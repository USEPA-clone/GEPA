{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded EPA Methane Inventory\n",
    "## Category: 1B2ab Abandoned Oil and Gas Wells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Authors: \n",
    "Erin E. McDuffie, Bram Maasakkers\n",
    "#### Date Last Updated: \n",
    "see Step 0\n",
    "#### Notebook Purpose: \n",
    "This Notebook calculates and reports annual gridded (0.1°x0.1°) methane emission fluxes (molec./cm2/s) from Abandoned Oil and Gas Wells in the CONUS region between 2012-2018. \n",
    "#### Summary & Notes:\n",
    "EPA GHGI emissions from Abandoned Oil and Gas wells (AOG) are read in at the national level. Unlike other sources in the GEPA, the national AOG emissions are first disaggregated into emissions are the region level (non-Appalachia vs. Appalachia), as a function of well type and plugging status, using regional well counts, plugging statues, and regional emission factors available in the GHGI workbook. Regional emissions are then allocated to the state level (as a function of well type and status) by using state level counts of abandoned wells from the national GHGI workbook (includes Enverus & historical ‘missing’ wells population). Resulting state-level emissions are then distributed onto a 0.1⁰x0.1⁰ grid using a map of grid-level AOG well locations based on the relative counts of total abandoned oil and gas wells in each state. Note that emissions from both plugged and unplugged are allocated using the same proxy (differentiated by oil vs. gas wells) as we do not have well-level time series information on the plugging status of each individual well. Emissions are converted to flux annual emission fluxes (molec./cm2/s) are written to final netCDFs in the ‘/code/Final_Gridded_Data/’ folder.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Step 0. Set-Up Notebook Modules, Functions, and Local Parameters and Constants\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm working directory & print last update time\n",
    "import os\n",
    "import time\n",
    "modtime = os.path.getmtime('./1B2ab_Abandoned_Oil_Gas.ipynb')\n",
    "modificationTime = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(modtime))\n",
    "print(\"This file was last modified on: \", modificationTime)\n",
    "print('')\n",
    "print(\"The directory we are working in is {}\" .format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Include plots within notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import pyodbc\n",
    "import PyPDF2 as pypdf\n",
    "import tabula as tb\n",
    "import shapefile as shp\n",
    "from datetime import datetime\n",
    "from copy import copy\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Import additional modules\n",
    "# Load plotting package Basemap \n",
    "# Must also specify project library path [unique to each user])\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# Load netCDF (for manipulating netCDF file types)\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Set up ticker\n",
    "#import matplotlib.ticker as ticker\n",
    "\n",
    "#add path for the global function module (file)\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../Global_Functions/'))\n",
    "#print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Load functions\n",
    "import data_load_functions as data_load_fn\n",
    "import data_functions as data_fn\n",
    "import data_IO_functions as data_IO_fn\n",
    "import data_plot_functions as data_plot_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT Files\n",
    "# Assign global file names\n",
    "global_filenames = data_load_fn.load_global_file_names()\n",
    "State_ANSI_inputfile = global_filenames[0]\n",
    "#County_ANSI_inputfile = global_filenames[1]\n",
    "#pop_map_inputfile = global_filenames[2]\n",
    "Grid_area01_inputfile = global_filenames[3]\n",
    "Grid_area001_inputfile = global_filenames[4]\n",
    "Grid_state001_ansi_inputfile = global_filenames[5]\n",
    "#Grid_county001_ansi_inputfile = global_filenames[6]\n",
    "globalinputlocation = global_filenames[0][0:20]\n",
    "print(globalinputlocation)\n",
    "\n",
    "# EPA Inventory Data\n",
    "EPA_AOG_inputfile = globalinputlocation+'GHGI/Ch3_Energy/FINAL Abandoned Wells Supporting Calcs_2019-12-03.xlsx'\n",
    "\n",
    "#proxy mapping file\n",
    "AOG_Mapping_inputfile = './InputData/Abandoned_OGwells_ProxyMapping.xlsx'\n",
    "\n",
    "#ERG Processed Well Count Notebook\n",
    "#ERG_statewellcounts_inputfile = globalinputlocation+'Enverus DrillingInfo Processing - Well Counts_2021-03-17.xlsx'\n",
    "#Activity Data - raw Enverus data (2019 data pull)\n",
    "Enverus_AOG_well_inputfile = globalinputlocation+'Enverus/AOG/DIDSK_HEADERS_API10_2019_abandoned_wells.csv'\n",
    "\n",
    "#NEI Data\n",
    "NEI_grid_ref_inputfile = globalinputlocation+'Gridded/NEI_Reference_Grid_LCC_to_WGS84_latlon.shp'\n",
    "ERG_NEI_inputloc = globalinputlocation+'NEI/ERG_ILINData/CONUS_SA_FILES_'\n",
    "ERG_NEI_inputloc_2018 = globalinputlocation+'NEI/ERG_ILINData/IL_IN_ALLOCATED_WELL_LEVEL_DATA_2018_2019/IL_IN_WELL_LEVEL_DATA.accdb'\n",
    "\n",
    "#OUTPUT FILES\n",
    "gridded_outputfile = '../Final_Gridded_Data/EPA_v2_1B2ab_Abandoned_Oil_Gas.nc'\n",
    "#gridded_monthly_outputfile = '../Final_Gridded_Data/EPA_v2_1B2b_Natural_Gas_Transmission_Month.nc'\n",
    "netCDF_description = 'Gridded EPA Inventory - Abandoned Oil and Gas Well Emissions - IPCC Source Category 1B2ab'\n",
    "title_str = \"EPA methane emissions from abandoned oil and gas wells\"\n",
    "title_diff_str = \"Emissions from abandoned oil and gas wells difference: 2018-2012\"\n",
    "\n",
    "#output gridded proxy data\n",
    "grid_emi_outputfile = '../Final_Gridded_Data/Extension/v2_input_data/AOG_Grid_Emi.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPECIFY RECALCS\n",
    "\n",
    "# Re-Calculate = 1, Load from /IntermediateOutput folder = 0\n",
    "ReCalc_NEI = 0\n",
    "ReCalc_Env = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local variables\n",
    "start_year = 2012  #First year in emission timeseries\n",
    "end_year = 2018    #Last year in emission timeseries\n",
    "year_range = [*range(start_year, end_year+1,1)] #List of emission years\n",
    "year_range_str=[str(i) for i in year_range]\n",
    "num_years = len(year_range)\n",
    "num_inv_years = len([*range(1990, end_year+1,1)]) #List of inventory years\n",
    "\n",
    "# Define constants\n",
    "Avogadro   = 6.02214129 * 10**(23)  #molecules/mol\n",
    "Molarch4   = 16.04                  #g/mol\n",
    "Res01      = 0.1                    # degrees\n",
    "Res_01     = 0.01                   # degrees\n",
    "hrs_to_yrs = 8760                   #number of hours in a year\n",
    "g_to_mt    = 1*10**(-6)             # grams to metric ton\n",
    "\n",
    "# Continental US Lat/Lon Limits (for netCDF files)\n",
    "Lon_left = -130       #deg\n",
    "Lon_right = -60       #deg\n",
    "Lat_low  = 20         #deg\n",
    "Lat_up  = 55          #deg\n",
    "loc_dimensions = [Lat_low, Lat_up, Lon_left, Lon_right]\n",
    "\n",
    "ilat_start = int((90+Lat_low)/Res01) #1100:1450 (continental US range)\n",
    "ilat_end = int((90+Lat_up)/Res01)\n",
    "ilon_start = abs(int((-180-Lon_left)/Res01)) #500:1200 (continental US range)\n",
    "ilon_end = abs(int((-180-Lon_right)/Res01))\n",
    "\n",
    "# Number of days in each month\n",
    "month_day_leap  = [  31,  29,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_day_nonleap = [  31,  28,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_tag = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "month_dict = {'January':1, 'February':2,'March':3,'April':4,'May':5,'June':6, 'July':7,'August':8,'September':9,'October':10,\\\n",
    "             'November':11,'December':12}\n",
    "\n",
    "# Month arrays\n",
    "month_range_str = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "num_months = len(month_range_str)\n",
    "\n",
    "num_regions = 2\n",
    "appalachia_states = ['OH','PA','WV','NY','KY','TN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;\n",
    "//prevent auto-scrolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track run time\n",
    "ct = datetime.now() \n",
    "it = ct.timestamp() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Step 1. Load in State ANSI data, and Area Maps\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State-level ANSI Data\n",
    "#Read the state ANSI file array\n",
    "State_ANSI, name_dict, abbr_dict = data_load_fn.load_state_ansi(State_ANSI_inputfile)[0:3]\n",
    "#QA: number of states\n",
    "print('Read input file: '+ f\"{State_ANSI_inputfile}\")\n",
    "print('Total \"States\" found: ' + '%.0f' % len(State_ANSI))\n",
    "print(' ')\n",
    "\n",
    "# 0.01 x0.01 degree Data\n",
    "# State ANSI IDs and grid cell area (m2) maps\n",
    "state_ANSI_map = data_load_fn.load_state_ansi_map(Grid_state001_ansi_inputfile)\n",
    "area_map, lat001, lon001 = data_load_fn.load_area_map_001(Grid_area001_inputfile)\n",
    "\n",
    "# 0.1 x0.1 degree data\n",
    "# grid cell area and state ANSI maps\n",
    "Lat01, Lon01 = data_load_fn.load_area_map_01(Grid_area01_inputfile)[1:3]\n",
    "#Select relevant Continental 0.1 x0.1 domain\n",
    "Lat_01 = Lat01[ilat_start:ilat_end]\n",
    "Lon_01 = Lon01[ilon_start:ilon_end]\n",
    "area_matrix_01 = data_fn.regrid001_to_01(area_map, Lat_01, Lon_01)\n",
    "area_matrix_01 *= 10000  #convert from m2 to cm2\n",
    "#state_ANSI_map_01 = data_fn.regrid001_to_01(state_ANSI_map, Lat_01, Lon_01)\n",
    "del area_map#, lat001, lon001, global_filenames\n",
    "\n",
    "# Print time\n",
    "ct = datetime.now() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 2: Read-in and Format Proxy Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 Read In Proxy Mapping File & Make Proxy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1 Format Proxy Group Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load GHGI Mapping Groups\n",
    "names = pd.read_excel(AOG_Mapping_inputfile, sheet_name = \"GHGI Map - AOG\", usecols = \"A:B\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "ghgi_aog_map = pd.read_excel(AOG_Mapping_inputfile, sheet_name = \"GHGI Map - AOG\", usecols = \"A:B\", skiprows = 2, names = colnames)\n",
    "#drop rows with no data, remove the parentheses and \"\"\n",
    "ghgi_aog_map = ghgi_aog_map[ghgi_aog_map['GHGI_Emi_Group'] != 'na']\n",
    "ghgi_aog_map = ghgi_aog_map[ghgi_aog_map['GHGI_Emi_Group'].notna()]\n",
    "ghgi_aog_map = ghgi_aog_map[ghgi_aog_map['GHGI_Emi_Group'] != '-']\n",
    "ghgi_aog_map['GHGI_Source']= ghgi_aog_map['GHGI_Source'].str.replace(r\"\\(\",\"\")\n",
    "ghgi_aog_map['GHGI_Source']= ghgi_aog_map['GHGI_Source'].str.replace(r\"\\)\",\"\")\n",
    "ghgi_aog_map['GHGI_Source']= ghgi_aog_map['GHGI_Source'].str.replace(r\"+\",\"\")\n",
    "ghgi_aog_map.reset_index(inplace=True, drop=True)\n",
    "display(ghgi_aog_map)\n",
    "\n",
    "#load emission group - proxy map\n",
    "names = pd.read_excel(AOG_Mapping_inputfile, sheet_name = \"Proxy Map - AOG\", usecols = \"A:E\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "proxy_aog_map = pd.read_excel(AOG_Mapping_inputfile, sheet_name = \"Proxy Map - AOG\", usecols = \"A:E\", skiprows = 1, names = colnames)\n",
    "display((proxy_aog_map))\n",
    "\n",
    "#create empty proxy and emission group arrays (for state and months, where needed)\n",
    "for igroup in np.arange(0,len(proxy_aog_map)):\n",
    "    if proxy_aog_map.loc[igroup, 'Grid_Month_Flag'] ==0:\n",
    "        vars()[proxy_aog_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        vars()[proxy_aog_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "    else:\n",
    "        vars()[proxy_aog_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "        vars()[proxy_aog_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years,num_months])\n",
    "        \n",
    "    vars()[proxy_aog_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])\n",
    "    \n",
    "    if proxy_aog_map.loc[igroup,'State_Proxy_Group'] != '-':\n",
    "        if proxy_aog_map.loc[igroup,'State_Month_Flag'] == 0:\n",
    "            vars()[proxy_aog_map.loc[igroup,'State_Proxy_Group']] = np.zeros([len(State_ANSI),num_years])\n",
    "        else:\n",
    "            vars()[proxy_aog_map.loc[igroup,'State_Proxy_Group']] = np.zeros([len(State_ANSI),num_years,num_months])\n",
    "    else:\n",
    "        continue # do not make state proxy variable if no variable assigned in mapping file\n",
    "        \n",
    "emi_group_names = np.unique(ghgi_aog_map['GHGI_Emi_Group'])\n",
    "\n",
    "print('QA/QC: Is the number of emission groups the same for the proxy and emissions tabs?')\n",
    "if (len(emi_group_names) == len(np.unique(proxy_aog_map['GHGI_Emi_Group']))):\n",
    "    print('PASS')\n",
    "else:\n",
    "    print('FAIL')\n",
    "    print(emi_group_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read In Proxy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2 Read in State-Level Well Counts from ERG Abandoned Wells Inventory Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2.1 Read in 1990 and 2015 State well count values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read In State-Level Well Counts from Workbook\n",
    "\n",
    "# As of the 2020 GHGI... \n",
    "# State Level Well Counts by well type are only available for 1990 and 2015. \n",
    "# Interpolate for all other years and hold 2015 values constant to 2018. Then calcualte the fraction in each\n",
    "# state for each year.\n",
    "\n",
    "EPA_AOG_state_wells_1990 = pd.read_excel(EPA_AOG_inputfile, sheet_name = \"GHGI Method Dev - StateLevel AD\", usecols = \"A,W:X\", skiprows = 6, nrows = 50)\n",
    "EPA_AOG_state_wells_1990.rename(columns={EPA_AOG_state_wells_1990.columns[0]:'State'}, inplace=True)\n",
    "EPA_AOG_state_wells_1990.rename(columns={EPA_AOG_state_wells_1990.columns[1]:'NG_wells'}, inplace=True)\n",
    "EPA_AOG_state_wells_1990.rename(columns={EPA_AOG_state_wells_1990.columns[2]:'Petr_wells'}, inplace=True)\n",
    "\n",
    "EPA_AOG_state_wells_2015 = pd.read_excel(EPA_AOG_inputfile, sheet_name = \"GHGI Method Dev - StateLevel AD\", usecols = \"A,AF:AG\", skiprows = 6, nrows = 50)\n",
    "EPA_AOG_state_wells_2015.rename(columns={EPA_AOG_state_wells_2015.columns[0]:'State'}, inplace=True)\n",
    "EPA_AOG_state_wells_2015.rename(columns={EPA_AOG_state_wells_2015.columns[1]:'NG_wells'}, inplace=True)\n",
    "EPA_AOG_state_wells_2015.rename(columns={EPA_AOG_state_wells_2015.columns[2]:'Petr_wells'}, inplace=True)\n",
    "\n",
    "display(EPA_AOG_state_wells_1990)\n",
    "display(EPA_AOG_state_wells_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.1.2 Make Timeseries Arrays of State-level NG and Petr well counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Interpolate for all other years and hold 2015 values constant to latest year. \n",
    "# These counts account for the historical adjustment factor (calcualted at the state level in the GHGI workbook)\n",
    "# as well as the adjustment factor for DI data (though the 1975 DI data are from a 2018 data pull)\n",
    "# Note that these counts do not represent the national total counts in the GHGI, but represent the most recent \n",
    "# information we have on the relative amount of abandoned oil and gas wells in each state for each year. \n",
    "# To update this work, we need:\n",
    "#     1) updated state level counts of abandoned oil and gas wells in 1975 from DI\n",
    "#     2) updated state level counts of abandoned oil and gas wells across the entire timeseries from DI\n",
    "\n",
    "\n",
    "\n",
    "state_ng_wells = np.zeros([len(State_ANSI), num_inv_years])\n",
    "state_petr_wells = np.zeros([len(State_ANSI), num_inv_years])\n",
    "\n",
    "start_year_idx = 0\n",
    "idx_2015 = 2015-1990\n",
    "\n",
    "for istate in np.arange(0,len(EPA_AOG_state_wells_1990)):\n",
    "    match_state = np.where(State_ANSI['abbr'] == EPA_AOG_state_wells_1990['State'][istate])[0][0]\n",
    "    state_ng_wells[match_state,0] = EPA_AOG_state_wells_1990['NG_wells'][istate]\n",
    "    state_ng_wells[match_state,idx_2015] = EPA_AOG_state_wells_2015['NG_wells'][istate]\n",
    "    state_petr_wells[match_state,0] = EPA_AOG_state_wells_1990['Petr_wells'][istate]\n",
    "    state_petr_wells[match_state,idx_2015] = EPA_AOG_state_wells_2015['Petr_wells'][istate]\n",
    "\n",
    "# Interpolate to fill missing years\n",
    "for istate in np.arange(0,len(state_ng_wells)):\n",
    "    ng_wells_temp = state_ng_wells[istate][:]  \n",
    "    ng_wells_temp[idx_2015:] = ng_wells_temp[idx_2015]      #extend 2015 data to the most recent year \n",
    "    ng_wells_temp = pd.Series(ng_wells_temp)  \n",
    "    ng_wells_temp.replace(0,np.NaN, inplace=True)\n",
    "    ng_wells_temp = ng_wells_temp.interpolate().values\n",
    "    ng_wells_temp = np.nan_to_num(ng_wells_temp)\n",
    "    state_ng_wells[istate][:] = ng_wells_temp\n",
    "    \n",
    "    petr_wells_temp = state_petr_wells[istate][:]     \n",
    "    petr_wells_temp[idx_2015:] = petr_wells_temp[idx_2015]      #extend 2015 data to the most recent year \n",
    "    petr_wells_temp = pd.Series(petr_wells_temp)  \n",
    "    petr_wells_temp.replace(0,np.NaN, inplace=True)\n",
    "    petr_wells_temp = petr_wells_temp.interpolate().values\n",
    "    petr_wells_temp = np.nan_to_num(petr_wells_temp)\n",
    "    state_petr_wells[istate][:] = petr_wells_temp\n",
    "    \n",
    "\n",
    "#Reduce arrays to relevant years\n",
    "state_ng_wells = state_ng_wells[:,start_year-1990:end_year-1990+1]\n",
    "state_petr_wells = state_petr_wells[:,start_year-1990:end_year-1990+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2.3. Read In State-Level Well Plugging Status Fractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2.3.1. Read In State-Level Fractions for 2016-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For years with avaialble DI Data (2016-2018), need to calculate fraction plugged vs. unplugged in each state\n",
    "# Note: Assume that all wells are unplugged if there is no plugging status in Enverus for a particular state\n",
    "\n",
    "# Since state level well counts by plugging type are only avialble for 2016-2019, interpolate for other years assuming\n",
    "# zero percent plugged in 1950.\n",
    "# This is how the current inventory also does this calculation\n",
    "# Ideally, all wells by type, plugging status, and state would be read from the raw Enverus data\n",
    "\n",
    "#Data are in units of number of wells of each different status type\n",
    "\n",
    "state_plugged_frac_well = np.zeros([len(State_ANSI), num_inv_years])\n",
    "\n",
    "start_year_idx = 0\n",
    "idx_2016 = 2016-1990\n",
    "\n",
    "#2016\n",
    "EPA_AOG_state_well_status_2016 = pd.read_excel(EPA_AOG_inputfile, sheet_name = \"Yr 2016 DI Status\", usecols = \"K:AV\", nrows = 10)\n",
    "EPA_AOG_state_well_status_2016 = EPA_AOG_state_well_status_2016.drop(columns = ['Townsend-Small','%','Cum %'])\n",
    "\n",
    "for istate in np.arange(0,len(State_ANSI)):\n",
    "    match_state_abbr = State_ANSI['abbr'][istate]\n",
    "    col_names = EPA_AOG_state_well_status_2016.columns\n",
    "    if match_state_abbr in col_names:\n",
    "        temp_plugged = np.sum(EPA_AOG_state_well_status_2016.loc[EPA_AOG_state_well_status_2016['EPA'] == 'Plugged',match_state_abbr])\n",
    "        temp_unplugged = np.sum(EPA_AOG_state_well_status_2016.loc[EPA_AOG_state_well_status_2016['EPA'] == 'Unplugged',match_state_abbr])\n",
    "        if np.isnan(temp_plugged):\n",
    "            temp_plugged =0\n",
    "        elif np.isnan(temp_unplugged):\n",
    "            temp_unplugged=1\n",
    "        state_plugged_frac_well[istate,idx_2016] = temp_plugged/(temp_plugged+temp_unplugged)\n",
    "        #state_unplugged_frac_well[istate,idx_2016] = temp_unplugged/ (temp_plugged+temp_unplugged)\n",
    "    else:\n",
    "        state_plugged_frac_well[istate,idx_2016] = 0\n",
    "        #state_unplugged_frac_well[istate,idx_2016] = 1\n",
    "\n",
    "#*********\n",
    "#2017 data\n",
    "idx_2017 = 2017-1990\n",
    "    \n",
    "EPA_AOG_state_well_status_2017 = pd.read_excel(EPA_AOG_inputfile, sheet_name = \"Yr 2017 DI Status\", usecols = \"K:AV\", nrows = 11)\n",
    "EPA_AOG_state_well_status_2017 = EPA_AOG_state_well_status_2017.drop(columns = ['Townsend-Small','%','Cum %'])\n",
    "\n",
    "for istate in np.arange(0,len(State_ANSI)):\n",
    "    match_state_abbr = State_ANSI['abbr'][istate]\n",
    "    col_names = EPA_AOG_state_well_status_2017.columns\n",
    "    if match_state_abbr in col_names:\n",
    "        temp_plugged = np.sum(EPA_AOG_state_well_status_2017.loc[EPA_AOG_state_well_status_2017['EPA'] == 'Plugged',match_state_abbr])\n",
    "        temp_unplugged = np.sum(EPA_AOG_state_well_status_2017.loc[EPA_AOG_state_well_status_2017['EPA'] == 'Unplugged',match_state_abbr])\n",
    "        state_plugged_frac_well[istate,idx_2017] = temp_plugged/(temp_plugged+temp_unplugged)\n",
    "    else:\n",
    "        state_plugged_frac_well[istate,idx_2017] = 0\n",
    "    \n",
    "#*****\n",
    "#2018 data\n",
    "idx_2018 = 2018-1990\n",
    "    \n",
    "EPA_AOG_state_well_status_2018 = pd.read_excel(EPA_AOG_inputfile, sheet_name = \"Yr 2018 DI Status\", usecols = \"B:D\", skiprows = 7)\n",
    "EPA_AOG_state_well_status_2018.rename(columns={EPA_AOG_state_well_status_2018.columns[0]:'State'}, inplace=True)\n",
    "EPA_AOG_state_well_status_2018.rename(columns={EPA_AOG_state_well_status_2018.columns[1]:'Count'}, inplace=True)\n",
    "EPA_AOG_state_well_status_2018.rename(columns={EPA_AOG_state_well_status_2018.columns[2]:'EPA Status'}, inplace=True)\n",
    "\n",
    "state_names = EPA_AOG_state_well_status_2018['State'].drop_duplicates().to_list()\n",
    "for istate in np.arange(0,len(State_ANSI)):\n",
    "    match_state_abbr = State_ANSI['abbr'][istate]\n",
    "    if match_state_abbr in state_names:\n",
    "        temp_plugged = np.sum(EPA_AOG_state_well_status_2018.loc[(EPA_AOG_state_well_status_2018['State'] == match_state_abbr) & (EPA_AOG_state_well_status_2018['EPA Status'] == 'Plugged'),'Count'])\n",
    "        temp_unplugged = np.sum(EPA_AOG_state_well_status_2018.loc[(EPA_AOG_state_well_status_2018['State'] == match_state_abbr) & (EPA_AOG_state_well_status_2018['EPA Status'] == 'Unplugged'),'Count'])\n",
    "        if temp_plugged ==0 and temp_unplugged ==0:\n",
    "            state_plugged_frac_well[istate,idx_2018] = 0\n",
    "        else:\n",
    "            state_plugged_frac_well[istate,idx_2018] = temp_plugged/(temp_plugged+temp_unplugged)\n",
    "    else:\n",
    "        state_plugged_frac_well[istate,idx_2018] = 0    \n",
    "\n",
    "#**********\n",
    "#(for future work)\n",
    "#2019 data\n",
    "#idx_2019 = 2019-start_year\n",
    "    \n",
    "#EPA_AOG_state_well_status_2019 = pd.read_excel(EPA_AOG_inputfile, sheet_name = \"YR 2020 DI Status\", usecols = \"B,D,F\", skiprows = 0)\n",
    "#EPA_AOG_state_well_status_2019.rename(columns={EPA_AOG_state_well_status_2019.columns[0]:'State'}, inplace=True)\n",
    "#EPA_AOG_state_well_status_2019.rename(columns={EPA_AOG_state_well_status_2019.columns[1]:'Count'}, inplace=True)\n",
    "#EPA_AOG_state_well_status_2019.rename(columns={EPA_AOG_state_well_status_2019.columns[2]:'EPA Status'}, inplace=True)\n",
    "\n",
    "#state_names = EPA_AOG_state_well_status_2019['State'].drop_duplicates().to_list()\n",
    "#for istate in np.arange(0,len(State_ANSI)):\n",
    "#    match_state_abbr = State_ANSI['abbr'][istate]\n",
    "#    if match_state_abbr in state_names:\n",
    "#        temp_plugged = np.sum(EPA_AOG_state_well_status_2019.loc[(EPA_AOG_state_well_status_2019['State'] == match_state_abbr) & (EPA_AOG_state_well_status_2019['EPA Status'] == 'Plugged'),'Count'])\n",
    "#        temp_unplugged = np.sum(EPA_AOG_state_well_status_2019.loc[(EPA_AOG_state_well_status_2019['State'] == match_state_abbr) & (EPA_AOG_state_well_status_2019['EPA Status'] == 'Unplugged'),'Count'])\n",
    "#         #NOTE: for 2019 - add in the wells from 2019 and assume all are unplugged (this was a correction made by ERG at the national level\n",
    "#        # to account for changes in reported plugging status in the DI/Prism dataset)\n",
    "#        #print(temp_unplugged)\n",
    "#        temp_unplugged = temp_unplugged + EPA_AOG_state_wells_1990[EPA_AOG_state_wells_1990['State']==match_state_abbr].sum(axis=1).values[0]\n",
    "#        #print(temp_unplugged)\n",
    "#        if temp_plugged ==0 and temp_unplugged ==0:\n",
    "#            aog_state_plugged_frac_well[istate,idx_2019] = 0\n",
    "#            aog_state_unplugged_frac_well[istate,idx_2019] = 0 \n",
    "#        else:\n",
    "#            aog_state_plugged_frac_well[istate,idx_2019] = temp_plugged/(temp_plugged+temp_unplugged)\n",
    "#            aog_state_unplugged_frac_well[istate,idx_2019] = temp_unplugged/ (temp_plugged+temp_unplugged)\n",
    "#        #if match_state_abbr == 'TX':\n",
    "#            #print(aog_state_plugged_frac_well[istate,idx_2019])\n",
    "#            #print(aog_state_unplugged_frac_well[istate,idx_2019])\n",
    "#            #print(aog_state_plugged_frac_well[istate,idx_2018])\n",
    "#            #print(aog_state_unplugged_frac_well[istate,idx_2018])\n",
    "#    else:\n",
    "#        aog_state_plugged_frac_well[istate,idx_2019] = 0\n",
    "#        aog_state_unplugged_frac_well[istate,idx_2019] = 0     \n",
    "#aog_state_plugged_frac_well[:,idx_2019] =  aog_state_plugged_frac_well[:,idx_2018]\n",
    "#aog_state_unplugged_frac_well[:,idx_2019] =  aog_state_unplugged_frac_well[:,idx_2018]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2.3.2. Make Timeseries Arrays of state-level plugging status fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Interpolate for years between 1950 and 2016, then remove all years prior to 'start year'\n",
    "# Following National Inventory approach, assume 0% of wells plugged in 1950. \n",
    "\n",
    "# interpolate between 1950 and 2016 values (assuming 0% plugged in 1950)\n",
    "x = [1950,2016]\n",
    "#print(idx_2016)\n",
    "xnew = np.linspace(1990, 2015, num=(2016-1990), endpoint=True)        #full timeseries\n",
    "for istate in np.arange(0, len(State_ANSI)):\n",
    "    frac_plugged_wells_temp = state_plugged_frac_well[istate,:] \n",
    "    y = [0,frac_plugged_wells_temp[idx_2016]]                         # the % in 1950 and 2016\n",
    "    f = interp1d(x,y)\n",
    "    frac_plugged_wells_temp[:idx_2016] = f(xnew)                      #evaluate the interpolation for each time series year \n",
    "    state_plugged_frac_well[istate,:] = frac_plugged_wells_temp\n",
    "\n",
    "#Reduce arrays to relevant years\n",
    "state_plugged_frac_well = state_plugged_frac_well[:,start_year-1990:end_year-1990+1]\n",
    "\n",
    "#print(np.shape(state_unplugged_frac_well))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(state_plugged_frac_well[:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2.4 Make Timeseries of Wells by State, plugging status, type, and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split counts for appalachia and non-appalachia states as well as for NG vs Petr and plugged vs unplugged wells\n",
    "\n",
    "app_mask = np.zeros(len(State_ANSI))\n",
    "nonapp_mask = np.ones(len(State_ANSI))\n",
    "state_ng_plugged_app = np.zeros([len(State_ANSI),num_years])\n",
    "state_ng_unplugged_app = np.zeros([len(State_ANSI),num_years])\n",
    "state_petr_plugged_app = np.zeros([len(State_ANSI),num_years])\n",
    "state_petr_unplugged_app = np.zeros([len(State_ANSI),num_years])\n",
    "state_ng_plugged_nonapp = np.zeros([len(State_ANSI),num_years])\n",
    "state_ng_unplugged_nonapp = np.zeros([len(State_ANSI),num_years])\n",
    "state_petr_plugged_nonapp = np.zeros([len(State_ANSI),num_years])\n",
    "state_petr_unplugged_nonapp = np.zeros([len(State_ANSI),num_years])\n",
    "\n",
    "# make mask arrays for app and non-app states\n",
    "for istate in np.arange(0,len(State_ANSI)):\n",
    "    if State_ANSI['abbr'][istate] in appalachia_states:\n",
    "        app_mask[istate]=1\n",
    "        nonapp_mask[istate]=0\n",
    "\n",
    "# make timeseries\n",
    "for iyear in np.arange(0,num_years):\n",
    "    state_ng_plugged_app[:,iyear] = state_ng_wells[:,iyear] * state_plugged_frac_well[:,iyear] * app_mask\n",
    "    state_ng_plugged_nonapp[:,iyear] = state_ng_wells[:,iyear] * state_plugged_frac_well[:,iyear] * nonapp_mask\n",
    "    state_ng_unplugged_app[:,iyear] = state_ng_wells[:,iyear] * (1-state_plugged_frac_well[:,iyear]) * app_mask\n",
    "    state_ng_unplugged_nonapp[:,iyear] = state_ng_wells[:,iyear] * (1-state_plugged_frac_well[:,iyear]) * nonapp_mask\n",
    "    state_petr_plugged_app[:,iyear] = state_petr_wells[:,iyear] * state_plugged_frac_well[:,iyear] *app_mask\n",
    "    state_petr_plugged_nonapp[:,iyear] = state_petr_wells[:,iyear] * state_plugged_frac_well[:,iyear] * nonapp_mask\n",
    "    state_petr_unplugged_app[:,iyear] = state_petr_wells[:,iyear] * (1-state_plugged_frac_well[:,iyear]) * app_mask\n",
    "    state_petr_unplugged_nonapp[:,iyear] = state_petr_wells[:,iyear] * (1-state_plugged_frac_well[:,iyear]) * nonapp_mask\n",
    "#display(np.shape(state_ng_plugged_app))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3. Make Grid Proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.1. Read in Enverus Abandoned well data (from ERG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read In Raw Enverus Data from ERG\n",
    "\n",
    "if ReCalc_Env ==1:\n",
    "    #DI data\n",
    "    DI_data = pd.read_csv(Enverus_AOG_well_inputfile,low_memory=False)\n",
    "    DI_data = DI_data.drop(columns=['API10','COUNTYPARISH','CUM_GAS','CUM_OIL','WELL_STATUS','PRODUCTION_TYPE','ERG_WELL_TYPE','GOR'])\n",
    "    DI_data.rename({'SURFACE_HOLE_LATITUDE_WGS84':'LAT','SURFACE_HOLE_LONGITUDE_WGS84':'LON'},axis=1, inplace=True)\n",
    "    DI_data['LAST_PROD_DATE'] = DI_data['LAST_PROD_DATE'].astype(str)\n",
    "    DI_data['COMPLETION_DATE'] = DI_data['COMPLETION_DATE'].astype(str)\n",
    "    DI_data['SPUD_DATE'] = DI_data['SPUD_DATE'].astype(str)\n",
    "    DI_data['OFFSHORE'] = DI_data['OFFSHORE'].astype(str)\n",
    "    DI_data = DI_data[DI_data['OFFSHORE'] == 'N']\n",
    "    DI_data.reset_index(inplace=True, drop=True)\n",
    "    # make columns to reflect the last production, completion, and spud years\n",
    "    DI_data['prod_year'] = DI_data['LAST_PROD_DATE'].str[:4]\n",
    "    DI_data['comp_year'] = DI_data['COMPLETION_DATE'].str[:4]\n",
    "    DI_data['spud_year'] = DI_data['SPUD_DATE'].str[:4]\n",
    "    DI_data['prod_year'] = DI_data['prod_year'].astype(float)\n",
    "    DI_data['comp_year'] = DI_data['comp_year'].astype(float)\n",
    "    DI_data['spud_year'] = DI_data['spud_year'].astype(float)\n",
    "    display(DI_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.2. Calculate the state-level fraction of abandoned gas to oil wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the state-level fraction of abandoned gas to oil wells\n",
    "# To be applied to the 'DRY' well counts in the next step - this approach is also used to allocate the \n",
    "# dry well population to either oil or gas types in the national GHGI\n",
    "# e.g., gas wells = gas wells + dry wells * (gas wells / (gas wells + oil wells))\n",
    "\n",
    "if ReCalc_Env ==1:\n",
    "    state_abd_gas_to_oil_ratio = np.zeros([len(State_ANSI),num_years])\n",
    "\n",
    "    for iyear in np.arange(0, num_years):\n",
    "        #abandoned wells by the given year\n",
    "            print('Year', iyear, 'of',num_years)\n",
    "            temp = DI_data[(DI_data['prod_year'] < year_range[iyear]) | \\\n",
    "               (((DI_data['prod_year'].isna()) & (DI_data['comp_year'] < year_range[iyear]))) |\\\n",
    "              ((DI_data['prod_year'].isna()) & (DI_data['comp_year'].isna()) & (DI_data['spud_year'] < year_range[iyear]))]\n",
    "            #total_well_count[iyear] = np.sum(temp['PRODUCING_ENTITY_COUNT'])\n",
    "        \n",
    "            # Calculate the fraction of abandoned wells in the given year that are gas\n",
    "            for istate in np.arange(0,len(State_ANSI)):\n",
    "                state_ag_count = 0\n",
    "                state_ao_count = 0\n",
    "                temp2 = temp[(temp['STATE'] == State_ANSI['abbr'][istate])]\n",
    "                temp3 = temp2[(temp2['ABANDONED_WELL_TYPE'] =='GAS')]\n",
    "                state_ag_count += np.sum(temp3['PRODUCING_ENTITY_COUNT'])\n",
    "                temp3 = temp2[(temp2['ABANDONED_WELL_TYPE'] =='OIL')]\n",
    "                state_ao_count += np.sum(temp3['PRODUCING_ENTITY_COUNT'])\n",
    "                state_abd_gas_to_oil_ratio[istate,iyear] = data_fn.safe_div(state_ag_count,(state_ag_count+state_ao_count))\n",
    "                #print(state_ag_count, state_ao_count,state_abd_gas_to_oil_ratio[istate,iyear]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.4.3. Make CONUS Grid Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for each year, calculate the total number of wells that were abandoned by that year\n",
    "\n",
    "# Logic:\n",
    "# If the well is onshore, extract the year of last production, completion, or spud date \n",
    "# Following the same logic as the GHGI, count the well as being abandoned if its last\n",
    "# production date was before the current year, of if the completion date is before the\n",
    "# current year (if the last prod date is missing), or if the spud date is before the\n",
    "# current year (if both the last prod date and completion date are missing)\n",
    "# Then add the relevant number of wells to the map if the location of that well is in the CONUS region\n",
    "# allocate dry wells based on the fraction of gas to oil wells in the given state\n",
    "\n",
    "if ReCalc_Env ==1:\n",
    "    map_agas_wells = np.zeros([len(lat001),len(lon001),num_years])\n",
    "    map_aoil_wells = np.zeros([len(lat001),len(lon001),num_years])\n",
    "    total_well_count = np.zeros(num_years)\n",
    "    ongrid_well_count = np.zeros(num_years)\n",
    "    nongrid_well_count = np.zeros(num_years)\n",
    "    \n",
    "    for iyear in np.arange(0,num_years):\n",
    "        #abandoned wells by the given year\n",
    "        print('Year', iyear, 'of',num_years)\n",
    "        temp = DI_data[(DI_data['prod_year'] < year_range[iyear]) | \\\n",
    "               (((DI_data['prod_year'].isna()) & (DI_data['comp_year'] < year_range[iyear]))) |\\\n",
    "              ((DI_data['prod_year'].isna()) & (DI_data['comp_year'].isna()) & (DI_data['spud_year'] < year_range[iyear]))]\n",
    "        total_well_count[iyear] = np.sum(temp['PRODUCING_ENTITY_COUNT'])\n",
    "        #subset on the CONUS grid\n",
    "        temp2 = temp[(temp['LON'] > Lon_left) & (temp['LON'] < Lon_right) & \\\n",
    "            (temp['LAT'] > Lat_low) & (temp['LAT'] < Lat_up)]\n",
    "        temp2.reset_index(inplace=True, drop=True)\n",
    "        #subset off the CONUS grid\n",
    "        temp3 = temp[~((temp['LON'] > Lon_left) & (temp['LON'] < Lon_right) & \\\n",
    "            (temp['LAT'] > Lat_low) & (temp['LAT'] < Lat_up))]\n",
    "        nongrid_well_count[iyear] = (np.sum(temp3['PRODUCING_ENTITY_COUNT']))\n",
    "    \n",
    "        for iwell in np.arange(0, len(temp2)):\n",
    "            ilat = int((temp2['LAT'][iwell] - Lat_low)/Res_01)\n",
    "            ilon = int((temp2['LON'][iwell] - Lon_left)/Res_01)\n",
    "            istate = np.where(temp2['STATE'][iwell] == State_ANSI['abbr'])[0][0]\n",
    "            if temp2.loc[iwell,'ABANDONED_WELL_TYPE'] =='GAS':\n",
    "                map_agas_wells[ilat,ilon,iyear] += temp2.loc[iwell,'PRODUCING_ENTITY_COUNT']\n",
    "            elif temp2.loc[iwell,'ABANDONED_WELL_TYPE'] =='OIL':\n",
    "                map_aoil_wells[ilat,ilon,iyear] += temp2.loc[iwell,'PRODUCING_ENTITY_COUNT']\n",
    "            elif temp2.loc[iwell,'ABANDONED_WELL_TYPE'] =='DRY':\n",
    "                map_agas_wells[ilat,ilon,iyear] += temp2.loc[iwell,'PRODUCING_ENTITY_COUNT']*state_abd_gas_to_oil_ratio[istate,iyear]\n",
    "                map_aoil_wells[ilat,ilon,iyear] += temp2.loc[iwell,'PRODUCING_ENTITY_COUNT']*(1-state_abd_gas_to_oil_ratio[istate,iyear])\n",
    "            ongrid_well_count[iyear] += temp2.loc[iwell,'PRODUCING_ENTITY_COUNT']\n",
    "            #print(iwell)\n",
    "        print('Complete:',year_range[iyear],', Number of AOG wells:',total_well_count[iyear], np.sum(map_agas_wells[:,:,iyear])+np.sum(map_aoil_wells[:,:,iyear])+nongrid_well_count[iyear])\n",
    "\n",
    "    np.save('./IntermediateOutputs/Enverus_agaswell_tempoutput', map_agas_wells)\n",
    "    np.save('./IntermediateOutputs/Enverus_aoilwell_tempoutput', map_aoil_wells)\n",
    "    del temp, DI_data, temp2, temp3\n",
    "    \n",
    "else:\n",
    "    map_agas_wells = np.load('./IntermediateOutputs/Enverus_agaswell_tempoutput.npy')\n",
    "    map_aoil_wells = np.load('./IntermediateOutputs/Enverus_aoilwell_tempoutput.npy')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# MS Access logic    \n",
    "#            WHERE (((ABANDONED_WELLS_ENVERUS.last_prod_year)<1975)) OR \\\n",
    "#            (((ABANDONED_WELLS_ENVERUS.last_prod_year) Is Null) AND ((ABANDONED_WELLS_ENVERUS.completion_year)<1975)) OR \\\n",
    "#            (((ABANDONED_WELLS_ENVERUS.last_prod_year) Is Null) AND ((ABANDONED_WELLS_ENVERUS.completion_year) Is Null) AND ((ABANDONED_WELLS_ENVERUS.spud_date_year)<1975))\n",
    "#GROUP BY ABANDONED_WELLS_ENVERUS.ABANDONED_WELL_TYPE;\n",
    "\n",
    "# Print time\n",
    "ct = datetime.now() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.4.3. Correct IL & IN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrememly limited well information in the Enverus wells dataset. Therefore, all AOG wells in the national \n",
    "# GHGI are from the historic estimates (from state-specific databases). Because we have very limited information\n",
    "# on where these wells are located, the AOG wells for IL/IN are allocated to the grid cell level assuming the same\n",
    "# spatial distribution as currently active wells in these states. \n",
    "# Other possible assumptions would be to spatially disaggregate based on production levels or data from IL/IN state\n",
    "# bases, which could be implemented in a future version of the GEPA. \n",
    "\n",
    "# General Process\n",
    "## NA Read the GHGI well and production statistics from the GHGI (contain corrected IL and IN data)\n",
    "# 1. Read in the relevant NEI data (from both file formats) and place onto GEPA grid (including reproj of NEI data)\n",
    "## NA  Scale the NEI prxy maps to the corresponding state level values from Step 1.\n",
    "## NA Calculate the lease condensate proxy for IL/IN using the same method as the Enverus data\n",
    "# 2. Place the NEI grid data on the appropriate Enverus proxy grids. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.4.3.1 Read in all data prior to 2018 (text file format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Read in relevant files by year (for all years before 2018 [2018 read from different file type])\n",
    "# Data are in a text file format where each row of data contains the surrogate code, FIPS code, column and row location\n",
    "# (on the NEI CONUS1 grid), and the absolute, fractional, and running sum of data (e.g., counts or production) in the\n",
    "# given FIPS region. \n",
    "# The absolute data are placed onto the GEPA grid by using an NEI reference map shapefile to map the data location\n",
    "# from the NEI CONUS grid cell indexes to the corresponding latitude and longitude values in the GEPA grid. \n",
    "### Note - the 2016 data from the NEI is on a non-standard grid where lat/lons are unknown. Can change later if needed, or\n",
    "# can interpolat ebetween years if more accurate\n",
    "\n",
    "NEI_files = ['/USA_698_NOFILL.txt','/USA_695_NOFILL.txt']\n",
    "\n",
    "\n",
    "# only recalc the data if required (set in Step 0)\n",
    "if ReCalc_NEI ==1:\n",
    "    \n",
    "    map_NEI_agas_wells = np.zeros([len(lat001),len(lon001),num_years])\n",
    "    map_NEI_aoil_wells = np.zeros([len(lat001),len(lon001),num_years])\n",
    "    \n",
    "    #read in the NEI grid refernece shapefile (contains the lat/lons of each NEI coordinate)\n",
    "    shape = shp.Reader(NEI_grid_ref_inputfile)\n",
    "\n",
    "    #make the map arrays of aboslute values (counts and mcf)\n",
    "    for ivar in np.arange(0,len(NEI_files)):\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            if year_range_str[iyear] == '2012':\n",
    "                year = '2011'\n",
    "            elif year_range_str[iyear] == '2013' or year_range_str[iyear] == '2014' or year_range_str[iyear] == '2015':\n",
    "                year = '2014'\n",
    "            #elif year_range_str[iyear] == '2015' or year_range_str[iyear] == '2016':\n",
    "             #    year = '2016'  \n",
    "            elif year_range_str[iyear] == '2016' or year_range_str[iyear] == '2017':\n",
    "                year = '2017'\n",
    "            elif year_range_str[iyear] == '2018':\n",
    "                continue\n",
    "            else:\n",
    "                print('NEI DATA MISSING FOR YEAR ',year_range_str[iyear])\n",
    "            path = ERG_NEI_inputloc+year+NEI_files[ivar]\n",
    "            data_temp = pd.read_csv(path, sep='\\t', skiprows = 25)\n",
    "            data_temp = data_temp.drop([\"!\"], axis=1)\n",
    "            data_temp.columns = ['Code','FIPS','COL','ROW','Frac','Abs','FIPS_Total','FIPS_Running_Sum']\n",
    "            data_temp['Lat'] = np.zeros([len(data_temp)])\n",
    "            data_temp['Lon'] = np.zeros([len(data_temp)])\n",
    "            colmin = 1332\n",
    "            colmax=0\n",
    "            rowmin = 1548\n",
    "            rowmax=0\n",
    "            counter =0\n",
    "        \n",
    "            #Create the boundary box\n",
    "            for idx in np.arange(0,len(data_temp)):\n",
    "                if str(data_temp['FIPS'][idx]).startswith('17') or str(data_temp['FIPS'][idx]).startswith('18'):\n",
    "                    icol = data_temp['COL'][idx]\n",
    "                    irow = data_temp['ROW'][idx]\n",
    "                    if icol > colmax:\n",
    "                        colmax =icol\n",
    "                    if icol < colmin:\n",
    "                        colmin = icol\n",
    "                    if irow > rowmax:\n",
    "                        rowmax = irow\n",
    "                    if irow < rowmin:\n",
    "                        rowmin  = irow\n",
    "            \n",
    "            #Extract the relevant indicies from the NEI reference shapefile\n",
    "            array_temp = np.zeros([4,((colmax+1-colmin)*(rowmax+1-rowmin))]) #make an array to save col, row, lat, lon\n",
    "            idx=0\n",
    "            for rec in shape.iterRecords():\n",
    "                if (int(rec['cellid'][0:4]) <= colmax and int(rec['cellid'][0:4]) >= colmin) \\\n",
    "                    and (int(rec['cellid'][5:]) <= rowmax and int(rec['cellid'][5:]) >= rowmin):\n",
    "                        array_temp[0,idx] = int(rec['cellid'][0:4])   #column index\n",
    "                        array_temp[1,idx] = int(rec['cellid'][5:])    #row index\n",
    "                        array_temp[2,idx] = rec['Latitude']           #latitude\n",
    "                        array_temp[3,idx] = rec['Longitude']          #longitude\n",
    "                        idx +=1\n",
    "                        #print(idx,int(rec['cellid'][0:4]),int(rec['cellid'][5:]))\n",
    "    \n",
    "            #Use this array to locate and assign the lat lon values to the NEI datafile and then place onto grid\n",
    "            for idx in np.arange(0,len(data_temp)):\n",
    "                if str(data_temp['FIPS'][idx]).startswith('17') or str(data_temp['FIPS'][idx]).startswith('18'):\n",
    "                    icol = data_temp['COL'][idx]\n",
    "                    irow = data_temp['ROW'][idx]\n",
    "                    match = np.where((icol == array_temp[0,:]) & (irow == array_temp[1,:]))[0][0]\n",
    "                    #print(match)\n",
    "                    data_temp.loc[idx,'Lat'] = array_temp[2,match]\n",
    "                    data_temp.loc[idx,'Lon'] = array_temp[3,match]\n",
    "                    ilat = int((data_temp['Lat'][idx] - Lat_low)/Res_01)\n",
    "                    ilon = int((data_temp['Lon'][idx] - Lon_left)/Res_01)\n",
    "                    #if str(data_temp['FIPS'][idx]).startswith('17'):\n",
    "                    if ivar ==0:\n",
    "                        map_NEI_agas_wells[ilat,ilon,iyear] += data_temp.loc[idx,'Abs']\n",
    "                    elif ivar ==1:\n",
    "                        map_NEI_aoil_wells[ilat,ilon,iyear] += data_temp.loc[idx,'Abs']\n",
    "                        #print(ilat,ilon,data_temp.loc[idx,'Abs'], vars()[data_names[ivar]][0,ilat,ilon,iyear])\n",
    "                    #else:\n",
    "                    #    vars()[data_names[ivar]][1,ilat,ilon,iyear] += data_temp.loc[idx,'Abs']\n",
    "                #else:\n",
    "                #    data_temp.loc[idx,'Abs'] = 0 #zero out the non IL/IN data\n",
    "\n",
    "    np.save('./IntermediateOutputs/NEI_agaswell_tempoutput', map_NEI_agas_wells)\n",
    "    np.save('./IntermediateOutputs/NEI_aoilwell_tempoutput', map_NEI_aoil_wells)\n",
    "\n",
    "else:\n",
    "    map_NEI_agas_wells = np.load('./IntermediateOutputs/NEI_agaswell_tempoutput.npy')\n",
    "    map_NEI_aoil_wells = np.load('./IntermediateOutputs/NEI_aoilwell_tempoutput.npy')\n",
    "            \n",
    "            \n",
    "print('IL/IN NEI totals')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    print('Year: ', year_range_str[iyear])\n",
    "    print('Non Associated Gas (Conv + HF): ',np.sum(map_NEI_agas_wells[:,:,iyear]))\n",
    "    print('Oil wells (Conv + HF):         ',np.sum(map_NEI_aoil_wells[:,:,iyear]))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.4.3.2 Read in 2018 data (MS Access data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in 2018 NEI data from different datafile format\n",
    "    \n",
    "if ReCalc_NEI ==1:    \n",
    "    #Read in the data\n",
    "    driver_str = r'Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ='+ERG_NEI_inputloc_2018+';'''\n",
    "    conn = pyodbc.connect(driver_str)\n",
    "    NEI_2018_ILIN_wells = pd.read_sql(\"SELECT * FROM 2018_IL_IN_WELLS\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    data_temp = NEI_2018_ILIN_wells[(NEI_2018_ILIN_wells['ACTIVE_WELL_FLAG'] ==1)]\n",
    "    data_temp.reset_index(inplace=True, drop=True)\n",
    "    data_temp.fillna(\"\",inplace=True)\n",
    "\n",
    "    #find 2018 index\n",
    "    year_diff = [abs(x - 2018) for x in year_range]\n",
    "    iyear = year_diff.index(min(year_diff))\n",
    "\n",
    "    # place data on map for each state (for active wells, production, completions, and drilled wells)\n",
    "    for iwell in np.arange(0,len(data_temp)):\n",
    "        ilat = int((data_temp['LATITUDE'][iwell] - Lat_low)/Res_01)\n",
    "        ilon = int((data_temp['LONGITUDE'][iwell] - Lon_left)/Res_01)\n",
    "        if str(data_temp['FIPS_CODE'][iwell]).startswith('17') or str(data_temp['FIPS_CODE'][iwell]).startswith('18'):\n",
    "            #istate = 0\n",
    "        #else:\n",
    "            #istate =1\n",
    "            if NEI_2018_ILIN_wells['WELL_TYPE'][iwell] == 'GAS':\n",
    "                map_NEI_agas_wells[ilat,ilon,iyear] += 1\n",
    "            elif NEI_2018_ILIN_wells['WELL_TYPE'][iwell] == 'OIL':\n",
    "                map_NEI_aoil_wells[ilat,ilon,iyear] += 1\n",
    "\n",
    "    np.save('./IntermediateOutputs/NEI_agaswell_w2018_tempoutput', map_NEI_agas_wells)\n",
    "    np.save('./IntermediateOutputs/NEI_aoilwell_w2018_tempoutput', map_NEI_aoil_wells)\n",
    "\n",
    "else:\n",
    "    map_NEI_agas_wells = np.load('./IntermediateOutputs/NEI_agaswell_w2018_tempoutput.npy')\n",
    "    map_NEI_aoil_wells = np.load('./IntermediateOutputs/NEI_aoilwell_w2018_tempoutput.npy')\n",
    "            \n",
    "            \n",
    "print('IL/IN NEI totals')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    print('Year: ', year_range_str[iyear])\n",
    "    print('Abandoned gas wells: ',np.sum(map_NEI_agas_wells[:,:,iyear]))\n",
    "    print('Abandoned oil wells: ',np.sum(map_NEI_aoil_wells[:,:,iyear]))\n",
    "    print(' ')\n",
    "\n",
    "#display(data_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.4.3.3 Add the NEI data to the relevant Enverus Proxy Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add maps to relevant Enverus maps\n",
    "# add absolute values to the Enverus maps above \n",
    "# Note: since this proxy is used to allocate emissions from the state to grid cell level, we don't need to \n",
    "# scale the IL/IN data to the historical state counts. This method assumes that the relative geographical \n",
    "# distribution of wells within these states is the same between abandoned and active wells \n",
    "# Note: This block should not be run more than once\n",
    "for iyear in np.arange(0,num_years):\n",
    "    map_agas_wells[:,:,iyear] += map_NEI_agas_wells[:,:,iyear]\n",
    "    map_aoil_wells[:,:,iyear] += map_NEI_aoil_wells[:,:,iyear]\n",
    "del map_NEI_agas_wells,map_NEI_aoil_wells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Step 3. Read In EPA GHGI Data\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1. Read-In National AOG Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read EPA AOG emissions data (units of metric tons)\n",
    "# Read from '2021 Time Series' tab. The data are weighted by regional emission factors following:\n",
    "# Emissions = national well counts * (Appalachia EF * fraction wells Appalachia + Non-Appalachia EF * fraction wells non-Appalachia)\n",
    "# Therefore, we need to first split/calculate national inventory emissions by region, well type, and plugging status\n",
    "# even though the workbook only reports by well type and plugging status\n",
    "\n",
    "#This is different than other sources in the GEPA since the Inventory data are not directly reported in Excel at the\n",
    "#finest level of spatial resolution. This detail therefore needs to first be calculated in this Notebook. \n",
    "\n",
    "#e.g.,\n",
    "# Emissions = national wells * fraction Appalachia *Appalachia EF + national wells * fraction non-Appalachia *Non-Appalachia EF\n",
    "\n",
    "names = pd.read_excel(EPA_AOG_inputfile, sheet_name = \"2020 PR Time Series\", usecols = \"A:AD\", skiprows = 4, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "EPA_emi_AOG = pd.read_excel(EPA_AOG_inputfile, sheet_name = \"2020 PR Time Series\", usecols = \"A:AD\", skiprows = 27, names = colnames, nrows = 7)\n",
    "EPA_emi_AOG.rename(columns={EPA_emi_AOG.columns[0]:'Source'}, inplace=True)\n",
    "EPA_emi_AOG = EPA_emi_AOG.fillna('')\n",
    "EPA_emi_AOG = EPA_emi_AOG.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_emi_AOG.reset_index(inplace=True, drop=True)\n",
    "display(EPA_emi_AOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2 Read In Region Specific Data (well counts and EFs by well type and gas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read In regional activity data (well counts and plugging status time series) and  EFs (constant over time series)\n",
    "\n",
    "# a) Read in National Well Counts and Plugging Status\n",
    "EPA_well_counts = pd.read_excel(EPA_AOG_inputfile, sheet_name = \"2020 PR Time Series\", usecols = \"A:AD\", skiprows = 5, names = colnames, nrows = 7)\n",
    "EPA_well_counts.rename(columns={EPA_well_counts.columns[0]:'Source'}, inplace=True)\n",
    "EPA_well_counts = EPA_well_counts.fillna('')\n",
    "EPA_well_counts = EPA_well_counts.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_well_counts.reset_index(inplace=True, drop=True)\n",
    "#print(EPA_well_counts)\n",
    "\n",
    "start_year_idx = EPA_well_counts.columns.get_loc(start_year)\n",
    "\n",
    "national_wells_ng_plugged = EPA_well_counts.iloc[EPA_well_counts.index[EPA_well_counts['Source'] == 'NG - Plugged'],start_year_idx:]\n",
    "national_wells_ng_unplugged = EPA_well_counts.iloc[EPA_well_counts.index[EPA_well_counts['Source'] == 'NG - Unplugged'],start_year_idx:]\n",
    "national_wells_petr_plugged = EPA_well_counts.iloc[EPA_well_counts.index[EPA_well_counts['Source'] == 'Petro - Plugged'],start_year_idx:]\n",
    "national_wells_petr_unplugged = EPA_well_counts.iloc[EPA_well_counts.index[EPA_well_counts['Source'] == 'Petro - Unplugged'],start_year_idx:]\n",
    "\n",
    "\n",
    "# b) Read in Activity Factors (fraction of wells in each region)\n",
    "EPA_well_region_fractions = pd.read_excel(EPA_AOG_inputfile, sheet_name = \"2020 PR Time Series\", usecols = \"A:AD\", skiprows = 13, names = colnames, nrows = 4)\n",
    "EPA_well_region_fractions.rename(columns={EPA_well_region_fractions.columns[0]:'Source'}, inplace=True)\n",
    "EPA_well_region_fractions = EPA_well_region_fractions.fillna('')\n",
    "EPA_well_region_fractions = EPA_well_region_fractions.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_well_region_fractions.reset_index(inplace=True, drop=True)\n",
    "#print(EPA_well_region_fractions)\n",
    "\n",
    "start_year_idx = EPA_well_counts.columns.get_loc(start_year)\n",
    "\n",
    "fraction_wells_ng_app = EPA_well_region_fractions.iloc[EPA_well_region_fractions.index[EPA_well_region_fractions['Source'] == 'NG - Appalachia'],start_year_idx:]\n",
    "fraction_wells_ng_nonapp = EPA_well_region_fractions.iloc[EPA_well_region_fractions.index[EPA_well_region_fractions['Source'] == 'NG - NonAppalcahia'],start_year_idx:]\n",
    "fraction_wells_petr_app = EPA_well_region_fractions.iloc[EPA_well_region_fractions.index[EPA_well_region_fractions['Source'] == 'Petro - Appalachia'],start_year_idx:]\n",
    "fraction_wells_petr_nonapp = EPA_well_region_fractions.iloc[EPA_well_region_fractions.index[EPA_well_region_fractions['Source'] == 'Petro - NonAppalachia'],start_year_idx:]\n",
    "\n",
    "\n",
    "#Read in regional emission factors (single value apllied to timeseries)\n",
    "EPA_plugged_App_EF = pd.read_excel(EPA_AOG_inputfile, sheet_name = \"EFs from Studies\", usecols = \"C\", skiprows = 35, nrows = 1)\n",
    "EPA_plugged_App_EF = EPA_plugged_App_EF.columns.values[0]\n",
    "EPA_unplugged_App_EF = pd.read_excel(EPA_AOG_inputfile, sheet_name = \"EFs from Studies\", usecols = \"C\",skiprows = 38, nrows = 1)\n",
    "EPA_unplugged_App_EF = EPA_unplugged_App_EF.columns.values[0]\n",
    "EPA_plugged_NonApp_EF = pd.read_excel(EPA_AOG_inputfile, sheet_name = \"EFs from Studies\", usecols = \"C\",skiprows = 24, nrows = 1)\n",
    "EPA_plugged_NonApp_EF = EPA_plugged_NonApp_EF.columns.values[0]\n",
    "EPA_unplugged_NonApp_EF = pd.read_excel(EPA_AOG_inputfile, sheet_name = \"EFs from Studies\", usecols = \"C\", skiprows = 25, nrows = 1)\n",
    "EPA_unplugged_NonApp_EF = EPA_unplugged_NonApp_EF.columns.values[0]\n",
    "\n",
    "print('QA/QC: Check EF data against GHGI Workbook')\n",
    "print('All units: g/hr/well')\n",
    "print('Appalachia EF (plugged):      ',EPA_plugged_App_EF)\n",
    "print('Appalachia EF (unplugged):    ', EPA_unplugged_App_EF)\n",
    "print('Non-Appalachia EF (plugged):  ',EPA_plugged_NonApp_EF)\n",
    "print('Non-Appalachia EF (unplugged):', EPA_unplugged_NonApp_EF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3. Calculate National EPA emissions as a function of region, well type, and status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split national emissions into regional Emissions using workbook data\n",
    "# Calculated in units of metric tons\n",
    "\n",
    "#emis_ng_plugged_app = national_ng_plugged * plugged_app_EF * fraction_wells_ng_app\n",
    "\n",
    "NG_plugged_app = national_wells_ng_plugged.to_numpy() * (fraction_wells_ng_app.to_numpy() * EPA_plugged_App_EF) *hrs_to_yrs*g_to_mt #convert to MT\n",
    "NG_plugged_app = NG_plugged_app[0,:]\n",
    "NG_unplugged_app = national_wells_ng_unplugged.to_numpy() * (fraction_wells_ng_app.to_numpy() * EPA_unplugged_App_EF)*hrs_to_yrs*g_to_mt\n",
    "NG_unplugged_app = NG_unplugged_app[0,:]\n",
    "Petro_plugged_app = national_wells_petr_plugged.to_numpy() * (fraction_wells_petr_app.to_numpy() * EPA_plugged_App_EF)*hrs_to_yrs*g_to_mt\n",
    "Petro_plugged_app = Petro_plugged_app[0,:]\n",
    "Petro_unplugged_app = national_wells_petr_unplugged.to_numpy() * (fraction_wells_petr_app.to_numpy() * EPA_unplugged_App_EF)*hrs_to_yrs*g_to_mt\n",
    "Petro_unplugged_app = Petro_unplugged_app[0,:]\n",
    "\n",
    "NG_plugged_nonapp = national_wells_ng_plugged.to_numpy() * (fraction_wells_ng_nonapp.to_numpy() * EPA_plugged_NonApp_EF)*hrs_to_yrs*g_to_mt\n",
    "NG_plugged_nonapp = NG_plugged_nonapp[0,:]\n",
    "NG_unplugged_nonapp = national_wells_ng_unplugged.to_numpy() * (fraction_wells_ng_nonapp.to_numpy() * EPA_unplugged_NonApp_EF)*hrs_to_yrs*g_to_mt\n",
    "NG_unplugged_nonapp = NG_unplugged_nonapp[0,:]\n",
    "Petro_plugged_nonapp = national_wells_petr_plugged.to_numpy() * (fraction_wells_petr_nonapp.to_numpy() * EPA_plugged_NonApp_EF)*hrs_to_yrs*g_to_mt\n",
    "Petro_plugged_nonapp = Petro_plugged_nonapp[0,:]\n",
    "Petro_unplugged_nonapp = national_wells_petr_unplugged.to_numpy() * (fraction_wells_petr_nonapp.to_numpy() * EPA_unplugged_NonApp_EF)*hrs_to_yrs*g_to_mt\n",
    "Petro_unplugged_nonapp = Petro_unplugged_nonapp[0,:]\n",
    "\n",
    "#Make GHGI DataFrame\n",
    "GHGI_Data = pd.DataFrame(columns = ['Source', *range(start_year, end_year+1,1)])\n",
    "GHGI_Data.loc[0] = ['Petro_plugged_app'] + Petro_plugged_app.tolist()\n",
    "GHGI_Data.loc[1] = ['Petro_plugged_nonapp'] + Petro_plugged_nonapp.tolist()\n",
    "GHGI_Data.loc[2] = ['Petro_unplugged_app'] + Petro_unplugged_app.tolist()\n",
    "GHGI_Data.loc[3] = ['Petro_unplugged_nonapp'] + Petro_unplugged_nonapp.tolist()\n",
    "GHGI_Data.loc[4] = ['NG_plugged_app'] + NG_plugged_app.tolist()\n",
    "GHGI_Data.loc[5] = ['NG_plugged_nonapp'] + NG_plugged_nonapp.tolist()\n",
    "GHGI_Data.loc[6] = ['NG_unplugged_app'] + NG_unplugged_app.tolist()\n",
    "GHGI_Data.loc[7] = ['NG_unplugged_nonapp'] + NG_unplugged_nonapp.tolist()\n",
    "#display(GHGI_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4. Split Emissions into Gridding Groups (each Group will have the same proxy applied during the gridding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final units are converted from metric ton to kt\n",
    "\n",
    "#sum_emi = np.zeros([num_years])\n",
    "ghgi_aog_groups = ghgi_aog_map['GHGI_Emi_Group'].unique()\n",
    "DEBUG=1\n",
    "\n",
    "for igroup in np.arange(0,len(ghgi_aog_groups)): #loop through all groups, finding the GHGI sources in that group and summing emissions for that region, year\n",
    "    vars()[ghgi_aog_groups[igroup]] = np.zeros([num_years])\n",
    "    source_temp = ghgi_aog_map.loc[ghgi_aog_map['GHGI_Emi_Group'] == ghgi_aog_groups[igroup], 'GHGI_Source']\n",
    "    pattern_temp  = '|'.join(source_temp)\n",
    "    emi_temp = GHGI_Data[GHGI_Data['Source'].str.contains(pattern_temp)]\n",
    "    vars()[ghgi_aog_groups[igroup]][:] = np.where(emi_temp.iloc[:,start_year_idx:] =='',[0],emi_temp.iloc[:,start_year_idx:]).sum(axis=0)/float(1000) #convert MT to kt\n",
    "    \n",
    "#Check again national totals\n",
    "print('QA/QC: Check Emissions Sum against GHGI Summary Emissions')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    sum_emi = 0\n",
    "    for igroup in np.arange(0,len(ghgi_aog_groups)):\n",
    "        sum_emi += vars()[ghgi_aog_groups[igroup]][iyear]\n",
    "    summary_emi = EPA_emi_AOG.iloc[-1,iyear+1]/1e3\n",
    "    diff1 = abs(sum_emi - summary_emi)/((sum_emi + summary_emi)/2)\n",
    "    if DEBUG==1:\n",
    "        print(summary_emi)\n",
    "        print(sum_emi)\n",
    "    if diff1 < 0.0001:\n",
    "        print('Year ', year_range[iyear],': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear],': FAIL (check Production & summary tabs): ', diff1,'%') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Step 4. Grid Data (using spatial proxies)\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1. Allocate emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.1.1 Assign the Appropriate Proxy Variable Names (state & grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names on the *left* need to match the 'Abandoned_OilGasWells_ProxyMapping' 'State_Proxy_Group' names \n",
    "# (these are initialized in Step 2). \n",
    "# The names on the *right* are the variable names used to caluclate the proxies in this code.\n",
    "# Names on the right need to match those from the code in Step 2\n",
    "\n",
    "    \n",
    "#national --> state proxies (state x year)\n",
    "State_Gas_Plugged_App = state_ng_plugged_app\n",
    "State_Gas_Plugged_NonApp = state_ng_plugged_nonapp\n",
    "State_Gas_Unplugged_App= state_ng_unplugged_app\n",
    "State_Gas_Unplugged_NonApp= state_ng_unplugged_nonapp\n",
    "State_Petr_Plugged_App = state_petr_plugged_app\n",
    "State_Petr_Plugged_NonApp = state_petr_plugged_nonapp\n",
    "State_Petr_Unplugged_App = state_petr_unplugged_app\n",
    "State_Petr_Unplugged_NonApp = state_petr_unplugged_nonapp\n",
    "\n",
    "#state --> grid proxies (0.01x0.01)\n",
    "Map_Gas_AbdWells = map_agas_wells\n",
    "Map_Oil_AbdWells = map_aoil_wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del map_agas_wells, map_aoil_wells, state_ng_plugged_app, state_ng_plugged_nonapp, state_ng_unplugged_app\n",
    "del state_petr_plugged_app,state_petr_plugged_nonapp, state_petr_unplugged_app,state_petr_unplugged_nonapp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.1.2 Allocate National EPA Emissions to the State-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate state-level emissions for aog wells (as a function of type and status and region)\n",
    "# Emissions in kt\n",
    "# State data = national GHGI emissions * state proxy/national total\n",
    "\n",
    "DEBUG =1\n",
    "\n",
    "# Make placeholder emission arrays for each group\n",
    "for igroup in np.arange(0,len(proxy_aog_map)):\n",
    "    #if proxy_stat_map.loc[igroup,'State_Month_Flag'] ==1:\n",
    "    vars()['State_'+proxy_aog_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(State_ANSI),num_years])\n",
    "    #else:\n",
    "        #vars()['State_'+proxy_aog_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(State_ANSI),num_years])\n",
    "    vars()['NonState_'+proxy_aog_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])\n",
    "        \n",
    "#Loop over years\n",
    "for iyear in np.arange(0,num_years):\n",
    "    #Loop over states\n",
    "    for istate in np.arange(0,len(State_ANSI)):\n",
    "        for igroup in np.arange(0,len(proxy_aog_map)):    \n",
    "            if proxy_aog_map.loc[igroup,'State_Proxy_Group'] != '-' and proxy_aog_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "                #if emission group has a state-level proxy\n",
    "                vars()['State_'+proxy_aog_map.loc[igroup,'GHGI_Emi_Group']][istate,iyear] = \\\n",
    "                            vars()[proxy_aog_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                            data_fn.safe_div(vars()[proxy_aog_map.loc[igroup,'State_Proxy_Group']][istate,iyear], \\\n",
    "                                             np.sum(vars()[proxy_aog_map.loc[igroup,'State_Proxy_Group']][:,iyear]))\n",
    "            else:\n",
    "                #retain emissions without state-level proxy from gridding later (not relevant here)\n",
    "                vars()['NonState_'+proxy_aog_map.loc[igroup,'GHGI_Emi_Group']][iyear] = vars()[proxy_aog_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "                \n",
    "# Check sum of all gridded emissions + emissions not included in state allocation\n",
    "print('QA/QC #1: Check weighted emissions against GHGI')   \n",
    "for iyear in np.arange(0,num_years):\n",
    "    summary_emi = EPA_emi_AOG.iloc[-1,iyear+1]/1e3 # convert MT to kt\n",
    "    calc_emi = 0\n",
    "    for igroup in np.arange(0,len(proxy_aog_map)):\n",
    "        #print(np.sum(vars()['State_'+proxy_aog_map.loc[igroup,'GHGI_Emi_Group']][:,iyear]))\n",
    "        calc_emi +=  np.sum(vars()['State_'+proxy_aog_map.loc[igroup,'GHGI_Emi_Group']][:,iyear])+\\\n",
    "            vars()['NonState_'+proxy_aog_map.loc[igroup,'GHGI_Emi_Group']][iyear] \n",
    "    if DEBUG ==1:\n",
    "        print(summary_emi)\n",
    "        print(calc_emi)\n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0001:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.3 Allocate emissions to the CONUS region (0.1x0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Allocate State-Level emissions (kt) onto a 0.1x0.1 grid using gridcell level 'Proxy_Groups'\n",
    "\n",
    "#Define emission arrays\n",
    "Emissions_array_001 = np.zeros([len(lat001),len(lon001),num_years])\n",
    "Emissions_array_01 = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Emissions_nongrid = np.zeros([num_years])\n",
    "\n",
    "DEBUG =1 \n",
    "\n",
    "# For each year, (2a) distribute state-level emissions onto a grid using proxies defined above ....\n",
    "# To speed up the code, masks are used rather than looping individually through each lat/lon. \n",
    "# In this case, a mask of 1's is made for the grid cells that match the ANSI values for a given state\n",
    "# The masked values are set to zero, remaining values = 1. \n",
    "# AK and HI and territories are removed from the analysis at this stage. \n",
    "# The emissions allocated to each state are at 0.01x0.01 degree resolution, as required to calculate accurate 'mask'\n",
    "# arrays for each state. \n",
    "# (2b - not applicable here) For emission groups that were not first allocated to states, national emissions for those groups are gridded\n",
    "# based on the relevant gridded proxy arrays (0.1x0.1 resolution). These emissions are at 0.1x0.1 degrees resolution. \n",
    "# (2c - not applicable here) - record 'not mapped' emission groups in the 'non-grid' array\n",
    "\n",
    "\n",
    "print('**QA/QC Check: Sum of national gridded emissions vs. GHGI national emissions')\n",
    "running_sum = np.zeros([len(proxy_aog_map),num_years])\n",
    "\n",
    "for igroup in np.arange(0,len(proxy_aog_map)):\n",
    "    proxy_temp = vars()[proxy_aog_map.loc[igroup,'Proxy_Group']]\n",
    "    proxy_temp_nongrid = vars()[proxy_aog_map.loc[igroup,'Proxy_Group']+'_nongrid']\n",
    "\n",
    "    #2a. Step through each state (if group was previously allocated to state level)\n",
    "    if proxy_aog_map.loc[igroup,'State_Proxy_Group'] != '-' and \\\n",
    "        proxy_aog_map.loc[igroup,'State_Proxy_Group'] != 'state_not_mapped':\n",
    "        print('Group:',igroup,'of ',len(proxy_aog_map))\n",
    "        vars()['Ext_'+proxy_aog_map.loc[igroup,'GHGI_Emi_Group']+'_01'] = np.zeros([len(lat001),len(lon001),num_years])\n",
    "\n",
    "        for istate in np.arange(0,len(State_ANSI)):\n",
    "            #print(igroup,istate)\n",
    "            \n",
    "            if State_ANSI['abbr'][istate] not in {'AK','HI'} and istate < 51:\n",
    "                mask_state = np.ma.ones(np.shape(state_ANSI_map))\n",
    "                mask_state = np.ma.masked_where(state_ANSI_map != State_ANSI['ansi'][istate], mask_state)\n",
    "                mask_state = np.ma.filled(mask_state,0) \n",
    "                for iyear in np.arange(0,num_years):\n",
    "                    emi_temp = vars()['State_'+proxy_aog_map.loc[igroup,'GHGI_Emi_Group']][istate,iyear]\n",
    "                    #print(emi_temp)\n",
    "                    if np.sum(mask_state*proxy_temp[:,:,iyear]) > 0 and emi_temp > 0: \n",
    "                    # if state is on grid and proxy for that state is non-zero\n",
    "                        weighted_array = data_fn.safe_div(mask_state*proxy_temp[:,:,iyear], \\\n",
    "                                            np.sum(mask_state*proxy_temp[:,:,iyear]))\n",
    "                        Emissions_array_001[:,:,iyear] += emi_temp*weighted_array#_01\n",
    "                        vars()['Ext_'+proxy_aog_map.loc[igroup,'GHGI_Emi_Group']+'_01'][:,:,iyear] += emi_temp*weighted_array\n",
    "                        running_sum[igroup,iyear] += np.sum(emi_temp*weighted_array)\n",
    "                    else:\n",
    "                        Emissions_nongrid[iyear] += emi_temp\n",
    "                        running_sum[igroup,iyear] += np.sum(emi_temp)                 \n",
    "\n",
    "            else:\n",
    "                for iyear in np.arange(0, num_years):\n",
    "                    Emissions_nongrid[iyear] += np.sum(vars()['State_'+proxy_aog_map.loc[igroup,'GHGI_Emi_Group']][istate,iyear])\n",
    "                    running_sum[igroup,iyear] += np.sum(vars()['State_'+proxy_aog_map.loc[igroup,'GHGI_Emi_Group']][istate,iyear])    \n",
    "    \n",
    "\n",
    "for igroup in np.arange(0,len(proxy_aog_map)):\n",
    "    vars()['Ext_'+proxy_aog_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "    \n",
    "for iyear in np.arange(0, num_years):    \n",
    "    Emissions_array_01[:,:,iyear] = data_fn.regrid001_to_01(Emissions_array_001[:,:,iyear], Lat_01, Lon_01)\n",
    "    #Emissions_array_01[:,:,iyear] += Emissions_array_01_temp[:,:,iyear]\n",
    "    calc_emi = np.sum(Emissions_array_01[:,:,iyear]) + np.sum(Emissions_nongrid[iyear]) \n",
    "    calc_emi2 = 0\n",
    "    for igroup in np.arange(0, len(proxy_aog_map)):\n",
    "        if proxy_aog_map.loc[igroup,'State_Proxy_Group'] != '-' and proxy_aog_map.loc[igroup,'State_Proxy_Group'] != 'state_not_mapped':\n",
    "            vars()['Ext_'+proxy_aog_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]= data_fn.regrid001_to_01(vars()['Ext_'+proxy_aog_map.loc[igroup,'GHGI_Emi_Group']+'_01'][:,:,iyear], Lat_01, Lon_01)\n",
    "            calc_emi2 += np.sum(vars()['Ext_'+proxy_aog_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])\n",
    "    calc_emi2 += np.sum(Emissions_nongrid[iyear]) \n",
    "    summary_emi = EPA_emi_AOG.iloc[-1,iyear+1]/1e3 #metric tons to kt\n",
    "    emi_diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    #check two\n",
    "    if DEBUG==1:\n",
    "        print(calc_emi)\n",
    "        print(calc_emi2)\n",
    "        print(summary_emi)\n",
    "    if abs(emi_diff) < 0.0001:\n",
    "        print('Year '+ year_range_str[iyear]+': Difference < 0.01%: PASS')\n",
    "    else: \n",
    "        print('Year '+ year_range_str[iyear]+': Difference > 0.01%: FAIL, diff: '+str(emi_diff))\n",
    "        \n",
    "ct = datetime.now() \n",
    "print(\"current time:\", ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1.4 Save gridded emissions (kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save gridded emissions for each gridding group - for extension\n",
    "\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(grid_emi_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "unique_groups = np.unique(proxy_aog_map['GHGI_Emi_Group'])\n",
    "unique_groups = unique_groups[unique_groups != 'Emi_not_mapped']\n",
    "\n",
    "nc_out = Dataset(grid_emi_outputfile, 'r+', format='NETCDF4')\n",
    "\n",
    "for igroup in np.arange(0,len(unique_groups)):\n",
    "    print('Ext_'+unique_groups[igroup])\n",
    "    if len(np.shape(vars()['Ext_'+unique_groups[igroup]])) ==4:\n",
    "        ghgi_temp = np.sum(vars()[unique_groups[igroup]],axis=3) #sum month data if data is monthly\n",
    "    else:\n",
    "        ghgi_temp = vars()['Ext_'+unique_groups[igroup]]\n",
    "\n",
    "    # Write data to netCDF\n",
    "    data_out = nc_out.createVariable('Ext_'+unique_groups[igroup], 'f8', ('lat', 'lon','year'), zlib=True)\n",
    "    data_out[:,:,:] = ghgi_temp[:,:,:]\n",
    "\n",
    "#save nongrid data to calculate non-grid fraction extension\n",
    "data_out = nc_out.createVariable('Emissions_nongrid', 'f8', ('year'), zlib=True)  \n",
    "data_out[:] = Emissions_nongrid[:]\n",
    "nc_out.close()\n",
    "\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded emissions (kt) written to file: {}\" .format(os.getcwd())+grid_emi_outputfile)\n",
    "print(' ')\n",
    "\n",
    "del data_out, ghgi_temp, nc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Calculate Gridded Emission Fluxes (molec./cm2/s) (0.1x0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert emissions to emission flux\n",
    "# conversion: kt emissions to molec/cm2/s flux\n",
    "\n",
    "Flux_array_01_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "print('**QA/QC Check: Sum of national gridded emissions vs. GHGI national emissions')\n",
    "  \n",
    "for iyear in np.arange(0,num_years):\n",
    "    calc_emi = 0\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "\n",
    "    conversion_factor_01 = 10**9 * Avogadro / float(Molarch4 *year_days * 24 * 60 *60) / area_matrix_01\n",
    "    Flux_array_01_annual[:,:,iyear] = Emissions_array_01[:,:,iyear]*conversion_factor_01\n",
    "    #convert back to mass to check\n",
    "    conversion_factor_annual = 10**9 * Avogadro / float(Molarch4 *year_days * 24 * 60 *60) / area_matrix_01\n",
    "    calc_emi = np.sum(Flux_array_01_annual[:,:,iyear]/conversion_factor_annual)+np.sum(Emissions_nongrid[iyear])\n",
    "    summary_emi = EPA_emi_AOG.iloc[-1,iyear+1]/1e3 #metric tons to kt\n",
    "    emi_diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if DEBUG==1:\n",
    "        print(calc_emi)\n",
    "        print(summary_emi)\n",
    "    if abs(emi_diff) < 0.0001:\n",
    "        print('Year '+ year_range_str[iyear]+': Difference < 0.01%: PASS')\n",
    "    else: \n",
    "        print('Year '+ year_range_str[iyear]+': Difference > 0.01%: FAIL, diff: '+str(emi_diff))\n",
    "        \n",
    "Flux_Emissions_Total_annual = Flux_array_01_annual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 5. Write netCDF\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearly data\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(gridded_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write data to netCDF\n",
    "nc_out = Dataset(gridded_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Total_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded total abandoned oil and gas well fluxes written to file: {}\" .format(os.getcwd())+gridded_outputfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Step 6. Plot Gridded Data\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.1. Plot Annual Emission Fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot Annual Data\n",
    "scale_max = 0.5\n",
    "save_flag = 0\n",
    "save_fig = ''\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_str,scale_max,save_flag,save_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.2 Plot Difference between first and last inventory year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot difference between last and first year\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_diff_str,save_flag,save_outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = datetime.now() \n",
    "ft = ct.timestamp() \n",
    "time_elapsed = (ft-it)/(60*60)\n",
    "print('Time to run: '+str(time_elapsed)+' hours')\n",
    "print('** GEPA_1B2ab_Abandoned_Oil_Gas: COMPLETE **')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

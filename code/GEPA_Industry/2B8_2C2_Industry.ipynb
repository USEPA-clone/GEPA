{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded EPA Methane Inventory\n",
    "## Category: 2B8 Petrochemical Productions & 2C2 Ferroalloy Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Authors: \n",
    "Joannes D. Maasakkers, Candice F. Z. Chen, Erin E. McDuffie, Revathi Muralidharan\n",
    "#### Date Last Updated: \n",
    "see Step 0\n",
    "#### Notebook Purpose: \n",
    "This Notebook calculates and reports annual gridded (0.1⁰x0.1⁰) methane emission fluxes (molec./cm2/s) from production of ferroalloy and petrochemicals in the CONUS region between 2012-2018. \n",
    "#### Summary & Notes:\n",
    "EPA GHGI emissions from petrochemical and ferroalloy production facilities are read in at the national level. Emissions are first allocated to the facility level using relative facility-level methane emissions from the Greenhouse Gas Reporting Program (GHGRP, Subparts X and K). Resulting facility-level emissions are then distributed onto a 0.1⁰x0.1⁰ grid using a map of grid-level petrochemical and ferroalloy facility locations (from GHGRP). Emissions are converted to flux annual emission fluxes (molec./cm2/s) are written to final netCDFs in the ‘/code/Final_Gridded_Data/’ folder.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Step 0. Set-Up Notebook Modules, Functions, and Local Parameters and Constants\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "modtime = os.path.getmtime('./2B8_2C2_Industry.ipynb')\n",
    "modificationTime = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(modtime))\n",
    "print(\"This file was last modified on: \", modificationTime)\n",
    "print('')\n",
    "print(\"The directory we are working in is {}\" .format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Include plots within notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from copy import copy\n",
    "\n",
    "# Import additional modules\n",
    "# Load plotting package Basemap \n",
    "# first, set the project library for basemap\n",
    "# 'r' in front of string is necessary for this block of code to run\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# Load netCDF (for manipulating netCDF file types)\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Set up ticker\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#add path for the global function module (file)\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../Global_Functions/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Load Tabula (for reading tables from PDFs)\n",
    "import tabula as tb   \n",
    "    \n",
    "# Load user-defined global functions (modules)\n",
    "import data_load_functions as data_load_fn\n",
    "import data_functions as data_fn\n",
    "import data_IO_functions as data_IO_fn\n",
    "import data_plot_functions as data_plot_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT Files\n",
    "# Assign global file names\n",
    "global_filenames = data_load_fn.load_global_file_names()\n",
    "State_ANSI_inputfile = global_filenames[0]\n",
    "County_ANSI_inputfile = global_filenames[1]\n",
    "pop_map_inputfile = global_filenames[2]\n",
    "Grid_area01_inputfile = global_filenames[3]\n",
    "Grid_area001_inputfile = global_filenames[4]\n",
    "Grid_state001_ansi_inputfile = global_filenames[5]\n",
    "Grid_county001_ansi_inputfile = global_filenames[6]\n",
    "\n",
    "# Specify names of inputs files used in this notebook\n",
    "EPA_inputfile = '../Global_InputData/GHGI/Ch2_Industry/CH4 emissions from ferroalloys and petrochemicals 1990-2018.xlsx'\n",
    "\n",
    "Ind_Mapping_inputfile = './InputData/Industry_ProxyMapping.xlsx'\n",
    "# Activity Data\n",
    "# GHGRP files\n",
    "EPA_ghgrp_petrofacility_inputfile = './InputData/SubpartX_Petrochemical_Facilities.csv'\n",
    "EPA_ghgrp_petro_inputfile = './InputData/SubpartX_Petrochemical.csv'\n",
    "EPA_ghgrp_ferrofacility_inputfile = './InputData/SubpartK_Ferroalloy_Facilities.csv'\n",
    "EPA_ghgrp_ferro_inputfile = './InputData/SubpartK_Ferroalloy.csv'\n",
    "\n",
    "\n",
    "#OUTPUT FILES\n",
    "gridded_outputfile = '../Final_Gridded_Data/EPA_v2_2B8_2C2_Industry.nc'\n",
    "netCDF_description = 'Gridded EPA Inventory - Industry - IPCC Source Category 2B5 and 2C1'\n",
    "gridded_petro_outputfile = '../Final_Gridded_Data/EPA_v2_2B8_Industry_Petrochemical.nc'\n",
    "netCDF_petro_description = 'Gridded EPA Inventory - Industry Emissions - IPCC Source Category 2B8 - Petrochemical'\n",
    "gridded_ferro_outputfile = '../Final_Gridded_Data/EPA_v2_2C2_Industry_Ferroalloy.nc'\n",
    "netCDF_ferro_description = 'Gridded EPA Inventory - Industry Emissions - IPCC Source Category 2C2 - Ferroalloy'\n",
    "title_str = \"EPA methane emissions from industry\"\n",
    "title_petro_str = \"EPA methane emissions from petrochemical industry\"\n",
    "title_ferro_str = \"EPA methane emissions from ferroalloy production\"\n",
    "title_diff_str = \"Emissions from industry total difference: 2018-2012\"\n",
    "title_petro_diff_str = \"Emissions from petrochemical difference: 2018-2012\"\n",
    "title_ferro_diff_str = \"Emissions from ferroalloy total difference: 2018-2012\"\n",
    "\n",
    "grid_emi_outputfile = '../Final_Gridded_Data/Extension/v2_input_data/Ind_Petro_Ferro_Grid_Emi2.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local variables\n",
    "start_year = 2012  #First year in emission timeseries\n",
    "end_year = 2018    #Last year in emission timeseries\n",
    "year_range = [*range(start_year, end_year+1,1)] #List of emission years\n",
    "year_range_str=[str(i) for i in year_range]\n",
    "num_years = len(year_range)\n",
    "\n",
    "# Define constants\n",
    "Avogadro   = 6.02214129 * 10**(23)  #molecules/mol\n",
    "Molarch4   = 16.04                  #g/mol\n",
    "Res01      = 0.1                    # degrees\n",
    "tg_scale   = 0.001                  #Tg scale number [New file allows for the exclusion of the territories] \n",
    "GWP_CH4    = 25                     #global warming potential of CH4 relative to CO2 (used to convert mass to CO2e units)\n",
    "\n",
    "# Continental US Lat/Lon Limits (for netCDF files)\n",
    "Lon_left = -130       #deg\n",
    "Lon_right = -60       #deg\n",
    "Lat_low  = 20         #deg\n",
    "Lat_up  = 55          #deg\n",
    "loc_dimensions = [Lat_low, Lat_up, Lon_left, Lon_right]\n",
    "\n",
    "ilat_start = int((90+Lat_low)/Res01) #1100:1450 (continental US range)\n",
    "ilat_end = int((90+Lat_up)/Res01)\n",
    "ilon_start = abs(int((-180-Lon_left)/Res01)) #500:1200 (continental US range)\n",
    "ilon_end = abs(int((-180-Lon_right)/Res01))\n",
    "\n",
    "# Number of days in each month\n",
    "month_day_leap  = [  31,  29,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_day_nonleap = [  31,  28,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "\n",
    "# Month arrays\n",
    "month_range_str = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "num_months = len(month_range_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track run time\n",
    "ct = datetime.datetime.now() \n",
    "it = ct.timestamp() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Step 1. Load in State ANSI data and Area Maps\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State-level ANSI Data\n",
    "#Read the state ANSI file array\n",
    "#State_ANSI, name_dict = data_load_fn.load_state_ansi(State_ANSI_inputfile)[0:2]\n",
    "#QA: number of states\n",
    "#print('Read input file: '+ f\"{State_ANSI_inputfile}\")\n",
    "#print('Total \"States\" found: ' + '%.0f' % len(State_ANSI))\n",
    "#print(' ')\n",
    "\n",
    "# 0.01 x0.01 degree Data\n",
    "# State ANSI IDs and grid cell area (m2) maps\n",
    "#state_ANSI_map = data_load_fn.load_state_ansi_map(Grid_state001_ansi_inputfile)\n",
    "area_map, lat001, lon001 = data_load_fn.load_area_map_001(Grid_area001_inputfile)\n",
    "\n",
    "#County ANSI Data\n",
    "#Includes State ANSI number, county ANSI number, county name, and country area (square miles)\n",
    "#pd_counties = pd.read_csv(County_ANSI_inputfile,encoding='latin-1')\n",
    "\n",
    "#QA: number of counties\n",
    "#print ('Read input file: ' + f\"{County_ANSI_inputfile}\")\n",
    "#print('Total \"Counties\" found (include PR): ' + '%.0f' % len(pd_counties))\n",
    "#print(' ')\n",
    "\n",
    "#Create a placeholder array for county data\n",
    "#county_array = np.zeros([len(pd_counties),3])\n",
    "\n",
    "#Populate array with State ANSI number (0), county ANSI number (1), and county area (2)\n",
    "#for icounty in np.arange(0,len(pd_counties)):\n",
    "#    county_array[icounty,0] = int(pd_counties.values[icounty,0])\n",
    "#    county_array[icounty,1] = int(pd_counties.values[icounty,1])\n",
    "#    county_array[icounty,2] = pd_counties.values[icounty,3]\n",
    "\n",
    "# 0.01 x0.01 degree Data\n",
    "# State ANSI IDs and grid cell area (m2) maps\n",
    "#state_ANSI_map = data_load_fn.load_state_ansi_map(Grid_state001_ansi_inputfile)\n",
    "#state_ANSI_map = state_ANSI_map.astype('int32')\n",
    "#county_ANSI_map = data_load_fn.load_county_ansi_map(Grid_county001_ansi_inputfile)\n",
    "#county_ANSI_map = county_ANSI_map.astype('int32')\n",
    "area_map, lat001, lon001 = data_load_fn.load_area_map_001(Grid_area001_inputfile)\n",
    "\n",
    "# 0.1 x0.1 degree data\n",
    "# grid cell area and state and county ANSI maps\n",
    "area_map01, Lat01, Lon01 = data_load_fn.load_area_map_01(Grid_area01_inputfile)[0:3]\n",
    "#Select relevant Continental 0.1 x0.1 domain\n",
    "Lat_01 = Lat01[ilat_start:ilat_end]\n",
    "Lon_01 = Lon01[ilon_start:ilon_end]\n",
    "area_matrix_01 = data_fn.regrid001_to_01(area_map, Lat_01, Lon_01)\n",
    "area_matrix_01 *= 10000  #convert from m2 to cm2\n",
    "\n",
    "#state_ANSI_map_01 = data_fn.regrid001_to_01(state_ANSI_map, Lat_01, Lon_01)\n",
    "\n",
    "# Print time\n",
    "ct = datetime.datetime.now() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 2: Read-in and Format Proxy Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1 Read In Proxy Mapping File & Make Proxy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load GHGI Mapping Groups\n",
    "names = pd.read_excel(Ind_Mapping_inputfile, sheet_name = \"GHGI Map - Ind\", usecols = \"A:B\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "ghgi_ind_map = pd.read_excel(Ind_Mapping_inputfile, sheet_name = \"GHGI Map - Ind\", usecols = \"A:B\", skiprows = 1, names = colnames)\n",
    "#drop rows with no data, remove the parentheses and \"\"\n",
    "ghgi_ind_map = ghgi_ind_map[ghgi_ind_map['GHGI_Emi_Group'] != 'na']\n",
    "ghgi_ind_map = ghgi_ind_map[ghgi_ind_map['GHGI_Emi_Group'].notna()]\n",
    "ghgi_ind_map['GHGI_Source']= ghgi_ind_map['GHGI_Source'].str.replace(r\"\\(\",\"\")\n",
    "ghgi_ind_map['GHGI_Source']= ghgi_ind_map['GHGI_Source'].str.replace(r\"\\)\",\"\")\n",
    "ghgi_ind_map.reset_index(inplace=True, drop=True)\n",
    "display(ghgi_ind_map)\n",
    "\n",
    "#load emission group - proxy map\n",
    "names = pd.read_excel(Ind_Mapping_inputfile, sheet_name = \"Proxy Map - Ind\", usecols = \"A:D\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "proxy_ind_map = pd.read_excel(Ind_Mapping_inputfile, sheet_name = \"Proxy Map - Ind\", usecols = \"A:D\", skiprows = 1, names = colnames)\n",
    "display((proxy_ind_map))\n",
    "\n",
    "#create empty proxy and emission group arrays (add months for proxy variables that have monthly data)\n",
    "for igroup in np.arange(0,len(proxy_ind_map)):\n",
    "    vars()[proxy_ind_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "    vars()[proxy_ind_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "    vars()[proxy_ind_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1. Read in GHGRP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Read in GHGRP Subpart X data CH4 based allocation (metric for carbon input)\n",
    "# NOTE that the facility level methane emissions are different when downloaded from ENVIRO facts vs FLIGHT\n",
    "# fewer facilities have emissions in Envirofacts than FLIGHT and emissions are overall lower\n",
    "# FLIGHT downloaded data contains Supart C and Supart X/K emissions. \n",
    "# Subpart C is general stational combustion emissions. \n",
    "# We have adjusted the code here to only read in Subpart X/K data from Envirofacts\n",
    "# We also confirmed that methane emissions from FLIGHT download = Envirofacts Subpart C + Subpart X in many cases\n",
    "\n",
    "#a) Read in the GHGRP facility data\n",
    "facility_info = pd.read_csv(EPA_ghgrp_petrofacility_inputfile)\n",
    "facility_emis = pd.read_csv(EPA_ghgrp_petro_inputfile)\n",
    "\n",
    "#filter emissions data for methane only (in metric tonnes CH4) and for years of interest\n",
    "facility_emis = facility_emis[facility_emis['X_SUBPART_LEVEL_INFORMATION.GHG_NAME'] == 'Methane']\n",
    "facility_emis = facility_emis[facility_emis['X_SUBPART_LEVEL_INFORMATION.REPORTING_YEAR'].isin(year_range)]\n",
    "facility_info = facility_info[facility_info['V_GHG_EMITTER_FACILITIES.YEAR'].isin(year_range)]\n",
    "facility_info.reset_index(inplace=True, drop=True)\n",
    "facility_emis.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#rename common columns and merge into one dataframe\n",
    "facility_info.rename(columns={'V_GHG_EMITTER_FACILITIES.YEAR':'Year', \\\n",
    "                             'V_GHG_EMITTER_FACILITIES.FACILITY_ID':'Facility_ID', \\\n",
    "                             'V_GHG_EMITTER_FACILITIES.LONGITUDE':'LONGITUDE',\n",
    "                             'V_GHG_EMITTER_FACILITIES.LATITUDE':'LATITUDE'},inplace=True)\n",
    "facility_emis.rename(columns={'X_SUBPART_LEVEL_INFORMATION.REPORTING_YEAR':'Year', \\\n",
    "                              'X_SUBPART_LEVEL_INFORMATION.FACILITY_ID':'Facility_ID'},inplace=True)\n",
    "ghgrp_petro = pd.merge(facility_info, facility_emis)\n",
    "ghgrp_petro['emis_tg_tot'] = ghgrp_petro['X_SUBPART_LEVEL_INFORMATION.GHG_QUANTITY']/1e6\n",
    "#display(ghgrp_petro)\n",
    "\n",
    "#Ferro alloy\n",
    "#a) Read in the GHGRP facility data\n",
    "facility_info = pd.read_csv(EPA_ghgrp_ferrofacility_inputfile)\n",
    "facility_emis = pd.read_csv(EPA_ghgrp_ferro_inputfile)\n",
    "\n",
    "#filter emissions data for methane only (in metric tonnes CH4) and for years of interest\n",
    "facility_emis = facility_emis[facility_emis['K_SUBPART_LEVEL_INFORMATION.GHG_NAME'] == 'Methane']\n",
    "facility_emis = facility_emis[facility_emis['K_SUBPART_LEVEL_INFORMATION.REPORTING_YEAR'].isin(year_range)]\n",
    "facility_info = facility_info[facility_info['V_GHG_EMITTER_FACILITIES.YEAR'].isin(year_range)]\n",
    "facility_info.reset_index(inplace=True, drop=True)\n",
    "facility_emis.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#rename common columns and merge into one dataframe\n",
    "facility_info.rename(columns={'V_GHG_EMITTER_FACILITIES.YEAR':'Year', \\\n",
    "                             'V_GHG_EMITTER_FACILITIES.FACILITY_ID':'Facility_ID', \\\n",
    "                             'V_GHG_EMITTER_FACILITIES.LONGITUDE':'LONGITUDE',\n",
    "                             'V_GHG_EMITTER_FACILITIES.LATITUDE':'LATITUDE'},inplace=True)\n",
    "facility_emis.rename(columns={'K_SUBPART_LEVEL_INFORMATION.REPORTING_YEAR':'Year', \\\n",
    "                              'K_SUBPART_LEVEL_INFORMATION.FACILITY_ID':'Facility_ID'},inplace=True)\n",
    "ghgrp_ferro = pd.merge(facility_info, facility_emis)\n",
    "ghgrp_ferro['emis_tg_tot'] = ghgrp_ferro['K_SUBPART_LEVEL_INFORMATION.GHG_QUANTITY']/1e6\n",
    "#display(ghgrp_ferro)\n",
    "\n",
    "\n",
    "#if reading in FLIGHT data: (includes subpart C emissions plus K and X)\n",
    "#Read Petrochemical GHGRP (convert metric tons CO2e to Tg)\n",
    "#ghgrp_petro = pd.read_csv('Data/Flight_SubpartX_Petrochemical.csv')\n",
    "#ghgrp_petro['emis_tg_tot'] = ghgrp_petro['GHG QUANTITY (METRIC TONS CO2e)']*0.000001/GWP_CH4 #tg\n",
    "        \n",
    "#Read Ferroalloy GHGRP\n",
    "#ghgrp_ferro = pd.read_csv('Data/Flight_SubpartK_Ferro.csv')\n",
    "#ghgrp_ferro['emis_tg_tot'] = ghgrp_ferro['GHG QUANTITY (METRIC TONS CO2e)']*0.000001/GWP_CH4 #tg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2 Allocate Proxy data to Grid Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each facility, use the fraction of reported emissions for each facility relative to the national total\n",
    "# (= reported facility emissions / sum of all reported facility emissions) to allocate the national GHGI emissions\n",
    "# Facility emissions for each year are first stored on a grid array for facilities in and outside CONUS region \n",
    "\n",
    "# Petro emissions\n",
    "Map_ghgrppetro = np.zeros([len(Lat_01),len(Lon_01),num_years]) #data represent a snapshot in time that is applied to entire timeseries\n",
    "Map_ghgrppetro_nongrid = np.zeros([num_years])\n",
    "\n",
    "for iyear in np.arange(0,len(year_range)):\n",
    "    petro_temp = ghgrp_petro[ghgrp_petro['Year'] ==year_range[iyear]]\n",
    "    petro_temp.reset_index(inplace=True, drop=True)\n",
    "    for ifacility in np.arange(0,len(petro_temp)): \n",
    "        if petro_temp['LONGITUDE'][ifacility] > Lon_left and petro_temp['LONGITUDE'][ifacility] < Lon_right \\\n",
    "            and petro_temp['LATITUDE'][ifacility] > Lat_low and petro_temp['LATITUDE'][ifacility] < Lat_up:\n",
    "            ilat = int((petro_temp['LATITUDE'][ifacility] - Lat_low)/Res01)\n",
    "            ilon = int((petro_temp['LONGITUDE'][ifacility] - Lon_left)/Res01)\n",
    "            Map_ghgrppetro[ilat,ilon,iyear] += petro_temp.loc[ifacility, 'emis_tg_tot']\n",
    "        else:\n",
    "            Map_ghgrppetro_nongrid[iyear] += petro_temp.loc[ifacility, 'emis_tg_tot']  \n",
    "\n",
    "# Ferro emissions\n",
    "Map_ghgrpferro = np.zeros([len(Lat_01),len(Lon_01),num_years]) #data represent a snapshot in time that is applied to entire timeseries\n",
    "Map_ghgrpferro_nongrid = np.zeros([num_years])\n",
    "\n",
    "for iyear in np.arange(0,len(year_range)):\n",
    "    ferro_temp = ghgrp_ferro[ghgrp_ferro['Year'] ==year_range[iyear]]\n",
    "    ferro_temp.reset_index(inplace=True, drop=True)\n",
    "    for ifacility in np.arange(0,len(ferro_temp)): \n",
    "        if ferro_temp['LONGITUDE'][ifacility] > Lon_left and ferro_temp['LONGITUDE'][ifacility] < Lon_right \\\n",
    "            and ferro_temp['LATITUDE'][ifacility] > Lat_low and ferro_temp['LATITUDE'][ifacility] < Lat_up:\n",
    "            ilat = int((petro_temp['LATITUDE'][ifacility] - Lat_low)/Res01)\n",
    "            ilon = int((petro_temp['LONGITUDE'][ifacility] - Lon_left)/Res01)\n",
    "            Map_ghgrpferro[ilat,ilon,iyear] += ferro_temp.loc[ifacility, 'emis_tg_tot']\n",
    "        else:\n",
    "            Map_ghgrpferro_nongrid[iyear] += ferro_temp.loc[ifacility, 'emis_tg_tot']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Step 3. Read In EPA GHGI Data\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1. Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Petrochemical GHGI emissions (1990-2018), in kt\n",
    "\n",
    "#Petrochemicals\n",
    "EPA_petro_emissions = pd.read_excel(EPA_inputfile, skiprows = 2, sheet_name = \"Petrochemicals\")\n",
    "EPA_petro_emissions.rename(columns={EPA_petro_emissions.columns[0]:'Source'}, inplace=True)\n",
    "EPA_petro_emissions = EPA_petro_emissions.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_petro_emissions['Source'] = 'Total Petrochemicals'\n",
    "\n",
    "#Ferroalloy\n",
    "EPA_ferro_emissions = pd.read_excel(EPA_inputfile, skiprows = 2, sheet_name = \"Ferroalloys\")\n",
    "EPA_ferro_emissions= EPA_ferro_emissions.drop(columns = ['Unnamed: 1'])\n",
    "EPA_ferro_emissions.rename(columns={EPA_ferro_emissions.columns[0]:'Source'}, inplace=True)\n",
    "EPA_ferro_emissions = EPA_ferro_emissions.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_ferro_emissions = EPA_ferro_emissions.drop([0,2], axis=0)\n",
    "EPA_ferro_emissions['Source'] = 'Total Ferroalloy'\n",
    "\n",
    "EPA_Industry = pd.concat([EPA_petro_emissions,EPA_ferro_emissions])\n",
    "display(EPA_Industry)\n",
    "\n",
    "Total_EPA_Industry_Emissions = EPA_ferro_emissions.iloc[0,1:]+EPA_petro_emissions.iloc[0,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Split Emissions into Gridding Groups (each Group will have the same proxy applied during the gridding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Emissions in Units of kt\n",
    "# Use mapping proxy and source files to split the GHGI emissions\n",
    "\n",
    "DEBUG =1\n",
    "\n",
    "start_year_idx = EPA_Industry.columns.get_loc(start_year)\n",
    "end_year_idx = EPA_Industry.columns.get_loc(end_year)+1\n",
    "sum_emi = np.zeros(num_years)\n",
    "\n",
    "ghgi_ind_groups = ghgi_ind_map['GHGI_Emi_Group'].unique()\n",
    "\n",
    "for igroup in np.arange(0,len(ghgi_ind_groups)): #loop through all groups, finding the GHGI sources in that group and summing emissions for that region, year\n",
    "        vars()[ghgi_ind_groups[igroup]] = np.zeros([num_years])\n",
    "        source_temp = ghgi_ind_map.loc[ghgi_ind_map['GHGI_Emi_Group'] == ghgi_ind_groups[igroup], 'GHGI_Source']\n",
    "        pattern_temp  = '|'.join(source_temp)\n",
    "        ##DEBUG## display(pattern_temp)\n",
    "        emi_temp = EPA_Industry[EPA_Industry['Source'].str.contains(pattern_temp)]\n",
    "        ##DEBUG## display(emi_temp)\n",
    "        vars()[ghgi_ind_groups[igroup]][:] = np.where(emi_temp.iloc[:,start_year_idx:] =='',[0],emi_temp.iloc[:,start_year_idx:]).sum(axis=0)#/float(1000) #convert Mg to kt\n",
    "\n",
    "#Check against total summary emissions \n",
    "print('QA/QC #1: Check Processing Emission Sum against GHGI Summary Emissions')\n",
    "for iyear in np.arange(0,num_years): \n",
    "    for igroup in np.arange(0,len(ghgi_ind_groups)):\n",
    "        sum_emi[iyear] += vars()[ghgi_ind_groups[igroup]][iyear]\n",
    "        \n",
    "    summary_emi = Total_EPA_Industry_Emissions[year_range[iyear]]  \n",
    "    #Check 1 - make sure that the sums from all the regions equal the totals reported\n",
    "    diff1 = abs(sum_emi[iyear] - summary_emi)/((sum_emi[iyear] + summary_emi)/2)\n",
    "    if DEBUG ==1:\n",
    "        print(summary_emi)\n",
    "        print(sum_emi[iyear])\n",
    "    if diff1 < 0.0001:\n",
    "        print('Year ', year_range[iyear],': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear],': FAIL (check Production & summary tabs): ', diff1,'%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Step 4. Grid Data (using spatial proxies)\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1.1 Define Proxy maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names on the *left* need to match the 'Industry_ProxyMapping' 'Proxy_Group' names \n",
    "# (these are initialized in Step 2). \n",
    "# The names on the right are the variable names used to caluclate the proxies in this code.\n",
    "# Names on the *right* need to match those from the code in Step 2.5\n",
    "\n",
    "Map_Ferro = Map_ghgrpferro\n",
    "Map_Ferro_nongrid = Map_ghgrpferro_nongrid\n",
    "Map_Petro = Map_ghgrppetro\n",
    "Map_Petro_nongrid = Map_ghgrppetro_nongrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1.2 Calculate the fractional proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate weighting arrays (i.e., fractional arrays)\n",
    "# also weight by the number of days in each year\n",
    "\n",
    "proxy_ind_map_unique = np.unique(proxy_ind_map['Proxy_Group'])\n",
    "#print(proxy_proc_map_unique)\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "        month_days = month_day_leap\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        month_days = month_day_nonleap  \n",
    "    \n",
    "    #Step 1a: weighted proxy ongrid = ongrid proxy * days each year\n",
    "    #Step 1b: weighted proxy offgrid = offgrid proxy * days each year\n",
    "    #Step 2a: noramlized weighted proxy ongrid = weighted proxy in each grid cell / (sum weighted proxy ongrid + weighted proxy offgrid)\n",
    "    #Step 2b: noramlized weighted proxy offgrid = weighted proxy offgrid / (sum weighted proxy ongrid + weighted proxy offgrid)\n",
    "    print('Check That Sum of Ind. Proxy Arrays = 1 for: ', year_range[iyear])\n",
    "    for iproxy in np.arange(0,len(proxy_ind_map_unique)):\n",
    "        #DEBUG## print(np.sum(vars()[proxy_ind_map.loc[iproxy,'Proxy_Group']][:,:,iyear]))\n",
    "        vars()[proxy_ind_map.loc[iproxy,'Proxy_Group']][:,:,iyear] *= np.sum(month_days)\n",
    "        ##DEBUG## print(np.sum(vars()[proxy_proc_map.loc[iproxy,'Proxy_Group']][:,:,iyear]))\n",
    "        vars()[proxy_ind_map.loc[iproxy,'Proxy_Group']+'_nongrid'][iyear] *= np.sum(month_days)\n",
    "        temp_sum = float(np.sum(vars()[proxy_ind_map.loc[iproxy,'Proxy_Group']][:,:,iyear]) + \\\n",
    "                    np.sum(vars()[proxy_ind_map.loc[iproxy,'Proxy_Group']+'_nongrid'][iyear]))\n",
    "        ##DEBUG## print(temp_sum)\n",
    "        vars()[proxy_ind_map.loc[iproxy,'Proxy_Group']][:,:,iyear] = \\\n",
    "                    data_fn.safe_div(vars()[proxy_ind_map.loc[iproxy,'Proxy_Group']][:,:,iyear], temp_sum)\n",
    "        vars()[proxy_ind_map.loc[iproxy,'Proxy_Group']+'_nongrid'][iyear] = \\\n",
    "                    data_fn.safe_div(vars()[proxy_ind_map.loc[iproxy,'Proxy_Group']+'_nongrid'][iyear], temp_sum)\n",
    "        proxy_sum = np.sum(vars()[proxy_ind_map.loc[iproxy,'Proxy_Group']][:,:,iyear])+np.sum(vars()[proxy_ind_map.loc[iproxy,'Proxy_Group']+'_nongrid'][iyear])\n",
    "        ##DEBUG## print(proxy_sum)\n",
    "        if proxy_sum >1.0001 or proxy_sum <0.9999:\n",
    "            print('Check ', proxy_ind_map.loc[iproxy,'Proxy_Group'],': ', proxy_sum)   \n",
    "        else:\n",
    "            print('Pass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step. 4.2. Allocate emissions to the CONUS region (0.1x0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process...\n",
    "# 1) make emissions array with correct dimensions\n",
    "# 2) weight monthly data by days in month (or year)\n",
    "# 3) caluclate emissions as emissions = GHGI emissions * Proxy Map\n",
    "\n",
    "DEBUG =1\n",
    "\n",
    "Emissions = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Emissions_Ferro = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Emissions_Petro = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Emissions_nongrid = np.zeros([num_years])\n",
    "Emi_not_mapped_sum = np.zeros(num_years)\n",
    "\n",
    "#loop through each emission group, where: Gridded emissions = National emissions * proxy map\n",
    "for igroup in np.arange(0,len(proxy_ind_map)):\n",
    "    vars()['Flux_'+proxy_ind_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "    vars()['Flux_'+proxy_ind_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "    vars()['Ext_'+proxy_ind_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "    for iyear in np.arange(0,num_years):\n",
    "        vars()['Flux_'+proxy_ind_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += \\\n",
    "            vars()[proxy_ind_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "            vars()[proxy_ind_map.loc[igroup,'Proxy_Group']][:,:,iyear]\n",
    "        vars()['Flux_'+proxy_ind_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear] += \\\n",
    "            vars()[proxy_ind_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "            vars()[proxy_ind_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear]\n",
    "        if 'Ferro' in proxy_ind_map.loc[igroup,'GHGI_Emi_Group']:\n",
    "            vars()['Ext_'+proxy_ind_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += vars()['Flux_'+proxy_ind_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]\n",
    "            Emissions_Ferro[:,:,iyear] += vars()['Flux_'+proxy_ind_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]\n",
    "        if 'Petro' in proxy_ind_map.loc[igroup,'GHGI_Emi_Group']:\n",
    "            #print(igroup)\n",
    "            vars()['Ext_'+proxy_ind_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += vars()['Flux_'+proxy_ind_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]\n",
    "            Emissions_Petro[:,:,iyear] += vars()['Flux_'+proxy_ind_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]\n",
    "        Emissions[:,:,iyear] += vars()['Flux_'+proxy_ind_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]\n",
    "        Emissions_nongrid[iyear] += vars()['Flux_'+proxy_ind_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear]\n",
    "\n",
    "    \n",
    "# QA/QC gridded emissions\n",
    "# Check sum of all gridded emissions + emissions not included in gridding (e.g., AK), and other non-gridded areas\n",
    "print('QA/QC #1: Check weighted emissions against GHGI')   \n",
    "for iyear in np.arange(0,num_years):\n",
    "    summary_emi = Total_EPA_Industry_Emissions[year_range[iyear]]\n",
    "    calc_emi =  np.sum(Emissions_Ferro[:,:,iyear]) +np.sum(Emissions_Petro[:,:,iyear]) + Emi_not_mapped_sum[iyear] + Emissions_nongrid[iyear]\n",
    "    if DEBUG==1:\n",
    "        print(summary_emi)\n",
    "        print(calc_emi)\n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0002:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2.2 Save gridded emissions (kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save gridded emissions for each gridding group - for extension\n",
    "\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(grid_emi_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "unique_groups = np.unique(proxy_ind_map['GHGI_Emi_Group'])\n",
    "unique_groups = unique_groups[unique_groups != 'Emi_not_mapped']\n",
    "\n",
    "nc_out = Dataset(grid_emi_outputfile, 'r+', format='NETCDF4')\n",
    "\n",
    "for igroup in np.arange(0,len(unique_groups)):\n",
    "    print('Ext_'+unique_groups[igroup])\n",
    "    if len(np.shape(vars()['Ext_'+unique_groups[igroup]])) ==3:\n",
    "        ghgi_temp = np.sum(vars()['Ext_'+unique_groups[igroup]],axis=2) #sum month data\n",
    "    else:\n",
    "        ghgi_temp = vars()['Ext_'+unique_groups[igroup]]\n",
    "\n",
    "    # Write data to netCDF\n",
    "    data_out = nc_out.createVariable('Ext_'+unique_groups[igroup], 'f8', ('lat', 'lon','year'), zlib=True)\n",
    "    data_out[:,:,:] = ghgi_temp\n",
    "\n",
    "#save nongrid data to calculate non-grid fraction extension\n",
    "data_out = nc_out.createVariable('Emissions_nongrid', 'f8', ('year'), zlib=True)  \n",
    "data_out[:] = Emissions_nongrid[:]\n",
    "nc_out.close()\n",
    "\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded emissions (kt) written to file: {}\" .format(os.getcwd())+grid_emi_outputfile)\n",
    "print(' ')\n",
    "\n",
    "del data_out, ghgi_temp, nc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.3 Calculate Gridded Emission Fluxes (molec./cm2/s) (0.1x0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 -- #Convert emissions to emission flux\n",
    "# conversion: kt emissions to molec/cm2/s flux\n",
    "\n",
    "DEBUG=1\n",
    "\n",
    "#Initialize arrays\n",
    "check_sum_annual = np.zeros([num_years])\n",
    "Flux_array_01_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Flux_array_01_ferro_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Flux_array_01_petro_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "for igroup in np.arange(0,len(proxy_ind_map)):\n",
    "    vars()['Flux_'+proxy_ind_map.loc[igroup,'GHGI_Emi_Group']+'_annual'] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "\n",
    "\n",
    "#Calculate fluxes\n",
    "for iyear in np.arange(0,num_years):\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "        month_days = month_day_leap\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        month_days = month_day_nonleap \n",
    "    \n",
    "    # calculate fluxes for annual data  (=kt * grams/kt *molec/mol *mol/g *s^-1 * cm^-2)\n",
    "    conversion_factor_annual = 10**9 * Avogadro / float(Molarch4 * np.sum(month_days) * 24 * 60 *60) / area_matrix_01\n",
    "    print(np.median(conversion_factor_annual))\n",
    "    for igroup in np.arange(0,len(proxy_ind_map)):\n",
    "        vars()['Flux_'+proxy_ind_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] *= conversion_factor_annual\n",
    "        vars()['Flux_'+proxy_ind_map.loc[igroup,'GHGI_Emi_Group']+'_annual'][:,:,iyear] = vars()['Flux_'+proxy_ind_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]\n",
    "        Flux_array_01_annual[:,:,iyear] = Emissions[:,:,iyear]*conversion_factor_annual\n",
    "        Flux_array_01_ferro_annual[:,:,iyear] = Emissions_Ferro[:,:,iyear]*conversion_factor_annual\n",
    "        Flux_array_01_petro_annual[:,:,iyear] = Emissions_Petro[:,:,iyear]*conversion_factor_annual\n",
    "    check_sum_annual[iyear] = np.sum(Flux_array_01_ferro_annual[:,:,iyear]/conversion_factor_annual) +\\\n",
    "                                np.sum(Flux_array_01_petro_annual[:,:,iyear]/conversion_factor_annual)#convert back to emissions to check at end\n",
    "\n",
    "print(' ')\n",
    "print('QA/QC #2: Check final gridded fluxes against GHGI')  \n",
    "# for the sum, check the converted annual emissions (convert back from flux) plus all the non-gridded emissions\n",
    "for iyear in np.arange(0,num_years):\n",
    "    calc_emi = check_sum_annual[iyear] + Emissions_nongrid[iyear]\n",
    "    summary_emi = Total_EPA_Industry_Emissions[year_range[iyear]]\n",
    "    if DEBUG==1:\n",
    "        print(calc_emi)\n",
    "        print(summary_emi)\n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0001:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')\n",
    "        \n",
    "Flux_Emissions_Total_annual = Flux_array_01_annual\n",
    "Flux_Emissions_Petro_annual = Flux_array_01_petro_annual\n",
    "Flux_Emissions_Ferro_annual = Flux_array_01_ferro_annual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 5. Write gridded (0.1⁰x0.1⁰) data to netCDF files.\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize netCDF files\n",
    "\n",
    "data_IO_fn.initialize_netCDF(gridded_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "data_IO_fn.initialize_netCDF(gridded_petro_outputfile, netCDF_petro_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "data_IO_fn.initialize_netCDF(gridded_ferro_outputfile, netCDF_ferro_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write the Data to netCDF\n",
    "nc_out = Dataset(gridded_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Total_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded industry fluxes written to file: {}\" .format(os.getcwd())+gridded_outputfile)\n",
    "print('')\n",
    "\n",
    "#Petro\n",
    "# Write the Data to netCDF\n",
    "nc_out = Dataset(gridded_petro_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Petro_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded industry fluxes written to file: {}\" .format(os.getcwd())+gridded_petro_outputfile)\n",
    "print('')\n",
    "\n",
    "#Ferro\n",
    "# Write the Data to netCDF\n",
    "nc_out = Dataset(gridded_ferro_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Ferro_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded industry fluxes written to file: {}\" .format(os.getcwd())+gridded_ferro_outputfile)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 6. Plot Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7.1. Plot Annual Emission Fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot annual data for entire timeseries\n",
    "scale_max = 2\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_str, scale_max,save_flag,save_outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot each individually (could change to plot each group)\n",
    "for igroup in np.arange(0,len(proxy_ind_map)):\n",
    "    temp_plot = vars()['Flux_'+proxy_ind_map.loc[igroup,'GHGI_Emi_Group']]\n",
    "    data_plot_fn.plot_annual_emission_flux_map(temp_plot, Lat_01, Lon_01, year_range, proxy_ind_map.loc[igroup,'GHGI_Emi_Group'], scale_max,save_flag,save_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7.2 Plot Difference between first and last inventory year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot difference between last and first year for the industry total\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_diff_str,save_flag,save_outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = datetime.datetime.now() \n",
    "ft = ct.timestamp() \n",
    "time_elapsed = (ft-it)/(60*60)\n",
    "print('Time to run: '+str(time_elapsed)+' hours')\n",
    "print('** GEPA_2B8_2C2_Industry: COMPLETE **')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

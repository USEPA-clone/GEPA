{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded EPA Methane Inventory\n",
    "## Category: 5D Wastewater Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Authors: \n",
    "Erin E. McDuffie, Bram Maasakkers, Maggie Schultz\n",
    "#### Date Last Updated: \n",
    "see Step 0\n",
    "#### Notebook Purpose: \n",
    "This Notebook calculates and reports annual gridded (0.1°x0.1°) methane emission fluxes (molec./cm2/s) from Wastewater treatment facilities (total, industrial, and domestic) in the CONUS region between 2012-2018. \n",
    "#### Summary & Notes:\n",
    "The national EPA GHGI emissions from domestic and industrial wastewater treatment facilities (split into sub-sources) are read in from the EPA GHG Inventory wastewater treatment workbook. Emissions are available as national totals (for entire time series). State-level allocations are also available from the 2021 State GHG Inventory for two industrial sectors (pulp and paper manufacturing and food and beverage manufacturing) within the industrial waste category. National industrial waste emissions are allocated to the state level (for each subgroup) using these relative state-level emissions data. State-level emissions for each subgroup are then allocated to the 0.01⁰x0.01⁰ CONUS grid using gridded data of facility-level emissions for each subgroup. Data are then re-gridded to 0.1⁰x0.1⁰. National MSW landfill emissions are allocated directly to the 0.1⁰x0.1⁰ CONUS grid using relative facility-level emissions for MSW landfills. All data are then converted to fluxes (molecules CH4/cm2/s). Annual emission fluxes (molecules CH4/cm2/s) for total landfills, MSW landfills, and industrial landfills are written to final netCDFs in the ‘/code/Final_Gridded_Data/’ folder. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Step 0. Set-Up Notebook Modules, Functions, and Local Parameters and Constants\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm working directory & print last update time\n",
    "import os\n",
    "import time\n",
    "modtime = os.path.getmtime('./5D_Wastewater.ipynb')\n",
    "modificationTime = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(modtime))\n",
    "print(\"This file was last modified on: \", modificationTime)\n",
    "print('')\n",
    "print(\"The directory we are working in is {}\" .format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Include plots within notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import pyodbc\n",
    "import PyPDF2 as pypdf\n",
    "import tabula as tb\n",
    "import shapefile as shp\n",
    "from datetime import datetime\n",
    "from copy import copy\n",
    "from scipy.interpolate import interp1d\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Import additional modules\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# Load netCDF (for manipulating netCDF file types)\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Set up ticker\n",
    "#import matplotlib.ticker as ticker\n",
    "\n",
    "#add path for the global function module (file)\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../Global_Functions/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Load functions\n",
    "import data_load_functions as data_load_fn\n",
    "import data_functions as data_fn\n",
    "import data_IO_functions as data_IO_fn\n",
    "import data_plot_functions as data_plot_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT Files\n",
    "# Assign global file names\n",
    "global_filenames = data_load_fn.load_global_file_names()\n",
    "State_ANSI_inputfile = global_filenames[0]\n",
    "#County_ANSI_inputfile = global_filenames[1]\n",
    "pop_map_inputfile = global_filenames[2]\n",
    "Grid_area01_inputfile = global_filenames[3]\n",
    "Grid_area001_inputfile = global_filenames[4]\n",
    "Grid_state001_ansi_inputfile = global_filenames[5]\n",
    "#Grid_county001_ansi_inputfile = global_filenames[6]\n",
    "globalinputlocation = global_filenames[0][0:20]\n",
    "print(globalinputlocation)\n",
    "\n",
    "# EPA Inventory Data\n",
    "EPA_ww_inputfile = globalinputlocation+'GHGI/Ch7_Waste/WastewaterTreatment18_for PR_CRF_REVISED_02032020.xlsx'\n",
    "\n",
    "#proxy mapping file\n",
    "Wastewater_Mapping_inputfile = './InputData/WastewaterTreatment_ProxyMapping.xlsx'\n",
    "\n",
    "# GHGRP Data\n",
    "ghgrp_emi_ii_inputfile = './InputData/ghgrp_subpart_II.csv'\n",
    "ghgrp_facility_ii_inputfile = './InputData/SubpartII_Facilities.csv'\n",
    "\n",
    "#ECHO Data\n",
    "echo_inputfile = './InputData/ECHO/ECHO_'\n",
    "\n",
    "cwns_inputfile = './InputData/CWNS/Final Clean CWNS 2004.mdb'\n",
    "\n",
    "#OUTPUT FILES\n",
    "gridded_outputfile = '../Final_Gridded_Data/EPA_v2_5D_Wastewater_Treatment.nc'\n",
    "gridded_dom_outputfile = '../Final_Gridded_Data/EPA_v2_5D_Wastewater_Treatment_Domestic.nc'\n",
    "gridded_ind_outputfile = '../Final_Gridded_Data/EPA_v2_5D_Wastewater_Treatment_Industrial.nc'\n",
    "\n",
    "netCDF_description = 'Gridded EPA Inventory - Wastewater Treatment Emissions - IPCC Source Category 5D'\n",
    "netCDF_dom_description = 'Gridded EPA Inventory - Wastewater Treatment and Discharge Emissions - IPCC Source Category 5D - Domestic'\n",
    "netCDF_ind_description = 'Gridded EPA Inventory - Wastewater Treatment and Discharge Emissions - IPCC Source Category 5D - Industrial'\n",
    "\n",
    "title_str = \"EPA methane emissions from wastewater treatment\"\n",
    "title_str_dom = \"EPA methane emissions from domestic wastewater\"\n",
    "title_str_ind = \"EPA methane emissions from industrial wastewater\"\n",
    "title_diff_str = \"Emissions from wastewater difference: 2018-2012\"\n",
    "title_diff_str_dom = \"Emissions from domestic wastewater treatment difference: 2018-2012\"\n",
    "title_diff_str_ind = \"Emissions from industrial wastewater treatment difference: 2018-2012\"\n",
    "\n",
    "#output gridded proxy data\n",
    "grid_emi_outputfile = '../Final_Gridded_Data/Extension/v2_input_data/Wastewater_Grid_Emi.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local variables\n",
    "start_year = 2012  #First year in emission timeseries\n",
    "end_year = 2018    #Last year in emission timeseries\n",
    "year_range = [*range(start_year, end_year+1,1)] #List of emission years\n",
    "year_range_str=[str(i) for i in year_range]\n",
    "num_years = len(year_range)\n",
    "num_inv_years = len([*range(1990, end_year+1,1)]) #List of inventory years\n",
    "\n",
    "# Define constants\n",
    "Avogadro   = 6.02214129 * 10**(23)  #molecules/mol\n",
    "Molarch4   = 16.04                  #g/mol\n",
    "Res01      = 0.1                    # degrees\n",
    "Res_01     = 0.01                   # degrees\n",
    "hrs_to_yrs = 8760                   #number of hours in a year\n",
    "g_to_mt    = 1*10**(-6)             # grams to metric ton\n",
    "\n",
    "# Continental US Lat/Lon Limits (for netCDF files)\n",
    "Lon_left = -130       #deg\n",
    "Lon_right = -60       #deg\n",
    "Lat_low  = 20         #deg\n",
    "Lat_up  = 55          #deg\n",
    "loc_dimensions = [Lat_low, Lat_up, Lon_left, Lon_right]\n",
    "\n",
    "ilat_start = int((90+Lat_low)/Res01) #1100:1450 (continental US range)\n",
    "ilat_end = int((90+Lat_up)/Res01)\n",
    "ilon_start = abs(int((-180-Lon_left)/Res01)) #500:1200 (continental US range)\n",
    "ilon_end = abs(int((-180-Lon_right)/Res01))\n",
    "\n",
    "# Number of days in each month\n",
    "month_day_leap  = [  31,  29,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_day_nonleap = [  31,  28,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_tag = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "month_dict = {'January':1, 'February':2,'March':3,'April':4,'May':5,'June':6, 'July':7,'August':8,'September':9,'October':10,\\\n",
    "             'November':11,'December':12}\n",
    "\n",
    "# Month arrays\n",
    "month_range_str = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "num_months = len(month_range_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;\n",
    "//prevent auto-scrolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track run time\n",
    "ct = datetime.now() \n",
    "it = ct.timestamp() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Step 1. Load in State ANSI data, and Area Maps\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State-level ANSI Data\n",
    "#Read the state ANSI file array\n",
    "State_ANSI, name_dict, abbr_dict = data_load_fn.load_state_ansi(State_ANSI_inputfile)[0:3]\n",
    "#QA: number of states\n",
    "print('Read input file: '+ f\"{State_ANSI_inputfile}\")\n",
    "print('Total \"States\" found: ' + '%.0f' % len(State_ANSI))\n",
    "print(' ')\n",
    "\n",
    "# 0.01 x0.01 degree Data\n",
    "# State ANSI IDs and grid cell area (m2) maps\n",
    "state_ANSI_map = data_load_fn.load_state_ansi_map(Grid_state001_ansi_inputfile)\n",
    "area_map, lat001, lon001 = data_load_fn.load_area_map_001(Grid_area001_inputfile)\n",
    "\n",
    "# 0.1 x0.1 degree data\n",
    "# grid cell area and state ANSI maps\n",
    "Lat01, Lon01 = data_load_fn.load_area_map_01(Grid_area01_inputfile)[1:3]\n",
    "#Select relevant Continental 0.1 x0.1 domain\n",
    "Lat_01 = Lat01[ilat_start:ilat_end]\n",
    "Lon_01 = Lon01[ilon_start:ilon_end]\n",
    "area_matrix_01 = data_fn.regrid001_to_01(area_map, Lat_01, Lon_01)\n",
    "area_matrix_01 *= 10000  #convert from m2 to cm2\n",
    "#state_ANSI_map_01 = data_fn.regrid001_to_01(state_ANSI_map, Lat_01, Lon_01)\n",
    "#del area_map#, lat001, lon001, global_filenames\n",
    "\n",
    "# Print time\n",
    "ct = datetime.now() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 2: Read-in and Format Proxy Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 Read In Proxy Mapping File & Make Proxy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load GHGI Mapping Groups\n",
    "names = pd.read_excel(Wastewater_Mapping_inputfile, sheet_name = \"GHGI Map - WW\", usecols = \"A:B\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "ghgi_wwt_map = pd.read_excel(Wastewater_Mapping_inputfile, sheet_name = \"GHGI Map - WW\", usecols = \"A:B\", skiprows = 1, names = colnames)\n",
    "#drop rows with no data, remove the parentheses and \"\"\n",
    "ghgi_wwt_map = ghgi_wwt_map[ghgi_wwt_map['GHGI_Emi_Group'] != 'na']\n",
    "ghgi_wwt_map = ghgi_wwt_map[ghgi_wwt_map['GHGI_Emi_Group'].notna()]\n",
    "ghgi_wwt_map = ghgi_wwt_map[ghgi_wwt_map['GHGI_Emi_Group'] != '-']\n",
    "ghgi_wwt_map['GHGI_Source']= ghgi_wwt_map['GHGI_Source'].str.replace(r\"\\(\",\"\")\n",
    "ghgi_wwt_map['GHGI_Source']= ghgi_wwt_map['GHGI_Source'].str.replace(r\"\\)\",\"\")\n",
    "ghgi_wwt_map['GHGI_Source']= ghgi_wwt_map['GHGI_Source'].str.replace(r\"+\",\"\")\n",
    "ghgi_wwt_map.reset_index(inplace=True, drop=True)\n",
    "display(ghgi_wwt_map)\n",
    "\n",
    "#load emission group - proxy map\n",
    "names = pd.read_excel(Wastewater_Mapping_inputfile, sheet_name = \"Proxy Map - WW\", usecols = \"A:C\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "proxy_wwt_map = pd.read_excel(Wastewater_Mapping_inputfile, sheet_name = \"Proxy Map - WW\", usecols = \"A:C\", skiprows = 1, names = colnames)\n",
    "display((proxy_wwt_map))\n",
    "\n",
    "#create empty proxy and emission group arrays (for state and months, where needed)\n",
    "for igroup in np.arange(0,len(proxy_wwt_map)):\n",
    "    if proxy_wwt_map.loc[igroup, 'Grid_Month_Flag'] ==0:\n",
    "        vars()[proxy_wwt_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        vars()[proxy_wwt_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "    else:\n",
    "        vars()[proxy_wwt_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "        vars()[proxy_wwt_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years,num_months])\n",
    "        \n",
    "    vars()[proxy_wwt_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])  \n",
    "        \n",
    "emi_group_names = np.unique(ghgi_wwt_map['GHGI_Emi_Group'])\n",
    "\n",
    "print('QA/QC: Is the number of emission groups the same for the proxy and emissions tabs?')\n",
    "if (len(emi_group_names) == len(np.unique(proxy_wwt_map['GHGI_Emi_Group']))):\n",
    "    print('PASS')\n",
    "else:\n",
    "    print('FAIL')\n",
    "    print(emi_group_names)\n",
    "    print(len(emi_group_names))\n",
    "    print(len(np.unique(proxy_wwt_map['GHGI_Emi_Group'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2 Reads In Domestic Wastewater Treatment Proxy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2.1. Read in Full ECHO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read in and combine ECHO data\n",
    "for iyear in np.arange(0, num_years):\n",
    "    print('Year:',year_range[iyear])\n",
    "    for iregion in np.arange(1, 11): #10 regions of data\n",
    "        if iregion ==1 and iyear == 0:\n",
    "            echo_full = pd.read_csv(echo_inputfile+'Region'+str(iregion)+'_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "        elif iregion ==3:\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'DE_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'DC_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'MD_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'PA_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'VA_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'WV_POTW_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'WV_nonPOTW_GPC_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'WV_nonPOTW_NPD_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "        elif iregion ==4:\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'AL_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'FL_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'GA_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'KY_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'MS_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'NC_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'SC_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'TN_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "        elif iregion ==6:\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'AR_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'LA_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'NM_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'OK_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'TX_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "        elif iregion ==5 or iregion ==9:\n",
    "            echo_temp1 = pd.read_csv(echo_inputfile+'Region'+str(iregion)+'_POTW_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_temp2 = pd.read_csv(echo_inputfile+'Region'+str(iregion)+'_nonPOTW_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp1)\n",
    "            echo_full = echo_full.append(echo_temp2)\n",
    "        else:\n",
    "            echo_temp = pd.read_csv(echo_inputfile+'Region'+str(iregion)+'_'+year_range_str[iyear]+'.csv',skiprows = 3,low_memory = False)\n",
    "            echo_full = echo_full.append(echo_temp)\n",
    "        echo_full.reset_index(inplace=True, drop=True)\n",
    "        #display(echo_full)\n",
    "        print('Loaded Region',iregion,'...')\n",
    "echo_full = echo_full[[\"Year\",\"NPDES Permit Number\",\"FRS ID\",\"CWNS ID(s)\",\"Facility Type Indicator\",\"SIC Code\",\"NAICS Code\",\\\n",
    "                        \"City\",\"State\",'County','Facility Latitude','Facility Longitude',\\\n",
    "                       'Average Daily Flow (MGD)','Actual Average Facility Flow (MGD)',\\\n",
    "                       'Total Facility Design Flow (MGD)']]\n",
    "\n",
    "echo_full = echo_full.fillna(0)\n",
    "\n",
    "\n",
    "display(echo_full)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use reported daily flow rate. If not reported, use total design flow rate (if available and adjusted based on\n",
    "# nationally-avaialble ratios of reported flow to capacity). Do for both POTW and non-POTW facilities\n",
    "# Also Group data based on Permit Number (retain the maximum flow reported for the permit)\n",
    "# also filter out water supply SIC codes\n",
    "\n",
    "echo_potw = echo_full[(echo_full['Facility Type Indicator'] == 'POTW')& (echo_full['SIC Code'] != 4941.0)].copy()\n",
    "echo_potw = echo_potw.groupby(['Year','NPDES Permit Number'], as_index = False).agg(\\\n",
    "               {'FRS ID':'first','CWNS ID(s)':'first','SIC Code':'max','NAICS Code':'max','City':'first',\\\n",
    "                'State':'first','County':'first','Facility Latitude':'max','Facility Longitude':'max',\\\n",
    "                'calc_flow_mgd': 'max','Total Facility Design Flow (MGD)': 'max','Average Daily Flow (MGD)': 'max',\\\n",
    "               'Actual Average Facility Flow (MGD)':'max'})\n",
    "echo_potw.reset_index(inplace=True, drop=True)\n",
    "#assume incorrect units if reported flows or design capacities are > 1000\n",
    "echo_potw.loc[(echo_potw['Average Daily Flow (MGD)']  >= 1000) ,'Average Daily Flow (MGD)'] = \\\n",
    "    echo_potw.loc[(echo_potw['Average Daily Flow (MGD)']  >= 1000) ,'Average Daily Flow (MGD)']/1e6\n",
    "#assume incorrect units if reported flows or design capacities are > 1000\n",
    "echo_potw.loc[(echo_potw['Total Facility Design Flow (MGD)']  >= 1000) ,'Total Facility Design Flow (MGD)'] = \\\n",
    "    echo_potw.loc[(echo_potw['Total Facility Design Flow (MGD)']  >= 1000) ,'Total Facility Design Flow (MGD)']/1e6\n",
    "echo_potw.loc[(echo_potw['Actual Average Facility Flow (MGD)']  >= 1000) ,'Actual Average Facility Flow (MGD)'] = \\\n",
    "    echo_potw.loc[(echo_potw['Actual Average Facility Flow (MGD)']  >= 1000) ,'Actual Average Facility Flow (MGD)']/1e6\n",
    "\n",
    "#find median ratio of flow to capacity for all facilities that report both\n",
    "flow_subset = echo_potw.copy()\n",
    "#find where actual average flow is less than design flow and calculate national median ratio\n",
    "flow_subset = flow_subset[(flow_subset['Actual Average Facility Flow (MGD)'] > 0) & (flow_subset['Total Facility Design Flow (MGD)'] > 0) &\\\n",
    "                         (flow_subset['Actual Average Facility Flow (MGD)'] < flow_subset['Total Facility Design Flow (MGD)'])]\n",
    "flow_subset['ratio1'] = flow_subset['Actual Average Facility Flow (MGD)']/ flow_subset['Total Facility Design Flow (MGD)']\n",
    "potw_ratio1 = np.mean(flow_subset['ratio1'])\n",
    "#find where the actual and average daily flow rates are non-zero and calulate national median\n",
    "flow_subset = echo_potw.copy()\n",
    "flow_subset = flow_subset[(flow_subset['Actual Average Facility Flow (MGD)'] > 0) & (flow_subset['Average Daily Flow (MGD)'] > 0)]\n",
    "flow_subset['ratio2'] = flow_subset['Actual Average Facility Flow (MGD)']/ flow_subset['Average Daily Flow (MGD)']\n",
    "potw_ratio2 = np.median(flow_subset['ratio2'])\n",
    "\n",
    "print(potw_ratio1, potw_ratio2)\n",
    "#if no facility flow or daily flow is greater than design flow, replace with scaled design flow\n",
    "echo_potw.loc[(echo_potw['calc_flow_mgd']  <= 0) | \\\n",
    "              (echo_potw['calc_flow_mgd'] > echo_potw['Total Facility Design Flow (MGD)']) ,'calc_flow_mgd'] = \\\n",
    "    echo_potw.loc[(echo_potw['calc_flow_mgd']  <= 0) | \\\n",
    "              (echo_potw['calc_flow_mgd'] > echo_potw['Total Facility Design Flow (MGD)']),'Total Facility Design Flow (MGD)']*potw_ratio1\n",
    "# if no design flow, replace with scaled average daily flow\n",
    "echo_potw.loc[(echo_potw['calc_flow_mgd']  <= 0) ,'calc_flow_mgd'] = \\\n",
    "    echo_potw.loc[(echo_potw['calc_flow_mgd']  <= 0) ,'Average Daily Flow (MGD)']*potw_ratio2\n",
    "\n",
    "\n",
    "#for facilities with no reported flow data, use the reported capacity data, adjusted down by the national median ratio\n",
    "count_temp = echo_potw[((echo_potw['Actual Average Facility Flow (MGD)'] == 0) & (echo_potw['Total Facility Design Flow (MGD)']==0))&\\\n",
    "                      (echo_potw['Average Daily Flow (MGD)']==0)]\n",
    "#filter for all data <= 0 and report how many facilities those are\n",
    "print(str(count_temp.shape[0]), ' of ', str(echo_potw.shape[0]) , ' POTW facilities have no flow/capacity data')\n",
    "echo_potw = echo_potw[echo_potw['calc_flow_mgd'] >0]\n",
    "echo_potw.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#do the same for non-POTW (filter out water supply facilities)\n",
    "echo_nonpotw = echo_full[(echo_full['Facility Type Indicator'] == 'NON-POTW')& (echo_full['SIC Code'] != 4941.0)].copy()\n",
    "echo_nonpotw = echo_nonpotw.groupby(['Year','NPDES Permit Number'], as_index = False).agg(\\\n",
    "               {'FRS ID':'first','CWNS ID(s)':'first','SIC Code':'max','NAICS Code':'max','City':'first',\\\n",
    "                'State':'first','County':'first','Facility Latitude':'max','Facility Longitude':'max',\\\n",
    "                'calc_flow_mgd': 'max','Total Facility Design Flow (MGD)': 'max','Average Daily Flow (MGD)': 'max',\\\n",
    "               'Actual Average Facility Flow (MGD)':'max'})\n",
    "echo_nonpotw.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#find median ratio of flow to capacity for all facilities that report both\n",
    "flow_subset = echo_nonpotw.copy()\n",
    "flow_subset = flow_subset[(flow_subset['Actual Average Facility Flow (MGD)'] > 0) & (flow_subset['Total Facility Design Flow (MGD)'] > 0) &\\\n",
    "                         (flow_subset['Actual Average Facility Flow (MGD)'] < flow_subset['Total Facility Design Flow (MGD)'])]\n",
    "flow_subset['ratio1'] = flow_subset['Actual Average Facility Flow (MGD)']/ flow_subset['Total Facility Design Flow (MGD)']\n",
    "nonpotw_ratio1 = np.mean(flow_subset['ratio1'])\n",
    "#find where the actual and average daily flow rates are non-zero and calulate national median\n",
    "flow_subset = echo_nonpotw.copy()\n",
    "flow_subset = flow_subset[(flow_subset['Actual Average Facility Flow (MGD)'] > 0) & (flow_subset['Average Daily Flow (MGD)'] > 0)]\n",
    "flow_subset['ratio2'] = flow_subset['Actual Average Facility Flow (MGD)']/ flow_subset['Average Daily Flow (MGD)']\n",
    "nonpotw_ratio2 = np.median(flow_subset['ratio2'])\n",
    "#if no facility flow or daily flow is greater than design flow, replace with scaled design flow\n",
    "echo_nonpotw.loc[(echo_nonpotw['calc_flow_mgd']  <= 0) | \\\n",
    "              (echo_nonpotw['calc_flow_mgd'] > echo_nonpotw['Total Facility Design Flow (MGD)']) ,'calc_flow_mgd'] = \\\n",
    "    echo_nonpotw.loc[(echo_nonpotw['calc_flow_mgd']  <= 0) | \\\n",
    "              (echo_nonpotw['calc_flow_mgd'] > echo_nonpotw['Total Facility Design Flow (MGD)']),'Total Facility Design Flow (MGD)']*nonpotw_ratio1\n",
    "# if no design flow, replace with scaled average daily flow\n",
    "echo_nonpotw.loc[(echo_nonpotw['calc_flow_mgd']  <= 0) ,'calc_flow_mgd'] = \\\n",
    "    echo_nonpotw.loc[(echo_nonpotw['calc_flow_mgd']  <= 0) ,'Average Daily Flow (MGD)']*nonpotw_ratio2\n",
    "count_temp = echo_nonpotw[(echo_nonpotw['Actual Average Facility Flow (MGD)'] == 0) & (echo_nonpotw['Total Facility Design Flow (MGD)']==0)&\\\n",
    "                         (echo_nonpotw['Average Daily Flow (MGD)']==0)]\n",
    "#filter for all data <= 0 and report how many facilities those are\n",
    "print(str(count_temp.shape[0]), ' of ', str(echo_nonpotw.shape[0]) , ' non-POTW facilities have no flow/capacity data')\n",
    "echo_nonpotw = echo_nonpotw[echo_nonpotw['calc_flow_mgd'] >0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2.2. Format POTW ECHO Data and Place onto CONUS Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place data (wastewater flow) on CONUS grid\n",
    "map_dom_wwf = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "map_dom_wwf_nongrid = np.zeros([num_years])\n",
    "\n",
    "for iyear in np.arange(0, num_years):\n",
    "    echo_temp = echo_potw[echo_potw['Year'] == year_range[iyear]]\n",
    "    echo_temp.reset_index(inplace=True, drop=True)\n",
    "    for ifacility in np.arange(0, len(echo_temp)):\n",
    "        if echo_temp['Facility Longitude'][ifacility] > Lon_left and \\\n",
    "            echo_temp['Facility Longitude'][ifacility] < Lon_right and \\\n",
    "            echo_temp['Facility Latitude'][ifacility] > Lat_low and \\\n",
    "            echo_temp['Facility Latitude'][ifacility] < Lat_up:\n",
    "                \n",
    "            ilat = int((echo_temp['Facility Latitude'][ifacility] - Lat_low)/Res01)\n",
    "            ilon = int((echo_temp['Facility Longitude'][ifacility] - Lon_left)/Res01)\n",
    "            map_dom_wwf[ilat,ilon,iyear] += echo_temp['calc_flow_mgd'][ifacility]\n",
    "        else:\n",
    "            map_dom_wwf_nongrid[iyear] += echo_temp['calc_flow_mgd'][ifacility]\n",
    "                \n",
    "    print(np.sum(map_dom_wwf[:,:,iyear])+map_dom_wwf_nongrid[iyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2.1. Read in CWNS 2004 Survey (to get classification types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_str = r'Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ='+cwns_inputfile+';'''\n",
    "conn = pyodbc.connect(driver_str)\n",
    "cwns_facility_treatment = pd.read_sql(\"SELECT * FROM [Unit Processes]\", conn)\n",
    "conn.close()\n",
    "#Select columns\n",
    "cwns_facility_treatment = cwns_facility_treatment[[\"AF_NBR\",\"UPB_NAME\"]]  \n",
    "cwns_facility_treatment['UPB_NAME']= cwns_facility_treatment['UPB_NAME'].str.replace(r\"\\(\",\"\")\n",
    "cwns_facility_treatment['UPB_NAME']= cwns_facility_treatment['UPB_NAME'].str.replace(r\"\\)\",\"\")\n",
    "\n",
    "# Determine classification (anaerobic/anaerobic digestor/aerobic)\n",
    "# based on AerovsAnaero document provided by ERG (in Additional Resources document)\n",
    "#constructed wetlands calssifications are from the 'CW Data' tab in the wastewater inventory workbook\n",
    "cwns_facility_treatment['aerobic_flag'] = 0\n",
    "cwns_facility_treatment['anaerobic_flag'] = 0\n",
    "cwns_facility_treatment['anaerobic_digestor_flag'] = 0\n",
    "cwns_facility_treatment['const_wetland_flag'] = 0\n",
    "\n",
    "list_aerobic = ['Activated Bio-Filter ABF','Activated Sludge-Anaerobic/Anoxic/Oxic', 'Activated Sludge-Complete Mix',\\\n",
    "                'Activated Sludge-Contact Stabilization', 'Activated Sludge-Conventional', \\\n",
    "                'Activated Sludge-Extended Aeration', 'Activated Sludge-High Rate', 'Activated Sludge-Other Mode',\\\n",
    "                'Activated Sludge-Pure Oxygen', 'Activated Sludge-Step Aeration', \\\n",
    "                'Activated Sludge With Biological Denitrification', 'Aerated Biosolids Storage', 'Aerated Lagoon', \\\n",
    "                'Aeration System','Aerobic Digestion-Air', 'Aerobic Digestion-Oxygen', \\\n",
    "                'Autothermal Thermophilic Aerobic Digestion-Air', 'Autothermal Thermophilic Aerobic Digestion-Oxygn',\\\n",
    "                'Biological Nitrification-Separate Stage', 'Biological Phosphorus Removal', \\\n",
    "                'Biological Phosphorus Removal-Modified Bardenpho', 'Biological Phosphorus Removal-Phostrip', \\\n",
    "                'Combined Biological Nitrification And BOD Reductn', 'Oxidation Ditch', 'Package Plant', 'Post Aeration',\\\n",
    "                'Preaeration', 'Rapid Infiltration System-No Underdrain', 'Rapid Infiltration System W/Underdrain',\\\n",
    "                'Rotating Biological Contactor RBC', 'Sequencing Batch Reactor SBR', \\\n",
    "                'Slow Rate Application System-No Underdrain', 'Slow Rate Land Application System W/Underdrain', \\\n",
    "                'Trickling Filter', 'Trickling Filter-Other Media', 'Trickling Filter-Plastic Media', \\\n",
    "                'Trickling Filter-Redwood Slats','Trickling Filter-Rock Media']\n",
    "list_anaerobic = ['Anaerobic Digestion', 'Anaerobic Digestion-Thermophilic', 'Anaerobic Lagoons', 'Biosolids Lagoons',\\\n",
    "                  'Constructed Wetlands', 'Constructed Wetlands Resource Extraction', \\\n",
    "                  'Denitrification Filter-Coarse Media', 'Design/Install Constructed Wetlands', 'Facultative Lagoon',\\\n",
    "                  'Freesurface/WetlandMarsh System', 'Lagoon, Polishing Lagoon', 'Stabilization Pond', \\\n",
    "                  'Waste Treatment Lagoon NO. 359']\n",
    "list_constructed_wetlands = ['01000039001','01000054004','01000244001','01000353001','04001506001','05000130001',\\\n",
    "                             '05000722001','12000001039','16000163001','19000735001','20002005012','22001335001',\\\n",
    "                             '28000035001','28000155001','28000255001','28000660001','28000890001','28000915001',\\\n",
    "                             '28001005001','28001275001','28001275002','28001275003','28001315001','28001578002',\\\n",
    "                             '28001848001','29001003023','29002313001','29002418002','29002473001','36003149001',\\\n",
    "                             '39004809001','39009009001','40000371001','45000607105','46000015001','46000028001',\\\n",
    "                             '46000057001','46000060001','46000290001','46000291001','46000299001','46000368001',\\\n",
    "                             '46000404001','46000441001','46000448001','46000450001','46000458001','46000486001',\\\n",
    "                             '46000498001','46000555001','48006001001','51000222001','53000095001','55004430001',\\\n",
    "                             '56000093001','01000038001','01000064001','01000086001','01000248001','01000308001',\\\n",
    "                             '01000338001','01000348001','12000119001','12000139001','20000980001','36003204003']\n",
    "\n",
    "for ifacility in np.arange(0, len(cwns_facility_treatment)):\n",
    "    #Aerobic\n",
    "    if cwns_facility_treatment.loc[ifacility,'UPB_NAME'] in list_aerobic:\n",
    "        cwns_facility_treatment.loc[ifacility,'aerobic_flag'] = 1\n",
    "    #Anerobic\n",
    "    if cwns_facility_treatment.loc[ifacility,'UPB_NAME'] in list_anaerobic:\n",
    "        #Anaerobic digestors\n",
    "        if 'anaerobic digest' in cwns_facility_treatment.loc[ifacility,'UPB_NAME'].lower():\n",
    "            cwns_facility_treatment.loc[ifacility,'anaerobic_digestor_flag'] = 1\n",
    "        #anaerobic systems without digestors\n",
    "        else:\n",
    "            cwns_facility_treatment.loc[ifacility,'anaerobic_flag'] = 1\n",
    "    #constructed wetlands\n",
    "    if cwns_facility_treatment.loc[ifacility,'AF_NBR'] in list_constructed_wetlands:\n",
    "        cwns_facility_treatment.loc[ifacility,'const_wetland_flag'] = 1\n",
    "\n",
    "#Compress on AF_NBR, using max of flags\n",
    "cwns_facility_treatment = cwns_facility_treatment.groupby(['AF_NBR'], as_index = False).agg(\\\n",
    "               {'aerobic_flag':'max','anaerobic_flag':'max','anaerobic_digestor_flag':'max','const_wetland_flag':'first'})\n",
    "display(cwns_facility_treatment)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2.2. Match ECHO and CWNS datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through ECHO Data - try to match by CWNS number and record classification type\n",
    "\n",
    "echo_potw['aerobic_flag'] = 0\n",
    "echo_potw['anaerobic_flag'] = 0\n",
    "echo_potw['anaerobic_digestor_flag'] = 0\n",
    "echo_potw['const_wetland_flag'] = 0\n",
    "echo_potw['cwns_match'] = 0\n",
    "\n",
    "for ifacility in np.arange(0, len(echo_potw)):\n",
    "    imatch = np.where(echo_potw['CWNS ID(s)'][ifacility] == cwns_facility_treatment['AF_NBR'])[0]\n",
    "    if len(imatch)>0:\n",
    "        echo_potw.loc[ifacility,'aerobic_flag'] = cwns_facility_treatment.loc[imatch[0],'aerobic_flag']\n",
    "        echo_potw.loc[ifacility,'anaerobic_flag'] = cwns_facility_treatment.loc[imatch[0],'anaerobic_flag']\n",
    "        echo_potw.loc[ifacility,'anaerobic_digestor_flag'] = cwns_facility_treatment.loc[imatch[0],'anaerobic_digestor_flag']\n",
    "        echo_potw.loc[ifacility,'const_wetland_flag'] = cwns_facility_treatment.loc[imatch[0],'const_wetland_flag']\n",
    "        echo_potw.loc[ifacility,'cwns_match'] = 1\n",
    "display(echo_potw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2.3. Place wastewater volumes onto CONUS grid for each classification type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Place wastewater flow onto grid for each type\n",
    "# for facilities that weren't matched, assign fraction of flow to each classification based on the ratio of flows \n",
    "# for each classification type each year (so a facility will be considered partially under multiple categories)\n",
    "\n",
    "# This approach assigns values to each facility type that it's classified as. For example, \n",
    "# if a facility is classified as aeroboc and anaeroboc, the wasterwater flow\n",
    "# is mapped to both the aerobic and anaerobic proxies. This means that the percentages between\n",
    "# the four categories won't sum to 100% and will double or triple count facilities that have\n",
    "# mutliple classification types\n",
    "\n",
    "# Place data (wastewater flow) on CONUS grid\n",
    "map_dom_aero = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "map_dom_aero_nongrid = np.zeros([num_years])\n",
    "map_dom_anaero = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "map_dom_anaero_nongrid = np.zeros([num_years])\n",
    "map_dom_ad = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "map_dom_ad_nongrid = np.zeros([num_years])\n",
    "map_dom_cw = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "map_dom_cw_nongrid = np.zeros([num_years])\n",
    "\n",
    "for iyear in np.arange(0, num_years):\n",
    "    echo_temp = echo_potw[echo_potw['Year'] == year_range[iyear]]\n",
    "    echo_temp.reset_index(inplace=True, drop=True)\n",
    "    total_flow = np.sum(echo_temp.loc[(echo_potw['cwns_match']==1) & ((echo_potw['aerobic_flag']==1) | \\\n",
    "                                      (echo_potw['anaerobic_flag']==1) |(echo_potw['anaerobic_digestor_flag']==1)),\\\n",
    "                                      'calc_flow_mgd'])#where matched, aerobic+anaerobic+ad\n",
    "    per_aero = np.sum(echo_temp.loc[(echo_potw['cwns_match']==1) & (echo_potw['aerobic_flag']==1),\\\n",
    "                                      'calc_flow_mgd'])/total_flow#where matched, aerobic+anaerobic+ad\n",
    "    #where matched, aero/total_flow\n",
    "    per_anaero = np.sum(echo_temp.loc[(echo_potw['cwns_match']==1) & (echo_potw['anaerobic_flag']==1) &\\\n",
    "                                      (echo_potw['anaerobic_digestor_flag']==0), 'calc_flow_mgd'])/total_flow#where matched, anaero/total_flow\n",
    "    per_ad = np.sum(echo_temp.loc[(echo_potw['cwns_match']==1) & (echo_potw['anaerobic_digestor_flag']==1),\\\n",
    "                                      'calc_flow_mgd'])/total_flow\n",
    "    per_cw = np.sum(echo_temp.loc[(echo_potw['cwns_match']==1) & (echo_potw['const_wetland_flag']==1),\\\n",
    "                                      'calc_flow_mgd'])/total_flow#where matched, cw/sum where not cw\n",
    "    print('Year:',year_range[iyear])\n",
    "    print('Percent aerobic flow:',per_aero)\n",
    "    print('Percent anaerobic flow:',per_anaero)\n",
    "    print('Percent AD flow:',per_ad)\n",
    "    print('Percent CW flow:',per_cw)\n",
    "    print(' ')\n",
    "    for ifacility in np.arange(0, len(echo_temp)):\n",
    "        if echo_temp['Facility Longitude'][ifacility] > Lon_left and \\\n",
    "            echo_temp['Facility Longitude'][ifacility] < Lon_right and \\\n",
    "            echo_temp['Facility Latitude'][ifacility] > Lat_low and \\\n",
    "            echo_temp['Facility Latitude'][ifacility] < Lat_up:\n",
    "\n",
    "            ilat = int((echo_temp['Facility Latitude'][ifacility] - Lat_low)/Res01)\n",
    "            ilon = int((echo_temp['Facility Longitude'][ifacility] - Lon_left)/Res01)\n",
    "            if echo_temp['cwns_match'][ifacility] == 0:\n",
    "                map_dom_aero[ilat,ilon,iyear] += echo_temp['calc_flow_mgd'][ifacility]*per_aero\n",
    "                map_dom_anaero[ilat,ilon,iyear] += echo_temp['calc_flow_mgd'][ifacility]*per_anaero\n",
    "                map_dom_ad[ilat,ilon,iyear] += echo_temp['calc_flow_mgd'][ifacility]*per_ad\n",
    "            if echo_temp['aerobic_flag'][ifacility] ==1:\n",
    "                map_dom_aero[ilat,ilon,iyear] += echo_temp['calc_flow_mgd'][ifacility]\n",
    "            if echo_temp['anaerobic_flag'][ifacility] ==1 and echo_temp['anaerobic_digestor_flag'][ifacility] ==0:\n",
    "                map_dom_anaero[ilat,ilon,iyear] += echo_temp['calc_flow_mgd'][ifacility]\n",
    "            if echo_temp['anaerobic_digestor_flag'][ifacility] ==1:\n",
    "                map_dom_ad[ilat,ilon,iyear] += echo_temp['calc_flow_mgd'][ifacility]\n",
    "            if echo_temp['const_wetland_flag'][ifacility] ==1:\n",
    "                map_dom_cw[ilat,ilon,iyear] += echo_temp['calc_flow_mgd'][ifacility]\n",
    "        else:\n",
    "            if echo_temp['cwns_match'][ifacility] == 0:\n",
    "                map_dom_aero_nongrid[iyear] += echo_temp['calc_flow_mgd'][ifacility]*per_aero\n",
    "                map_dom_anaero_nongrid[iyear] += echo_temp['calc_flow_mgd'][ifacility]*per_anaero\n",
    "                map_dom_ad_nongrid[iyear] += echo_temp['calc_flow_mgd'][ifacility]*per_ad\n",
    "            if echo_temp['aerobic_flag'][ifacility] ==1:\n",
    "                map_dom_aero_nongrid[iyear] += echo_temp['calc_flow_mgd'][ifacility]\n",
    "            if echo_temp['anaerobic_flag'][ifacility] ==1 and echo_temp['anaerobic_digestor_flag'][ifacility] ==0:\n",
    "                map_dom_anaero_nongrid[iyear] += echo_temp['calc_flow_mgd'][ifacility]\n",
    "            if echo_temp['anaerobic_digestor_flag'][ifacility] ==1:\n",
    "                map_dom_ad_nongrid[iyear] += echo_temp['calc_flow_mgd'][ifacility]\n",
    "            if echo_temp['const_wetland_flag'][ifacility] ==1:\n",
    "                map_dom_cw_nongrid[iyear] += echo_temp['calc_flow_mgd'][ifacility]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2.4. Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read population density map\n",
    "pop_den_map = data_load_fn.load_pop_den_map(pop_map_inputfile)\n",
    "\n",
    "# convert to absolute population and re-grid to 0.1x0.1 degrees (hold population constant over all years)\n",
    "map_pop_001 = pop_den_map * area_map\n",
    "map_pop_01 = data_fn.regrid001_to_01(map_pop_001, Lat_01, Lon_01)\n",
    "\n",
    "map_pop = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "\n",
    "for iyear in np.arange(0, num_years):\n",
    "    map_pop[:,:,iyear] = map_pop_01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3 Reads In Industrial Wastewater Treatment Proxy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3.1 Read In GHGRP Subpart II Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read in GHGRP Subpart II Emissions #and place onto CONUS grid\n",
    "\n",
    "#a) Read in the GHGRP facility data (Subpart II)\n",
    "facility_info = pd.read_csv(ghgrp_facility_ii_inputfile)\n",
    "facility_emis = pd.read_csv(ghgrp_emi_ii_inputfile)\n",
    "#filter emissions data for methane only (in metric tonnes CH4) and for years of interest\n",
    "facility_emis = facility_emis[facility_emis['II_SUBPART_LEVEL_INFORMATION.GHG_NAME'] == 'Methane']\n",
    "facility_emis = facility_emis[facility_emis['II_SUBPART_LEVEL_INFORMATION.REPORTING_YEAR'].isin(year_range)]\n",
    "facility_info = facility_info[facility_info['V_GHG_EMITTER_FACILITIES.YEAR'].isin(year_range)]\n",
    "facility_info.reset_index(inplace=True, drop=True)\n",
    "facility_emis.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#rename common columns and merge into one dataframe\n",
    "facility_info.rename(columns={'V_GHG_EMITTER_FACILITIES.YEAR':'Year', \\\n",
    "                             'V_GHG_EMITTER_FACILITIES.FACILITY_ID':'Facility_ID', \\\n",
    "                             'V_GHG_EMITTER_FACILITIES.LONGITUDE':'LONGITUDE',\n",
    "                             'V_GHG_EMITTER_FACILITIES.LATITUDE':'LATITUDE',\n",
    "                             'V_GHG_EMITTER_FACILITIES.PRIMARY_NAICS_CODE':'NAICS_CODE',\n",
    "                             'V_GHG_EMITTER_FACILITIES.COUNTY':'COUNTY',\n",
    "                             'V_GHG_EMITTER_FACILITIES.CITY':'CITY',\n",
    "                             'V_GHG_EMITTER_FACILITIES.STATE':'STATE'},inplace=True)\n",
    "facility_emis.rename(columns={'II_SUBPART_LEVEL_INFORMATION.REPORTING_YEAR':'Year', \\\n",
    "                              'II_SUBPART_LEVEL_INFORMATION.FACILITY_ID':'Facility_ID'},inplace=True)\n",
    "ghgrp_ind = pd.merge(facility_info, facility_emis)\n",
    "ghgrp_ind['emis_kt_tot'] = ghgrp_ind['II_SUBPART_LEVEL_INFORMATION.GHG_QUANTITY']/1e3 #convert metric tonnes to kt\n",
    "\n",
    "print('NAICES CODES in Subpart II:', np.unique(ghgrp_ind['NAICS_CODE']))\n",
    "#split into different industry sectors (to later match with ECHO plant data)\n",
    "ghgrp_ind['NAICS_CODE'] = ghgrp_ind['NAICS_CODE'].astype(str)\n",
    "\n",
    "#pulp and paper\n",
    "ghgrp_pp = ghgrp_ind[ghgrp_ind['NAICS_CODE'].str.startswith('322')].copy()\n",
    "ghgrp_pp.reset_index(inplace=True, drop=True)\n",
    "#red meat and poultry\n",
    "ghgrp_mp = ghgrp_ind[ghgrp_ind['NAICS_CODE'].str.startswith('3116')].copy()\n",
    "ghgrp_mp.reset_index(inplace=True, drop=True)\n",
    "#fruits and veggies\n",
    "ghgrp_fv = ghgrp_ind[ghgrp_ind['NAICS_CODE'].str.startswith('3114')].copy()\n",
    "ghgrp_fv.reset_index(inplace=True, drop=True)\n",
    "#ethanol production\n",
    "ghgrp_eth = ghgrp_ind[ghgrp_ind['NAICS_CODE'].str.startswith('325193')].copy()\n",
    "ghgrp_eth.reset_index(inplace=True, drop=True)\n",
    "##CORRECT REPORTING ERROR IN 2014 & negative reported value###\n",
    "ghgrp_eth.loc[(ghgrp_eth['Year']==2014) & (ghgrp_eth['V_GHG_EMITTER_FACILITIES.FACILITY_NAME']=='Hub City Energy'),'emis_kt_tot'] /= 100\n",
    "ghgrp_eth.loc[:,'emis_kt_tot'] = abs(ghgrp_eth.loc[:,'emis_kt_tot'])\n",
    "###\n",
    "#breweries\n",
    "ghgrp_brew = ghgrp_ind[ghgrp_ind['NAICS_CODE'].str.startswith('312120')].copy()\n",
    "ghgrp_brew.reset_index(inplace=True, drop=True)\n",
    "#petroleum refining\n",
    "ghgrp_ref = ghgrp_ind[ghgrp_ind['NAICS_CODE'].str.startswith('324121')].copy()\n",
    "ghgrp_ref.reset_index(inplace=True, drop=True)\n",
    "if len(ghgrp_ref) ==0: #if no data, create blank dataframe\n",
    "    df2 = {'Year':2012}\n",
    "    ghgrp_ref = ghgrp_ref.append(df2, ignore_index = True)\n",
    "    ghgrp_ref = ghgrp_ref.fillna(0)\n",
    "    \n",
    "print(' ')\n",
    "print('GHGRP Emissions (kt)')\n",
    "for iyear in np.arange(0, num_years):\n",
    "    print('Year',year_range[iyear],':')\n",
    "    print('Pulp & Paper   :',np.sum(ghgrp_pp.loc[ghgrp_pp['Year']==year_range[iyear],'emis_kt_tot']))\n",
    "    print('Meat & Poultry :',np.sum(ghgrp_mp.loc[ghgrp_mp['Year']==year_range[iyear],'emis_kt_tot']))\n",
    "    print('Fruit & Veggies:',np.sum(ghgrp_fv.loc[ghgrp_fv['Year']==year_range[iyear],'emis_kt_tot']))\n",
    "    print('Ethanol        :',np.sum(ghgrp_eth.loc[ghgrp_eth['Year']==year_range[iyear],'emis_kt_tot']))\n",
    "    print('Breweries      :',np.sum(ghgrp_brew.loc[ghgrp_brew['Year']==year_range[iyear],'emis_kt_tot']))\n",
    "    print('Refining (kt)  :',np.sum(ghgrp_ref.loc[ghgrp_ref['Year']==year_range[iyear],'emis_kt_tot']))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3.2. Read in EPA Industrial Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 read in EPA industr emissions\n",
    "names = pd.read_excel(EPA_ww_inputfile, sheet_name = \"InvDB\", usecols = \"E:AJ\",skiprows = 15, header = 0)\n",
    "colnames = names.columns.values\n",
    "EPA_emi_ind_ww_CH4 = pd.read_excel(EPA_ww_inputfile, sheet_name = \"InvDB\", usecols = \"E:AJ\", skiprows = 15, names = colnames)\n",
    "#drop rows with no data, remove the parentheses and \"\"\n",
    "EPA_emi_ind_ww_CH4 = EPA_emi_ind_ww_CH4.fillna('')\n",
    "EPA_emi_ind_ww_CH4.rename(columns={EPA_emi_ind_ww_CH4.columns[0]:'Source'}, inplace=True)\n",
    "EPA_emi_ind_ww_CH4['Source']= EPA_emi_ind_ww_CH4['Source'].str.replace(r\"\\(\",\"\")\n",
    "EPA_emi_ind_ww_CH4['Source']= EPA_emi_ind_ww_CH4['Source'].str.replace(r\"\\)\",\"\")\n",
    "EPA_emi_ind_ww_CH4.drop(EPA_emi_ind_ww_CH4.columns[[1,2]], axis =1, inplace= True)\n",
    "EPA_emi_ind_ww_CH4 = EPA_emi_ind_ww_CH4.drop(columns = [n for n in range(1990, start_year,1)])\n",
    "EPA_emi_ind_ww_CH4.iloc[:,1:] = EPA_emi_ind_ww_CH4.iloc[:,1:]*1000 #convert Tg to kt\n",
    "display(EPA_emi_ind_ww_CH4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3.3. Format ECHO data by industry segment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.3.1. Format ECHO Non-POTW Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "echo_nonpotw['NAICS Code'] = echo_nonpotw['NAICS Code'].astype(str)\n",
    "echo_nonpotw['SIC Code'] = echo_nonpotw['SIC Code'].astype(str)\n",
    "\n",
    "# extract industry specific facilities from non-POTW list\n",
    "# SIC codes determined by cross-walking with relevant NAICS codes\n",
    "#pulp and paper\n",
    "echo_pp = echo_nonpotw[(echo_nonpotw['NAICS Code'].str.startswith('3221'))\\\n",
    "                       |(echo_nonpotw['SIC Code'].str.contains('|'.join(['2611','2621','2631'])))].copy()\n",
    "echo_pp.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#meat and poultry\n",
    "echo_mp = echo_nonpotw[(echo_nonpotw['NAICS Code'].str.startswith('3116'))\\\n",
    "                       |(echo_nonpotw['SIC Code'].str.contains('|'.join(['0751','2011','2048','2013','5147','2077','2015'])))].copy()\n",
    "echo_mp.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#fruit and veggies\n",
    "echo_fv = echo_nonpotw[(echo_nonpotw['NAICS Code'].str.startswith('3114'))\\\n",
    "                       |(echo_nonpotw['SIC Code'].str.contains('|'.join(['2037','2038','2033','2035','2032','2034','2099'])))].copy()\n",
    "echo_fv.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#ethanol\n",
    "echo_eth = echo_nonpotw[(echo_nonpotw['NAICS Code'].str.startswith('325193'))\\\n",
    "                       |(echo_nonpotw['SIC Code'].str.contains('|'.join(['2869'])))].copy()\n",
    "echo_eth.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#breweries\n",
    "echo_brew = echo_nonpotw[(echo_nonpotw['NAICS Code'].str.startswith('312120'))\\\n",
    "                       |(echo_nonpotw['SIC Code'].str.contains('|'.join(['2082'])))].copy()\n",
    "echo_brew.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#petroleum refining\n",
    "echo_ref = echo_nonpotw[(echo_nonpotw['NAICS Code'].str.startswith('32411'))\\\n",
    "                       |(echo_nonpotw['SIC Code'].str.contains('|'.join(['2911'])))].copy()\n",
    "echo_ref.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.3.2. Pulp and Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1 -  try to match facilities to ghgrp based on location \n",
    "# If there is no match with a GHGRP facility, add it to the full list of facilities\n",
    "\n",
    "echo_pp['ghgrp_match'] = 0\n",
    "echo_pp['emis_kt'] = 0\n",
    "echo_pp['EF'] = 0\n",
    "ghgrp_pp.loc[:,'found']=0\n",
    "\n",
    "for ifacility in np.arange(0,len(echo_pp)):\n",
    "    for ighgrp in np.arange(0,len(ghgrp_pp)):\n",
    "        dist = np.sqrt((ghgrp_pp.loc[ighgrp,'LATITUDE']-echo_pp.loc[ifacility,'Facility Latitude'])**2\\\n",
    "                       +(ghgrp_pp.loc[ighgrp,'LONGITUDE']-echo_pp.loc[ifacility,'Facility Longitude'])**2)\n",
    "        if dist < 0.025 and ghgrp_pp.loc[ighgrp,'Year'] == echo_pp.loc[ifacility,'Year']:\n",
    "            ghgrp_pp.loc[ighgrp,'found'] = 1\n",
    "            echo_pp.loc[ifacility,'ghgrp_match'] = 1\n",
    "            echo_pp.loc[ifacility,'emis_kt'] = ghgrp_pp.loc[ighgrp,'emis_kt_tot']\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "print('Found (%):',100*np.sum(echo_pp['ghgrp_match']/len(echo_pp)))\n",
    "print('GHGRP not found:',len(ghgrp_pp[ghgrp_pp['found']==0]))\n",
    "print('GHGRP found:',len(ghgrp_pp[ghgrp_pp['found']==1]))\n",
    "print('Total Emis (kt):',np.sum(echo_pp['emis_kt']))\n",
    "\n",
    "# Step 2 - add additional GHGRP facilities (to capture all GHGRP emissions - deviates from GHGI methods)\n",
    "# for each extra facility in the ghgrp dataset, append an extra row, then fill in the emissions values for each year\n",
    "for ifacility in np.arange(0,len(ghgrp_pp)):\n",
    "    if ghgrp_pp.loc[ifacility,'found'] ==0:\n",
    "        facility_id = ghgrp_pp.loc[ifacility,'Facility_ID']\n",
    "        df2 = {'Year':ghgrp_pp.loc[ifacility,'Year'],'State':ghgrp_pp.loc[ifacility,'STATE'], 'City':ghgrp_pp.loc[ifacility,'CITY'],\\\n",
    "               'County':ghgrp_pp.loc[ifacility,'COUNTY'],'ghgrp_match': 1, 'Facility Latitude': ghgrp_pp.loc[ifacility,'LATITUDE'], \\\n",
    "               'Facility Longitude':ghgrp_pp.loc[ifacility,'LONGITUDE'], 'EF':0,\\\n",
    "               'calc_flow_mgd':0,'emis_kt':ghgrp_pp.loc[ifacility,'emis_kt_tot']}\n",
    "        echo_pp = echo_pp.append(df2, ignore_index = True)\n",
    "        ghgrp_pp.loc[ifacility,'found'] =1\n",
    "\n",
    "#Step 3 -  distribute remaining emissions difference based on the relative wastewater flow at each facility\n",
    "for iyear in np.arange(0, num_years):\n",
    "    epa_emi  = EPA_emi_ind_ww_CH4.loc[EPA_emi_ind_ww_CH4['Source'] == 'Pulp and Paper',year_range[iyear]].values[0]\n",
    "    #print(epa_emi)\n",
    "    sum_emi = np.sum(echo_pp.loc[echo_pp['Year']==year_range[iyear],'emis_kt'])\n",
    "    emi_diff = max((epa_emi-sum_emi),0)\n",
    "    print(emi_diff)\n",
    "    total_flow = np.sum(echo_pp.loc[(echo_pp['Year']==year_range[iyear]) & (echo_pp['ghgrp_match']==0),'calc_flow_mgd'])\n",
    "    \n",
    "    for ifacility in np.arange(0, len(echo_pp)):\n",
    "        if echo_pp.loc[ifacility,'Year']==year_range[iyear] and echo_pp.loc[ifacility,'ghgrp_match']==0:\n",
    "            echo_pp.loc[ifacility,'emis_kt'] = emi_diff * echo_pp.loc[ifacility,'calc_flow_mgd']/total_flow\n",
    "    \n",
    "# Step 4- Place data on CONUS grid\n",
    "map_ind_pp_emis = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "map_ind_pp_emis_nongrid = np.zeros([num_years])\n",
    "\n",
    "for iyear in np.arange(0, num_years):\n",
    "    echo_temp = echo_pp[echo_pp['Year'] == year_range[iyear]]\n",
    "    echo_temp.reset_index(inplace=True, drop=True)\n",
    "    #display(ghgrp_temp)\n",
    "    for ifacility in np.arange(0, len(echo_temp)):\n",
    "        if echo_temp['Facility Longitude'][ifacility] > Lon_left and \\\n",
    "            echo_temp['Facility Longitude'][ifacility] < Lon_right and \\\n",
    "            echo_temp['Facility Latitude'][ifacility] > Lat_low and \\\n",
    "            echo_temp['Facility Latitude'][ifacility] < Lat_up:\n",
    "\n",
    "            ilat = int((echo_temp['Facility Latitude'][ifacility] - Lat_low)/Res01)\n",
    "            ilon = int((echo_temp['Facility Longitude'][ifacility] - Lon_left)/Res01)\n",
    "            map_ind_pp_emis[ilat,ilon,iyear] += echo_temp['emis_kt'][ifacility]\n",
    "        else:\n",
    "            map_ind_pp_emis_nongrid[iyear] += echo_temp['emis_kt'][ifacility]\n",
    "    print('Year',year_range[iyear],'Emissions (kt):',np.sum(map_ind_pp_emis[:,:,iyear])+map_ind_pp_emis_nongrid[iyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.3.3.  Meat and Poultry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#) Step 1 -  try to match facilities to ghgrp based on location \n",
    "# If there is no match with a GHGRP facility, add it to the full list of facilities\n",
    "echo_mp['ghgrp_match'] = 0\n",
    "echo_mp['emis_kt'] = 0\n",
    "echo_mp['EF'] = 0\n",
    "ghgrp_mp.loc[:,'found']=0\n",
    "\n",
    "for ifacility in np.arange(0,len(echo_mp)):\n",
    "    for ighgrp in np.arange(0,len(ghgrp_mp)):\n",
    "        dist = np.sqrt((ghgrp_mp.loc[ighgrp,'LATITUDE']-echo_mp.loc[ifacility,'Facility Latitude'])**2\\\n",
    "                       +(ghgrp_mp.loc[ighgrp,'LONGITUDE']-echo_mp.loc[ifacility,'Facility Longitude'])**2)\n",
    "        if dist < 0.025 and ghgrp_mp.loc[ighgrp,'Year'] == echo_mp.loc[ifacility,'Year']:\n",
    "            ghgrp_mp.loc[ighgrp,'found'] = 1\n",
    "            echo_mp.loc[ifacility,'ghgrp_match'] = 1\n",
    "            echo_mp.loc[ifacility,'emis_kt'] = ghgrp_mp.loc[ighgrp,'emis_kt_tot']\n",
    "        else:\n",
    "            continue\n",
    "print('Found (%):',100*np.sum(echo_mp['ghgrp_match']/len(echo_mp)))\n",
    "print('GHGRP not found:',len(ghgrp_mp[ghgrp_mp['found']==0]))\n",
    "print('GHGRP found:',len(ghgrp_mp[ghgrp_mp['found']==1]))\n",
    "print('Total Emis (kt):',np.sum(echo_mp['emis_kt']))\n",
    "\n",
    "\n",
    "# Step 2 - add additional GHGRP facilities (to capture all GHGRP emissions - deviates from GHGI methods)\n",
    "# for each extra facility in the ghgrp dataset, append an extra row, then fill in the emissions values for each year\n",
    "for ifacility in np.arange(0,len(ghgrp_mp)):\n",
    "    if ghgrp_mp.loc[ifacility,'found'] ==0:\n",
    "        facility_id = ghgrp_mp.loc[ifacility,'Facility_ID']\n",
    "        df2 = {'Year':ghgrp_mp.loc[ifacility,'Year'],'State':ghgrp_mp.loc[ifacility,'STATE'], 'City':ghgrp_mp.loc[ifacility,'CITY'],\\\n",
    "               'County':ghgrp_mp.loc[ifacility,'COUNTY'],'ghgrp_match': 1, 'Facility Latitude': ghgrp_mp.loc[ifacility,'LATITUDE'], \\\n",
    "               'Facility Longitude':ghgrp_mp.loc[ifacility,'LONGITUDE'], 'EF':0,\\\n",
    "               'calc_flow_mgd':0,'emis_kt':ghgrp_mp.loc[ifacility,'emis_kt_tot']}\n",
    "        echo_mp = echo_mp.append(df2, ignore_index = True)\n",
    "        ghgrp_mp.loc[ifacility,'found'] =1\n",
    "\n",
    "        \n",
    "#Step 3 -  distribute remaining emissions difference based on the relative wastewater flow at each facility\n",
    "for iyear in np.arange(0, num_years):\n",
    "    epa_emi  = EPA_emi_ind_ww_CH4.loc[EPA_emi_ind_ww_CH4['Source'] == 'Red Meat and Poultry',year_range[iyear]].values[0]\n",
    "    sum_emi = np.sum(echo_mp.loc[echo_mp['Year']==year_range[iyear],'emis_kt'])\n",
    "    emi_diff = max((epa_emi-sum_emi),0)\n",
    "    print(emi_diff)\n",
    "    total_flow = np.sum(echo_mp.loc[(echo_mp['Year']==year_range[iyear]) & (echo_mp['ghgrp_match']==0),'calc_flow_mgd'])\n",
    "    for ifacility in np.arange(0, len(echo_mp)):\n",
    "        if echo_mp.loc[ifacility,'Year']==year_range[iyear] and echo_mp.loc[ifacility,'ghgrp_match']==0:\n",
    "            echo_mp.loc[ifacility,'emis_kt'] = emi_diff * echo_mp.loc[ifacility,'calc_flow_mgd']/total_flow\n",
    "    \n",
    "\n",
    "# Step 4- Place data on CONUS grid\n",
    "map_ind_mp_emis = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "map_ind_mp_emis_nongrid = np.zeros([num_years])\n",
    "\n",
    "for iyear in np.arange(0, num_years):\n",
    "    echo_temp = echo_mp[echo_mp['Year'] == year_range[iyear]]\n",
    "    echo_temp.reset_index(inplace=True, drop=True)\n",
    "    for ifacility in np.arange(0, len(echo_temp)):\n",
    "        if echo_temp['Facility Longitude'][ifacility] > Lon_left and \\\n",
    "            echo_temp['Facility Longitude'][ifacility] < Lon_right and \\\n",
    "            echo_temp['Facility Latitude'][ifacility] > Lat_low and \\\n",
    "            echo_temp['Facility Latitude'][ifacility] < Lat_up:\n",
    "\n",
    "            ilat = int((echo_temp['Facility Latitude'][ifacility] - Lat_low)/Res01)\n",
    "            ilon = int((echo_temp['Facility Longitude'][ifacility] - Lon_left)/Res01)\n",
    "            map_ind_mp_emis[ilat,ilon,iyear] += echo_temp['emis_kt'][ifacility]\n",
    "        else:\n",
    "            map_ind_mp_emis_nongrid[iyear] += echo_temp['emis_kt'][ifacility]\n",
    "    print('Year',year_range[iyear],'Emissions (kt):',np.sum(map_ind_mp_emis[:,:,iyear])+map_ind_mp_emis_nongrid[iyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.3.4.  Fruit and Vegtables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#) Step 1 -  try to match facilities to ghgrp based on location \n",
    "# If there is no match with a GHGRP facility, add it to the full list of facilities\n",
    "echo_fv['ghgrp_match'] = 0\n",
    "echo_fv['emis_kt'] = 0\n",
    "echo_fv['EF'] = 0\n",
    "ghgrp_fv.loc[:,'found']=0\n",
    "\n",
    "for ifacility in np.arange(0,len(echo_fv)):\n",
    "    for ighgrp in np.arange(0,len(ghgrp_fv)):\n",
    "        dist = np.sqrt((ghgrp_fv.loc[ighgrp,'LATITUDE']-echo_fv.loc[ifacility,'Facility Latitude'])**2\\\n",
    "                       +(ghgrp_fv.loc[ighgrp,'LONGITUDE']-echo_fv.loc[ifacility,'Facility Longitude'])**2)\n",
    "        if dist < 0.025 and ghgrp_fv.loc[ighgrp,'Year'] == echo_fv.loc[ifacility,'Year']:\n",
    "            ghgrp_fv.loc[ighgrp,'found'] = 1\n",
    "            echo_fv.loc[ifacility,'ghgrp_match'] = 1\n",
    "            echo_fv.loc[ifacility,'emis_kt'] = ghgrp_fv.loc[ighgrp,'emis_kt_tot']\n",
    "        else:\n",
    "            continue\n",
    "print('Found (%):',100*np.sum(echo_fv['ghgrp_match']/len(echo_fv)))\n",
    "print('GHGRP not found:',len(ghgrp_fv[ghgrp_fv['found']==0]))\n",
    "print('GHGRP found:',len(ghgrp_fv[ghgrp_fv['found']==1]))\n",
    "print('Total Emis (kt):',np.sum(echo_fv['emis_kt']))\n",
    "\n",
    "\n",
    "# Step 2 - add additional GHGRP facilities (to capture all GHGRP emissions - deviates from GHGI methods)\n",
    "# for each extra facility in the ghgrp dataset, append an extra row, then fill in the emissions values for each year\n",
    "for ifacility in np.arange(0,len(ghgrp_fv)):\n",
    "    if ghgrp_fv.loc[ifacility,'found'] ==0:\n",
    "        facility_id = ghgrp_fv.loc[ifacility,'Facility_ID']\n",
    "        df2 = {'Year':ghgrp_fv.loc[ifacility,'Year'],'State':ghgrp_fv.loc[ifacility,'STATE'], 'City':ghgrp_fv.loc[ifacility,'CITY'],\\\n",
    "               'County':ghgrp_fv.loc[ifacility,'COUNTY'],'ghgrp_match': 1, 'Facility Latitude': ghgrp_fv.loc[ifacility,'LATITUDE'], \\\n",
    "               'Facility Longitude':ghgrp_fv.loc[ifacility,'LONGITUDE'], 'EF':0,\\\n",
    "               'calc_flow_mgd':0,'emis_kt':ghgrp_fv.loc[ifacility,'emis_kt_tot']}\n",
    "        echo_fv = echo_fv.append(df2, ignore_index = True)\n",
    "        ghgrp_fv.loc[ifacility,'found'] =1\n",
    "\n",
    "        \n",
    "#Step 3 -  distribute remaining emissions difference based on the relative wastewater flow at each facility\n",
    "for iyear in np.arange(0, num_years):\n",
    "    epa_emi  = EPA_emi_ind_ww_CH4.loc[EPA_emi_ind_ww_CH4['Source'] == 'Fruits and Vegtables',year_range[iyear]].values[0]\n",
    "    sum_emi = np.sum(echo_fv.loc[echo_fv['Year']==year_range[iyear],'emis_kt'])\n",
    "    emi_diff = max((epa_emi-sum_emi),0)\n",
    "    print(emi_diff)\n",
    "    total_flow = np.sum(echo_fv.loc[(echo_fv['Year']==year_range[iyear]) & (echo_fv['ghgrp_match']==0),'calc_flow_mgd'])\n",
    "    for ifacility in np.arange(0, len(echo_fv)):\n",
    "        if echo_fv.loc[ifacility,'Year']==year_range[iyear] and echo_fv.loc[ifacility,'ghgrp_match']==0:\n",
    "            echo_fv.loc[ifacility,'emis_kt'] = emi_diff * echo_fv.loc[ifacility,'calc_flow_mgd']/total_flow\n",
    "    \n",
    "\n",
    "# Step 4- Place data on CONUS grid\n",
    "map_ind_fv_emis = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "map_ind_fv_emis_nongrid = np.zeros([num_years])\n",
    "\n",
    "for iyear in np.arange(0, num_years):\n",
    "    echo_tefv = echo_fv[echo_fv['Year'] == year_range[iyear]]\n",
    "    echo_tefv.reset_index(inplace=True, drop=True)\n",
    "    for ifacility in np.arange(0, len(echo_tefv)):\n",
    "        if echo_tefv['Facility Longitude'][ifacility] > Lon_left and \\\n",
    "            echo_tefv['Facility Longitude'][ifacility] < Lon_right and \\\n",
    "            echo_tefv['Facility Latitude'][ifacility] > Lat_low and \\\n",
    "            echo_tefv['Facility Latitude'][ifacility] < Lat_up:\n",
    "\n",
    "            ilat = int((echo_tefv['Facility Latitude'][ifacility] - Lat_low)/Res01)\n",
    "            ilon = int((echo_tefv['Facility Longitude'][ifacility] - Lon_left)/Res01)\n",
    "            map_ind_fv_emis[ilat,ilon,iyear] += echo_tefv['emis_kt'][ifacility]\n",
    "        else:\n",
    "            map_ind_fv_emis_nongrid[iyear] += echo_tefv['emis_kt'][ifacility]\n",
    "    print('Year',year_range[iyear],'Emissions (kt):',np.sum(map_ind_fv_emis[:,:,iyear])+map_ind_fv_emis_nongrid[iyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.3.5. Ethanol Prodution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#) Step 1 -  try to match facilities to ghgrp based on location \n",
    "# If there is no match with a GHGRP facility, add it to the full list of facilities\n",
    "echo_eth['ghgrp_match'] = 0\n",
    "echo_eth['emis_kt'] = 0\n",
    "echo_eth['EF'] = 0\n",
    "ghgrp_eth.loc[:,'found']=0\n",
    "\n",
    "for ifacility in np.arange(0,len(echo_eth)):\n",
    "    for ighgrp in np.arange(0,len(ghgrp_eth)):\n",
    "        dist = np.sqrt((ghgrp_eth.loc[ighgrp,'LATITUDE']-echo_eth.loc[ifacility,'Facility Latitude'])**2\\\n",
    "                       +(ghgrp_eth.loc[ighgrp,'LONGITUDE']-echo_eth.loc[ifacility,'Facility Longitude'])**2)\n",
    "        if dist < 0.025 and ghgrp_eth.loc[ighgrp,'Year'] == echo_eth.loc[ifacility,'Year']:\n",
    "            ghgrp_eth.loc[ighgrp,'found'] = 1\n",
    "            echo_eth.loc[ifacility,'ghgrp_match'] = 1\n",
    "            echo_eth.loc[ifacility,'emis_kt'] = abs(ghgrp_eth.loc[ighgrp,'emis_kt_tot']) #some ghgrp data were negative\n",
    "        else:\n",
    "            continue\n",
    "print('Found (%):',100*np.sum(echo_eth['ghgrp_match']/len(echo_eth)))\n",
    "print('GHGRP not found:',len(ghgrp_eth[ghgrp_eth['found']==0]))\n",
    "print('GHGRP found:',len(ghgrp_eth[ghgrp_eth['found']==1]))\n",
    "print('Total Emis (kt):',np.sum(echo_eth['emis_kt']))\n",
    "\n",
    "\n",
    "\n",
    "# Step 2 - add additional GHGRP facilities (to capture all GHGRP emissions - deviates from GHGI methods)\n",
    "# for each extra facility in the ghgrp dataset, append an extra row, then fill in the emissions values for each year\n",
    "for ifacility in np.arange(0,len(ghgrp_eth)):\n",
    "    if ghgrp_eth.loc[ifacility,'found'] ==0:\n",
    "        facility_id = ghgrp_eth.loc[ifacility,'Facility_ID']\n",
    "        df2 = {'Year':ghgrp_eth.loc[ifacility,'Year'],'State':ghgrp_eth.loc[ifacility,'STATE'], 'City':ghgrp_eth.loc[ifacility,'CITY'],\\\n",
    "               'County':ghgrp_eth.loc[ifacility,'COUNTY'],'ghgrp_match': 1, 'Facility Latitude': ghgrp_eth.loc[ifacility,'LATITUDE'], \\\n",
    "               'Facility Longitude':ghgrp_eth.loc[ifacility,'LONGITUDE'], 'EF':0,\\\n",
    "               'calc_flow_mgd':0,'emis_kt':ghgrp_eth.loc[ifacility,'emis_kt_tot']}\n",
    "        echo_eth = echo_eth.append(df2, ignore_index = True)\n",
    "        ghgrp_eth.loc[ifacility,'found'] =1\n",
    "\n",
    "        \n",
    "#Step 3 -  distribute remaining emissions difference based on the relative wastewater flow at each facility\n",
    "for iyear in np.arange(0, num_years):\n",
    "    epa_emi  = EPA_emi_ind_ww_CH4.loc[EPA_emi_ind_ww_CH4['Source'] == 'Ethanol Production',year_range[iyear]].values[0]\n",
    "    sum_emi = np.sum(echo_eth.loc[echo_eth['Year']==year_range[iyear],'emis_kt'])\n",
    "    emi_diff = max((epa_emi-sum_emi),0)\n",
    "    print(emi_diff)\n",
    "    total_flow = np.sum(echo_eth.loc[(echo_eth['Year']==year_range[iyear]) & (echo_eth['ghgrp_match']==0),'calc_flow_mgd'])\n",
    "    for ifacility in np.arange(0, len(echo_eth)):\n",
    "        if echo_eth.loc[ifacility,'Year']==year_range[iyear] and echo_eth.loc[ifacility,'ghgrp_match']==0:\n",
    "            echo_eth.loc[ifacility,'emis_kt'] = emi_diff * echo_eth.loc[ifacility,'calc_flow_mgd']/total_flow\n",
    "\n",
    "# Step 4- Place data on CONUS grid\n",
    "map_ind_eth_emis = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "map_ind_eth_emis_nongrid = np.zeros([num_years])\n",
    "\n",
    "for iyear in np.arange(0, num_years):\n",
    "    echo_temp = echo_eth[echo_eth['Year'] == year_range[iyear]]\n",
    "    echo_temp.reset_index(inplace=True, drop=True)\n",
    "    for ifacility in np.arange(0, len(echo_temp)):\n",
    "        if echo_temp['Facility Longitude'][ifacility] > Lon_left and \\\n",
    "            echo_temp['Facility Longitude'][ifacility] < Lon_right and \\\n",
    "            echo_temp['Facility Latitude'][ifacility] > Lat_low and \\\n",
    "            echo_temp['Facility Latitude'][ifacility] < Lat_up:\n",
    "\n",
    "            ilat = int((echo_temp['Facility Latitude'][ifacility] - Lat_low)/Res01)\n",
    "            ilon = int((echo_temp['Facility Longitude'][ifacility] - Lon_left)/Res01)\n",
    "            map_ind_eth_emis[ilat,ilon,iyear] += echo_temp['emis_kt'][ifacility]\n",
    "        else:\n",
    "            map_ind_eth_emis_nongrid[iyear] += echo_temp['emis_kt'][ifacility]\n",
    "    print('Year',year_range[iyear],'Emissions (kt):',np.sum(map_ind_eth_emis[:,:,iyear])+map_ind_eth_emis_nongrid[iyear])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.3.6. Breweries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#) Step 1 -  try to match facilities to ghgrp based on location \n",
    "# If there is no match with a GHGRP facility, add it to the full list of facilities\n",
    "echo_brew['ghgrp_match'] = 0\n",
    "echo_brew['emis_kt'] = 0\n",
    "echo_brew['EF'] = 0\n",
    "ghgrp_brew.loc[:,'found']=0\n",
    "\n",
    "for ifacility in np.arange(0,len(echo_brew)):\n",
    "    for ighgrp in np.arange(0,len(ghgrp_brew)):\n",
    "        dist = np.sqrt((ghgrp_brew.loc[ighgrp,'LATITUDE']-echo_brew.loc[ifacility,'Facility Latitude'])**2\\\n",
    "                       +(ghgrp_brew.loc[ighgrp,'LONGITUDE']-echo_brew.loc[ifacility,'Facility Longitude'])**2)\n",
    "        if dist < 0.025 and ghgrp_brew.loc[ighgrp,'Year'] == echo_brew.loc[ifacility,'Year']:\n",
    "            ghgrp_brew.loc[ighgrp,'found'] = 1\n",
    "            echo_brew.loc[ifacility,'ghgrp_match'] = 1\n",
    "            echo_brew.loc[ifacility,'emis_kt'] = ghgrp_brew.loc[ighgrp,'emis_kt_tot']\n",
    "        else:\n",
    "            continue\n",
    "print('Found (%):',100*np.sum(echo_brew['ghgrp_match']/len(echo_brew)))\n",
    "print('GHGRP not found:',len(ghgrp_brew[ghgrp_brew['found']==0]))\n",
    "print('GHGRP found:',len(ghgrp_brew[ghgrp_brew['found']==1]))\n",
    "print('Total Emis (kt):',np.sum(echo_brew['emis_kt']))\n",
    "\n",
    "\n",
    "# Step 2 - add additional GHGRP facilities (to capture all GHGRP emissions - deviates from GHGI methods)\n",
    "# for each extra facility in the ghgrp dataset, append an extra row, then fill in the emissions values for each year\n",
    "for ifacility in np.arange(0,len(ghgrp_brew)):\n",
    "    if ghgrp_brew.loc[ifacility,'found'] ==0:\n",
    "        facility_id = ghgrp_brew.loc[ifacility,'Facility_ID']\n",
    "        df2 = {'Year':ghgrp_brew.loc[ifacility,'Year'],'State':ghgrp_brew.loc[ifacility,'STATE'], 'City':ghgrp_brew.loc[ifacility,'CITY'],\\\n",
    "               'County':ghgrp_brew.loc[ifacility,'COUNTY'],'ghgrp_match': 1, 'Facility Latitude': ghgrp_brew.loc[ifacility,'LATITUDE'], \\\n",
    "               'Facility Longitude':ghgrp_brew.loc[ifacility,'LONGITUDE'], 'EF':0,\\\n",
    "               'calc_flow_mgd':0,'emis_kt':ghgrp_brew.loc[ifacility,'emis_kt_tot']}\n",
    "        echo_brew = echo_brew.append(df2, ignore_index = True)\n",
    "        ghgrp_brew.loc[ifacility,'found'] =1\n",
    "\n",
    "        \n",
    "#Step 3 -  distribute remaining emissions difference based on the relative wastewater flow at each facility\n",
    "for iyear in np.arange(0, num_years):\n",
    "    epa_emi  = EPA_emi_ind_ww_CH4.loc[EPA_emi_ind_ww_CH4['Source'] == 'Breweries',year_range[iyear]].values[0]\n",
    "    sum_emi = np.sum(echo_brew.loc[echo_brew['Year']==year_range[iyear],'emis_kt'])\n",
    "    emi_diff = max((epa_emi-sum_emi),0)\n",
    "    print(emi_diff)\n",
    "    total_flow = np.sum(echo_brew.loc[(echo_brew['Year']==year_range[iyear]) & (echo_brew['ghgrp_match']==0),'calc_flow_mgd'])\n",
    "    for ifacility in np.arange(0, len(echo_brew)):\n",
    "        if echo_brew.loc[ifacility,'Year']==year_range[iyear] and echo_brew.loc[ifacility,'ghgrp_match']==0:\n",
    "            echo_brew.loc[ifacility,'emis_kt'] = emi_diff * echo_brew.loc[ifacility,'calc_flow_mgd']/total_flow\n",
    "    \n",
    "\n",
    "# Step 4- Place data on CONUS grid\n",
    "map_ind_brew_emis = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "map_ind_brew_emis_nongrid = np.zeros([num_years])\n",
    "\n",
    "for iyear in np.arange(0, num_years):\n",
    "    echo_temp = echo_brew[echo_brew['Year'] == year_range[iyear]]\n",
    "    echo_temp.reset_index(inplace=True, drop=True)\n",
    "    for ifacility in np.arange(0, len(echo_temp)):\n",
    "        if echo_temp['Facility Longitude'][ifacility] > Lon_left and \\\n",
    "            echo_temp['Facility Longitude'][ifacility] < Lon_right and \\\n",
    "            echo_temp['Facility Latitude'][ifacility] > Lat_low and \\\n",
    "            echo_temp['Facility Latitude'][ifacility] < Lat_up:\n",
    "\n",
    "            ilat = int((echo_temp['Facility Latitude'][ifacility] - Lat_low)/Res01)\n",
    "            ilon = int((echo_temp['Facility Longitude'][ifacility] - Lon_left)/Res01)\n",
    "            map_ind_brew_emis[ilat,ilon,iyear] += echo_temp['emis_kt'][ifacility]\n",
    "        else:\n",
    "            map_ind_brew_emis_nongrid[iyear] += echo_temp['emis_kt'][ifacility]\n",
    "    print('Year',year_range[iyear],'Emissions (kt):',np.sum(map_ind_brew_emis[:,:,iyear])+map_ind_brew_emis_nongrid[iyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.3.7. Petroleum Refining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Petroleeum Refining - GHGRP emissions are from Subpart Y. There are no \n",
    "#) Step 1 -  try to match facilities to ghgrp based on location \n",
    "# If there is no match with a GHGRP facility, add it to the full list of facilities\n",
    "echo_ref['ghgrp_match'] = 0\n",
    "echo_ref['emis_kt'] = 0\n",
    "echo_ref['EF'] = 0\n",
    "ghgrp_ref.loc[:,'found']=0\n",
    "\n",
    "for ifacility in np.arange(0,len(echo_ref)):\n",
    "    for ighgrp in np.arange(0,len(ghgrp_ref)):\n",
    "        dist = np.sqrt((ghgrp_ref.loc[ighgrp,'LATITUDE']-echo_ref.loc[ifacility,'Facility Latitude'])**2\\\n",
    "                       +(ghgrp_ref.loc[ighgrp,'LONGITUDE']-echo_ref.loc[ifacility,'Facility Longitude'])**2)\n",
    "        if dist < 0.025 and ghgrp_ref.loc[ighgrp,'Year'] == echo_ref.loc[ifacility,'Year']:\n",
    "            ghgrp_ref.loc[ighgrp,'found'] = 1\n",
    "            echo_ref.loc[ifacility,'ghgrp_match'] = 1\n",
    "            echo_ref.loc[ifacility,'emis_kt'] = ghgrp_ref.loc[ighgrp,'emis_kt_tot']\n",
    "        else:\n",
    "            continue\n",
    "print('Found (%):',100*np.sum(echo_ref['ghgrp_match']/len(echo_ref)))\n",
    "print('GHGRP not found:',len(ghgrp_ref[ghgrp_ref['found']==0]))\n",
    "print('GHGRP found:',len(ghgrp_ref[ghgrp_ref['found']==1]))\n",
    "print('Total Emis (kt):',np.sum(echo_ref['emis_kt']))\n",
    "\n",
    "\n",
    "# Step 2 - add additional GHGRP facilities (to capture all GHGRP emissions - deviates from GHGI methods)\n",
    "# for each extra facility in the ghgrp dataset, append an extra row, then fill in the emissions values for each year\n",
    "for ifacility in np.arange(0,len(ghgrp_ref)):\n",
    "    if ghgrp_ref.loc[ifacility,'found'] ==0:\n",
    "        facility_id = ghgrp_ref.loc[ifacility,'Facility_ID']\n",
    "        df2 = {'Year':ghgrp_ref.loc[ifacility,'Year'],'State':ghgrp_ref.loc[ifacility,'STATE'], 'City':ghgrp_ref.loc[ifacility,'CITY'],\\\n",
    "               'County':ghgrp_ref.loc[ifacility,'COUNTY'],'ghgrp_match': 1, 'Facility Latitude': ghgrp_ref.loc[ifacility,'LATITUDE'], \\\n",
    "               'Facility Longitude':ghgrp_ref.loc[ifacility,'LONGITUDE'], 'EF':0,\\\n",
    "               'calc_flow_mgd':0,'emis_kt':ghgrp_ref.loc[ifacility,'emis_kt_tot']}\n",
    "        echo_ref = echo_ref.append(df2, ignore_index = True)\n",
    "        ghgrp_ref.loc[ifacility,'found'] =1\n",
    "\n",
    "        \n",
    "#Step 3 -  distribute remaining emissions difference based on the relative wastewater flow at each facility\n",
    "for iyear in np.arange(0, num_years):\n",
    "    epa_emi  = EPA_emi_ind_ww_CH4.loc[EPA_emi_ind_ww_CH4['Source'] == 'Petroleum Refining',year_range[iyear]].values[0]\n",
    "    sum_emi = np.sum(echo_ref.loc[echo_ref['Year']==year_range[iyear],'emis_kt'])\n",
    "    emi_diff = max((epa_emi-sum_emi),0)\n",
    "    print(emi_diff)\n",
    "    total_flow = np.sum(echo_ref.loc[(echo_ref['Year']==year_range[iyear]) & (echo_ref['ghgrp_match']==0),'calc_flow_mgd'])\n",
    "    for ifacility in np.arange(0, len(echo_ref)):\n",
    "        if echo_ref.loc[ifacility,'Year']==year_range[iyear] and echo_ref.loc[ifacility,'ghgrp_match']==0:\n",
    "            echo_ref.loc[ifacility,'emis_kt'] = emi_diff * echo_ref.loc[ifacility,'calc_flow_mgd']/total_flow\n",
    "    \n",
    "\n",
    "# Step 4- Place data on CONUS grid\n",
    "map_ind_ref_emis = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "map_ind_ref_emis_nongrid = np.zeros([num_years])\n",
    "\n",
    "for iyear in np.arange(0, num_years):\n",
    "    echo_temp = echo_ref[echo_ref['Year'] == year_range[iyear]]\n",
    "    echo_temp.reset_index(inplace=True, drop=True)\n",
    "    for ifacility in np.arange(0, len(echo_temp)):\n",
    "        if echo_temp['Facility Longitude'][ifacility] > Lon_left and \\\n",
    "            echo_temp['Facility Longitude'][ifacility] < Lon_right and \\\n",
    "            echo_temp['Facility Latitude'][ifacility] > Lat_low and \\\n",
    "            echo_temp['Facility Latitude'][ifacility] < Lat_up:\n",
    "\n",
    "            ilat = int((echo_temp['Facility Latitude'][ifacility] - Lat_low)/Res01)\n",
    "            ilon = int((echo_temp['Facility Longitude'][ifacility] - Lon_left)/Res01)\n",
    "            map_ind_ref_emis[ilat,ilon,iyear] += echo_temp['emis_kt'][ifacility]\n",
    "        else:\n",
    "            map_ind_ref_emis_nongrid[iyear] += echo_temp['emis_kt'][ifacility]\n",
    "    print('Year',year_range[iyear],'Emissions (kt):',np.sum(map_ind_ref_emis[:,:,iyear])+map_ind_ref_emis_nongrid[iyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "## Step 3. Read in and Format US EPA GHGI Emissions\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the emissions data from the GHGI workbook (in kt)\n",
    "\n",
    "names = pd.read_excel(EPA_ww_inputfile, sheet_name = \"Dom Calcs\", usecols = \"B:AE\",skiprows = 14, header = 0)\n",
    "colnames = names.columns.values\n",
    "EPA_emi_dom_ww_CH4 = pd.read_excel(EPA_ww_inputfile, sheet_name = \"Dom Calcs\", usecols = \"B:AE\", skiprows = 14, names = colnames)\n",
    "#drop rows with no data, remove the parentheses and \"\"\n",
    "EPA_emi_dom_ww_CH4 = EPA_emi_dom_ww_CH4.fillna('')\n",
    "EPA_emi_dom_ww_CH4.rename(columns={EPA_emi_dom_ww_CH4.columns[0]:'Source'}, inplace=True)\n",
    "EPA_emi_dom_ww_CH4['Source']= EPA_emi_dom_ww_CH4['Source'].str.replace(r\"\\(\",\"\")\n",
    "EPA_emi_dom_ww_CH4['Source']= EPA_emi_dom_ww_CH4['Source'].str.replace(r\"\\)\",\"\")\n",
    "EPA_emi_dom_ww_CH4 = EPA_emi_dom_ww_CH4.drop(columns = [n for n in range(1990, start_year,1)])\n",
    "\n",
    "names = pd.read_excel(EPA_ww_inputfile, sheet_name = \"InvDB\", usecols = \"E:AJ\",skiprows = 15, header = 0)\n",
    "colnames = names.columns.values\n",
    "EPA_emi_ind_ww_CH4 = pd.read_excel(EPA_ww_inputfile, sheet_name = \"InvDB\", usecols = \"E:AJ\", skiprows = 15, names = colnames)\n",
    "#drop rows with no data, remove the parentheses and \"\"\n",
    "EPA_emi_ind_ww_CH4 = EPA_emi_ind_ww_CH4.fillna('')\n",
    "EPA_emi_ind_ww_CH4.rename(columns={EPA_emi_ind_ww_CH4.columns[0]:'Source'}, inplace=True)\n",
    "EPA_emi_ind_ww_CH4['Source']= EPA_emi_ind_ww_CH4['Source'].str.replace(r\"\\(\",\"\")\n",
    "EPA_emi_ind_ww_CH4['Source']= EPA_emi_ind_ww_CH4['Source'].str.replace(r\"\\)\",\"\")\n",
    "EPA_emi_ind_ww_CH4.drop(EPA_emi_ind_ww_CH4.columns[[1,2]], axis =1, inplace= True)\n",
    "EPA_emi_ind_ww_CH4 = EPA_emi_ind_ww_CH4.drop(columns = [n for n in range(1990, start_year,1)])\n",
    "EPA_emi_ind_ww_CH4.iloc[:,1:] = EPA_emi_ind_ww_CH4.iloc[:,1:]*1000 #convert Tg to kt\n",
    "EPA_emi_ww_CH4 = EPA_emi_dom_ww_CH4.append(EPA_emi_ind_ww_CH4)\n",
    "EPA_emi_ww_CH4.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#calculate national total values\n",
    "temp = EPA_emi_ind_ww_CH4.sum(axis=0)\n",
    "EPA_emi_ind_ww_CH4 = EPA_emi_ind_ww_CH4.append(temp, ignore_index=True)\n",
    "EPA_emi_ind_ww_CH4.iloc[-1,0] = 'Total'\n",
    "EPA_emi_ww_total = EPA_emi_ind_ww_CH4[EPA_emi_ind_ww_CH4['Source'] == 'Total']\n",
    "display(EPA_emi_ww_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Split Emissions into Gridding Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split GHG emissions into gridding groups, based on Coal Proxy Mapping file\n",
    "\n",
    "DEBUG =1\n",
    "start_year_idx = EPA_emi_ww_CH4.columns.get_loc((start_year))\n",
    "end_year_idx = EPA_emi_ww_CH4.columns.get_loc((end_year))+1\n",
    "ghgi_wwt_groups = ghgi_wwt_map['GHGI_Emi_Group'].unique()\n",
    "sum_emi = np.zeros([num_years])\n",
    "\n",
    "for igroup in np.arange(0,len(ghgi_wwt_groups)): #loop through all groups, finding the GHGI sources in that group and summing emissions for that region, year        vars()[ghgi_prod_groups[igroup]] = np.zeros([num_regions-1,num_years])\n",
    "    ##DEBUG## print(ghgi_stat_groups[igroup])\n",
    "    vars()[ghgi_wwt_groups[igroup]] = np.zeros([num_years])\n",
    "    source_temp = ghgi_wwt_map.loc[ghgi_wwt_map['GHGI_Emi_Group'] == ghgi_wwt_groups[igroup], 'GHGI_Source']\n",
    "    pattern_temp  = '|'.join(source_temp) \n",
    "    emi_temp =EPA_emi_ww_CH4[EPA_emi_ww_CH4['Source'].str.contains(pattern_temp)]\n",
    "    vars()[ghgi_wwt_groups[igroup]][:] = emi_temp.iloc[:,start_year_idx:].sum()\n",
    "        \n",
    "        \n",
    "#Check against total summary emissions \n",
    "print('QA/QC #1: Check Processing Emission Sum against GHGI Summary Emissions')\n",
    "for iyear in np.arange(0,num_years): \n",
    "    for igroup in np.arange(0,len(ghgi_wwt_groups)):\n",
    "        sum_emi[iyear] += vars()[ghgi_wwt_groups[igroup]][iyear]\n",
    "        \n",
    "    summary_emi = EPA_emi_ww_total.iloc[0,iyear+1]  \n",
    "    #Check 1 - make sure that the sums from all the regions equal the totals reported\n",
    "    diff1 = abs(sum_emi[iyear] - summary_emi)/((sum_emi[iyear] + summary_emi)/2)\n",
    "    if DEBUG==1:\n",
    "        print(summary_emi)\n",
    "        print(sum_emi[iyear])\n",
    "    if diff1 < 0.0001:\n",
    "        print('Year ', year_range[iyear],': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear],': FAIL (check Production & summary tabs): ', diff1,'%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Step 4. Grid Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1. Allocate emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.1.1 Assign the Appropriate Proxy Variable Names (state & grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names on the *left* need to match the 'ProxyMapping' 'State_Proxy_Group' names \n",
    "# (these are initialized in Step 2). \n",
    "# The names on the *right* are the variable names used to caluclate the proxies in this code.\n",
    "# Names on the right need to match those from the code in Step 2\n",
    "\n",
    "#national --> grid (0.01) proxies (lat x lon x year)\n",
    "Map_pop = map_pop\n",
    "Map_Dom_Aero = map_dom_cw\n",
    "Map_Dom_Anaero = map_dom_anaero\n",
    "Map_Dom_AD = map_dom_ad\n",
    "Map_Dom_WWF = map_dom_wwf\n",
    "Map_PP = map_ind_pp_emis\n",
    "Map_MP =map_ind_mp_emis\n",
    "Map_FV = map_ind_fv_emis\n",
    "Map_Brew = map_ind_brew_emis\n",
    "Map_Ethanol = map_ind_eth_emis\n",
    "Map_PetrRef = map_ind_ref_emis\n",
    "\n",
    "Map_pop_nongrid = np.zeros([num_years])\n",
    "Map_Dom_Aero_nongrid = map_dom_cw_nongrid\n",
    "Map_Dom_Anaero_nongrid = map_dom_anaero_nongrid\n",
    "Map_Dom_AD_nongrid = map_dom_ad_nongrid\n",
    "Map_Dom_WWF_nongrid = map_dom_wwf_nongrid\n",
    "Map_PP_nongrid= map_ind_pp_emis_nongrid\n",
    "Map_MP_nongrid= map_ind_mp_emis_nongrid\n",
    "Map_FV_nongrid = map_ind_fv_emis_nongrid\n",
    "Map_Brew_nongrid = map_ind_brew_emis_nongrid\n",
    "Map_Ethanol_nongrid= map_ind_eth_emis_nongrid\n",
    "Map_PetrRef_nongrid= map_ind_ref_emis_nongrid \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1.2 Allocate emissions to the CONUS region (0.1x0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate national emissions (kt) onto a 0.1x0.1 grid using gridcell level 'Proxy_Groups'\n",
    "\n",
    "DEBUG =1\n",
    "#Define emission arrays\n",
    "Emissions_array_01 = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "\n",
    "Emissions_array_01_dom = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "\n",
    "Emissions_array_01_ind = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "\n",
    "Emissions_nongrid = np.zeros([num_years])\n",
    "\n",
    "# For each year, (2a) [NOT RELEVANT TO THIS SOURCE] distribute state-level emissions onto a grid using proxies defined above ....\n",
    "# To speed up the code, masks are used rather than looping individually through each lat/lon. \n",
    "# In this case, a mask of 1's is made for the grid cells that match the ANSI values for a given state\n",
    "# The masked values are set to zero, remaining values = 1. \n",
    "# AK and HI and territories are removed from the analysis at this stage. \n",
    "# The emissions allocated to each state are at 0.01x0.01 degree resolution, as required to calculate accurate 'mask'\n",
    "# arrays for each state. \n",
    "# (2b) For emission groups that were not first allocated to states [RELEVENT HERE], national emissions for those groups are gridded\n",
    "# based on the relevant gridded proxy arrays (0.1x0.1 resolution). These emissions are at 0.1x0.1 degrees resolution. \n",
    "# (2c) - record 'not mapped' emission groups in the 'non-grid' array (not relevant here)\n",
    "\n",
    "print('**QA/QC Check: Sum of national gridded emissions vs. GHGI national emissions')\n",
    "running_sum = np.zeros([len(proxy_wwt_map),num_years])\n",
    "\n",
    "for igroup in np.arange(0,len(proxy_wwt_map)):\n",
    "    proxy_temp = vars()[proxy_wwt_map.loc[igroup,'Proxy_Group']]\n",
    "    proxy_temp_nongrid = vars()[proxy_wwt_map.loc[igroup,'Proxy_Group']+'_nongrid']\n",
    "    #vars()['Ext_'+proxy_wwt_map.loc[igroup,'GHGI_Emi_Group']+'_01'] = np.zeros([len(lat001),len(lon001),num_years])\n",
    "    vars()['Ext_'+proxy_wwt_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    " \n",
    "         \n",
    "    #2b. if emissions were not allocated to state, allocate national total to grid here (these are in 0.1x0.1 resolution)\n",
    "    if proxy_wwt_map.loc[igroup,'Proxy_Group'] != 'Map_not_mapped':\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            temp_sum = np.sum(vars()[proxy_wwt_map.loc[igroup,'Proxy_Group']][:,:,iyear])+np.sum(vars()[proxy_wwt_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear])\n",
    "            emi_temp = vars()[proxy_wwt_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                   data_fn.safe_div(vars()[proxy_wwt_map.loc[igroup,'Proxy_Group']][:,:,iyear], temp_sum)\n",
    "            Emissions_array_01[:,:,iyear] += emi_temp\n",
    "            vars()['Ext_'+proxy_wwt_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += emi_temp\n",
    "            if '_Dom_' in proxy_wwt_map.loc[igroup, 'GHGI_Emi_Group']:\n",
    "                Emissions_array_01_dom[:,:,iyear] += vars()[proxy_wwt_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                    data_fn.safe_div(vars()[proxy_wwt_map.loc[igroup,'Proxy_Group']][:,:,iyear], temp_sum)\n",
    "            elif 'Ind' in proxy_wwt_map.loc[igroup, 'GHGI_Emi_Group']:\n",
    "                Emissions_array_01_ind[:,:,iyear] += vars()[proxy_wwt_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                    data_fn.safe_div(vars()[proxy_wwt_map.loc[igroup,'Proxy_Group']][:,:,iyear], temp_sum)\n",
    "            \n",
    "            Emissions_nongrid[iyear] += vars()[proxy_wwt_map.loc[igroup,'GHGI_Emi_Group']][iyear] *\\\n",
    "                    data_fn.safe_div(vars()[proxy_wwt_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear], temp_sum)\n",
    "            ##DEBUG## running_count += vars()[proxy_wwt_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "            running_sum[igroup,iyear] += np.sum(vars()[proxy_wwt_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                   data_fn.safe_div(vars()[proxy_wwt_map.loc[igroup,'Proxy_Group']][:,:,iyear], temp_sum)) + \\\n",
    "                    (vars()[proxy_wwt_map.loc[igroup,'GHGI_Emi_Group']][iyear] *\\\n",
    "                     data_fn.safe_div(vars()[proxy_wwt_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear], temp_sum))    \n",
    "\n",
    "    #2c. this is the case that GHGI emissions are not mapped (e.g., specified outside of CONUS in the GHGI)\n",
    "    elif proxy_wwt_map.loc[igroup,'Proxy_Group'] == 'Map_not_mapped':  \n",
    "        for iyear in np.arange(0, num_years):\n",
    "            Emissions_nongrid[iyear] += vars()[proxy_wwt_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "            running_sum[igroup,iyear] += vars()[proxy_wwt_map.loc[igroup,'GHGI_Emi_Group']][iyear] \n",
    "    #print(running_sum[igroup,iyear])\n",
    "\n",
    "for iyear in np.arange(0, num_years):    \n",
    "    calc_emi = np.sum(Emissions_array_01[:,:,iyear]) + np.sum(Emissions_nongrid[iyear]) \n",
    "    calc_emi2 = np.sum(Emissions_array_01_dom[:,:,iyear]) +np.sum(Emissions_array_01_ind[:,:,iyear])+ \\\n",
    "                np.sum(Emissions_nongrid[iyear]) \n",
    "    calc_emi3 = 0\n",
    "    for igroup in np.arange(0,len(proxy_wwt_map)):\n",
    "        calc_emi3 += np.sum(vars()['Ext_'+proxy_wwt_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])\n",
    "    calc_emi3 += np.sum(Emissions_nongrid[iyear]) \n",
    "    summary_emi = EPA_emi_ww_total.iloc[0,iyear+1] \n",
    "    emi_diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if DEBUG==1:\n",
    "        print(calc_emi)\n",
    "        print(calc_emi2)\n",
    "        print(calc_emi3)\n",
    "        print(summary_emi)\n",
    "    if abs(emi_diff) < 0.0001:\n",
    "        print('Year '+ year_range_str[iyear]+': Difference < 0.01%: PASS')\n",
    "    else: \n",
    "        print('Year '+ year_range_str[iyear]+': Difference > 0.01%: FAIL, diff: '+str(emi_diff))\n",
    "        \n",
    "ct = datetime.now() \n",
    "print(\"current time:\", ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1.4 Save gridded emissions (kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save gridded emissions for each gridding group - for extension\n",
    "\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(grid_emi_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "unique_groups = np.unique(proxy_wwt_map['GHGI_Emi_Group'])\n",
    "unique_groups = unique_groups[unique_groups != 'Emi_not_mapped']\n",
    "\n",
    "nc_out = Dataset(grid_emi_outputfile, 'r+', format='NETCDF4')\n",
    "\n",
    "for igroup in np.arange(0,len(unique_groups)):\n",
    "    print('Ext_'+unique_groups[igroup])\n",
    "    if len(np.shape(vars()['Ext_'+unique_groups[igroup]])) ==4:\n",
    "        ghgi_temp = np.sum(vars()[unique_groups[igroup]],axis=3) #sum month data if data is monthly\n",
    "    else:\n",
    "        ghgi_temp = vars()['Ext_'+unique_groups[igroup]]\n",
    "\n",
    "    # Write data to netCDF\n",
    "    data_out = nc_out.createVariable('Ext_'+unique_groups[igroup], 'f8', ('lat', 'lon','year'), zlib=True)\n",
    "    data_out[:,:,:] = ghgi_temp[:,:,:]\n",
    "\n",
    "#save nongrid data to calculate non-grid fraction extension\n",
    "data_out = nc_out.createVariable('Emissions_nongrid', 'f8', ('year'), zlib=True)  \n",
    "data_out[:] = Emissions_nongrid[:]\n",
    "nc_out.close()\n",
    "\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded emissions (kt) written to file: {}\" .format(os.getcwd())+grid_emi_outputfile)\n",
    "print(' ')\n",
    "\n",
    "del data_out, ghgi_temp, nc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Calculate Gridded Emission Fluxes (molec./cm2/s) (0.1x0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert emissions to emission flux\n",
    "# convert kt to molec/cm2/s\n",
    "\n",
    "Flux_array_01_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Flux_array_01_annual_dom = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Flux_array_01_annual_ind = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "print('**QA/QC Check: Sum of national gridded emissions vs. GHGI national emissions')\n",
    "  \n",
    "for iyear in np.arange(0,num_years):\n",
    "    calc_emi = 0\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "        #month_days = month_day_leap\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        #month_days = month_day_nonleap\n",
    "        \n",
    "    #for imonth in np.arange(0,num_months):\n",
    "    conversion_factor_01 = 10**9 * Avogadro / float(Molarch4 *year_days * 24 * 60 *60) / area_matrix_01\n",
    "    Flux_array_01_annual[:,:,iyear] = Emissions_array_01[:,:,iyear]*conversion_factor_01\n",
    "    Flux_array_01_annual_dom[:,:,iyear] = Emissions_array_01_dom[:,:,iyear]*conversion_factor_01\n",
    "    Flux_array_01_annual_ind[:,:,iyear] = Emissions_array_01_ind[:,:,iyear]*conversion_factor_01\n",
    "    #convert back to mass to check\n",
    "    conversion_factor_annual = 10**9 * Avogadro / float(Molarch4 *year_days * 24 * 60 *60) / area_matrix_01\n",
    "    calc_emi = np.sum(Flux_array_01_annual[:,:,iyear]/conversion_factor_annual)+np.sum(Emissions_nongrid[iyear])\n",
    "    calc_emi2 = np.sum(Flux_array_01_annual_dom[:,:,iyear]/conversion_factor_annual)+\\\n",
    "                np.sum(Flux_array_01_annual_ind[:,:,iyear]/conversion_factor_annual)+np.sum(Emissions_nongrid[iyear])\n",
    "    summary_emi = EPA_emi_ww_total.iloc[0,iyear+1] \n",
    "    emi_diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if DEBUG==1:\n",
    "        print(calc_emi)\n",
    "        print(calc_emi2)\n",
    "        print(summary_emi)\n",
    "    if abs(emi_diff) < 0.0001:\n",
    "        print('Year '+ year_range_str[iyear]+': Difference < 0.01%: PASS')\n",
    "    else: \n",
    "        print('Year '+ year_range_str[iyear]+': Difference > 0.01%: FAIL, diff: '+str(emi_diff))\n",
    "        \n",
    "Flux_Emissions_Total_annual = Flux_array_01_annual\n",
    "Flux_Emissions_Total_annual_dom = Flux_array_01_annual_dom\n",
    "Flux_Emissions_Total_annual_ind = Flux_array_01_annual_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 5. Write netCDF\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearly data\n",
    "\n",
    "#Total\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(gridded_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "# Write data to netCDF\n",
    "nc_out = Dataset(gridded_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Total_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded stationary combustion fluxes written to file: {}\" .format(os.getcwd())+gridded_outputfile)\n",
    "\n",
    "#MSW Landfills\n",
    "# yearly data\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(gridded_dom_outputfile, netCDF_dom_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "# Write data to netCDF\n",
    "nc_out = Dataset(gridded_dom_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Total_annual_dom\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded stationary combustion fluxes written to file: {}\" .format(os.getcwd())+gridded_dom_outputfile)\n",
    "\n",
    "#Industrial Landfills\n",
    "# yearly data\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(gridded_ind_outputfile, netCDF_ind_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "# Write data to netCDF\n",
    "nc_out = Dataset(gridded_ind_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Total_annual_ind\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded stationary combustion fluxes written to file: {}\" .format(os.getcwd())+gridded_ind_outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Step 6. Plot Gridded Data\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.1. Plot Annual Emission Fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot Annual Data\n",
    "#Total\n",
    "scale_max = 10\n",
    "save_flag = 0\n",
    "save_fig = ''\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_str,scale_max,save_flag,save_fig)\n",
    "\n",
    "# Dom\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Total_annual_dom, Lat_01, Lon_01, year_range, title_str_dom,scale_max,save_flag,save_fig)\n",
    "\n",
    "#IND\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Total_annual_ind, Lat_01, Lon_01, year_range, title_str_ind,scale_max,save_flag,save_fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.2 Plot Difference between first and last inventory year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot difference between last and first year\n",
    "save_flag =0\n",
    "save_fig = ''\n",
    "#Total\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_diff_str,save_flag,save_fig)\n",
    "\n",
    "#MSW\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Total_annual_dom, Lat_01, Lon_01, year_range, title_diff_str_dom,save_flag,save_fig)\n",
    "\n",
    "#IND\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Total_annual_ind, Lat_01, Lon_01, year_range, title_diff_str_ind,save_flag,save_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = datetime.now() \n",
    "ft = ct.timestamp() \n",
    "time_elapsed = (ft-it)/(60*60)\n",
    "print('Time to run: '+str(time_elapsed)+' hours')\n",
    "print('** GEPA_5D_Wastewater: COMPLETE **')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

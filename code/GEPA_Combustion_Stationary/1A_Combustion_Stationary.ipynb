{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded EPA Methane Inventory\n",
    "## Category: 1A Stationary Combustion of Fuels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Authors: \n",
    "Joannes D. Maasakkers, Candice F. Z. Chen, Erin E. McDuffie\n",
    "#### Date Last Updated: \n",
    "see Step 0\n",
    "#### Notebook Purpose: \n",
    "This Notebook calculates and reports annual and monthly gridded (0.1°x0.1°) methane emission fluxes (molec./cm2/s) from stationary combustion sources in the CONUS region between 2012-2018.   \n",
    "#### Summary & Notes:\n",
    "EPA GHGI stationary combustion emissions from Electric Energy Generation, Commercial, Residential, and Industrial processes within the Energy Combustion sector are read in at the national level. Emissions for residential, commercial, and industrial sectors are first allocated to the state level as a function of proxy group. Residential emissions are also allocated to the county level using NEI wood combustion as a proxy. The activity/proxy data used to allocate emissions from each group include EIA State Energy Data System data (for energy sector) and facility level methane fluxes and emission from EPA’s Acid Rain Program and GHGRP (Subpart’s C and D), as a function of sector and fuel type (for commercial, residential, industrial sectors). State-level emissions are spatially distributed onto a 0.1°x0.1° grid based on population density (residential/commercial) and GHGRP facility locations/emissions (industrial). Emissions are converted to emission flux. Annual emission and monthly emission fluxes (molec./cm2/s) are written to final netCDFs in the ‘/code/Final_Gridded_Data/’ folder.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Step 0. Set-Up Notebook Modules, Functions, and Local Parameters and Constants\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Confirm working directory & print last update time\n",
    "import os\n",
    "import time\n",
    "modtime = os.path.getmtime('./1A_Combustion_Stationary.ipynb')\n",
    "modificationTime = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(modtime))\n",
    "print(\"This file was last modified on: \", modificationTime)\n",
    "print('')\n",
    "print(\"The directory we are working in is {}\" .format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Include plots within notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from copy import copy\n",
    "\n",
    "# Import additional modules\n",
    "# Load plotting package Basemap \n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# Load netCDF (for manipulating netCDF file types)\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Set up ticker\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#add path for the global function module (file)\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../Global_Functions/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Load Tabula (for reading tables from PDFs)\n",
    "import tabula as tb \n",
    "import PyPDF2 as pypdf\n",
    "    \n",
    "# Load user-defined global functions (modules)\n",
    "import data_load_functions as data_load_fn\n",
    "import data_functions as data_fn\n",
    "import data_IO_functions as data_IO_fn\n",
    "import data_plot_functions as data_plot_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT Files\n",
    "# Assign global file names\n",
    "global_filenames = data_load_fn.load_global_file_names()\n",
    "State_ANSI_inputfile = global_filenames[0]\n",
    "County_ANSI_inputfile = global_filenames[1]\n",
    "pop_map_inputfile = global_filenames[2]\n",
    "Grid_area01_inputfile = global_filenames[3]\n",
    "Grid_area001_inputfile = global_filenames[4]\n",
    "Grid_state001_ansi_inputfile = global_filenames[5]\n",
    "Grid_county001_ansi_inputfile = global_filenames[6]\n",
    "globalinputlocation = global_filenames[0][0:20]\n",
    "\n",
    "# Specify names of inputs files used in this notebook\n",
    "EPA_stat_inputfile = globalinputlocation+ 'GHGI/Ch3_Energy/EPA_Table_3-10_kt.csv'\n",
    "\n",
    "StatComb_Mapping_inputfile = './InputData/StationaryCombustion_ProxyMapping.xlsx'\n",
    "\n",
    "#Activity Data\n",
    "EPA_ARP_inputfile = './InputData/ARP_Data/EPA_ARP_FacilityEmissions.csv'\n",
    "EIA_SEDS_commconsump_inputfile = \"./InputData/EIA_SEDS/Commercial/sum_btu_com_\"\n",
    "EIA_SEDS_resconsump_inputfile = \"./InputData/EIA_SEDS/Residential/sum_btu_res_\"\n",
    "EIA_SEDS_indconsump_inputfile = \"./InputData/EIA_SEDS/Industrial/sum_btu_ind_\"\n",
    "NEI_resi_wood_inputfile = \"./InputData/NEI 2020 RWC Throughputs.xlsx\"\n",
    "\n",
    "#GHGRP Data (reporting format changed in 2015)\n",
    "GHGRP_subCfacility_inputfile = \"./InputData/GHGRP/GHGRP_SubpartCEmissions.csv\" #subpart C facility IDs and emissions (locations not available)\n",
    "GHGRP_subDfacility_inputfile = \"./InputData/GHGRP/GHGRP_SubpartDEmissions.csv\" #subpart D facility IDs and emissions \n",
    "GHGRP_subDfacility_loc_inputfile = \"./InputData/GHGRP/GHGRP_FacilityInfo.csv\" #subpart D facility info (for all years, with ID & lat and lons)\n",
    "\n",
    "#OUTPUT FILES\n",
    "gridded_int_outputfile = 'EPA_v2_1A_Combustion_Stationary_int.nc'\n",
    "\n",
    "gridded_outputfile = '../Final_Gridded_Data/EPA_v2_1A_Combustion_Stationary.nc'\n",
    "gridded_month_outputfile = '../Final_Gridded_Data/EPA_v2_1A_Combustion_Stationary_Monthly.nc'\n",
    "netCDF_description = 'Gridded EPA Inventory - Stationary Combustion Emissions - IPCC Source Category 1A'\n",
    "netCDF_description_m = 'Gridded EPA Inventory - Monthly Stationary Combustion Emissions - IPCC Source Category 1A'\n",
    "title_str = \"EPA methane emissions from stationary combustion\"\n",
    "title_diff_str = \"Emissions from stationary combustion difference: 2018-2012\"\n",
    "\n",
    "#output gridded proxy data\n",
    "grid_emi_outputfile = '../Final_Gridded_Data/Extension/v2_input_data/Comb_Stationary_Grid_Emi.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local variables\n",
    "start_year = 2012  #First year in emission timeseries\n",
    "end_year = 2018    #Last year in emission timeseries\n",
    "year_range = [*range(start_year, end_year+1,1)] #List of emission years\n",
    "year_range_str=[str(i) for i in year_range]\n",
    "num_years = len(year_range)\n",
    "\n",
    "# Define constants\n",
    "Avogadro   = 6.02214129 * 10**(23)  #molecules/mol\n",
    "Molarch4   = 16.04                  #g/mol\n",
    "Res01      = 0.1                    # degrees\n",
    "Res_01     = 0.01                   # degrees\n",
    "tg_scale   = 0.001                  #Tg scale number [New file allows for the exclusion of the territories] \n",
    "\n",
    "# Continental US Lat/Lon Limits (for netCDF files)\n",
    "Lon_left = -130       #deg\n",
    "Lon_right = -60       #deg\n",
    "Lat_low  = 20         #deg\n",
    "Lat_up  = 55          #deg\n",
    "loc_dimensions = [Lat_low, Lat_up, Lon_left, Lon_right]\n",
    "\n",
    "ilat_start = int((90+Lat_low)/Res01) #1100:1450 (continental US range)\n",
    "ilat_end = int((90+Lat_up)/Res01)\n",
    "ilon_start = abs(int((-180-Lon_left)/Res01)) #500:1200 (continental US range)\n",
    "ilon_end = abs(int((-180-Lon_right)/Res01))\n",
    "\n",
    "# Number of days in each month\n",
    "month_day_leap  = [  31,  29,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_day_nonleap = [  31,  28,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "\n",
    "# Month arrays\n",
    "month_range_str = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "num_months = len(month_range_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;\n",
    "//prevent auto-scrolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track run time\n",
    "ct = datetime.datetime.now() \n",
    "it = ct.timestamp() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Step 1. Load in State ANSI data and Area\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State-level ANSI Data\n",
    "#Read the state ANSI file array\n",
    "State_ANSI, name_dict = data_load_fn.load_state_ansi(State_ANSI_inputfile)[0:2]\n",
    "#QA: number of states\n",
    "print('Read input file: '+ f\"{State_ANSI_inputfile}\")\n",
    "print('Total \"States\" found: ' + '%.0f' % len(State_ANSI))\n",
    "print(' ')\n",
    "\n",
    "#County ANSI Data\n",
    "#Includes State ANSI number, county ANSI number, county name, and country area (square miles)\n",
    "County_ANSI = pd.read_csv(County_ANSI_inputfile,encoding='latin-1')\n",
    "\n",
    "#QA: number of counties\n",
    "print ('Read input file: ' + f\"{County_ANSI_inputfile}\")\n",
    "print('Total \"Counties\" found (include PR): ' + '%.0f' % len(County_ANSI))\n",
    "print(' ')\n",
    "\n",
    "#Create a placeholder array for county data\n",
    "county_array = np.zeros([len(County_ANSI),3])\n",
    "\n",
    "# 0.01 x0.01 degree Data\n",
    "# State ANSI IDs and grid cell area (m2) maps\n",
    "state_ANSI_map = data_load_fn.load_state_ansi_map(Grid_state001_ansi_inputfile)\n",
    "state_ANSI_map = state_ANSI_map.astype('int32')\n",
    "county_ANSI_map = data_load_fn.load_county_ansi_map(Grid_county001_ansi_inputfile)\n",
    "county_ANSI_map = county_ANSI_map.astype('int32')\n",
    "area_map, lat001, lon001 = data_load_fn.load_area_map_001(Grid_area001_inputfile)\n",
    "\n",
    "# 0.1 x0.1 degree data\n",
    "# grid cell area and state ANSI maps\n",
    "area_map01, Lat01, Lon01 = data_load_fn.load_area_map_01(Grid_area01_inputfile)[0:3]\n",
    "#Select relevant Continental 0.1 x0.1 domain\n",
    "Lat_01 = Lat01[ilat_start:ilat_end]\n",
    "Lon_01 = Lon01[ilon_start:ilon_end]\n",
    "area_matrix_01 = data_fn.regrid001_to_01(area_map, Lat_01, Lon_01)\n",
    "area_matrix_01 *= 10000  #convert from m2 to cm2\n",
    "\n",
    "state_ANSI_map_01 = data_fn.regrid001_to_01(state_ANSI_map, Lat_01, Lon_01)\n",
    "\n",
    "\n",
    "# Print time\n",
    "ct = datetime.datetime.now() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 2: Read-in and Format Proxy Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1 Read In Proxy Mapping File & Make Proxy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load GHGI Mapping Groups\n",
    "names = pd.read_excel(StatComb_Mapping_inputfile, sheet_name = \"GHGI Map - Stat\", usecols = \"A:B\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "ghgi_stat_map = pd.read_excel(StatComb_Mapping_inputfile, sheet_name = \"GHGI Map - Stat\", usecols = \"A:B\", skiprows = 1, names = colnames)\n",
    "#drop rows with no data, remove the parentheses and \"\"\n",
    "ghgi_stat_map = ghgi_stat_map[ghgi_stat_map['GHGI_Emi_Group'] != 'na']\n",
    "ghgi_stat_map = ghgi_stat_map[ghgi_stat_map['GHGI_Emi_Group'].notna()]\n",
    "ghgi_stat_map['GHGI_Source']= ghgi_stat_map['GHGI_Source'].str.replace(r\"\\(\",\"\")\n",
    "ghgi_stat_map['GHGI_Source']= ghgi_stat_map['GHGI_Source'].str.replace(r\"\\)\",\"\")\n",
    "ghgi_stat_map.reset_index(inplace=True, drop=True)\n",
    "display(ghgi_stat_map)\n",
    "\n",
    "#load emission group - proxy map\n",
    "names = pd.read_excel(StatComb_Mapping_inputfile, sheet_name = \"Proxy Map - Stat\", usecols = \"A:G\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "proxy_stat_map = pd.read_excel(StatComb_Mapping_inputfile, sheet_name = \"Proxy Map - Stat\", usecols = \"A:G\", skiprows = 1, names = colnames)\n",
    "display((proxy_stat_map))\n",
    "\n",
    "#create empty proxy and emission group arrays (add months for proxy variables that have monthly data)\n",
    "for igroup in np.arange(0,len(proxy_stat_map)):\n",
    "    if proxy_stat_map.loc[igroup, 'Grid_Month_Flag'] ==0:\n",
    "        vars()[proxy_stat_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        vars()[proxy_stat_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "    else:\n",
    "        vars()[proxy_stat_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "        vars()[proxy_stat_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years,num_months])\n",
    "        \n",
    "    vars()[proxy_stat_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])\n",
    "    \n",
    "    if proxy_stat_map.loc[igroup,'State_Proxy_Group'] != '-':\n",
    "        if proxy_stat_map.loc[igroup,'State_Month_Flag'] == 0:\n",
    "            vars()[proxy_stat_map.loc[igroup,'State_Proxy_Group']] = np.zeros([len(State_ANSI),num_years])\n",
    "        else:\n",
    "            vars()[proxy_stat_map.loc[igroup,'State_Proxy_Group']] = np.zeros([len(State_ANSI),num_years,num_months])\n",
    "    else:\n",
    "        continue # do not make state proxy variable if no variable assigned in mapping file\n",
    "    \n",
    "    if proxy_stat_map.loc[igroup,'County_Proxy_Group'] != '-':\n",
    "        if proxy_stat_map.loc[igroup,'County_Month_Flag'] == 0:\n",
    "            vars()[proxy_stat_map.loc[igroup,'County_Proxy_Group']] = np.zeros([len(State_ANSI),len(County_ANSI),num_years])\n",
    "        else:\n",
    "            vars()[proxy_stat_map.loc[igroup,'County_Proxy_Group']] = np.zeros([len(State_ANSI),len(County_ANSI),num_years,num_months])\n",
    "    else:\n",
    "        continue # do not make county proxy variable if no variable assigned in mapping file\n",
    "\n",
    "emi_group_names = np.unique(proxy_stat_map['GHGI_Emi_Group'])\n",
    "\n",
    "print('QA/QC: Is the number of emission groups the same for the proxy and emissions tabs?')\n",
    "if (len(emi_group_names) == len(np.unique(proxy_stat_map['GHGI_Emi_Group']))):\n",
    "    print('PASS')\n",
    "else:\n",
    "    print('FAIL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Read In and Format EPA (Acid Rain Program) Electric Power Emissions (Electric Energy Proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 Read in EPA power plant facility information and calculate facility-level emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read EPA ARP data for individual power plants. \n",
    "# Calculate emissions from the unit type and fuel type to calculate CH4 emission factor to apply to Heat Input.\n",
    "#https://ampd.epa.gov/ampd/\n",
    "\n",
    "fields = ['State', ' Year',' Month', ' Facility Name',' Facility Latitude',' Facility Longitude',' Unit Type', \\\n",
    "          ' Fuel Type (Primary)', ' Heat Input (MMBtu)']\n",
    "ARP_Raw = pd.read_csv(EPA_ARP_inputfile, usecols = fields, index_col=False, na_filter = False)\n",
    "\n",
    "# make a multidimensional dictionary that contains the data for each year\n",
    "# for calculations later, replace empty heat input values with NaNs and convert\n",
    "# to numeric (otherwise data in scientific notation are read in as strings)\n",
    "ARP_facilities = dict()\n",
    "for iyear in np.arange(num_years):\n",
    "    ARP_facilities[iyear] = ARP_Raw[ARP_Raw[' Year'] == year_range[iyear]]\n",
    "    ARP_facilities[iyear].fillna(0)#,inplace = True)\n",
    "    ARP_facilities[iyear].reset_index(inplace=True)\n",
    "    temp = pd.to_numeric(ARP_facilities[iyear].loc[:,' Heat Input (MMBtu)'], errors='coerce')\n",
    "    temp.fillna(0,inplace=True)\n",
    "    ARP_facilities[iyear].loc[:,' Heat Input (MMBtu)'] = temp\n",
    "\n",
    "\n",
    "# Clean up and standardize the Unit Type labels\n",
    "# Assign values to a temporary dataframe to avoid settingwithcopy warning\n",
    "\n",
    "# For each year, check the unit types and report a clean string version in a new column\n",
    "for iyear in np.arange(num_years):\n",
    "    temp = pd.DataFrame.from_dict(ARP_facilities[iyear])\n",
    "\n",
    "    for ifacility in np.arange(len(ARP_facilities[iyear])):\n",
    "        if re.search('combustion turbine',ARP_facilities[iyear].loc[ifacility,' Unit Type'].lower()) != None:\n",
    "            temp.loc[ifacility,'Unit_clean'] = 'combustion turbine'\n",
    "        elif re.search('combined cycle',ARP_facilities[iyear].loc[ifacility,' Unit Type'].lower()) != None:\n",
    "            temp.loc[ifacility,'Unit_clean'] = 'combined cycle'\n",
    "        elif re.search('wet bottom',ARP_facilities[iyear].loc[ifacility,' Unit Type'].lower()) != None:\n",
    "            temp.loc[ifacility,'Unit_clean'] = 'wet bottom'\n",
    "        elif re.search('dry bottom',ARP_facilities[iyear].loc[ifacility,' Unit Type'].lower()) != None:\n",
    "            temp.loc[ifacility,'Unit_clean'] = 'dry bottom'\n",
    "        elif re.search('bubbling',ARP_facilities[iyear].loc[ifacility,' Unit Type'].lower()) != None:\n",
    "            temp.loc[ifacility,'Unit_clean'] = 'bubbling'\n",
    "        else:\n",
    "            temp.loc[ifacility,'Unit_clean'] = ARP_facilities[iyear].loc[ifacility,' Unit Type'].lower()  \n",
    "\n",
    "        ARP_facilities[iyear] = temp.copy()\n",
    "\n",
    "#Clean up and standardize the fuel type labels\n",
    "# Assign values to a temporary dataframe to avoid settingwithcopy warning\n",
    "\n",
    "# For each year, check the primary fuel types and consolidate into Gas, Coal, Oil, and Wood fuel categories\n",
    "for iyear in np.arange(num_years):\n",
    "    \n",
    "    temp = pd.DataFrame.from_dict(ARP_facilities[iyear])\n",
    "    for ifacility in np.arange(len(ARP_facilities[iyear])):\n",
    "        if ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Pipeline Natural Gas' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Natural Gas' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Other Gas' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Other Gas, Pipeline Natural Gas' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Natural Gas, Pipeline Natural Gas' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Process Gas':\n",
    "            temp.loc[ifacility,'Fuel_clean'] = 'Gas'\n",
    "        elif ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Petroleum Coke' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Coal' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Coal, Pipeline Natural Gas' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Coal, Natural Gas' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Coal, Wood' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Coal, Coal Refuse' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Coal Refuse' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Other Solid Fuel':\n",
    "            temp.loc[ifacility,'Fuel_clean'] = 'Coal'\n",
    "        elif ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Other Oil' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Diesel Oil' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Diesel Oil, Residual Oil' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Diesel Oil, Pipeline Natural Gas' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Residual Oil' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Residual Oil, Pipeline Natural Gas':\n",
    "            temp.loc[ifacility,'Fuel_clean'] = 'Oil'\n",
    "        elif ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Wood' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Other Solid Fuel, Wood':\n",
    "            temp.loc[ifacility,'Fuel_clean'] = 'Wood'\n",
    "        else:\n",
    "            if ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] != '':\n",
    "                print(ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'])  \n",
    "                \n",
    "    ARP_facilities[iyear] = temp.copy()  \n",
    "\n",
    "del ARP_Raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2. Add the CH4 factor (kg gas/ TJ energy input) to the data dictionary, then calculate methane flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CH4 factor is based on the unit type and fuel used\n",
    "# From 'Acid Rain Prog - Unit-level Fuel+Technology' file in InputData Folder\n",
    "\n",
    "for iyear in np.arange(num_years):\n",
    "    ARP_facilities[iyear].loc[:,'CH4_f'] = 0.0\n",
    "\n",
    "    for ifacility in np.arange(len(ARP_facilities[iyear])):\n",
    "        # Gas: combined cycle or turbine= 3.7, \n",
    "        # Gas: others (Assume stoker, tangentially-fired, dry bottom, & wet bottom are boilers) = 1\n",
    "        if ARP_facilities[iyear].loc[ifacility,'Fuel_clean'] == 'Gas':\n",
    "            if ARP_facilities[iyear].loc[ifacility,'Unit_clean'] == 'combined cycle' \\\n",
    "             or re.search('turbine',ARP_facilities[iyear].loc[ifacility,'Unit_clean'].lower()) != None:\n",
    "                ARP_facilities[iyear].loc[ifacility,'CH4_f'] = 3.7     \n",
    "            else:\n",
    "                ARP_facilities[iyear].loc[ifacility,'CH4_f'] = 1\n",
    "        # Coal: tangentially-fired, dry bottom = 0.7\n",
    "        # Coal: wet bottom = 0.9\n",
    "        # Coal: Cyclone boiler = 0.2\n",
    "        # Coal: others (boilers, combined cycle) = 1\n",
    "        elif ARP_facilities[iyear].loc[ifacility,'Fuel_clean'] == 'Coal':\n",
    "            if ARP_facilities[iyear].loc[ifacility,'Unit_clean'] == 'tangentially-fired' \\\n",
    "             or ARP_facilities[iyear].loc[ifacility,'Unit_clean'] == 'dry bottom':\n",
    "                ARP_facilities[iyear].loc[ifacility,'CH4_f'] = 0.7\n",
    "            elif ARP_facilities[iyear].loc[ifacility,'Unit_clean'] == 'wet bottom':   \n",
    "                ARP_facilities[iyear].loc[ifacility,'CH4_f'] = 0.9\n",
    "            elif ARP_facilities[iyear].loc[ifacility,'Unit_clean'] == 'cyclone boiler':   \n",
    "                ARP_facilities[iyear].loc[ifacility,'CH4_f'] = 0.2\n",
    "            else:\n",
    "                ARP_facilities[iyear].loc[ifacility,'CH4_f'] = 1   \n",
    "        # Wood: assume all are recover boilders = 1\n",
    "        elif ARP_facilities[iyear].loc[ifacility,'Fuel_clean'] == 'Wood':\n",
    "            ARP_facilities[iyear].loc[ifacility,'CH4_f'] = 1   \n",
    "        # Oil: Reisdual oil, pipeline natural gas = 0.8\n",
    "        # Oil: Others = 0.9\n",
    "        elif ARP_facilities[iyear].loc[ifacility,'Fuel_clean'] == 'Oil':\n",
    "            if ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Residual Oil' \\\n",
    "             or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Residual Oil, Pipeline Natural Gas':\n",
    "                ARP_facilities[iyear].loc[ifacility,'CH4_f'] = 0.8\n",
    "            else:\n",
    "                ARP_facilities[iyear].loc[ifacility,'CH4_f'] = 0.9\n",
    "        else:\n",
    "            if ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] != '':\n",
    "                print(ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'])\n",
    "    \n",
    "\n",
    "#Calculate the methane flux at each facility and the flux by fuel at each facility relative to the national total. \n",
    "for iyear in np.arange(num_years):\n",
    "    # Calculate fluxes\n",
    "    ARP_facilities[iyear]['CH4_flux'] = 0.0\n",
    "    ARP_facilities[iyear]['CH4_flux'] = ARP_facilities[iyear]['CH4_f'] * ARP_facilities[iyear][' Heat Input (MMBtu)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2.3 Allocate flux data (as a function of fuel type) to grid arrays and to the state level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arp_wood_array = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "arp_wood_array_nongrid = np.zeros([num_years,num_months])\n",
    "arp_coal_array = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "arp_coal_array_nongrid = np.zeros([num_years,num_months])\n",
    "arp_oil_array = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "arp_oil_array_nongrid = np.zeros([num_years,num_months])\n",
    "arp_gas_array = np.zeros([len(Lat_01),len(Lon_01),num_years, num_months])\n",
    "arp_gas_array_nongrid = np.zeros([num_years,num_months])\n",
    "\n",
    "\n",
    "for iyear in np.arange(num_years):\n",
    "    #count=0\n",
    "    var = 'CH4_flux'\n",
    "    for ifacility in np.arange(len(ARP_facilities[iyear])):\n",
    "        imon = int(ARP_facilities[iyear].loc[ifacility,' Month'] - 1)\n",
    "        istate = np.where(ARP_facilities[iyear].loc[ifacility,'State'] == State_ANSI['abbr'])[0][0]\n",
    "        #Filter inside Continental US domain\n",
    "        if ARP_facilities[iyear].loc[ifacility,' Facility Longitude'] > Lon_left \\\n",
    "         and ARP_facilities[iyear].loc[ifacility,' Facility Longitude'] < Lon_right \\\n",
    "         and ARP_facilities[iyear].loc[ifacility,' Facility Latitude'] > Lat_low \\\n",
    "         and ARP_facilities[iyear].loc[ifacility,' Facility Latitude'] < Lat_up:\n",
    "            #Find the index values of each facility lat and lon within the Continental US grid \n",
    "            ilat = int((ARP_facilities[iyear].loc[ifacility,' Facility Latitude'] - Lat_low)/Res01)\n",
    "            ilon = int((ARP_facilities[iyear].loc[ifacility,' Facility Longitude'] - Lon_left)/Res01)\n",
    "            if ARP_facilities[iyear].loc[ifacility, 'Fuel_clean'] == 'Gas':\n",
    "                arp_gas_array[ilat,ilon,iyear,imon] += ARP_facilities[iyear].loc[ifacility,var]\n",
    "            elif ARP_facilities[iyear].loc[ifacility, 'Fuel_clean'] == 'Coal':\n",
    "                arp_coal_array[ilat,ilon,iyear,imon] += ARP_facilities[iyear].loc[ifacility,var] \n",
    "            elif ARP_facilities[iyear].loc[ifacility, 'Fuel_clean'] == 'Oil':\n",
    "                arp_oil_array[ilat,ilon,iyear,imon] += ARP_facilities[iyear].loc[ifacility,var] \n",
    "            elif ARP_facilities[iyear].loc[ifacility, 'Fuel_clean'] == 'Wood':\n",
    "                arp_wood_array[ilat,ilon,iyear,imon] += ARP_facilities[iyear].loc[ifacility,var] \n",
    "        else:    \n",
    "            if ARP_facilities[iyear].loc[ifacility, 'Fuel_clean'] == 'Gas':\n",
    "                arp_gas_array_nongrid[iyear,imon] += ARP_facilities[iyear].loc[ifacility,var] \n",
    "            elif ARP_facilities[iyear].loc[ifacility, 'Fuel_clean'] == 'Coal':\n",
    "                arp_coal_array_nongrid[iyear,imon] += ARP_facilities[iyear].loc[ifacility,var]\n",
    "            elif ARP_facilities[iyear].loc[ifacility, 'Fuel_clean'] == 'Oil':\n",
    "                arp_oil_array_nongrid[iyear,imon] += ARP_facilities[iyear].loc[ifacility,var] \n",
    "            elif ARP_facilities[iyear].loc[ifacility, 'Fuel_clean'] == 'Wood':\n",
    "                arp_wood_array_nongrid[iyear,imon] += ARP_facilities[iyear].loc[ifacility,var] \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3 Read In and Format EIA SEDS (State Engery Data System) Energy Consumption Data (Commercial, Residential, Industrial Proxies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.1 Read In EIA SEDS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Read state-level energy consumption data (Commercial)\n",
    "#All SEDS data: https://www.eia.gov/state/seds/archive/\n",
    "\n",
    "SEDS_com = dict()\n",
    "for iyear in np.arange(0,num_years):\n",
    "    #tb.convert_into(\"./InputData_Stationary/Data_stat/EIA_SEDS_Data/seds2012.pdf\",'./test.csv', output_format = 'csv', stream=True, guess = False, pages = 11)\n",
    "    SEDS_com[iyear] = pd.read_csv(EIA_SEDS_commconsump_inputfile+year_range_str[iyear]+'.csv',nrows=51)\n",
    "    SEDS_com[iyear]['ASCI'] = 0\n",
    "    for istate in np.arange(len(SEDS_com[iyear])):\n",
    "        SEDS_com[iyear].loc[istate,'ASCI'] = name_dict[SEDS_com[iyear].loc[istate,'State'].strip()]\n",
    "#SEDS_com[1].head(1)\n",
    "\n",
    "# 2) Read state-level energy consumption data (Residential)\n",
    "SEDS_res = dict()\n",
    "for iyear in np.arange(num_years):\n",
    "    SEDS_res[iyear] = pd.read_csv(EIA_SEDS_resconsump_inputfile+year_range_str[iyear]+'.csv',nrows=51)\n",
    "    #SEDS_res[i] = pd.read_csv(f\"./Data_stat/Residential/sum_btu_res_201{i+2}.csv\",nrows=51)\n",
    "    SEDS_res[iyear]['ASCI'] = 0\n",
    "    for istate in np.arange(len(SEDS_res[iyear])):\n",
    "        SEDS_res[iyear].loc[istate,'ASCI'] = name_dict[SEDS_res[iyear].loc[istate,'State'].strip()]\n",
    "#SEDS_res[0].head(1)\n",
    "\n",
    "# 3) Read state-level energy consumption data (Industrial)\n",
    "SEDS_ind = dict()\n",
    "for iyear in np.arange(0, num_years):\n",
    "    SEDS_ind[iyear] = pd.read_csv(EIA_SEDS_indconsump_inputfile+year_range_str[iyear]+'.csv',nrows=51)\n",
    "    SEDS_ind[iyear]['ASCI'] = 0\n",
    "    for istate in np.arange(len(SEDS_ind[iyear])):\n",
    "        SEDS_ind[iyear].loc[istate,'ASCI'] = name_dict[SEDS_ind[iyear].loc[istate,'State'].strip()]\n",
    "        #SEDS_ind[iyear].loc[istate,'ASCI'] = int(SEDS_ind[iyear].loc[istate,'ASCI'])\n",
    "#SEDS_ind[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.2 Allocate BTUs to the state level (commercial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcualte state level BTU levels for commercial SEDS data, by state and fuel type\n",
    "\n",
    "sedscom_wood_state = np.zeros([len(State_ANSI), num_years])\n",
    "sedscom_coal_state = np.zeros([len(State_ANSI), num_years])\n",
    "sedscom_oil_state = np.zeros([len(State_ANSI), num_years])\n",
    "sedscom_gas_state = np.zeros([len(State_ANSI), num_years])\n",
    "\n",
    "for iyear in np.arange(num_years):\n",
    "    #Calculate emissions\n",
    "    \n",
    "    for istate in np.arange(len(SEDS_com[iyear])):\n",
    "        state_str = SEDS_com[iyear].loc[istate,'State']\n",
    "        state_str = state_str.strip()\n",
    "        matchstate = np.where(state_str == State_ANSI['name'])[0][0]\n",
    "        #if SEDS_com[iyear].loc[istate,'State'] not in {'Alaska', 'Hawaii'}:\n",
    "            #SEDS_com[iyear].loc[istate,'ASCI'] = name_dict[SEDS_com[iyear].loc[istate,'State'].strip()]\n",
    "        sedscom_coal_state[matchstate,iyear] += SEDS_com[iyear].loc[istate,'Coal'] #/Coal_sum[iyear] \n",
    "        sedscom_oil_state[matchstate,iyear] += SEDS_com[iyear].loc[istate,'Total Petroleum'] #/Fuel_sum[iyear]\n",
    "        sedscom_gas_state[matchstate,iyear] += SEDS_com[iyear].loc[istate,'Natural Gas'] #/NGas_sum[iyear]\n",
    "        sedscom_wood_state[matchstate,iyear] += SEDS_com[iyear].loc[istate,'Wood and Waste'] #/Wood_sum[iyear]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.3 Allocate BTUs to the state level (residential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcualte state level BTU levels for residential SEDS data, by state and fuel type\n",
    "\n",
    "sedsres_wood_state = np.zeros([len(State_ANSI), num_years])\n",
    "sedsres_coal_state = np.zeros([len(State_ANSI), num_years])\n",
    "sedsres_oil_state = np.zeros([len(State_ANSI), num_years])\n",
    "sedsres_gas_state = np.zeros([len(State_ANSI), num_years])\n",
    "\n",
    "for iyear in np.arange(num_years):\n",
    "    #Calculate emissions\n",
    "    \n",
    "    for istate in np.arange(len(SEDS_res[iyear])):\n",
    "        state_str = SEDS_res[iyear].loc[istate,'State']\n",
    "        state_str = state_str.strip()\n",
    "        matchstate = np.where(state_str == State_ANSI['name'])[0][0]\n",
    "        #if SEDS_com[iyear].loc[istate,'State'] not in {'Alaska', 'Hawaii'}:\n",
    "            #SEDS_com[iyear].loc[istate,'ASCI'] = name_dict[SEDS_com[iyear].loc[istate,'State'].strip()]\n",
    "        sedsres_coal_state[matchstate,iyear] += SEDS_res[iyear].loc[istate,'Coal']#,Coal_sum[iyear]) \n",
    "        sedsres_oil_state[matchstate,iyear] += SEDS_res[iyear].loc[istate,'Total Petroleum']# /Fuel_sum[iyear]\n",
    "        sedsres_gas_state[matchstate,iyear] += SEDS_res[iyear].loc[istate,'Natural Gas']# /NGas_sum[iyear]\n",
    "        sedsres_wood_state[matchstate,iyear] += SEDS_res[iyear].loc[istate,'Wood']# /Wood_sum[iyear]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.4 Allocate BTUs to the state level (industrial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcualte relative state level BTU levels for industrial SEDS data, by state and fuel type\n",
    "\n",
    "sedsind_wood_state = np.zeros([len(State_ANSI), num_years])\n",
    "sedsind_coal_state = np.zeros([len(State_ANSI), num_years])\n",
    "sedsind_oil_state = np.zeros([len(State_ANSI), num_years])\n",
    "sedsind_gas_state = np.zeros([len(State_ANSI), num_years])\n",
    "\n",
    "\n",
    "#print('**QA/QC: Check allocated state-level commercial emissions against GHGI')\n",
    "#print('')\n",
    "for iyear in np.arange(num_years):\n",
    "    #Calculate emissions\n",
    "    \n",
    "    for istate in np.arange(len(SEDS_ind[iyear])):\n",
    "        state_str = SEDS_ind[iyear].loc[istate,'State']\n",
    "        state_str = state_str.strip()\n",
    "        matchstate = np.where(state_str == State_ANSI['name'])[0][0]\n",
    "        #if SEDS_com[iyear].loc[istate,'State'] not in {'Alaska', 'Hawaii'}:\n",
    "            #SEDS_com[iyear].loc[istate,'ASCI'] = name_dict[SEDS_com[iyear].loc[istate,'State'].strip()]\n",
    "        sedsind_coal_state[matchstate,iyear] += SEDS_ind[iyear].loc[istate,'Coal'] #/Coal_sum[iyear] \n",
    "        sedsind_oil_state[matchstate,iyear] += SEDS_ind[iyear].loc[istate,'Total Petroleum'] #/Fuel_sum[iyear]\n",
    "        sedsind_gas_state[matchstate,iyear] += SEDS_ind[iyear].loc[istate,'Natural Gas'] #/NGas_sum[iyear]\n",
    "        sedsind_wood_state[matchstate,iyear] += SEDS_ind[iyear].loc[istate,'Wood and Waste'] #/Wood_sum[iyear]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.4. Read In GHGRP Subpart C and D Data (Industrial Proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.4.1 Read in Subpart C and D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in Subpart D and C facility lists, find the subpart C facilities that were not in subpart D list\n",
    "# and then merge with subpart D list to create a complete facility information array\n",
    "\n",
    "#facility level data for Subpart C\n",
    "GHGRP_all = pd.read_csv(GHGRP_subCfacility_inputfile) \n",
    "#filter for methane emissions only\n",
    "GHGRP_all = GHGRP_all[GHGRP_all['C_SUBPART_LEVEL_INFORMATION.GHG_GAS_NAME'] == 'Methane']\n",
    "GHGRP_all = GHGRP_all.drop(columns = ['C_SUBPART_LEVEL_INFORMATION.FACILITY_NAME'])\n",
    "GHGRP_all.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#facility level data for subpart D\n",
    "GHGRP_elec = pd.read_csv(GHGRP_subDfacility_inputfile) \n",
    "#filter for methane emissions only\n",
    "GHGRP_elec = GHGRP_elec[GHGRP_elec['D_SUBPART_LEVEL_INFORMATION.GHG_NAME'] == 'Methane']\n",
    "GHGRP_elec = GHGRP_elec.drop(columns = ['D_SUBPART_LEVEL_INFORMATION.FACILITY_NAME'])\n",
    "GHGRP_elec.reset_index(inplace=True, drop=True)\n",
    "GHGRP_elec_fac = np.array(GHGRP_elec['D_SUBPART_LEVEL_INFORMATION.FACILITY_ID'])\n",
    "GHGRP_elec_fac = np.unique(GHGRP_elec_fac)\n",
    "\n",
    "GHGRP_comb_noloc = dict()\n",
    "\n",
    "#make a list of facilities that report to subpart C that are not in the subpart D facility list\n",
    "for iyear in np.arange(0,num_years):\n",
    "    temp = list()\n",
    "    for ifacility in np.arange(len(GHGRP_all)):       \n",
    "        if GHGRP_all.loc[ifacility,'C_SUBPART_LEVEL_INFORMATION.REPORTING_YEAR'] == year_range[iyear]:\n",
    "            if GHGRP_all.loc[ifacility,'C_SUBPART_LEVEL_INFORMATION.FACILITY_ID'] not in GHGRP_elec_fac:\n",
    "                temp.append(GHGRP_all.loc[ifacility])\n",
    "    GHGRP_comb_noloc[iyear] = pd.DataFrame(temp)\n",
    "\n",
    "GHGRP_comb_noloc[0].head(1)\n",
    "\n",
    "# Read Facility Info file that contains lat and lon for GHGRP facilities\n",
    "#extract the reporting facilities for the most recent year\n",
    "GHGRP_facloc = pd.read_csv(GHGRP_subDfacility_loc_inputfile)\n",
    "sort = GHGRP_facloc.sort_values(by=['V_GHG_EMITTER_FACILITIES.YEAR'])\n",
    "filter1 = sort.drop_duplicates(subset = 'V_GHG_EMITTER_FACILITIES.FACILITY_ID' , keep = 'last')\n",
    "filter2 = filter1.drop(columns=['V_GHG_EMITTER_FACILITIES.YEAR','V_GHG_EMITTER_FACILITIES.STATE'])\n",
    "Fac_rename = filter2.rename(columns={'V_GHG_EMITTER_FACILITIES.FACILITY_ID': 'FACILITY_ID'})\n",
    "\n",
    "#merge the missing subpart C facility list with the Subpart D facility list to get lat and lon values for all facilities\n",
    "GHGRP_combfac = dict()\n",
    "for iyear in np.arange(num_years):\n",
    "    Comb_rename = GHGRP_comb_noloc[iyear].rename(columns={'C_SUBPART_LEVEL_INFORMATION.FACILITY_ID': 'FACILITY_ID'})\n",
    "    temp = Comb_rename.merge(Fac_rename, on = 'FACILITY_ID')\n",
    "    GHGRP_combfac[iyear] = temp\n",
    "\n",
    "#Check that no repeats were added\n",
    "for iyear in np.arange(num_years):\n",
    "    if  GHGRP_combfac[iyear].shape[0] != GHGRP_comb_noloc[iyear].shape[0]:\n",
    "        print('Dataframe size discrepancy')\n",
    "\n",
    "display(GHGRP_combfac[0])#.head(1)\n",
    "del Fac_rename, GHGRP_all, GHGRP_elec, GHGRP_elec_fac,GHGRP_facloc,filter1,filter2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.4.2 Allocate facility level methane emissions to the CONUS grid (0.01x0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a 0.1x0.1 gridded map of GHGRP facility-level emissions\n",
    "# also record the emissions that are not within the CONUS grid \n",
    "\n",
    "ghgrp_emi_array = np.zeros([area_map.shape[0],area_map.shape[1], num_years])\n",
    "ghgrp_emi_array_nongrid = np.zeros([num_years])\n",
    "\n",
    "for iyear in np.arange(num_years):\n",
    "    n_plants = 0\n",
    "    for ifacility in np.arange(len(GHGRP_combfac[iyear])):\n",
    "        #Filter inside domain\n",
    "        if GHGRP_combfac[iyear].loc[ifacility,'V_GHG_EMITTER_FACILITIES.LONGITUDE'] > Lon_left \\\n",
    "         and GHGRP_combfac[iyear].loc[ifacility,'V_GHG_EMITTER_FACILITIES.LONGITUDE'] < Lon_right \\\n",
    "         and GHGRP_combfac[iyear].loc[ifacility,'V_GHG_EMITTER_FACILITIES.LATITUDE'] > Lat_low \\\n",
    "         and GHGRP_combfac[iyear].loc[ifacility,'V_GHG_EMITTER_FACILITIES.LATITUDE'] < Lat_up:\n",
    "            #find the corresponding plant ilon and ilat, record the emissions at that location\n",
    "            ilat = int((GHGRP_combfac[iyear].loc[ifacility,'V_GHG_EMITTER_FACILITIES.LATITUDE'] - Lat_low)/Res_01)\n",
    "            ilon = int((GHGRP_combfac[iyear].loc[ifacility,'V_GHG_EMITTER_FACILITIES.LONGITUDE'] - Lon_left)/Res_01)\n",
    "            ghgrp_emi_array[ilat,ilon,iyear] += GHGRP_combfac[iyear].loc[ifacility,'C_SUBPART_LEVEL_INFORMATION.GHG_QUANTITY']\n",
    "            n_plants += 1\n",
    "        else:\n",
    "            ghgrp_emi_array_nongrid[iyear] += GHGRP_combfac[iyear].loc[ifacility,'C_SUBPART_LEVEL_INFORMATION.GHG_QUANTITY']\n",
    "    print (year_range_str[iyear]+' Facilities: ', n_plants)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.5 Read in county-level residential wood thruput (from NEI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load GHGI Mapping Groups\n",
    "names = pd.read_excel(NEI_resi_wood_inputfile, usecols = \"A,D,F:G\", header = 0)\n",
    "colnames = names.columns.values\n",
    "nei_rwc_thruput = pd.read_excel(NEI_resi_wood_inputfile, usecols = \"A,D,F:G\", \\\n",
    "                                converters = {\"StateAndCountyFIPSCode\": str},names = colnames)\n",
    "nei_rwc_thruput.rename(columns={'StateAndCountyFIPSCode':'FIPS','SourceClassificationCode':'SCC'},inplace=True)\n",
    "#remove wax logs - per recommendation from Rich Mason\n",
    "#also select all data in units of TON\n",
    "nei_rwc_thruput = nei_rwc_thruput.loc[nei_rwc_thruput['SCC'] != 2104009000]\n",
    "nei_rwc_thruput = nei_rwc_thruput.loc[nei_rwc_thruput['ThroughputUnit'] == 'TON']\n",
    "nei_rwc_thruput.reset_index(inplace=True, drop=True)\n",
    "display(nei_rwc_thruput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "county_rwc_thruput = np.zeros([len(State_ANSI),len(County_ANSI),num_years])\n",
    "\n",
    "for irow in np.arange(0, len(nei_rwc_thruput)):\n",
    "    county_str = nei_rwc_thruput['FIPS'][irow][2:5].lstrip(\"0\")\n",
    "    state_str = int(nei_rwc_thruput['FIPS'][irow][0:2].lstrip(\"0\"))\n",
    "    #print(state_str)\n",
    "    #print(county_str)\n",
    "    matchstate = np.where(state_str == State_ANSI['ansi'])[0]\n",
    "    #print(matchstate)\n",
    "    matchcounty = np.where((County_ANSI['State']==int(state_str)) &\\\n",
    "                           (County_ANSI['County'] ==int(county_str)))[0]\n",
    "    #print(matchcounty)\n",
    "    if len(matchcounty) != 1:\n",
    "        #print(county_str, matchcounty, state_str)\n",
    "        if state_str ==2 and county_str == '63': #if AK, just assign to nearest approx. region\n",
    "            county_str = '20'\n",
    "            #print(county_str)\n",
    "        elif state_str ==2 and county_str == '66': #if AK, just assign to nearest approx. region\n",
    "            county_str = '261'\n",
    "            #print(county_str)\n",
    "        elif state_str ==2 and county_str == '158': #if AK, just assign to nearest approx. region\n",
    "            county_str = '60'\n",
    "            #print(county_str)\n",
    "        elif state_str ==46 and county_str == '102': #Shannon county renamed in 2015 and has new FIPS code (new:102)\n",
    "            county_str = '113'\n",
    "            #print(county_str)\n",
    "    else:\n",
    "        county_rwc_thruput[matchstate[0],matchcounty[0],:] += nei_rwc_thruput['Throughput'][irow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.6 Read in gridded population density data and calculate state-level populations, regrid to 0.1x0.1 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read population density map\n",
    "pop_den_map = data_load_fn.load_pop_den_map(pop_map_inputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "## Step 3. Read in and Format US EPA GHGI Emissions\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1. Read in the GHGI data (in kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read stationary combustion emissions (units = kt)\n",
    "# For electricity, industrial, commercial, residential, and total sources.\n",
    "# Total emissions include U.S. territories, while the sum of combustion sources do not. \n",
    "\n",
    "names = pd.read_csv(EPA_stat_inputfile,  skiprows = 2, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "EPA_emi_statcomb_CH4 = pd.read_csv(EPA_stat_inputfile, skiprows = 3, names = colnames, nrows = 26)\n",
    "EPA_emi_statcomb_CH4 = EPA_emi_statcomb_CH4.fillna('')\n",
    "EPA_emi_statcomb_CH4 = EPA_emi_statcomb_CH4.drop(columns = [str(n) for n in range(1990, start_year,1)])\n",
    "EPA_emi_statcomb_CH4.reset_index(inplace=True, drop=True)\n",
    "EPA_statcom_total = EPA_emi_statcomb_CH4[EPA_emi_statcomb_CH4['Sector/Fuel Type'] == 'Total']\n",
    "\n",
    "##DEBUG## print('EPA GHGI Emissions (kt)')\n",
    "##DEBUG## display(EPA_emi_statcomb_CH4)\n",
    "##DEBUG## display(EPA_statcom_total)\n",
    "\n",
    "display(EPA_emi_statcomb_CH4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Split Emissions into Gridding Groups (each Group will have the same proxy applied during the state allocation/gridding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_year_idx = EPA_emi_statcomb_CH4.columns.get_loc(str(start_year))\n",
    "end_year_idx = EPA_emi_statcomb_CH4.columns.get_loc(str(end_year))+1\n",
    "ghgi_stat_groups = ghgi_stat_map['GHGI_Emi_Group'].unique()\n",
    "sum_emi = np.zeros([num_years])\n",
    "\n",
    "DEBUG =1\n",
    "\n",
    "for igroup in np.arange(0,len(ghgi_stat_groups)): #loop through all groups, finding the GHGI sources in that group and summing emissions for that region, year        vars()[ghgi_prod_groups[igroup]] = np.zeros([num_regions-1,num_years])\n",
    "    ##DEBUG## print(ghgi_stat_groups[igroup])\n",
    "    vars()[ghgi_stat_groups[igroup]] = np.zeros([num_years])\n",
    "    source_temp = ghgi_stat_map.loc[ghgi_stat_map['GHGI_Emi_Group'] == ghgi_stat_groups[igroup], 'GHGI_Source']\n",
    "    pattern_temp  = '|'.join(source_temp) \n",
    "    ##DEBUG## display(pattern_temp)\n",
    "    if 'elec' in ghgi_stat_groups[igroup]:\n",
    "        isector = EPA_emi_statcomb_CH4.index[EPA_emi_statcomb_CH4['Sector/Fuel Type'].str.contains('Electric Power')][0]            \n",
    "    elif 'ind' in ghgi_stat_groups[igroup]: \n",
    "        isector = EPA_emi_statcomb_CH4.index[EPA_emi_statcomb_CH4['Sector/Fuel Type'].str.contains('Industrial')][0]\n",
    "    elif 'com' in ghgi_stat_groups[igroup]:    \n",
    "        isector = EPA_emi_statcomb_CH4.index[EPA_emi_statcomb_CH4['Sector/Fuel Type'].str.contains('Commercial/Institutional')][0]\n",
    "    elif 'res' in ghgi_stat_groups[igroup]:    \n",
    "        isector = EPA_emi_statcomb_CH4.index[EPA_emi_statcomb_CH4['Sector/Fuel Type'].str.contains('Residential')][0]  \n",
    "    elif 'not' in ghgi_stat_groups[igroup]:\n",
    "        isector = EPA_emi_statcomb_CH4.index[EPA_emi_statcomb_CH4['Sector/Fuel Type'].str.contains('U.S. Territories')][0] \n",
    "    EPA_emi_stat_temp = EPA_emi_statcomb_CH4.loc[isector+1:isector+5,] \n",
    "    emi_temp = EPA_emi_stat_temp[EPA_emi_stat_temp['Sector/Fuel Type'].str.contains(pattern_temp)]\n",
    "    ##DEBUG## display(emi_temp)\n",
    "    vars()[ghgi_stat_groups[igroup]][:] = emi_temp.iloc[:,start_year_idx:].sum()\n",
    "    ##DEBUG## display(vars()[ghgi_stat_groups[igroup]][:])\n",
    "        \n",
    "        \n",
    "#Check against total summary emissions \n",
    "print('QA/QC #1: Check Processing Emission Sum against GHGI Summary Emissions')\n",
    "for iyear in np.arange(0,num_years): \n",
    "    for igroup in np.arange(0,len(ghgi_stat_groups)):\n",
    "        sum_emi[iyear] += vars()[ghgi_stat_groups[igroup]][iyear]\n",
    "        \n",
    "    summary_emi = EPA_statcom_total.iloc[0,iyear+1]  \n",
    "    #Check 1 - make sure that the sums from all the regions equal the totals reported\n",
    "    diff1 = abs(sum_emi[iyear] - summary_emi)/((sum_emi[iyear] + summary_emi)/2)\n",
    "    if DEBUG ==1:\n",
    "        print(summary_emi)\n",
    "        print(sum_emi[iyear])\n",
    "    if diff1 < 0.0001:\n",
    "        print('Year ', year_range[iyear],': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear],': FAIL (check Production & summary tabs): ', diff1,'%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Step 4. Grid Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1. Allocate emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.1.1 Assign the Appropriate Proxy Variable Names (state & grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names on the *left* need to match the 'Stationary_ProxyMapping' 'State_Proxy_Group' names \n",
    "# (these are initialized in Step 2). \n",
    "# The names on the *right* are the variable names used to caluclate the proxies in this code.\n",
    "# Names on the right need to match those from the code in Step 2\n",
    "\n",
    "#national --> state proxies (state x year X month)\n",
    "state_indu_coal = sedsind_coal_state\n",
    "state_indu_wood = sedsind_wood_state\n",
    "state_indu_oil= sedsind_oil_state\n",
    "state_indu_gas= sedsind_gas_state\n",
    "state_resi_coal = sedsres_coal_state\n",
    "state_resi_wood = sedsres_wood_state\n",
    "state_resi_oil = sedsres_oil_state\n",
    "state_resi_gas = sedsres_gas_state\n",
    "state_comm_coal = sedscom_coal_state\n",
    "state_comm_wood= sedscom_wood_state\n",
    "state_comm_oil= sedscom_oil_state\n",
    "state_comm_gas= sedscom_gas_state\n",
    "\n",
    "#state --> county proxies (stat X county X year (X month))\n",
    "county_resi_wood = county_rwc_thruput\n",
    "\n",
    "#national --> grid proxies (0.1x0.1)\n",
    "Map_elec_wood = arp_wood_array \n",
    "Map_elec_wood_nongrid = arp_wood_array_nongrid\n",
    "Map_elec_coal = arp_coal_array\n",
    "Map_elec_coal_nongrid = arp_coal_array_nongrid \n",
    "Map_elec_oil = arp_oil_array\n",
    "Map_elec_oil_nongrid = arp_oil_array_nongrid\n",
    "Map_elec_gas = arp_gas_array\n",
    "Map_elec_gas_nongrid = arp_gas_array_nongrid\n",
    "\n",
    "#state --> grid proxies (0.01x0.01)\n",
    "Map_indu = ghgrp_emi_array\n",
    "Map_indu_nongrid = ghgrp_emi_array_nongrid\n",
    "Map_population = np.zeros([area_map.shape[0], area_map.shape[1], num_years])\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    Map_population[:,:,iyear] = pop_den_map*area_map\n",
    "    \n",
    "    \n",
    "\n",
    "# remove variables to clear space for larger arrays \n",
    "#del sedsind_coal_state,sedsind_wood_state,sedsind_oil_state,sedsind_gas_state,sedsres_coal_state,sedsres_wood_state\n",
    "#del sedsres_oil_state,sedsres_gas_state,sedscom_coal_state,sedscom_wood_state,sedscom_oil_state,sedscom_gas_state\n",
    "#del arp_wood_array,arp_wood_array_nongrid,arp_coal_array,arp_coal_array_nongrid,arp_oil_array,arp_oil_array_nongrid\n",
    "#del arp_gas_array,arp_gas_array_nongrid,ghgrp_emi_array,ghgrp_emi_array_nongrid,pop_den_map\n",
    "#del county_rwc_thruput"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# extra code - save state, county, and gridded proxies for each emission group\n",
    "\n",
    "#1) State\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(state_proxy_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "unique_proxies = np.unique(proxy_stat_map['State_Proxy_Group'])\n",
    "unique_proxies = unique_proxies[unique_proxies != '-']\n",
    "#print(unique_proxies)\n",
    "\n",
    "nc_out = Dataset(state_proxy_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.createDimension('state', len(State_ANSI))\n",
    "\n",
    "for iproxy in np.arange(0,len(unique_proxies)):\n",
    "    print(unique_proxies[iproxy])\n",
    "    if len(np.shape(vars()[unique_proxies[iproxy]])) ==3:\n",
    "        #print(np.sum(vars()[unique_grid_proxies[iproxy]]))\n",
    "        proxy_temp = np.sum(vars()[unique_proxies[iproxy]],axis=2) #sum month data\n",
    "        #print(np.shape(proxy_temp))\n",
    "        #print(np.sum(proxy_temp))\n",
    "    else:\n",
    "        proxy_temp = vars()[unique_proxies[iproxy]]\n",
    "        #print(np.shape(proxy_temp))\n",
    "    \n",
    "    #print(np.shape(proxy_temp))\n",
    "    # Write data to netCDF\n",
    "    data_out = nc_out.createVariable(unique_proxies[iproxy], 'f8', ('state', 'year'), zlib=True)\n",
    "    #print(nc_out)\n",
    "    data_out[:,:] = proxy_temp\n",
    "\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"State stationary combustion proxies written to file: {}\" .format(os.getcwd())+state_proxy_outputfile)\n",
    "\n",
    "\n",
    "\n",
    "#2) County\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(county_proxy_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "unique_proxies = np.unique(proxy_stat_map['County_Proxy_Group'])\n",
    "unique_proxies = unique_proxies[unique_proxies != '-']\n",
    "#print(unique_proxies)\n",
    "\n",
    "nc_out = Dataset(county_proxy_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.createDimension('state', len(State_ANSI))\n",
    "nc_out.createDimension('county', len(County_ANSI))\n",
    "\n",
    "for iproxy in np.arange(0,len(unique_proxies)):\n",
    "    print(unique_proxies[iproxy])\n",
    "    if len(np.shape(vars()[unique_proxies[iproxy]])) ==4:\n",
    "        #print(np.sum(vars()[unique_grid_proxies[iproxy]]))\n",
    "        proxy_temp = np.sum(vars()[unique_proxies[iproxy]],axis=3) #sum month data\n",
    "    else:\n",
    "        proxy_temp = vars()[unique_proxies[iproxy]]\n",
    "\n",
    "    #print(np.shape(proxy_temp))\n",
    "    # Write data to netCDF\n",
    "    data_out = nc_out.createVariable(unique_proxies[iproxy], 'f8', ('state','county','year'), zlib=True)\n",
    "    data_out[:,:,:] = proxy_temp\n",
    "\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"State stationary combustion proxies written to file: {}\" .format(os.getcwd())+county_proxy_outputfile)\n",
    "\n",
    "\n",
    "\n",
    "#3) Grid\n",
    "\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(gridded_proxy_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "unique_grid_proxies = np.unique(proxy_stat_map['Proxy_Group'])\n",
    "#print(unique_grid_proxies)\n",
    "\n",
    "#nc_out = Dataset(gridded_proxy_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out = Dataset(gridded_proxy_outputfile, 'r+', format='NETCDF4')\n",
    "\n",
    "for iproxy in np.arange(0,len(unique_grid_proxies)):\n",
    "    print(unique_grid_proxies[iproxy])\n",
    "    if len(np.shape(vars()[unique_grid_proxies[iproxy]])) ==4:\n",
    "        #print(np.sum(vars()[unique_grid_proxies[iproxy]]))\n",
    "        proxy_temp = np.sum(vars()[unique_grid_proxies[iproxy]],axis=3) #sum month data\n",
    "        #print(np.shape(proxy_temp))\n",
    "        #print(np.sum(proxy_temp))\n",
    "    else:\n",
    "        proxy_temp = vars()[unique_grid_proxies[iproxy]]\n",
    "        #print(np.shape(proxy_temp))\n",
    "        \n",
    "    if np.shape(proxy_temp)[0] ==3500:\n",
    "        proxy_temp2 = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            proxy_temp2[:,:,iyear] = data_fn.regrid001_to_01(proxy_temp[:,:,iyear], Lat_01, Lon_01)\n",
    "        proxy_temp = proxy_temp2.copy()\n",
    "        #print(np.shape(proxy_temp))\n",
    "    \n",
    "    # Write data to netCDF\n",
    "    data_out = nc_out.createVariable(unique_grid_proxies[iproxy], 'f8', ('lat', 'lon', 'year'), zlib=True)\n",
    "    data_out[:,:,:] = proxy_temp\n",
    "    #print(np.sum(data_out))\n",
    "    #nc_out.close()\n",
    "\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded stationary combustion proxies written to file: {}\" .format(os.getcwd())+gridded_proxy_outputfile)\n",
    "\n",
    "del proxy_temp, proxy_temp2, data_out, unique_grid_proxies, unique_proxies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.1.2 Allocate National EPA Emissions to the State-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate state-level emissions for commencial, residential, and industrial sectors\n",
    "# Emissions in kt\n",
    "# State data = national GHGI emissions * state proxy/national total\n",
    "\n",
    "\n",
    "# Note that national emissions are retained for groups that do not have state proxies (identified in the mapping file)\n",
    "# and are gridded in the next step\n",
    "DEBUG =1\n",
    "\n",
    "# Make placeholder emission arrays for each group\n",
    "for igroup in np.arange(0,len(proxy_stat_map)):\n",
    "    vars()['State_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(State_ANSI),num_years,num_months])\n",
    "    vars()['NonState_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])\n",
    "        \n",
    "#Loop over years\n",
    "for iyear in np.arange(num_years):\n",
    "    #Loop over states\n",
    "    for istate in np.arange(len(State_ANSI)):\n",
    "        for igroup in np.arange(0,len(proxy_stat_map)):    \n",
    "            if proxy_stat_map.loc[igroup,'State_Proxy_Group'] != '-' and proxy_stat_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "                if proxy_stat_map.loc[igroup,'State_Month_Flag'] ==1:\n",
    "                    for imonth in np.arange(0,num_months):\n",
    "                        vars()['State_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][istate,iyear,imonth] = vars()[proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][iyear]* \\\n",
    "                            data_fn.safe_div(vars()[proxy_stat_map.loc[igroup,'State_Proxy_Group']][istate,iyear,imonth], np.sum(vars()[proxy_stat_map.loc[igroup,'State_Proxy_Group']][:,iyear,:]))   \n",
    "                else:\n",
    "                    for imonth in np.arange(0,num_months):\n",
    "                        vars()['State_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][istate,iyear,imonth] = (1/12) * vars()[proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                            data_fn.safe_div(vars()[proxy_stat_map.loc[igroup,'State_Proxy_Group']][istate,iyear], np.sum(vars()[proxy_stat_map.loc[igroup,'State_Proxy_Group']][:,iyear]))\n",
    "            else:\n",
    "                vars()['NonState_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][iyear] = vars()[proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "                \n",
    "# Check sum of all gridded emissions + emissions not included in state allocation\n",
    "print('QA/QC #1: Check weighted emissions against GHGI')   \n",
    "for iyear in np.arange(0,num_years):\n",
    "    summary_emi = EPA_statcom_total.iloc[0,iyear+1] \n",
    "    calc_emi = 0\n",
    "    for igroup in np.arange(0,len(proxy_stat_map)):\n",
    "        calc_emi +=  np.sum(vars()['State_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][:,iyear,:])+\\\n",
    "            vars()['NonState_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][iyear] #np.sum(Emissions[:,iyear]) + Emissions_nongrid[iyear] + Emissions_nonstate[iyear]\n",
    "    if DEBUG ==1:\n",
    "        print(summary_emi)\n",
    "        print(calc_emi)\n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0002:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del state_indu_coal,state_indu_wood ,state_indu_oil,state_indu_gas,state_resi_coal,state_resi_wood,state_resi_oil\n",
    "del state_resi_gas,state_comm_coal,state_comm_wood,state_comm_oil,state_comm_gas, pop_den_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.3 4.1.3 Allocate emissions to the county level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate county-level emissions (kt)\n",
    "# Emissions in kt\n",
    "# County data = state emissions * county proxy /state total\n",
    "\n",
    "# If there are emissions in a state but no proxy data (e.g., wood thruput) available in the entire state, \n",
    "# emissions are allocated within that state by relative county areas \n",
    "\n",
    "DEBUG = 1\n",
    "\n",
    "# Note that national emissions are retained for groups that do not have state or county proxies (identified in the mapping file)\n",
    "# and are gridded in the next step\n",
    "\n",
    "# Make placeholder emission arrays for each group\n",
    "for igroup in np.arange(0,len(proxy_stat_map)):\n",
    "    #if proxy_rice_map.loc[igroup,'State_Month_Flag'] ==1:\n",
    "    vars()['County_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']] = \\\n",
    "            np.zeros([len(State_ANSI),len(County_ANSI),num_years,num_months])\n",
    "    #else:\n",
    "    #    vars()['State_'+proxy_rice_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(State_ANSI),num_years])\n",
    "    vars()['NonCounty_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])\n",
    "        \n",
    "#Loop over years\n",
    "for iyear in np.arange(0,num_years):\n",
    "    running_sum = np.zeros([num_years])\n",
    "    \n",
    "    for igroup in np.arange(0,len(proxy_stat_map)): \n",
    "        #print(igroup)\n",
    "        if proxy_stat_map.loc[igroup,'County_Proxy_Group'] != '-' and proxy_stat_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "            #print(proxy_stat_map.loc[igroup,'County_Proxy_Group'])\n",
    "            \n",
    "            for icounty in np.arange(0,len(County_ANSI)):\n",
    "                istate = np.where(State_ANSI['ansi']==County_ANSI['State'][icounty])[0][0]\n",
    "                state_ansi = State_ANSI['ansi'][istate]\n",
    "                #print(icounty, istate)            \n",
    "                emi_temp = vars()['State_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][istate,iyear,:]\n",
    "                frac_temp = data_fn.safe_div(vars()[proxy_stat_map.loc[igroup,'County_Proxy_Group']][istate,icounty,iyear], \\\n",
    "                            np.sum(vars()[proxy_stat_map.loc[igroup,'County_Proxy_Group']][istate,:,iyear]))\n",
    "                #print(np.sum(emi_temp))\n",
    "                for imonth in np.arange(0,num_months):\n",
    "                    if np.sum(emi_temp) > 0 and frac_temp > 0:\n",
    "                        vars()['County_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][istate,icounty,iyear,imonth] = emi_temp[imonth] * frac_temp\n",
    "                    elif np.sum(emi_temp) > 0 and np.sum(vars()[proxy_stat_map.loc[igroup,'County_Proxy_Group']][istate,:,iyear]) == 0:\n",
    "                        frac_temp = data_fn.safe_div(County_ANSI.loc[icounty,'Area'],np.sum(County_ANSI['Area'][County_ANSI['State'] == state_ansi]))\n",
    "                        vars()['County_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][istate,icounty,iyear,imonth] = emi_temp[imonth] * frac_temp  \n",
    "        \n",
    "        else: #add data not allocated to county\n",
    "            if proxy_stat_map.loc[igroup,'State_Proxy_Group'] != '-'and proxy_stat_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "                running_sum[iyear] += (np.sum(vars()['State_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][:,iyear,:]))\n",
    "            else:\n",
    "                running_sum[iyear] += vars()[proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "\n",
    "                \n",
    "    vars()['NonCounty_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][iyear] = running_sum[iyear]\n",
    "\n",
    "# Check sum of all gridded emissions + emissions not included in state allocation\n",
    "print('QA/QC #1: Check weighted emissions against GHGI')   \n",
    "for iyear in np.arange(0,num_years):\n",
    "    summary_emi = EPA_statcom_total.iloc[0,iyear+1] \n",
    "    calc_emi = 0\n",
    "    for igroup in np.arange(0,len(proxy_stat_map)):\n",
    "        calc_emi +=  np.sum(vars()['County_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,:])#+\\    \n",
    "    calc_emi += vars()['NonCounty_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "    \n",
    "    if DEBUG ==1:\n",
    "        print(summary_emi)\n",
    "        print(calc_emi)\n",
    "        #print(running_sum)\n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0001:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.4 Allocate emissions to the CONUS region (0.1x0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To speed up the code, this notebook does not loop through each county, but instead loops through\n",
    "# each lat/lon value in the CONUS region. Emissions are allocated based on the fraction of \n",
    "# the proxy that is in each grid cell relative to the total in that county. \n",
    "# Since the code is not using county masks, the sum of each proxy for each county/state pair\n",
    "# must first be calcualted. \n",
    "# This chunk calculates the county totals for county-level proxy group\n",
    "\n",
    "\n",
    "#for each group that was allocated to county level,...\n",
    "#For each grid box that falls within the continental US geographic bounds, keep a running sum of grid proxy to calculate \n",
    "# the total proxy value within each state and county. \n",
    "for igroup in np.arange(0,len(proxy_stat_map)):\n",
    "    if proxy_stat_map.loc[igroup,'County_Proxy_Group'] != '-' and \\\n",
    "            proxy_stat_map.loc[igroup,'County_Proxy_Group'] != 'county_not_mapped':\n",
    "        vars()[proxy_stat_map.loc[igroup,'Proxy_Group']+'_countysum'] = np.zeros([len(State_ANSI),len(County_ANSI),num_years])\n",
    "        print(proxy_stat_map.loc[igroup,'Proxy_Group'])\n",
    "        for ilat in np.arange(0, len(lat001)):\n",
    "            print(ilat, 'of',len(lat001))\n",
    "            for ilon in np.arange(0, len(lon001)):\n",
    "                if state_ANSI_map[ilat,ilon] > 0: #only includes CONUS region\n",
    "                    istate = np.where(State_ANSI['ansi']==state_ANSI_map[ilat,ilon])[0][0]\n",
    "                    icounty = np.where((County_ANSI['State']==state_ANSI_map[ilat,ilon]) & \\\n",
    "                                    (County_ANSI['County']==county_ANSI_map[ilat,ilon]))[0][0]\n",
    "                    #Area_sum[istate,icounty] += area_map[ilat,ilon]\n",
    "                    #for iyear in np.arange(0, num_years):\n",
    "                    vars()[proxy_stat_map.loc[igroup,'Proxy_Group']+'_countysum'][istate,icounty,:] += \\\n",
    "                        vars()[proxy_stat_map.loc[igroup,'Proxy_Group']][ilat,ilon,:]\n",
    "            print(np.sum(vars()[proxy_stat_map.loc[igroup,'Proxy_Group']+'_countysum'][:,:,0]))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Allocate State-Level emissions (kt) onto a 0.1x0.1 grid using gridcell level 'Proxy_Groups'\n",
    "\n",
    "#Define emission arrays\n",
    "#Emissions_array = np.zeros([area_map.shape[0],area_map.shape[1],num_years,num_months])\n",
    "Emissions_array_01 = np.zeros([len(Lat_01),len(Lon_01),num_years, num_months])\n",
    "Emissions_nongrid = np.zeros([num_years])\n",
    "month_days = np.zeros([num_years, num_months])\n",
    "\n",
    "DEBUG=1 \n",
    "# For each year, (2a) distribute county-level emissions onto a grid using proxies defined above ....\n",
    "# To speed up the code, masks are used rather than looping individually through each lat/lon (for state gridding only). \n",
    "# In this case, a mask of 1's is made for the grid cells that match the ANSI values for a given state\n",
    "# The masked values are set to zero, remaining values = 1. \n",
    "# For each year, (2b), if emission groups have been previously allocated to the state-level, then allocate to grid\n",
    "# AK and HI and territories are removed from the analysis at this stage. \n",
    "# The final emissions allocated to the grid are at 0.01x0.01 degree resolution, as required to calculate accurate 'mask'\n",
    "# arrays for each state. \n",
    "######Emission arrays are re-gridded to 0.1x0.1 degrees as looping through monthly high-resolution\n",
    "# grids was prohibitively slow\n",
    "# (2c) For emission groups that were not first allocated to states, national emissions for those groups are gridded\n",
    "# based on the relevant gridded proxy arrays (0.1x0.1 resolution). These emissions are at 0.1x0.1 degrees resolution. \n",
    "# (2d ) - record 'not mapped' emission groups in the 'non-grid' array\n",
    "\n",
    "\n",
    "print('**QA/QC Check: Sum of national gridded emissions vs. GHGI national emissions')\n",
    "running_sum = np.zeros([len(proxy_stat_map),num_years])\n",
    "#running_sum2 = np.zeros([len(proxy_stat_map),num_years])\n",
    "for igroup in np.arange(len(proxy_stat_map)):\n",
    "    vars()['Ext_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "\n",
    "\n",
    "#first calculate the number of days in each month and year (to weight proxy)\n",
    "for iyear in np.arange(0,num_years):\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        #year_days = np.sum(month_day_leap)\n",
    "        month_days[iyear,:] = month_day_leap\n",
    "    else:\n",
    "        #year_days = np.sum(month_day_nonleap)\n",
    "        month_days[iyear,:] = month_day_nonleap \n",
    "    #running_count = 0\n",
    "    \n",
    "#1. Step through each gridding group\n",
    "for igroup in np.arange(0,len(proxy_stat_map)):\n",
    "    print(igroup, 'of',len(proxy_stat_map))\n",
    "    # 1. weight proxy by the number of days in each month (depending on whether proxy has month res or not)\n",
    "    proxy_temp = vars()[proxy_stat_map.loc[igroup,'Proxy_Group']].copy()\n",
    "    proxy_temp_nongrid = vars()[proxy_stat_map.loc[igroup,'Proxy_Group']+'_nongrid'].copy()\n",
    "    if proxy_stat_map.loc[igroup,'Grid_Month_Flag'] ==1:\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            for imonth in np.arange(0, num_months):\n",
    "                proxy_temp[:,:,iyear,imonth] *= month_days[iyear,imonth]\n",
    "                proxy_temp_nongrid[iyear,imonth] *= month_days[iyear,imonth]\n",
    "    else:\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            proxy_temp[:,:,iyear] *= np.sum(month_days[iyear,:])\n",
    "            proxy_temp_nongrid[iyear] *= np.sum(month_days[iyear,:])\n",
    "        ##DEBUG## print(\"group \" + str(igroup) +' of '+ str(len(proxy_stat_map)))\n",
    "        \n",
    "        #2a. first check if allocated to county level, step through each county...\n",
    "    if proxy_stat_map.loc[igroup,'County_Proxy_Group'] != '-' and proxy_stat_map.loc[igroup,'County_Proxy_Group'] != 'county_not_mapped':\n",
    "        \n",
    "        ##****\n",
    "        #proxy_temp = Map_animal_area_rank\n",
    "        #proxy_temp_nongrid = Map_animal_area_rank_nongrid\n",
    "        #calculated in script above\n",
    "        proxy_temp = vars()[proxy_stat_map.loc[igroup,'Proxy_Group']].copy()\n",
    "        proxy_temp_sum = vars()[proxy_stat_map.loc[igroup,'Proxy_Group']+'_countysum'].copy()\n",
    "        emi_temp = np.zeros([len(lat001),len(lon001),num_years])\n",
    "        #area_map_sum = Area_sum\n",
    "        \n",
    "        for ilat in np.arange(0,len(lat001)):\n",
    "            print(ilat, 'of',len(lat001))\n",
    "            for ilon in np.arange(0,len(lon001)):\n",
    "                if state_ANSI_map[ilat,ilon] > 0:\n",
    "                    istate = np.where(State_ANSI['ansi']==state_ANSI_map[ilat,ilon])[0][0]\n",
    "                    icounty = np.where((County_ANSI['State']==state_ANSI_map[ilat,ilon]) & \\\n",
    "                                    (County_ANSI['County']==county_ANSI_map[ilat,ilon]))[0][0]\n",
    "                    county_temp = vars()['County_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][istate,icounty,:,:]\n",
    "                    if np.sum(county_temp) > 0:\n",
    "                        for iyear in np.arange(0,num_years):\n",
    "                            if np.sum(proxy_temp_sum[istate,icounty,iyear]) >0: # if there is proxy data in the county, allocate by that proxy in each grid cell relative to county sum\n",
    "                                weighted_array = data_fn.safe_div(proxy_temp[ilat,ilon,iyear],\\\n",
    "                                                          proxy_temp_sum[istate,icounty,iyear]) #counts at grid cell/counts in county\n",
    "                                emi_temp[ilat,ilon,iyear] += np.sum(county_temp[iyear,:])*weighted_array\n",
    "                                running_sum[igroup,iyear] += np.sum(weighted_array*county_temp[iyear,:])\n",
    "                                \n",
    "                            elif proxy_temp_sum[istate,icounty,iyear] == 0: # if no proxy data in county, #FLAG## use relative area as proxy\n",
    "                                print('check',istate,icounty)\n",
    "\n",
    "            print(running_sum[igroup,0])\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            emi_01 = data_fn.regrid001_to_01(emi_temp[:,:,iyear],Lat_01, Lon_01)\n",
    "            for imonth in np.arange(0,num_months):\n",
    "                Emissions_array_01[:,:,iyear,imonth]+= (1/12)*emi_01\n",
    "            vars()['Ext_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += emi_01\n",
    "            Emissions_nongrid[iyear] += np.sum(vars()['County_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])- np.sum(emi_01) \n",
    "            \n",
    "            print(Emissions_nongrid[iyear])\n",
    "            print(igroup, np.sum(Emissions_array_01[:,:,iyear,:]))\n",
    "                \n",
    "        \n",
    "    #2b.if instead allocated to state-level, Step through each state (if group was previously allocated to state level)\n",
    "    elif proxy_stat_map.loc[igroup,'State_Proxy_Group'] != '-' and proxy_stat_map.loc[igroup,'State_Proxy_Group'] != 'state_not_mapped':\n",
    "        for istate in np.arange(0,len(State_ANSI)):\n",
    "            mask_state = np.ma.ones(np.shape(state_ANSI_map))\n",
    "            mask_state = np.ma.masked_where(state_ANSI_map != State_ANSI['ansi'][istate], mask_state)\n",
    "            mask_state = np.ma.filled(mask_state,0)   \n",
    "            ##DEBUG## print(\"state \" + str(istate) +' of '+ str(len(State_ANSI)))\n",
    "            state_temp = vars()['State_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][istate,:,:]\n",
    "            for iyear in np.arange(0,num_years):\n",
    "                if State_ANSI['abbr'][istate] not in {'AK','HI'} and istate < 51 : \n",
    "                    if proxy_stat_map.loc[igroup, 'Grid_Month_Flag'] == 1:\n",
    "                        for imonth in np.arange(0,num_months):\n",
    "                            if np.sum(mask_state*proxy_temp[:,:,iyear,imonth]) > 0:\n",
    "                                # if state is on grid and proxy for that state is non-zero\n",
    "                                weighted_array = data_fn.safe_div(mask_state*proxy_temp[:,:,iyear,imonth], np.sum(mask_state*proxy_temp[:,:,iyear,imonth]))\n",
    "                                weighted_array_01 = data_fn.regrid001_to_01(weighted_array, Lat_01, Lon_01)\n",
    "                                emi_temp = state_temp[iyear,imonth]*weighted_array_01\n",
    "                                Emissions_array_01[:,:,iyear,imonth] += emi_temp\n",
    "                                vars()['Ext_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += emi_01\n",
    "                            else:\n",
    "                                #for imonth in np.arange(0,num_months):\n",
    "                                Emissions_nongrid[iyear] += state_temp[iyear,imonth]\n",
    "                    \n",
    "                    else:\n",
    "                        if np.sum(mask_state*proxy_temp[:,:,iyear]) > 0 and State_ANSI['abbr'][istate] not in {'AK','HI'} and istate < 51: \n",
    "                            weighted_array = data_fn.safe_div(mask_state*proxy_temp[:,:,iyear], np.sum(mask_state*proxy_temp[:,:,iyear]))\n",
    "                            weighted_array_01 = data_fn.regrid001_to_01(weighted_array, Lat_01, Lon_01)\n",
    "                            for imonth in np.arange(0,num_months):\n",
    "                                emi_temp = state_temp[iyear,imonth]*weighted_array_01\n",
    "                                Emissions_array_01[:,:,iyear,imonth] += emi_temp\n",
    "                                vars()['Ext_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += emi_temp\n",
    "                        else: \n",
    "                            #for imonth in np.arange(0,num_months):\n",
    "                            Emissions_nongrid[iyear] += np.sum(state_temp[iyear,:])\n",
    "                else: \n",
    "                        #for imonth in np.arange(0,num_months):\n",
    "                        Emissions_nongrid[iyear] += np.sum(state_temp[iyear,:])\n",
    "                ##DEBUG## running_count += np.sum(vars()['State_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][istate,iyear,:])\n",
    "                \n",
    "                ##DEBUG## print(running_count)\n",
    "                ##DEBUG## print(np.sum(Emissions_array_01[:,:,iyear,:]) +np.sum(Emissions_nongrid[iyear,:]))\n",
    "        print(igroup, np.sum(Emissions_array_01[:,:,iyear,:]))\n",
    "        print(Emissions_nongrid[iyear])\n",
    "                \n",
    "    #2c. if instead emissions are not allocated to state or county, allocate national total to grid here\n",
    "    elif proxy_stat_map.loc[igroup,'State_Proxy_Group'] == '-':\n",
    "        nat_temp = vars()[proxy_stat_map.loc[igroup,'GHGI_Emi_Group']]\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            if proxy_stat_map.loc[igroup, 'Grid_Month_Flag'] == 1: \n",
    "                temp_sum = np.sum(vars()[proxy_stat_map.loc[igroup,'Proxy_Group']][:,:,iyear,:])+np.sum(vars()[proxy_stat_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear,:])\n",
    "                for imonth in np.arange(0, num_months):\n",
    "                    emi_temp = nat_temp[iyear] * data_fn.safe_div(vars()[proxy_stat_map.loc[igroup,'Proxy_Group']][:,:,iyear,imonth], temp_sum)\n",
    "                    Emissions_array_01[:,:,iyear,imonth] += emi_temp\n",
    "                    vars()['Ext_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += emi_temp\n",
    "                    Emissions_nongrid[iyear] += nat_temp[iyear] * \\\n",
    "                        data_fn.safe_div(vars()[proxy_stat_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear,imonth], temp_sum)\n",
    "            else:\n",
    "                temp_sum = np.sum(vars()[proxy_stat_map.loc[igroup,'Proxy_Group']][:,:,iyear])+np.sum(vars()[proxy_stat_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear])\n",
    "                for imonth in np.arange(0,num_months):\n",
    "                    emi_temp = (1/12) * nat_temp[iyear] * data_fn.safe_div(vars()[proxy_stat_map.loc[igroup,'Proxy_Group']][:,:,iyear], temp_sum)\n",
    "                    Emissions_array_01[:,:,iyear,imonth] += emi_temp\n",
    "                    vars()['Ext_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += emi_temp\n",
    "                    Emissions_nongrid[iyear] += (1/12) * nat_temp[iyear] *\\\n",
    "                        data_fn.safe_div(vars()[proxy_stat_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear], temp_sum)\n",
    "            ##DEBUG## running_count += vars()[proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "        print(igroup, np.sum(Emissions_array_01[:,:,iyear,:]))\n",
    "        print(Emissions_nongrid[iyear])\n",
    "                \n",
    "    #2d. this is the case that GHGI emissions are not mapped (e.g., specified outside of CONUS in the GHGI)\n",
    "    elif proxy_stat_map.loc[igroup,'Proxy_Group'] == 'Map_not_mapped':    \n",
    "        for iyear in np.arange(0,num_years):\n",
    "            #for imonth in np.arange(0,num_months):\n",
    "            Emissions_nongrid[iyear] += vars()[proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "            ##DEBUG## running_count += vars()[proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "        ##DEBUG## print(running_count)\n",
    "        ##DEBUG## print(np.sum(Emissions_array_01[:,:,iyear,:]) +np.sum(Emissions_nongrid[iyear,:]))\n",
    "        print(igroup, np.sum(Emissions_array_01[:,:,iyear,:]))\n",
    "        print(Emissions_nongrid[iyear])\n",
    "            \n",
    "\n",
    "for iyear in np.arange(0,num_years):    \n",
    "    calc_emi = np.sum(Emissions_array_01[:,:,iyear,:]) + np.sum(Emissions_nongrid[iyear]) \n",
    "    summary_emi = EPA_statcom_total.iloc[0,iyear+1] \n",
    "    emi_diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if DEBUG ==1:\n",
    "        print(calc_emi)\n",
    "        print(summary_emi)\n",
    "    if abs(emi_diff) < 0.0001:\n",
    "        print('Year '+ year_range_str[iyear]+': Difference < 0.01%: PASS')\n",
    "    else: \n",
    "        print('Year '+ year_range_str[iyear]+': Difference > 0.01%: FAIL, diff: '+str(emi_diff))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for iyear in np.arange(0,num_years):  \n",
    "    calc_emi = 0\n",
    "    for igroup in np.arange(0,len(proxy_stat_map)):\n",
    "        calc_emi += np.sum(vars()['Ext_'+proxy_stat_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])\n",
    "    calc_emi+= np.sum(Emissions_nongrid[iyear]) \n",
    "    summary_emi = EPA_statcom_total.iloc[0,iyear+1] \n",
    "    emi_diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if DEBUG ==1:\n",
    "        print(calc_emi)\n",
    "        print(summary_emi)\n",
    "    if abs(emi_diff) < 0.0001:\n",
    "        print('Year '+ year_range_str[iyear]+': Difference < 0.01%: PASS')\n",
    "    else: \n",
    "        print('Year '+ year_range_str[iyear]+': Difference > 0.01%: FAIL, diff: '+str(emi_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1.4 Save gridded emissions (kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save gridded emissions for each gridding group - for extension\n",
    "\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(grid_emi_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "unique_groups = np.unique(proxy_stat_map['GHGI_Emi_Group'])\n",
    "unique_groups = unique_groups[unique_groups != 'Emi_not_mapped']\n",
    "\n",
    "nc_out = Dataset(grid_emi_outputfile, 'r+', format='NETCDF4')\n",
    "#nc_out.createDimension('state', len(State_ANSI))\n",
    "\n",
    "for igroup in np.arange(0,len(unique_groups)):\n",
    "    print('Ext_'+unique_groups[igroup])\n",
    "    if len(np.shape(vars()['Ext_'+unique_groups[igroup]])) ==4:\n",
    "        ghgi_temp = np.sum(vars()[unique_groups[igroup]],axis=3) #sum month data if data is monthly\n",
    "    else:\n",
    "        ghgi_temp = vars()['Ext_'+unique_groups[igroup]]\n",
    "\n",
    "    # Write data to netCDF\n",
    "    data_out = nc_out.createVariable('Ext_'+unique_groups[igroup], 'f8', ('lat', 'lon','year'), zlib=True)\n",
    "    data_out[:,:,:] = ghgi_temp[:,:,:]\n",
    "\n",
    "#save nongrid data to calculate non-grid fraction extension\n",
    "data_out = nc_out.createVariable('Emissions_nongrid', 'f8', ('year'), zlib=True)  \n",
    "data_out[:] = Emissions_nongrid[:]\n",
    "nc_out.close()\n",
    "\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded emissions (kt) written to file: {}\" .format(os.getcwd())+grid_emi_outputfile)\n",
    "print(' ')\n",
    "\n",
    "del data_out, ghgi_temp, nc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Calculate Gridded Emission Fluxes (molec./cm2/s) (0.1x0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert emissions to emission flux\n",
    "# conversion: kt emissions to molec/cm2/s flux\n",
    "\n",
    "DEBUG=1\n",
    "\n",
    "Flux_array_01 = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Flux_array_01_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "print('**QA/QC Check: Sum of national gridded emissions vs. GHGI national emissions')\n",
    "  \n",
    "for iyear in np.arange(0,num_years):\n",
    "    calc_emi = 0\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "        month_days = month_day_leap\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        month_days = month_day_nonleap\n",
    "        \n",
    "    for imonth in np.arange(0,num_months):\n",
    "        conversion_factor_01 = 10**9 * Avogadro / float(Molarch4 *month_days[imonth] * 24 * 60 *60) / area_matrix_01\n",
    "        conv_factor2 = month_days[imonth]/year_days\n",
    "        Flux_array_01[:,:,iyear,imonth] = Emissions_array_01[:,:,iyear,imonth]*conversion_factor_01\n",
    "        Flux_array_01_annual[:,:,iyear] += Flux_array_01[:,:,iyear,imonth]*conv_factor2\n",
    "        calc_emi += np.sum(Flux_array_01[:,:,iyear,imonth]/conversion_factor_01)\n",
    "    #convert back to mass to check\n",
    "    conversion_factor_annual = 10**9 * Avogadro / float(Molarch4 *year_days * 24 * 60 *60) / area_matrix_01\n",
    "    calc_emi += np.sum(Emissions_nongrid[iyear])\n",
    "    summary_emi = EPA_statcom_total.iloc[0,iyear+1] \n",
    "    emi_diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if DEBUG==1:\n",
    "        print(calc_emi)\n",
    "        print(summary_emi)\n",
    "    if abs(emi_diff) < 0.0001:\n",
    "        print('Year '+ year_range_str[iyear]+': Difference < 0.01%: PASS')\n",
    "    else: \n",
    "        print('Year '+ year_range_str[iyear]+': Difference > 0.01%: FAIL, diff: '+str(emi_diff))\n",
    "        \n",
    "Flux_Emissions_Total_annual = Flux_array_01_annual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 5. Write netCDF\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly data\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(gridded_month_outputfile, netCDF_description_m, 1, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write data to netCDF\n",
    "nc_out = Dataset(gridded_month_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:,:] = Flux_array_01\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded stationary combustion fluxes written to file: {}\" .format(os.getcwd())+gridded_month_outputfile)\n",
    "\n",
    "# yearly data\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(gridded_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write data to netCDF\n",
    "nc_out = Dataset(gridded_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Total_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded stationary combustion fluxes written to file: {}\" .format(os.getcwd())+gridded_outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Step 6. Plot Gridded Data\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.1. Plot Annual Emission Fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot Annual Data\n",
    "scale_max = 10\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_str, scale_max,save_flag,save_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.2 Plot Difference between first and last inventory year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot difference between last and first year\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_diff_str,save_flag,save_outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = datetime.datetime.now() \n",
    "ft = ct.timestamp() \n",
    "time_elapsed = (ft-it)/(60*60)\n",
    "print('Time to run: '+str(time_elapsed)+' hours')\n",
    "print('** GEPA_1A_Stationary_Combustion: COMPLETE **')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

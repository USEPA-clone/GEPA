{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded EPA Methane Inventory\n",
    "## Category: 3F Field Burning of Agricultural Residues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Authors: \n",
    "Joannes D. Maasakkers, Erin E. McDuffie\n",
    "#### Date Last Updated: \n",
    "see Step 0\n",
    "#### Notebook Purpose: \n",
    "This notebook calculates gridded (0.1⁰x0.1⁰) annual and monthly emission fluxes of methane (molecules CH4/cm2/s) from agricultural field burning activities in the CONUS region for the years 2012 - 2018. Emission fluxes are reported at both annual and monthly time resolution.  \n",
    "#### Summary & Notes:\n",
    "The national EPA GHGI emissions from field burning of agricultural residues are read in from the EPA GHG Inventory workbook. Emissions are available as national totals (for entire time series) and state-level allocations (until 2014). National emissions are allocated to the state level using relative state-level emissions data. State-level emissions are allocated to the 0.01⁰x0.01⁰ grid using county and gridded (annual and monthly) data of crop burning emissions from 2003-2007 from McCarty, 2011. Data are then re-gridded to 0.1⁰x0.1⁰ and converted to fluxes (molecules CH4/cm2/s). Annual and monthly emission fluxes (molecules CH4/cm2/s) are written to final netCDFs in the ‘/code/Final_Gridded_Data/’ folder.  \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Step 0. Set-Up Notebook Modules, Functions, and Local Parameters and Constants\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm working directory\n",
    "import os\n",
    "import time\n",
    "modtime = os.path.getmtime('./3F_Field_Burning.ipynb')\n",
    "modificationTime = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(modtime))\n",
    "print(\"This file was last modified on: \", modificationTime)\n",
    "print('')\n",
    "print(\"The directory we are working in is {}\" .format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from copy import copy\n",
    "\n",
    "# Import additional modules\n",
    "# Load plotting package Basemap \n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# Load netCDF (for manipulating netCDF file types)\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Set up ticker\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#add path for the global function module (file)\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../Global_Functions/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Load Tabula (for reading tables from PDFs)\n",
    "import tabula as tb   \n",
    "    \n",
    "# Load user-defined global functions (modules)\n",
    "import data_load_functions as data_load_fn\n",
    "import data_functions as data_fn\n",
    "import data_IO_functions as data_IO_fn\n",
    "import data_plot_functions as data_plot_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT Files\n",
    "# Assign global file names\n",
    "global_filenames = data_load_fn.load_global_file_names()\n",
    "State_ANSI_inputfile = global_filenames[0]\n",
    "County_ANSI_inputfile = global_filenames[1]\n",
    "pop_map_inputfile = global_filenames[2]\n",
    "Grid_area01_inputfile = global_filenames[3]\n",
    "Grid_area001_inputfile = global_filenames[4]\n",
    "Grid_state001_ansi_inputfile = global_filenames[5]\n",
    "Grid_county001_ansi_inputfile = global_filenames[6]\n",
    "\n",
    "# Specify names of inputs files used in this notebook\n",
    "#EPA Data\n",
    "EPA_burning_inputfile_csv = '../Global_InputData/GHGI/Ch5_Agriculture/Table 5-30.csv'\n",
    "EPA_burning_inputfile = '../Global_InputData/GHGI/Ch5_Agriculture/FBAR_1990-2018_PR_FINAL_2March2020.xlsx'\n",
    "\n",
    "#Proxy Data file\n",
    "Burning_Mapping_inputfile = \"./InputData/FieldBurning_ProxyMapping.xlsx\"\n",
    "\n",
    "#Gridded Crop Data\n",
    "Grid_crop_files = [\"./InputData/2003_ResidueBurning_AllCrops.csv\",\"./InputData/2004_ResidueBurning_AllCrops.csv\",\n",
    "                 \"./InputData/2005_ResidueBurning_AllCrops.csv\",\"./InputData/2006_ResidueBurning_AllCrops.csv\",\n",
    "                 \"./InputData/2007_ResidueBurning_AllCrops.csv\"]\n",
    "\n",
    "#intermediate output file\n",
    "burning_int_out = './IntermediateOutputs/Intermediate_EPA_v2_4F_Field_burning.nc'\n",
    "\n",
    "#OUTPUT FILES\n",
    "gridded_outputfile = '../Final_Gridded_Data/EPA_v2_3F_Field_Burning.nc'\n",
    "netCDF_description = 'Gridded EPA Inventory - Field Burning of Agricultural Residues Emissions - IPCC Source Category 3F'\n",
    "netCDF_description_m = 'Gridded EPA Inventory - Monthly Field Burning of Agricultural Residues Emissions - IPCC Source Category 3F'\n",
    "gridded_month_outputfile = '../Final_Gridded_Data/EPA_v2_3F_Field_Burning_Monthly.nc'\n",
    "title_str = \"EPA methane emissions from the burning of agricultural residues\"\n",
    "title_diff_str = \"Emissions from field burning difference: 2018-2012\"\n",
    "\n",
    "#output gridded proxy data\n",
    "grid_emi_outputfile = '../Final_Gridded_Data/Extension/v2_input_data/Field_Burning_Grid_Emi.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local variables\n",
    "start_year = 2012  #First year in emission timeseries\n",
    "end_year = 2018    #Last year in emission timeseries\n",
    "year_range = [*range(start_year, end_year+1,1)] #List of emission years\n",
    "year_range_str=[str(i) for i in year_range]\n",
    "num_years = len(year_range)\n",
    "\n",
    "# Define constants\n",
    "Avogadro   = 6.02214129 * 10**(23)  #molecules/mol\n",
    "Molarch4   = 16.04                  #g/mol\n",
    "Res01      = 0.1                    # degrees\n",
    "Res_01     = 0.01\n",
    "tg_scale   = 0.001                  #Tg scale number [New file allows for the exclusion of the territories] \n",
    "\n",
    "# Continental US Lat/Lon Limits (for netCDF files)\n",
    "Lon_left = -130       #deg\n",
    "Lon_right = -60       #deg\n",
    "Lat_low  = 20         #deg\n",
    "Lat_up  = 55          #deg\n",
    "loc_dimensions = [Lat_low, Lat_up, Lon_left, Lon_right]\n",
    "\n",
    "ilat_start = int((90+Lat_low)/Res01) #1100:1450 (continental US range)\n",
    "ilat_end = int((90+Lat_up)/Res01)\n",
    "ilon_start = abs(int((-180-Lon_left)/Res01)) #500:1200 (continental US range)\n",
    "ilon_end = abs(int((-180-Lon_right)/Res01))\n",
    "\n",
    "# Number of days in each month\n",
    "month_day_leap  = [  31,  29,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_day_nonleap = [  31,  28,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "\n",
    "# Month arrays\n",
    "month_range_str = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "num_months = len(month_range_str)\n",
    "\n",
    "#Initialize month dictionary\n",
    "month_dict = {'Jan': 0, 'Feb': 1, 'Mar': 2, 'Apr': 3, 'May': 4, 'Jun': 5, 'Jul': 6, 'Aug': 7, 'Sep': 8, 'Oct': 9, 'Nov': 10, 'Dec': 11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track run time\n",
    "ct = datetime.datetime.now() \n",
    "it = ct.timestamp() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale gridded emissions so they match state-level crop totals\n",
    "Scale_To_State = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Step 1. Load in State ANSI data and Area Maps\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State-level ANSI Data\n",
    "#Read the state ANSI file array\n",
    "State_ANSI, name_dict = data_load_fn.load_state_ansi(State_ANSI_inputfile)[0:2]\n",
    "#QA: number of states\n",
    "print('Read input file: '+ f\"{State_ANSI_inputfile}\")\n",
    "print('Total \"States\" found: ' + '%.0f' % len(State_ANSI))\n",
    "print(' ')\n",
    "\n",
    "#County ANSI Data\n",
    "#Includes State ANSI number, county ANSI number, county name, and country area (square miles)\n",
    "County_ANSI = pd.read_csv(County_ANSI_inputfile,encoding='latin-1')\n",
    "\n",
    "\n",
    "# 0.01 x0.01 degree Data\n",
    "# State ANSI IDs and grid cell area (m2) maps\n",
    "state_ANSI_map = data_load_fn.load_state_ansi_map(Grid_state001_ansi_inputfile)\n",
    "state_ANSI_map = state_ANSI_map.astype('int32')\n",
    "#county_ANSI_map = data_load_fn.load_county_ansi_map(Grid_county001_ansi_inputfile)\n",
    "#county_ANSI_map = county_ANSI_map.astype('int32')\n",
    "area_map, lat001, lon001 = data_load_fn.load_area_map_001(Grid_area001_inputfile)\n",
    "\n",
    "# 0.1 x0.1 degree data\n",
    "# grid cell area and state and county ANSI maps\n",
    "area_map01, Lat01, Lon01 = data_load_fn.load_area_map_01(Grid_area01_inputfile)[0:3]\n",
    "#Select relevant Continental 0.1 x0.1 domain\n",
    "Lat_01 = Lat01[ilat_start:ilat_end]\n",
    "Lon_01 = Lon01[ilon_start:ilon_end]\n",
    "area_matrix_01 = data_fn.regrid001_to_01(area_map, Lat_01, Lon_01)\n",
    "area_matrix_01 *= 10000  #convert from m2 to cm2\n",
    "\n",
    "state_ANSI_map_01 = data_fn.regrid001_to_01(state_ANSI_map, Lat_01, Lon_01)\n",
    "\n",
    "# Print time\n",
    "ct = datetime.datetime.now() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 2: Read-in and Format Proxy Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1 Read In Proxy Mapping File & Make Proxy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load GHGI Mapping Groups\n",
    "names = pd.read_excel(Burning_Mapping_inputfile, sheet_name = \"GHGI Map - Burning\", usecols = \"A:B\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "ghgi_burning_map = pd.read_excel(Burning_Mapping_inputfile, sheet_name = \"GHGI Map - Burning\", usecols = \"A:B\", skiprows = 1, names = colnames)\n",
    "#drop rows with no data, remove the parentheses and \"\"\n",
    "ghgi_burning_map = ghgi_burning_map[ghgi_burning_map['GHGI_Emi_Group'] != 'na']\n",
    "ghgi_burning_map = ghgi_burning_map[ghgi_burning_map['GHGI_Emi_Group'].notna()]\n",
    "ghgi_burning_map['GHGI_Source']= ghgi_burning_map['GHGI_Source'].str.replace(r\"\\(\",\"\")\n",
    "ghgi_burning_map['GHGI_Source']= ghgi_burning_map['GHGI_Source'].str.replace(r\"\\)\",\"\")\n",
    "ghgi_burning_map.reset_index(inplace=True, drop=True)\n",
    "display(ghgi_burning_map)\n",
    "\n",
    "#load emission group - proxy map\n",
    "names = pd.read_excel(Burning_Mapping_inputfile, sheet_name = \"Proxy Map - Burning\", usecols = \"A:E\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "proxy_burning_map = pd.read_excel(Burning_Mapping_inputfile, sheet_name = \"Proxy Map - Burning\", usecols = \"A:E\", skiprows = 1, names = colnames)\n",
    "display((proxy_burning_map))\n",
    "\n",
    "#create empty proxy and emission group arrays (add months for proxy variables that have monthly data)\n",
    "for igroup in np.arange(0,len(proxy_burning_map)):\n",
    "    if proxy_burning_map.loc[igroup, 'Grid_Month_Flag'] ==0:\n",
    "        vars()[proxy_burning_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        vars()[proxy_burning_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "    else:\n",
    "        vars()[proxy_burning_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "        vars()[proxy_burning_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years,num_months])\n",
    "        \n",
    "    vars()[proxy_burning_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])\n",
    "    \n",
    "    if proxy_burning_map.loc[igroup,'State_Proxy_Group'] != '-':\n",
    "        if proxy_burning_map.loc[igroup,'State_Month_Flag'] == 0:\n",
    "            vars()[proxy_burning_map.loc[igroup,'State_Proxy_Group']] = np.zeros([len(State_ANSI),num_years])\n",
    "        else:\n",
    "            vars()[proxy_burning_map.loc[igroup,'State_Proxy_Group']] = np.zeros([len(State_ANSI),num_years,num_months])\n",
    "    else:\n",
    "        continue # do not make state proxy variable if no variable assigned in mapping file\n",
    "        \n",
    "emi_group_names = np.unique(ghgi_burning_map['GHGI_Emi_Group'])\n",
    "\n",
    "print('QA/QC: Is the number of emission groups the same for the proxy and emissions tabs?')\n",
    "if (len(emi_group_names) == len(np.unique(proxy_burning_map['GHGI_Emi_Group']))):\n",
    "    print('PASS')\n",
    "else:\n",
    "    print('FAIL')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2. Read in the State Emissions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#State Emissions data from the GHGI workbook will be used as the State-level proxy here\n",
    "# EPA methane emissions in units of kt\n",
    "\n",
    "#initialize proxy arrays\n",
    "state_wheat = np.zeros([len(State_ANSI),num_years])\n",
    "state_maize = np.zeros([len(State_ANSI),num_years])\n",
    "state_rice = np.zeros([len(State_ANSI),num_years])\n",
    "state_soybeans = np.zeros([len(State_ANSI),num_years])\n",
    "state_cotton = np.zeros([len(State_ANSI),num_years])\n",
    "state_sorghum = np.zeros([len(State_ANSI),num_years])\n",
    "state_peanuts = np.zeros([len(State_ANSI),num_years])\n",
    "state_other = np.zeros([len(State_ANSI),num_years])\n",
    "state_leg = np.zeros([len(State_ANSI),num_years])\n",
    "state_bar = np.zeros([len(State_ANSI),num_years])\n",
    "state_oats = np.zeros([len(State_ANSI),num_years])\n",
    "state_grass = np.zeros([len(State_ANSI),num_years])\n",
    "state_veg = np.zeros([len(State_ANSI),num_years])\n",
    "state_tob = np.zeros([len(State_ANSI),num_years])\n",
    "state_sun = np.zeros([len(State_ANSI),num_years])\n",
    "state_pot = np.zeros([len(State_ANSI),num_years])\n",
    "state_pea = np.zeros([len(State_ANSI),num_years])\n",
    "state_drybean = np.zeros([len(State_ANSI),num_years])\n",
    "state_beets = np.zeros([len(State_ANSI),num_years])\n",
    "state_lentils = np.zeros([len(State_ANSI),num_years])\n",
    "state_chickpeas = np.zeros([len(State_ANSI),num_years])\n",
    "\n",
    "\n",
    "#initialize (crop x state x year)\n",
    "#state_emis = np.zeros([21,len(State_ANSI),num_years])\n",
    "idx_2014 = year_range.index(2014)\n",
    "xl = pd.ExcelFile(EPA_burning_inputfile)\n",
    "state_list = xl.sheet_names\n",
    "#Read in emissions\n",
    "for istate in np.arange(0, len(State_ANSI)):\n",
    "    if State_ANSI['name'][istate] in state_list:\n",
    "        state_name = State_ANSI['name'][istate]\n",
    "        print(state_name)\n",
    "        epa_state_temp = pd.read_excel(EPA_burning_inputfile,skiprows=78,nrows=22, sheet_name = state_name)\n",
    "        epa_state_temp.dropna(axis=0,inplace=True)\n",
    "        epa_state_temp.rename(columns={'1999':1999}, inplace=True)\n",
    "        epa_state_temp = epa_state_temp.drop(columns = [*range(1990, start_year,1)])\n",
    "        epa_state_temp.rename(columns={epa_state_temp.columns[0]:'Source'}, inplace=True)\n",
    "        epa_state_temp.reset_index(inplace=True,drop=True)\n",
    "        #display(epa_state_temp)\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            if year_range[iyear] > 2014:\n",
    "                year_idx = idx_2014\n",
    "            else:\n",
    "                year_idx = iyear\n",
    "            for icrop in np.arange(0,len(epa_state_temp)):\n",
    "                if epa_state_temp.loc[icrop,'Source'] == 'Cereal/Wheat':\n",
    "                    state_wheat[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Cereal/Maize':\n",
    "                    state_maize[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Cereal/Other/Rice':\n",
    "                    state_rice[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Pulses/Other/Soybeans':\n",
    "                    state_soybeans[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Other/Cotton':\n",
    "                    state_cotton[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Cereals/Other/Sorghum':\n",
    "                    state_sorghum[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Pulses/Other/Peanuts':\n",
    "                    state_peanuts[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Cereals/Other/Other Small Grains':\n",
    "                    state_other[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Other/Legume Hay':\n",
    "                    state_leg[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Cereals/Barley':\n",
    "                    state_bar[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Cereals/Oats':\n",
    "                    state_oats[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Other/Grass Hay':\n",
    "                    state_grass[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Other/Vegetables':\n",
    "                    state_veg[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Other/Tobacco':\n",
    "                    state_tob[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Other/Sunflower':\n",
    "                    state_sun[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Tubers And Roots/Other/Potatoes':\n",
    "                    state_pot[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Other/Pulses/Peas':\n",
    "                    state_pea[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Pulses/Other/Dry Beans':\n",
    "                    state_drybean[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Tubers And Roots/Other/Sugarbeets':\n",
    "                    state_beets[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Pulses/Other/Lentils':\n",
    "                    state_lentils[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                elif epa_state_temp.loc[icrop,'Source'] == 'Pulses/Other/Chickpeas':\n",
    "                    state_chickpeas[istate,iyear] = epa_state_temp.iloc[icrop,year_idx+1]\n",
    "                else:\n",
    "                    print(istate, icrop, epa_state_temp.loc[icrop,'Source'])\n",
    "\n",
    "    #display(state_emis[:,0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3. Read McCarty burning data (2003-2007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_files = 5\n",
    "\n",
    "#initialize proxy arrays\n",
    "grid_maize_mon = np.zeros([len(lat001),len(lon001),num_months])\n",
    "grid_cotton_mon = np.zeros([len(lat001),len(lon001),num_months])\n",
    "grid_rice_mon = np.zeros([len(lat001),len(lon001),num_months])\n",
    "grid_soybean_mon = np.zeros([len(lat001),len(lon001),num_months])\n",
    "grid_wheat_mon = np.zeros([len(lat001),len(lon001),num_months])\n",
    "grid_other_mon = np.zeros([len(lat001),len(lon001),num_months])\n",
    "\n",
    "for ifile in np.arange(0,num_files):\n",
    "    McC_temp = pd.read_csv(Grid_crop_files[ifile])\n",
    "    #Remove observations without date\n",
    "    McC_temp = McC_temp[McC_temp['Burn_Date'] != ' ']\n",
    "    McC_temp.reset_index(inplace=True, drop=True)\n",
    "    print(ifile)\n",
    "    for irow in np.arange(len(McC_temp)):\n",
    "        #Decide month based on burn date\n",
    "        if ifile == 3:\n",
    "            imonth = month_dict[McC_temp['Burn_Date'][irow][-3:]]\n",
    "        else:\n",
    "            imonth = month_dict[McC_temp['Burn_Date'][irow][:3]]\n",
    "        #Decide whether in domain\n",
    "        if McC_temp['Longitude'][irow] > Lon_left and \\\n",
    "            McC_temp['Longitude'][irow] < Lon_right and \\\n",
    "            McC_temp['Latitude'][irow] > Lat_low and  \\\n",
    "            McC_temp['Latitude'][irow] < Lat_up:\n",
    "            \n",
    "            #Calculate lat/lon\n",
    "            ilat = int((McC_temp['Latitude'][irow] - Lat_low)/Res_01)\n",
    "            ilon = int((McC_temp['Longitude'][irow] - Lon_left)/Res_01)\n",
    "            \n",
    "            #Create running sum for each group\n",
    "            if McC_temp['Crop_Type'][irow] == 'corn':\n",
    "                grid_maize_mon[ilat,ilon,imonth] += McC_temp['EMCH4_Gg'][irow]\n",
    "            elif McC_temp['Crop_Type'][irow] == 'rice':\n",
    "                grid_rice_mon[ilat,ilon,imonth] += McC_temp['EMCH4_Gg'][irow]\n",
    "            elif McC_temp['Crop_Type'][irow] == 'soybean':\n",
    "                grid_soybean_mon[ilat,ilon,imonth] += McC_temp['EMCH4_Gg'][irow]\n",
    "            elif McC_temp['Crop_Type'][irow] == 'cotton':\n",
    "                grid_cotton_mon[ilat,ilon,imonth] += McC_temp['EMCH4_Gg'][irow]\n",
    "            elif McC_temp['Crop_Type'][irow] == 'wheat':\n",
    "                grid_wheat_mon[ilat,ilon,imonth] += McC_temp['EMCH4_Gg'][irow]     \n",
    "            elif McC_temp['Crop_Type'][irow] in ['Kentucky bluegrass', 'other crop/fallow']:\n",
    "                grid_other_mon[ilat,ilon,imonth] += McC_temp['EMCH4_Gg'][irow]     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "## Step 3. Read in and Format US EPA GHGI Emissions\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read in data from the GHGI (in units of kt)\n",
    "\n",
    "EPA_emi_burning_CH4 = pd.read_excel(EPA_burning_inputfile,skiprows=78,nrows=23, sheet_name = 'National Emissions')\n",
    "EPA_emi_burning_CH4.dropna(axis=0,inplace=True)\n",
    "EPA_emi_burning_CH4.rename(columns={'1999':1999}, inplace=True)\n",
    "EPA_emi_burning_CH4 = EPA_emi_burning_CH4.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_emi_burning_CH4.rename(columns={EPA_emi_burning_CH4.columns[0]:'Source'}, inplace=True)\n",
    "display(EPA_emi_burning_CH4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Split Emissions into Gridding Groups (each Group will have the same proxy applied during the state allocation/gridding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split emissions into scaling groups\n",
    "\n",
    "DEBUG =1\n",
    "\n",
    "start_year_idx = EPA_emi_burning_CH4.columns.get_loc((start_year))\n",
    "end_year_idx = EPA_emi_burning_CH4.columns.get_loc((end_year))+1\n",
    "ghgi_burning_groups = ghgi_burning_map['GHGI_Emi_Group'].unique()\n",
    "sum_emi = np.zeros([num_years])\n",
    "\n",
    "\n",
    "for igroup in np.arange(0,len(ghgi_burning_groups)): #loop through all groups, finding the GHGI sources in that group and summing emissions for that region, year        vars()[ghgi_prod_groups[igroup]] = np.zeros([num_regions-1,num_years])\n",
    "    ##DEBUG## print(ghgi_burning_groups[igroup])\n",
    "    vars()[ghgi_burning_groups[igroup]] = np.zeros([num_years])\n",
    "    source_temp = ghgi_burning_map.loc[ghgi_burning_map['GHGI_Emi_Group'] == ghgi_burning_groups[igroup], 'GHGI_Source']\n",
    "    pattern_temp  = '|'.join(source_temp) \n",
    "    emi_temp = EPA_emi_burning_CH4[EPA_emi_burning_CH4['Source'].str.contains(pattern_temp)]\n",
    "    ##DEBUG## display(emi_temp)\n",
    "    vars()[ghgi_burning_groups[igroup]][:] = emi_temp.iloc[:,start_year_idx:].sum()\n",
    "    ##DEBUG## display(vars()[ghgi_burning_groups[igroup]][:])\n",
    "        \n",
    "        \n",
    "#Check against total summary emissions \n",
    "print('QA/QC #1: Check Processing Emission Sum against GHGI Summary Emissions')\n",
    "for iyear in np.arange(0,num_years): \n",
    "    for igroup in np.arange(0,len(ghgi_burning_groups)):\n",
    "        sum_emi[iyear] += vars()[ghgi_burning_groups[igroup]][iyear]\n",
    "        \n",
    "    summary_emi = EPA_emi_burning_CH4.iloc[-1,iyear+1]  \n",
    "    diff1 = abs(sum_emi[iyear] - summary_emi)/((sum_emi[iyear] + summary_emi)/2)\n",
    "    if DEBUG ==1:\n",
    "        print(summary_emi)\n",
    "        print(sum_emi[iyear])\n",
    "    if diff1 < 0.0001:\n",
    "        print('Year ', year_range[iyear],': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear],': FAIL (check Production & summary tabs): ', diff1,'%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Step 4. Grid Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1. Allocate emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.1.1 Assign the Appropriate Proxy Variable Names (state & grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names on the *left* need to match the 'FieldBurning_ProxyMapping' 'State_Proxy_Group' names \n",
    "# (these are initialized in Step 2). \n",
    "# The names on the *right* are the variable names used to caluclate the proxies in this code.\n",
    "# Names on the right need to match those from the code in Step 2\n",
    "\n",
    "#national --> state proxies (state x year [X month])\n",
    "State_wheat = state_wheat\n",
    "State_maize = state_maize\n",
    "State_rice = state_rice\n",
    "State_soybeans = state_soybeans\n",
    "State_cotton = state_cotton\n",
    "State_sorghum = state_sorghum\n",
    "State_peanuts = state_peanuts\n",
    "State_other_grains = state_other\n",
    "State_leghay = state_leg\n",
    "State_barley = state_bar\n",
    "State_oats = state_oats\n",
    "State_grasshay = state_grass\n",
    "State_vegetables = state_veg\n",
    "State_tobacco = state_tob\n",
    "State_sunflower = state_sun\n",
    "State_potatoes = state_pot\n",
    "State_peas = state_pea\n",
    "State_drybeans = state_drybean\n",
    "State_sugarbeets = state_beets\n",
    "State_lentils = state_lentils\n",
    "State_chickpeas = state_chickpeas\n",
    "\n",
    "#county --> grid proxies (0.01x0.01 [xmonth])\n",
    "Map_wheat = grid_wheat_mon\n",
    "Map_maize = grid_maize_mon\n",
    "Map_rice = grid_rice_mon\n",
    "Map_soybeans = grid_soybean_mon\n",
    "Map_cotton = grid_cotton_mon\n",
    "Map_other = grid_other_mon\n",
    "\n",
    "# remove variables to clear space for larger arrays \n",
    "del grid_wheat_mon, grid_maize_mon, grid_rice_mon, grid_soybean_mon, grid_cotton_mon, grid_other_mon\n",
    "\n",
    "Map_total = Map_wheat+Map_maize+Map_rice+Map_soybeans+Map_cotton+Map_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.1.2 Allocate National EPA Emissions to the State-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate state-level emissions \n",
    "# Emissions in kt\n",
    "# State data = national GHGI emissions * state proxy/national total\n",
    "\n",
    "DEBUG = 1\n",
    "\n",
    "# Note that national emissions are retained for groups that do not have state proxies (identified in the mapping file)\n",
    "# and are gridded in the next step\n",
    "\n",
    "# Make placeholder emission arrays for each group\n",
    "for igroup in np.arange(0,len(proxy_burning_map)):\n",
    "    #if proxy_burning_map.loc[igroup,'State_Month_Flag'] ==1:\n",
    "    vars()['State_'+proxy_burning_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(State_ANSI),num_years])\n",
    "    #else:\n",
    "    #    vars()['State_'+proxy_burning_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(State_ANSI),num_years])\n",
    "    vars()['NonState_'+proxy_burning_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])\n",
    "        \n",
    "#Loop over years\n",
    "for iyear in np.arange(num_years):\n",
    "    #Loop over states\n",
    "    for istate in np.arange(len(State_ANSI)):\n",
    "        for igroup in np.arange(0,len(proxy_burning_map)):    \n",
    "            if proxy_burning_map.loc[igroup,'State_Proxy_Group'] != '-' and proxy_burning_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "                vars()['State_'+proxy_burning_map.loc[igroup,'GHGI_Emi_Group']][istate,iyear] = \\\n",
    "                    vars()[proxy_burning_map.loc[igroup,'GHGI_Emi_Group']][iyear]* \\\n",
    "                    data_fn.safe_div(vars()[proxy_burning_map.loc[igroup,'State_Proxy_Group']][istate,iyear], \\\n",
    "                                     np.sum(vars()[proxy_burning_map.loc[igroup,'State_Proxy_Group']][:,iyear]))   \n",
    "            else:\n",
    "                vars()['NonState_'+proxy_burning_map.loc[igroup,'GHGI_Emi_Group']][iyear] = vars()[proxy_burning_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "                \n",
    "# Check sum of all gridded emissions + emissions not included in state allocation\n",
    "print('QA/QC #1: Check weighted emissions against GHGI')   \n",
    "for iyear in np.arange(0,num_years):\n",
    "    summary_emi = EPA_emi_burning_CH4.iloc[-1,iyear+1] \n",
    "    calc_emi = 0\n",
    "    for igroup in np.arange(0,len(proxy_burning_map)):\n",
    "        calc_emi +=  np.sum(vars()['State_'+proxy_burning_map.loc[igroup,'GHGI_Emi_Group']][:,iyear])+\\\n",
    "            vars()['NonState_'+proxy_burning_map.loc[igroup,'GHGI_Emi_Group']][iyear] #np.sum(Emissions[:,iyear]) + Emissions_nongrid[iyear] + Emissions_nonstate[iyear]\n",
    "    if DEBUG ==1:\n",
    "        print(summary_emi)\n",
    "        print(calc_emi)\n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0001:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.3 Allocate state emissions to the CONUS region (0.1x0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will need to save yearly emissions as intermediate output and read back in due to memory limits\n",
    "data_IO_fn.initialize_netCDF001(burning_int_out, netCDF_description, 1, year_range, loc_dimensions, lat001, lon001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Allocate County-Level emissions (kt) onto a 0.1x0.1 grid using gridcell level 'Proxy_Groups'\n",
    "\n",
    "DEBUG =1\n",
    "#Define emission arrays\n",
    "Emissions_array_01 = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Emissions_nongrid = np.zeros([num_years])\n",
    "running_sum = np.zeros([len(proxy_burning_map),num_years])\n",
    "\n",
    "\n",
    "# For each year, (2a) distribute state-level emissions onto a grid using proxies defined above ....\n",
    "# To speed up the code, masks are used rather than looping individually through each lat/lon. \n",
    "# In this case, a mask of 1's is made for the grid cells that match the ANSI values for a given state\n",
    "# The masked values are set to zero, remaining values = 1. \n",
    "# AK and HI and territories are removed from the analysis at this stage. \n",
    "# The emissions allocated to each state are at 0.01x0.01 degree resolution, as required to calculate accurate 'mask'\n",
    "# arrays for each state. \n",
    "# (2b - not applicable here) For emission groups that were not first allocated to states, national emissions for those groups are gridded\n",
    "# based on the relevant gridded proxy arrays (0.1x0.1 resolution). These emissions are at 0.1x0.1 degrees resolution. \n",
    "# (2c - not applicable here) - record 'not mapped' emission groups in the 'non-grid' array#\n",
    "\n",
    "print('**QA/QC Check: Sum of national gridded emissions vs. GHGI national emissions')\n",
    "     \n",
    "    \n",
    "#1. Step through each gridding group\n",
    "for igroup in np.arange(0,len(proxy_burning_map)):\n",
    "    print(igroup, 'of', len(proxy_burning_map))\n",
    "    vars()['Ext_'+proxy_burning_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "    \n",
    "    proxy_temp = vars()[proxy_burning_map.loc[igroup,'Proxy_Group']] #latxlonxmonth\n",
    "    #2a. Step through each state (if group was previously allocated to state level)\n",
    "    if proxy_burning_map.loc[igroup,'State_Proxy_Group'] != '-' and proxy_burning_map.loc[igroup,'State_Proxy_Group'] != 'state_not_mapped':\n",
    "        for istate in np.arange(0,len(State_ANSI)):\n",
    "            if State_ANSI['abbr'][istate] not in {'AK','HI'} and istate < 51:\n",
    "                #print()\n",
    "                \n",
    "                mask_state = np.ma.ones(np.shape(state_ANSI_map))\n",
    "                mask_state = np.ma.masked_where(state_ANSI_map != State_ANSI['ansi'][istate], mask_state)\n",
    "                mask_state = np.ma.filled(mask_state,0)\n",
    "                state_temp = vars()['State_'+proxy_burning_map.loc[igroup,'GHGI_Emi_Group']][istate,:]\n",
    "                if np.sum(state_temp[:]) > 0 :\n",
    "                    if np.sum(mask_state*np.sum(proxy_temp[:,:,:],axis=2)) > 0:\n",
    "                        for imonth in np.arange(0, num_months):\n",
    "                            weighted_array = data_fn.safe_div(mask_state*proxy_temp[:,:,imonth],np.sum(mask_state*np.sum(proxy_temp[:,:,:],axis=2)))\n",
    "                            weighted_array_01 = data_fn.regrid001_to_01(weighted_array[:,:], Lat_01, Lon_01)\n",
    "                            for iyear in np.arange(0, num_years):\n",
    "                                Emissions_array_01[:,:,iyear,imonth] += state_temp[iyear]*weighted_array_01\n",
    "                                running_sum[igroup,iyear] += np.sum(state_temp[iyear]*weighted_array_01)\n",
    "                                vars()['Ext_'+proxy_burning_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += state_temp[iyear]*weighted_array_01 #record running sum of month emissions for that state\n",
    "                                                            \n",
    "                    elif np.sum(mask_state*np.sum(proxy_temp[:,:,:],axis=2)) == 0:\n",
    "                        if np.sum(mask_state*np.sum(Map_total[:,:,:],axis=2)) == 0:\n",
    "                            for iyear in np.arange(0, num_years):\n",
    "                                Emissions_nongrid[iyear] += state_temp[iyear] #if no crop bruning data in that state...\n",
    "                        else:\n",
    "                            for imonth in np.arange(0, num_months):\n",
    "                                # if there is no burning data for that crop, but there are state emissions, weight by relative total burning emissions... \n",
    "                                weighted_array = data_fn.safe_div(mask_state*Map_total[:,:,imonth],np.sum(mask_state*np.sum(Map_total[:,:,:],axis=2)))       \n",
    "                                weighted_array_01 = data_fn.regrid001_to_01(weighted_array[:,:], Lat_01, Lon_01)\n",
    "                                for iyear in np.arange(0, num_years):\n",
    "                                    Emissions_array_01[:,:,iyear,imonth] += state_temp[iyear]*weighted_array_01\n",
    "                                    running_sum[igroup,iyear] += np.sum(state_temp[iyear]*weighted_array_01)\n",
    "                                    vars()['Ext_'+proxy_burning_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += state_temp[iyear]*weighted_array_01 #keep running sum of month emissions for that state\n",
    "                    else:\n",
    "                        for iyear in np.arange(0, num_years):\n",
    "                            Emissions_nongrid[iyear] += state_temp[iyear]\n",
    "                \n",
    "            else:\n",
    "                for iyear in np.arange(0, num_years):\n",
    "                    state_temp = vars()['State_'+proxy_burning_map.loc[igroup,'GHGI_Emi_Group']][istate,iyear]\n",
    "                    Emissions_nongrid[iyear] += state_temp \n",
    "        \n",
    "        print(igroup)\n",
    "        print(running_sum[igroup,0])\n",
    "        print(np.sum(Emissions_array_01[:,:,0,:]))\n",
    "        print(np.sum(vars()['Ext_'+proxy_burning_map.loc[igroup,'GHGI_Emi_Group']][:,:,0]))\n",
    "        \n",
    "    \n",
    "for iyear in np.arange(0, num_years): \n",
    "    calc_emi2 = 0\n",
    "    calc_emi = np.sum(Emissions_array_01[:,:,iyear,:]) + np.sum(Emissions_nongrid[iyear]) \n",
    "    for igroup in np.arange(0, len(proxy_burning_map)):\n",
    "        calc_emi2 += np.sum(vars()['Ext_'+proxy_burning_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])\n",
    "    calc_emi2 += np.sum(Emissions_nongrid[iyear])\n",
    "    summary_emi = summary_emi = EPA_emi_burning_CH4.iloc[-1,iyear+1] \n",
    "    emi_diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if DEBUG ==1:\n",
    "        print(summary_emi)\n",
    "        print(calc_emi)\n",
    "        print(calc_emi2)\n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0001:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iyear in np.arange(0, num_years): \n",
    "    calc_emi2 = 0\n",
    "    calc_emi = np.sum(Emissions_array_01[:,:,iyear,:]) + np.sum(Emissions_nongrid[iyear]) \n",
    "    for igroup in np.arange(0, len(proxy_burning_map)):\n",
    "        calc_emi2 += np.sum(vars()['Ext_'+proxy_burning_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])\n",
    "    calc_emi2 += np.sum(Emissions_nongrid[iyear])\n",
    "    summary_emi = summary_emi = EPA_emi_burning_CH4.iloc[-1,iyear+1] \n",
    "    emi_diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if DEBUG ==1:\n",
    "        print(summary_emi)\n",
    "        print(calc_emi)\n",
    "        print(calc_emi2)\n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0001:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2.2 Save gridded emissions (kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save gridded emissions for each gridding group - for extension\n",
    "\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(grid_emi_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "unique_groups = np.unique(proxy_burning_map['GHGI_Emi_Group'])\n",
    "unique_groups = list(unique_groups[unique_groups != 'Emi_not_mapped'])\n",
    "print(unique_groups)\n",
    "\n",
    "nc_out = Dataset(grid_emi_outputfile, 'r+', format='NETCDF4')\n",
    "#nc_out.createDimension('state', len(State_ANSI))\n",
    "\n",
    "for igroup in np.arange(0,len(unique_groups)):\n",
    "    print('Ext_'+unique_groups[igroup])\n",
    "    if len(np.shape(vars()['Ext_'+unique_groups[igroup]])) ==4:\n",
    "        ghgi_temp = np.sum(vars()['Ext_'+unique_groups[igroup]],axis=3) #sum month data if data is monthly\n",
    "    else:\n",
    "        ghgi_temp = vars()['Ext_'+unique_groups[igroup]]\n",
    "\n",
    "    # Write data to netCDF\n",
    "    data_out = nc_out.createVariable('Ext_'+unique_groups[igroup], 'f8', ('lat', 'lon','year'), zlib=True)\n",
    "    data_out[:,:,:] = ghgi_temp[:,:,:]\n",
    "\n",
    "#save nongrid data to calculate non-grid fraction extension\n",
    "data_out = nc_out.createVariable('Emissions_nongrid', 'f8', ('year'), zlib=True)  \n",
    "data_out[:] = np.sum(Emissions_nongrid[:])\n",
    "\n",
    "nc_out.close()\n",
    "\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded emissions (kt) written to file: {}\" .format(os.getcwd())+grid_emi_outputfile)\n",
    "print(' ')\n",
    "\n",
    "del data_out, ghgi_temp, nc_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Calculate Gridded Emission Fluxes (molec./cm2/s) (0.1x0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert emissions to emission flux\n",
    "# conversion: kt emissions to molec/cm2/s flux\n",
    "\n",
    "DEBUG = 1\n",
    "\n",
    "Flux_array_01 = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Flux_array_01_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "check_sum = np.zeros([num_years])\n",
    "check_sum_annual = np.zeros([num_years])\n",
    "\n",
    "print('**QA/QC Check: Sum of national gridded emissions vs. GHGI national emissions')\n",
    "  \n",
    "for iyear in np.arange(0,num_years):\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "        month_days = month_day_leap\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        month_days = month_day_nonleap\n",
    "        \n",
    "    # calculate fluxes for each emissions group and national sum  (=kt * grams/kt *molec/mol *mol/g *s^-1 * cm^-2)\n",
    "    conversion_factor_annual = 10**9 * Avogadro / float(Molarch4 * np.sum(month_days) * 24 * 60 *60) / area_matrix_01\n",
    "    \n",
    "    #if proxy_livestock_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "    for imonth in np.arange(0,num_months):\n",
    "        conversion_factor_month = 10**9 * Avogadro / float(Molarch4 * month_days[imonth] * 24 * 60 *60) / area_matrix_01\n",
    "        conv_factor2 = month_days[imonth]/year_days\n",
    "        Flux_array_01[:,:,iyear,imonth] = Emissions_array_01[:,:,iyear,imonth] * conversion_factor_month\n",
    "        Flux_array_01_annual[:,:,iyear] += Flux_array_01[:,:,iyear,imonth]*conv_factor2        \n",
    "        #calculate the monthly running flux totals and convert from flux back to mass (also calc annual sum)    \n",
    "        check_sum[iyear] += np.sum(Flux_array_01[:,:,iyear,imonth]/conversion_factor_month)\n",
    "    check_sum_annual[iyear] += np.sum(Flux_array_01_annual[:,:,iyear]/conversion_factor_annual)\n",
    "        \n",
    "    #convert back to mass to check\n",
    "    calc_emi = check_sum_annual[iyear] +np.sum(Emissions_nongrid[iyear]) \n",
    "    calc_emi2 = check_sum[iyear] +np.sum(Emissions_nongrid[iyear]) \n",
    "    \n",
    "    summary_emi = EPA_emi_burning_CH4.iloc[-1,iyear+1] \n",
    "    emi_diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if DEBUG ==1:\n",
    "        print(calc_emi)\n",
    "        print(calc_emi2)\n",
    "        print(summary_emi)\n",
    "    if abs(emi_diff) < 0.00015:\n",
    "        print('Year '+ year_range_str[iyear]+': Difference < 0.01%: PASS')\n",
    "    else: \n",
    "        print('Year '+ year_range_str[iyear]+': Difference > 0.01%: FAIL, diff: '+str(emi_diff))\n",
    "        \n",
    "Flux_Emissions_Total_annual = Flux_array_01_annual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 5. Write netCDF\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly data\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(gridded_month_outputfile, netCDF_description_m, 1, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write data to netCDF\n",
    "nc_out = Dataset(gridded_month_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:,:] = Flux_array_01\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded fluxes written to file: {}\" .format(os.getcwd())+gridded_month_outputfile)\n",
    "\n",
    "# yearly data\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(gridded_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write data to netCDF\n",
    "nc_out = Dataset(gridded_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Total_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded field burning fluxes written to file: {}\" .format(os.getcwd())+gridded_outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Step 6. Plot Gridded Data\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.1. Plot Annual Emission Fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot Annual Data\n",
    "scale_max = 0.10\n",
    "save_flag =0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_str,scale_max,save_flag,save_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.2 Plot Difference between first and last inventory year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot difference between last and first year\n",
    "save_flag =0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_diff_str,save_flag, save_outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = datetime.datetime.now() \n",
    "ft = ct.timestamp() \n",
    "time_elapsed = (ft-it)/(60*60)\n",
    "print('Time to run: '+str(time_elapsed)+' hours')\n",
    "print('** GEPA_3F_Field_Burning_of_Agricultural_Residues: COMPLETE **')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded EPA Methane Inventory\n",
    "## Category: 5B1 Composting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Authors: \n",
    "Joannes D. Maasakkers, Candice F. Z. Chen, Erin E. McDuffie\n",
    "#### Date Last Updated: \n",
    "See Step 0\n",
    "#### Notebook Purpose: \n",
    "This notebook calculates gridded (0.1⁰x0.1⁰) annual emission fluxes of methane (molecules CH4/cm2/s) from composting facilities in the CONUS region for years 2012 - 2018. \n",
    "#### Summary & Notes:\n",
    "The national EPA GHGI emissions from composting facilities are read in from the published GHGI Chapter 7 data tables. National emissions are then allocated to the state level using state-level composting and recycling (converted to composting) emissions using the tonnes of garbage composted in each state. State-level emissions are then distributed onto a 0.01⁰x0.01⁰ grid using a map of composting facility locations (aggregated from four facility-level datasets) and gridded population data (U.S. Census), for states where no facilities exist. Data are then re-gridded to 0.1⁰x0.1⁰ and converted to fluxes (molecules CH4/cm2/s). Annual emission fluxes (molec./cm2/s) are written to final netCDFs in the ‘/code/Final_Gridded_Data/’ folder.\n",
    "\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Set-Up Notebook Modules, Functions, and Local Parameters and Constants\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm working directory\n",
    "import os\n",
    "import time\n",
    "modtime = os.path.getmtime('./5B1_Composting.ipynb')\n",
    "modificationTime = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(modtime))\n",
    "print(\"This file was last modified on: \", modificationTime)\n",
    "print('')\n",
    "print(\"The directory we are working in is {}\" .format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Include plots within notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import base modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from copy import copy\n",
    "\n",
    "# Import additional modules\n",
    "# Load plotting package Basemap \n",
    "# Must also specify project library path [unique to each user])\n",
    "# os.environ[\"PROJ_LIB\"] = r'C:\\Users\\candicechen\\Anaconda3\\pkgs\\basemap-1.2.2-py38haf86b8b_0\\Library\\share'\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# Load netCDF (for manipulating netCDF file types)\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Set up ticker\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#add path for the global function module (file)\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../Global_Functions/'))\n",
    "#print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Load functions\n",
    "import data_load_functions as data_load_fn\n",
    "import data_functions as data_fn\n",
    "import data_IO_functions as data_IO_fn\n",
    "import data_plot_functions as data_plot_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT Files\n",
    "# Assign global file names\n",
    "global_filenames = data_load_fn.load_global_file_names()\n",
    "State_ANSI_inputfile = global_filenames[0]\n",
    "#County_ANSI_inputfile = global_filenames[1]\n",
    "pop_map_inputfile = global_filenames[2]\n",
    "Grid_area01_inputfile = global_filenames[3]\n",
    "Grid_area001_inputfile = global_filenames[4]\n",
    "Grid_state001_ansi_inputfile = global_filenames[5]\n",
    "#Grid_county001_ansi_inputfile = global_filenames[6]\n",
    "\n",
    "# Specify names of inputs files used in this notebook\n",
    "# EPA National emissions\n",
    "EPA_comp_inputfile = '../Global_InputData/GHGI/Ch7_Waste/Table 7-19.csv'\n",
    "\n",
    "#Proxy Map\n",
    "Comp_Mapping_inputfile = './InputData/Composting_ProxyMapping.xlsx'\n",
    "\n",
    "#Activity Data (facility level)\n",
    "EPA_compost_facility_inputfile = './InputData/Composting Facilities.csv'\n",
    "compost_council_facility_inputfile = \"./InputData/Ad_compostingcouncil.txt\"\n",
    "biocycle_facilitylocs_inputfile = \"./InputData/biocycle_locs_clean.csv\"\n",
    "FRS_inputfile = \"../Global_InputData/FRS/national_single/NATIONAL_SINGLE.csv\"\n",
    "#State level\n",
    "stateofgarbage_inputfile = \"./InputData/Shin_State-of-Garbage_2014_Table3.csv\"\n",
    "epa_state_composting_inputfile = \"./InputData/Appendix F Waste Sector Estimates.xlsx\"\n",
    "\n",
    "#Specify names of intermediate files\n",
    "EPA_facility_loc_intfile = \"./Intermediate_Data/EPA_compost_locs.csv\"\n",
    "compost_council_facility_intfile = \"./Intermediate_Data/compost_council_facilitylocs_new.csv\"\n",
    "\n",
    "#Specify names of gridded output files\n",
    "gridded_outputfile = '../Final_Gridded_Data/EPA_v2_5B1_Composting.nc'\n",
    "netCDF_description = 'Gridded EPA Inventory - Composting Emissions - IPCC Source Category 5B1'\n",
    "title_str = \"EPA methane emissions from composting\"\n",
    "title_diff_str = \"Emissions from composting difference: 2018-2012\"\n",
    "\n",
    "#output gridded proxy data\n",
    "grid_emi_outputfile = '../Final_Gridded_Data/Extension/v2_input_data/Composting_Grid_Emi.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local variables\n",
    "start_year = 2012  #First year in emission timeseries\n",
    "end_year = 2018    #Last year in emission timeseries\n",
    "year_range = [*range(start_year, end_year+1,1)] #List of emission years\n",
    "year_range_str=[str(i) for i in year_range]\n",
    "num_years = len(year_range)\n",
    "\n",
    "# Define constants\n",
    "Avogadro   = 6.02214129 * 10**(23)  #molecules/mol\n",
    "Molarch4   = 16.04                  #g/mol\n",
    "Res01      = 0.1                    # degrees\n",
    "Res_01     = 0.01                   # degrees\n",
    "\n",
    "# Continental US Lat/Lon Limits (for netCDF files)\n",
    "Lon_left = -130       #deg\n",
    "Lon_right = -60       #deg\n",
    "Lat_low  = 20         #deg\n",
    "Lat_up  = 55          #deg\n",
    "loc_dimensions = [Lat_low, Lat_up, Lon_left, Lon_right]\n",
    "\n",
    "ilat_start = int((90+Lat_low)/Res01) #1100:1450 (continental US range)\n",
    "ilat_end = int((90+Lat_up)/Res01)\n",
    "ilon_start = abs(int((-180-Lon_left)/Res01)) #500:1200 (continental US range)\n",
    "ilon_end = abs(int((-180-Lon_right)/Res01))\n",
    "\n",
    "# Number of days in each month\n",
    "month_day_leap  = [  31,  29,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_day_nonleap = [  31,  28,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "\n",
    "# Month arrays\n",
    "#month_range_str = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "#num_months = len(month_range_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;\n",
    "//prevent auto-scrolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track run time\n",
    "ct = datetime.datetime.now() \n",
    "it = ct.timestamp() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Step 1. Load in State ANSI data and Area Maps\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the state ANSI file array\n",
    "State_ANSI, name_dict = data_load_fn.load_state_ansi(State_ANSI_inputfile)[0:2]\n",
    "#QA: number of states\n",
    "print('Read input file: '+ f\"{State_ANSI_inputfile}\")\n",
    "print('Total \"States\" found: ' + '%.0f' % len(State_ANSI))\n",
    "print(' ')\n",
    "\n",
    "# Load/Format Gridded Data Maps (0.01x0.01 and 0.1x0.1)\n",
    "# 0.01 x0.01 degree Data\n",
    "# State ANSI IDs and grid cell area (m2) maps\n",
    "state_ANSI_map = data_load_fn.load_state_ansi_map(Grid_state001_ansi_inputfile)\n",
    "area_map, lat001, lon001 = data_load_fn.load_area_map_001(Grid_area001_inputfile)\n",
    "\n",
    "# 0.1 x0.1 degree data\n",
    "# grid cell area and state ANSI maps\n",
    "Lat01, Lon01 = data_load_fn.load_area_map_01(Grid_area01_inputfile)[1:3]\n",
    "#Select relevant Continental US 0.1 x0.1 domain\n",
    "Lat_01 = Lat01[ilat_start:ilat_end]\n",
    "Lon_01 = Lon01[ilon_start:ilon_end]\n",
    "area_matrix_01 = data_fn.regrid001_to_01(area_map, Lat_01, Lon_01)\n",
    "area_matrix_01 *= 10000 #convert m2 to cm2\n",
    "state_ANSI_map_01 = data_fn.regrid001_to_01(state_ANSI_map, Lat_01, Lon_01)\n",
    "\n",
    "# Print time\n",
    "ct = datetime.datetime.now() \n",
    "print(\"current time:\", ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 2: Read-in and Format Proxy Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 Read In Proxy Mapping File & Make Proxy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load GHGI Mapping Groups\n",
    "names = pd.read_excel(Comp_Mapping_inputfile, sheet_name = \"GHGI Map - Comp\", usecols = \"A:B\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "ghgi_comp_map = pd.read_excel(Comp_Mapping_inputfile, sheet_name = \"GHGI Map - Comp\", usecols = \"A:B\", skiprows = 1, names = colnames)\n",
    "#drop rows with no data, remove the parentheses and \"\"\n",
    "ghgi_comp_map = ghgi_comp_map[ghgi_comp_map['GHGI_Emi_Group'] != 'na']\n",
    "ghgi_comp_map = ghgi_comp_map[ghgi_comp_map['GHGI_Emi_Group'].notna()]\n",
    "ghgi_comp_map['GHGI_Source']= ghgi_comp_map['GHGI_Source'].str.replace(r\"\\(\",\"\")\n",
    "ghgi_comp_map['GHGI_Source']= ghgi_comp_map['GHGI_Source'].str.replace(r\"\\)\",\"\")\n",
    "ghgi_comp_map.reset_index(inplace=True, drop=True)\n",
    "display(ghgi_comp_map)\n",
    "\n",
    "#load emission group - proxy map\n",
    "names = pd.read_excel(Comp_Mapping_inputfile, sheet_name = \"Proxy Map - Comp\", usecols = \"A:E\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "proxy_comp_map = pd.read_excel(Comp_Mapping_inputfile, sheet_name = \"Proxy Map - Comp\", usecols = \"A:E\", skiprows = 1, names = colnames)\n",
    "display((proxy_comp_map))\n",
    "\n",
    "#create empty proxy and emission group arrays (add months for proxy variables that have monthly data)\n",
    "for igroup in np.arange(0,len(proxy_comp_map)):\n",
    "    if proxy_comp_map.loc[igroup, 'Grid_Month_Flag'] ==0:\n",
    "        vars()[proxy_comp_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        vars()[proxy_comp_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "    else:\n",
    "        vars()[proxy_comp_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "        vars()[proxy_comp_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years,num_months])\n",
    "        \n",
    "    vars()[proxy_comp_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])\n",
    "    \n",
    "    if proxy_comp_map.loc[igroup,'State_Proxy_Group'] != '-':\n",
    "        if proxy_comp_map.loc[igroup,'State_Month_Flag'] == 0:\n",
    "            vars()[proxy_comp_map.loc[igroup,'State_Proxy_Group']] = np.zeros([len(State_ANSI),num_years])\n",
    "        else:\n",
    "            vars()[proxy_comp_map.loc[igroup,'State_Proxy_Group']] = np.zeros([len(State_ANSI),num_years,num_months])\n",
    "    else:\n",
    "        continue # do not make state proxy variable if no variable assigned in mapping file\n",
    "        \n",
    "#emi_group_names = np.unique(proxy_comp_map['GHGI_Emi_Group'])\n",
    "emi_group_names = np.unique(ghgi_comp_map['GHGI_Emi_Group'])\n",
    "print('QA/QC: Is the number of emission groups the same for the proxy and emissions tabs?')\n",
    "if (len(emi_group_names) == len(np.unique(proxy_comp_map['GHGI_Emi_Group']))):\n",
    "    print('PASS')\n",
    "else:\n",
    "    print('FAIL')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 State Population Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read population map\n",
    "pop_den_map = data_load_fn.load_pop_den_map(pop_map_inputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 EPA Composting Locations Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read EPA Composting Facility Location Information (2018 data file)\n",
    "# data from https://edg.epa.gov/metadata/catalog/search/resource/details.page?uuid=%7BBEC8068F-2F89-429D-B5DC-FD8147C17101%7D\n",
    "\n",
    "# Load in the first 10 columns of data with location information, drop duplicate entries based on location and name\n",
    "# Note: Some of the duplicates removed by name could be the same company with multiple sites. \n",
    "# These might be worth keeping in a future inventory\n",
    "EPA_facility_info = pd.read_csv(EPA_compost_facility_inputfile,usecols = [0,1,2,3,4,5,6,7,8,9], encoding='latin-1')\n",
    "EPA_facility_info = EPA_facility_info.drop_duplicates(subset=['XCoord','YCoord','Name'],ignore_index=True)\n",
    "\n",
    "#Convert coordinates to lat/lon\n",
    "faclat, faclon = data_fn.meters2degrees(EPA_facility_info.loc[:,'XCoord'], EPA_facility_info.loc[:,'YCoord'])\n",
    "EPA_facility_info.loc[:,('XCoord','YCoord')] = pd.DataFrame([faclon, faclat]).T\n",
    "\n",
    "#Rename columns\n",
    "cols = list(EPA_facility_info.columns)\n",
    "cols[0],cols[1] = cols[1],cols[0]\n",
    "EPA_facility_info = EPA_facility_info[cols].rename(columns={'XCoord': 'lon', 'YCoord': 'lat'})\n",
    "\n",
    "# Remove more possible duplicates. The distance value could be played with more\n",
    "# Identify and remove possible duplicates by indicating sites that are within 0.0025 degrees from each other (<~250m) \n",
    "# Write the formating location data to an intermediate csv file\n",
    "EPA_facility_info['Dupl'] = 0\n",
    "for index in np.arange(len(EPA_facility_info)):\n",
    "    for index2 in np.arange(index+1,len(EPA_facility_info)):\n",
    "        dist = np.sqrt((EPA_facility_info.loc[index,'lon']-EPA_facility_info.loc[index2,'lon'])**2+(EPA_facility_info.loc[index,'lat']-EPA_facility_info.loc[index2,'lat'])**2)\n",
    "        if dist < 0.0025:\n",
    "            EPA_facility_info.loc[index,'Dupl'] = 1\n",
    "\n",
    "# Remove duplicates\n",
    "EPA_facility_info = EPA_facility_info[EPA_facility_info['Dupl'] == 0]\n",
    "EPA_facility_info.reset_index(inplace=True, drop=True)\n",
    "EPA_facility_info.to_csv(EPA_facility_loc_intfile)\n",
    "print('Locations Found: ', len(EPA_facility_info))\n",
    "EPA_facility_info.head(1)\n",
    "\n",
    "# Print time\n",
    "ct = datetime.datetime.now() \n",
    "print(\"current time:\", ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Read in and Format the U.S. Composting Council Facility Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Compost Council facility information textfile (includes web based code and requires reformatting)\n",
    "# Data is broken up into lines using the \\\\n new line character\n",
    "# search each line of strings for the pattern matching lat, lon locations\n",
    "# recond those data and save an intermediate file containing newly formated lat/lon data (after\n",
    "# dropping duplicate location data)\n",
    "\n",
    "with open (compost_council_facility_inputfile, \"r\") as myfile:\n",
    "    compost_council_web=myfile.read()\n",
    "    \n",
    "compost_council_split = re.split('n', compost_council_web)\n",
    "#print ('Lines found:     ', len(compost_council_split))\n",
    "\n",
    "compost_council_facilitylocs = []\n",
    "for iline in np.arange(len(compost_council_split)):\n",
    "    temp = re.search('[-+]?([1-8]?\\d(\\.\\d+)?|90(\\.0+)?),*[-+]?(180(\\.0+)?|((1[0-7]\\d)|([1-9]?\\d))(\\.\\d+)?)', compost_council_split[iline])\n",
    "    if temp != None:\n",
    "        if len(temp.group(0)) > 8:\n",
    "            temp2 = re.split(',',temp.group(0))\n",
    "            compost_council_facilitylocs.append(temp2)\n",
    "#print ('Locations found: ', len(compost_council_facilitylocs))\n",
    "\n",
    "compost_council_facilitylocs = pd.DataFrame(compost_council_facilitylocs, columns = ['lat','lon']).astype('float')\n",
    "compost_council_facilitylocs = compost_council_facilitylocs.drop_duplicates(ignore_index=True)\n",
    "print ('Compost Council locations:  ', len(compost_council_facilitylocs))\n",
    "compost_council_facilitylocs.to_csv(compost_council_facility_intfile)\n",
    "compost_council_facilitylocs.head()\n",
    "\n",
    "# Remove Composting Council Facilities that are within 0.025 degrees (~1-5km) of EPA composting locations\n",
    "compost_council_facilitylocs['Dupl'] = 0\n",
    "\n",
    "# check the each composting council location against each EPA facility location, record if \n",
    "# within 0.025 degrees and then remove from compost council list. \n",
    "for index_cc in np.arange(len(compost_council_facilitylocs)):\n",
    "    for index_epa in np.arange(len(EPA_facility_info)):\n",
    "        dist = np.sqrt((EPA_facility_info.loc[index_epa,'lon']-compost_council_facilitylocs.loc[index_cc,'lon'])**2\\\n",
    "                       +(EPA_facility_info.loc[index_epa,'lat']-compost_council_facilitylocs.loc[index_cc,'lat'])**2)\n",
    "        if dist < 0.025:\n",
    "            compost_council_facilitylocs.loc[index_cc,'Dupl'] = 1\n",
    "\n",
    "print ('Duplicates to be removed: ', compost_council_facilitylocs['Dupl'].sum())\n",
    "compost_council_facilitylocs = compost_council_facilitylocs[compost_council_facilitylocs['Dupl'] == 0]\n",
    "compost_council_facilitylocs.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print ('Total Facilities: ', len(compost_council_facilitylocs)+len(EPA_facility_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Read in and Format Biocycle locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the locations for the Biocycle facilities\n",
    "\n",
    "# Read in data, then find duplicates within 0.025 degrees of the EPA and\n",
    "# Compost Council facility locations. Remove the duplicates from the biocycle data.\n",
    "\n",
    "#Database from findacomposter.com, geocoded in Matlab. This is not updated from the original version\n",
    "biocycle_facility_locs = pd.read_csv(biocycle_facilitylocs_inputfile)\n",
    "biocycle_facility_locs = biocycle_facility_locs[biocycle_facility_locs['lat'] > 0]\n",
    "biocycle_facility_locs.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print ('Biocycle locations: ', len(biocycle_facility_locs))\n",
    "biocycle_facility_locs.head(1)\n",
    "\n",
    "# Find and remove duplicates\n",
    "biocycle_facility_locs['Dupl'] = 0\n",
    "\n",
    "for index_bc in np.arange(len(biocycle_facility_locs)):\n",
    "    for index_epa in np.arange(len(EPA_facility_info)):\n",
    "        dist = np.sqrt((biocycle_facility_locs.loc[index_bc,'lon']-EPA_facility_info.loc[index_epa,'lon'])**2\\\n",
    "                       +(biocycle_facility_locs.loc[index_bc,'lat']-EPA_facility_info.loc[index_epa,'lat'])**2)\n",
    "        if dist < 0.025:\n",
    "            biocycle_facility_locs.loc[index_bc,'Dupl'] = 1\n",
    "\n",
    "for index_bc in np.arange(len(biocycle_facility_locs)):    \n",
    "    for index_cc in np.arange(len(compost_council_facilitylocs)):\n",
    "        dist = np.sqrt((biocycle_facility_locs.loc[index_bc,'lon']-compost_council_facilitylocs.loc[index_cc,'lon'])**2\\\n",
    "                       +(biocycle_facility_locs.loc[index_bc,'lat']-compost_council_facilitylocs.loc[index_cc,'lat'])**2)\n",
    "        if dist < 0.025:\n",
    "            biocycle_facility_locs.loc[index_bc,'Dupl'] = 1\n",
    "\n",
    "print ('Duplicates to be removed: ', biocycle_facility_locs['Dupl'].sum())\n",
    "biocycle_facility_locs = biocycle_facility_locs[biocycle_facility_locs['Dupl'] == 0]\n",
    "biocycle_facility_locs.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print ('Total Facilities: ', len(EPA_facility_info)+len(compost_council_facilitylocs)+len(biocycle_facility_locs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Read in and Format EPA Federal Registry Service (FRS) Facility Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data and format the FRS composting facility data\n",
    "# Remove duplicates by comparing locations to EPA, CC, and biocycle facility locations\n",
    "\n",
    "FRS_facility_locs = pd.read_csv(FRS_inputfile, usecols = [2,3,5,7,8,10,17,20,21,26,27,28,31,32,34,35,36],low_memory=False)\n",
    "FRS_facility_locs.fillna(0, inplace = True)\n",
    "FRS_facility_locs = FRS_facility_locs[FRS_facility_locs['LATITUDE83'] > 0]\n",
    "FRS_facility_locs = FRS_facility_locs[FRS_facility_locs['NAICS_CODES'] != 0]\n",
    "FRS_facility_locs.reset_index(inplace=True, drop=True)\n",
    "\n",
    "FRS_facility_locs['Comp_Flag'] = 0\n",
    "for i in np.arange(len(FRS_facility_locs)):\n",
    "    if re.search('562219',FRS_facility_locs.loc[i,'NAICS_CODES'].lower()) != None:\n",
    "        FRS_facility_locs.loc[i,'Comp_Flag'] = 1\n",
    "FRS_facility_locs = FRS_facility_locs[FRS_facility_locs['Comp_Flag'] == 1]\n",
    "FRS_facility_locs = FRS_facility_locs[FRS_facility_locs['COLLECT_DESC'] != 'INTERPOLATION-OTHER']\n",
    "FRS_facility_locs.reset_index(inplace=True, drop=True)\n",
    "\n",
    "FRS_facility_locs = FRS_facility_locs.drop_duplicates(subset=['LATITUDE83','LONGITUDE83'],ignore_index=True)\n",
    "\n",
    "# remove duplicates within FRS dataset based on two facilities with similar location\n",
    "FRS_facility_locs['Dupl'] = 0\n",
    "for index in np.arange(len(FRS_facility_locs)):\n",
    "    for index2 in np.arange(index+1,len(FRS_facility_locs)):\n",
    "        dist = np.sqrt((FRS_facility_locs.loc[index,'LONGITUDE83']-FRS_facility_locs.loc[index2,'LONGITUDE83'])**2+(FRS_facility_locs.loc[index,'LATITUDE83']-FRS_facility_locs.loc[index2,'LATITUDE83'])**2)\n",
    "        if dist < 0.0025:\n",
    "            FRS_facility_locs.loc[index,'Dupl'] = 1\n",
    "FRS_facility_locs = FRS_facility_locs[FRS_facility_locs['Dupl'] == 0]\n",
    "FRS_facility_locs.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "# Remove duplicates with other dataset\n",
    "for index_FRS in np.arange(len(FRS_facility_locs)):\n",
    "    for index_bc in np.arange(len(biocycle_facility_locs)):\n",
    "        dist = np.sqrt((biocycle_facility_locs.loc[index_bc,'lon']-FRS_facility_locs.loc[index_FRS,'LONGITUDE83'])**2+(biocycle_facility_locs.loc[index_bc,'lat']-FRS_facility_locs.loc[index_FRS,'LATITUDE83'])**2)\n",
    "        if dist < 0.025:\n",
    "            FRS_facility_locs.loc[index_FRS,'Dupl'] = 1\n",
    "            \n",
    "for index_FRS in np.arange(len(FRS_facility_locs)):\n",
    "    for index_cc in np.arange(len(compost_council_facilitylocs)):\n",
    "        dist = np.sqrt((compost_council_facilitylocs.loc[index_cc,'lon']-FRS_facility_locs.loc[index_FRS,'LONGITUDE83'])**2+(compost_council_facilitylocs.loc[index_cc,'lat']-FRS_facility_locs.loc[index_FRS,'LATITUDE83'])**2)\n",
    "        if dist < 0.025:\n",
    "            FRS_facility_locs.loc[index_FRS,'Dupl'] = 1\n",
    "\n",
    "for index_FRS in np.arange(len(FRS_facility_locs)):\n",
    "    for index_epa in np.arange(len(EPA_facility_info)):\n",
    "        dist = np.sqrt((EPA_facility_info.loc[index_epa,'lon']-FRS_facility_locs.loc[index_FRS,'LONGITUDE83'])**2+(EPA_facility_info.loc[index_epa,'lat']-FRS_facility_locs.loc[index_FRS,'LATITUDE83'])**2)\n",
    "        if dist < 0.025:\n",
    "            FRS_facility_locs.loc[index_FRS,'Dupl'] = 1\n",
    "            \n",
    "print ('FRS locations: ', len(FRS_facility_locs))\n",
    "print ('Duplicates to be removed: ', FRS_facility_locs['Dupl'].sum())\n",
    "FRS_facility_locs = FRS_facility_locs[FRS_facility_locs['Dupl'] == 0]\n",
    "FRS_facility_locs.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print ('Total Facilities: ', len(EPA_facility_info)+len(compost_council_facilitylocs)+len(biocycle_facility_locs)+len(FRS_facility_locs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.7. Make a gridded Map of Composting Facility Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put facilities on our map (does this need to be at high res???)\n",
    "comp_facility_map = np.zeros([len(lat001),len(lon001)])\n",
    "comp_facility_nongrid = 0\n",
    "\n",
    "# EPA facilities\n",
    "# If on continental US map, add to comp_facilities map\n",
    "for ifacility in np.arange(len(EPA_facility_info)):\n",
    "    if EPA_facility_info.loc[ifacility,'lon'] > Lon_left and \\\n",
    "     EPA_facility_info.loc[ifacility,'lon'] < Lon_right and \\\n",
    "     EPA_facility_info.loc[ifacility,'lat'] > Lat_low and \\\n",
    "     EPA_facility_info.loc[ifacility,'lat'] < Lat_up:\n",
    "        ilat = int((EPA_facility_info.loc[ifacility,'lat'] - Lat_low)/Res_01)\n",
    "        ilon = int((EPA_facility_info.loc[ifacility,'lon'] - Lon_left)/Res_01)\n",
    "        comp_facility_map[ilat,ilon] += 1\n",
    "    else:\n",
    "        comp_facility_nongrid +=1\n",
    "        \n",
    "# Biocycle facilities\n",
    "# If on continental US map, add to comp_facilities map\n",
    "for ifacility in np.arange(len(biocycle_facility_locs)):\n",
    "    #Check if on the grid\n",
    "    if biocycle_facility_locs.loc[ifacility,'lon'] > Lon_left and \\\n",
    "     biocycle_facility_locs.loc[ifacility,'lon'] < Lon_right and \\\n",
    "     biocycle_facility_locs.loc[ifacility,'lat'] > Lat_low and \\\n",
    "     biocycle_facility_locs.loc[ifacility,'lat'] < Lat_up:\n",
    "        ilat = int((biocycle_facility_locs.loc[ifacility,'lat'] - Lat_low)/Res_01)\n",
    "        ilon = int((biocycle_facility_locs.loc[ifacility,'lon'] - Lon_left)/Res_01)\n",
    "        comp_facility_map[ilat,ilon] += 1\n",
    "    else:\n",
    "        comp_facility_nongrid += 1\n",
    "\n",
    "# Composting Council facilities\n",
    "# If on continental US map, add to comp_facilities map\n",
    "for ifacility in np.arange(len(compost_council_facilitylocs)):\n",
    "    if compost_council_facilitylocs.loc[ifacility,'lon'] > Lon_left and \\\n",
    "     compost_council_facilitylocs.loc[ifacility,'lon'] < Lon_right and \\\n",
    "     compost_council_facilitylocs.loc[ifacility,'lat'] > Lat_low and \\\n",
    "     compost_council_facilitylocs.loc[ifacility,'lat'] < Lat_up:\n",
    "        ilat = int((compost_council_facilitylocs.loc[ifacility,'lat'] - Lat_low)/Res_01)\n",
    "        ilon = int((compost_council_facilitylocs.loc[ifacility,'lon'] - Lon_left)/Res_01)\n",
    "        comp_facility_map[ilat,ilon] += 1\n",
    "    else:\n",
    "        comp_facility_nongrid +=1\n",
    "\n",
    "## FRS facilities\n",
    "## If on continental US map, add to comp_facilities map\n",
    "for ifacility in np.arange(len(FRS_facility_locs)):\n",
    "    #Check if on the grid\n",
    "    if FRS_facility_locs.loc[ifacility,'LONGITUDE83'] > Lon_left and \\\n",
    "     FRS_facility_locs.loc[ifacility,'LONGITUDE83'] < Lon_right and \\\n",
    "     FRS_facility_locs.loc[ifacility,'LATITUDE83'] > Lat_low and \\\n",
    "     FRS_facility_locs.loc[ifacility,'LATITUDE83'] < Lat_up:\n",
    "        ilat = int((FRS_facility_locs.loc[ifacility,'LATITUDE83'] - Lat_low)/Res_01)\n",
    "        ilon = int((FRS_facility_locs.loc[ifacility,'LONGITUDE83'] - Lon_left)/Res_01)\n",
    "        comp_facility_map[ilat,ilon] += 1\n",
    "    else:\n",
    "        comp_facility_nongrid +=1\n",
    "        \n",
    "print('Gridded Number of Facilities: ', np.sum(comp_facility_map))\n",
    "print('Nongrid Number of Facilities: ', comp_facility_nongrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.8 Merge Facility data and Population Data (by state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To take the emissions from the state to the gridded level, Step 4 will use a combination of facility locations (for \n",
    "# states where facilities are available) and population density (for states where facilities are not avaialble).\n",
    "# In this case, it is ok to mix variables of different units in a single gridded matrix, because the data that will\n",
    "# be used to allocated emissions from a single state will either be facility locations or population, not both. \n",
    "\n",
    "#Initialize array\n",
    "facility_pop_map = np.zeros([len(lat001),len(lon001),num_years])\n",
    "\n",
    "# 1) Create mask for a given state\n",
    "# 2) Cacluate the number of facilities within that state\n",
    "# 3) If less than 2 facilities in the state, replace with population data for that state\n",
    "# 4) If ore than two facilities in that state, use facility location information for that state\n",
    "for istate in np.arange(0, len(State_ANSI)):\n",
    "    mask_state = np.ma.ones(np.shape(state_ANSI_map))\n",
    "    mask_state = np.ma.masked_where(state_ANSI_map != State_ANSI['ansi'][istate], mask_state)\n",
    "    mask_state = np.ma.filled(mask_state,0)\n",
    "    num_state_facilities = np.sum(mask_state*comp_facility_map)\n",
    "    #print(num_state_facilities)\n",
    "    if num_state_facilities <= 2:\n",
    "        facility_pop_map[:,:,0] += mask_state*pop_den_map*area_map\n",
    "        #print(np.sum(mask_state*pop_den_map*area_map))\n",
    "    else:\n",
    "        facility_pop_map[:,:,0] += mask_state*comp_facility_map\n",
    "        #print(np.sum(mask_state*comp_facility_map))\n",
    "    #print('State', State_ANSI['abbr'][istate], 'of', len(State_ANSI))   \n",
    "\n",
    "#fill in remaining years (proxy held constant over time)\n",
    "for iyear in np.arange(0, num_years):\n",
    "    facility_pop_map[:,:,iyear] = facility_pop_map[:,:,0]\n",
    "    print('Year',year_range_str[iyear],'proxy sum:',np.sum(facility_pop_map[:,:,iyear]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.9. Read In State-Level Recyling & Composting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Option to Use EPA state-level estimates for composting (use Shin et al., 2014 and more recent sources)\n",
    "\n",
    "#State Emissions data from the GHGI workbook will be used as the State-level proxy here\n",
    "# EPA methane emission fractions by state\n",
    "\n",
    "compost_state = np.zeros([len(State_ANSI), num_years])\n",
    "\n",
    "#read in the data file\n",
    "EPA_stateComp_Emissions = pd.read_excel(epa_state_composting_inputfile,skiprows=2, sheet_name = 'Composting (F-4) ')\n",
    "EPA_stateComp_Emissions.dropna(axis=0,inplace=True)\n",
    "EPA_stateComp_Emissions.rename(columns={EPA_stateComp_Emissions.columns[0]:'State'}, inplace=True)\n",
    "\n",
    "EPA_stateComp_Emissions = EPA_stateComp_Emissions.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_stateComp_Emissions.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#make state array\n",
    "for iyear in np.arange(0, num_years):\n",
    "    for istate in np.arange(0, len(EPA_stateComp_Emissions)):\n",
    "        #print(EPA_stateComp_Emissions['State'][istate])\n",
    "        match_state = np.where(EPA_stateComp_Emissions['State'][istate].strip() == State_ANSI['name'])[0][0]\n",
    "        #print(match_state)\n",
    "        compost_state[match_state,iyear] = EPA_stateComp_Emissions.loc[istate,year_range[iyear]]#/(25*1e-3) #covert from MMT CO2e to kt\n",
    "\n",
    "    print('Emissions fraction:', year_range_str[iyear],np.sum(compost_state[:,iyear]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "## Step 3. Read in and Format US EPA GHGI Emissions\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in Table 7-19:  CH4 and N2O Emissions from Composting (kt) \n",
    "#Just one number for each year \n",
    "EPA_emis_kt =pd.read_csv(EPA_comp_inputfile, skiprows=2, nrows=1) #,usecols = [24,25,26,27,28,29,30]\n",
    "EPA_emis_kt = EPA_emis_kt.drop(['Unnamed: 0'], axis=1)\n",
    "EPA_emis_kt.rename(columns={EPA_emis_kt.columns[0]:'Source'}, inplace=True)\n",
    "EPA_emis_kt = EPA_emis_kt.fillna('')\n",
    "temprange = [*range(1990, start_year,1)]\n",
    "dropnames=[str(i) for i in temprange]\n",
    "EPA_emis_kt = EPA_emis_kt.drop(columns = dropnames)\n",
    "print('EPA GHGI National CH4 Emissions (kt):')\n",
    "display(EPA_emis_kt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Split Emissions into Gridding Groups (each Group will have the same proxy applied during the state allocation/gridding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split emissions into scaling groups\n",
    "# In this case, data are only availabe for total emissions\n",
    "\n",
    "DEBUG =1\n",
    "\n",
    "start_year_idx = EPA_emis_kt.columns.get_loc(str(start_year))\n",
    "end_year_idx = EPA_emis_kt.columns.get_loc(str(end_year))+1\n",
    "ghgi_comp_groups = ghgi_comp_map['GHGI_Emi_Group'].unique()\n",
    "sum_emi = np.zeros([num_years])\n",
    "\n",
    "\n",
    "for igroup in np.arange(0,len(ghgi_comp_groups)): #loop through all groups, finding the GHGI sources in that group and summing emissions for that region, year        vars()[ghgi_prod_groups[igroup]] = np.zeros([num_regions-1,num_years])\n",
    "    ##DEBUG## print(ghgi_comp_groups[igroup])\n",
    "    vars()[ghgi_comp_groups[igroup]] = np.zeros([num_years])\n",
    "    source_temp = ghgi_comp_map.loc[ghgi_comp_map['GHGI_Emi_Group'] == ghgi_comp_groups[igroup], 'GHGI_Source']\n",
    "    pattern_temp  = '|'.join(source_temp) \n",
    "    emi_temp = EPA_emis_kt[EPA_emis_kt['Source'].str.contains(pattern_temp)]\n",
    "    ##DEBUG## display(emi_temp)\n",
    "    vars()[ghgi_comp_groups[igroup]][:] = emi_temp.iloc[:,start_year_idx:].sum()\n",
    "    ##DEBUG## display(vars()[ghgi_comp_groups[igroup]][:])\n",
    "        \n",
    "        \n",
    "#Check against total summary emissions \n",
    "print('QA/QC #1: Check Processing Emission Sum against GHGI Summary Emissions')\n",
    "for iyear in np.arange(0,num_years): \n",
    "    for igroup in np.arange(0,len(ghgi_comp_groups)):\n",
    "        sum_emi[iyear] += vars()[ghgi_comp_groups[igroup]][iyear]\n",
    "        \n",
    "    summary_emi = EPA_emis_kt.iloc[0,iyear+1]  \n",
    "    #Check 1 - make sure that the sums from all the regions equal the totals reported\n",
    "    diff1 = abs(sum_emi[iyear] - summary_emi)/((sum_emi[iyear] + summary_emi)/2)\n",
    "    if DEBUG ==1:\n",
    "        print(summary_emi)\n",
    "        print(sum_emi[iyear])\n",
    "    if diff1 < 0.0001:\n",
    "        print('Year ', year_range[iyear],': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear],': FAIL (check Production & summary tabs): ', diff1,'%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Step 4. Grid Emissions\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1. Allocate emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.1.1 Assign the Appropriate Proxy Variable Names (state & grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names on the *left* need to match the 'Composting_ProxyMapping' 'State_Proxy_Group' names \n",
    "# (these are initialized in Step 2). \n",
    "# The names on the *right* are the variable names used to caluclate the proxies in this code.\n",
    "# Names on the right need to match those from the code in Step 2\n",
    "\n",
    "#national --> state proxies (state x year (X month))\n",
    "State_WasteComposted = compost_state\n",
    "\n",
    "#state --> grid proxies (0.01x0.01)\n",
    "Map_Facility_Population = facility_pop_map\n",
    "\n",
    "# remove variables to clear space for larger arrays \n",
    "del facility_pop_map,comp_facility_map,pop_den_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.1.2 Allocate National EPA Emissions to the State-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate state-level emissions (in kt)\n",
    "# State data = national GHGI emissions * state proxy/national total\n",
    "\n",
    "DEBUG =1\n",
    "\n",
    "# Note that national emissions are retained for groups that do not have state proxies (identified in the mapping file)\n",
    "# and are gridded in the next step (not applicable to this composting)\n",
    "\n",
    "# Make placeholder emission arrays for each group\n",
    "for igroup in np.arange(0,len(proxy_comp_map)):\n",
    "    vars()['State_'+proxy_comp_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(State_ANSI),num_years])\n",
    "    vars()['NonState_'+proxy_comp_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])\n",
    "        \n",
    "#Loop over years\n",
    "for iyear in np.arange(num_years):\n",
    "    #Loop over states\n",
    "    for istate in np.arange(len(State_ANSI)):\n",
    "        for igroup in np.arange(0,len(proxy_comp_map)):    \n",
    "            if proxy_comp_map.loc[igroup,'State_Proxy_Group'] != '-' and \\\n",
    "                proxy_comp_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "                vars()['State_'+proxy_comp_map.loc[igroup,'GHGI_Emi_Group']][istate,iyear] = vars()[proxy_comp_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                            data_fn.safe_div(vars()[proxy_comp_map.loc[igroup,'State_Proxy_Group']][istate,iyear], np.sum(vars()[proxy_comp_map.loc[igroup,'State_Proxy_Group']][:,iyear]))\n",
    "            else:\n",
    "                vars()['NonState_'+proxy_comp_map.loc[igroup,'GHGI_Emi_Group']][iyear] = vars()[proxy_comp_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "                \n",
    "# Check sum of all gridded emissions + emissions not included in state allocation\n",
    "print('QA/QC #1: Check weighted emissions against GHGI')   \n",
    "for iyear in np.arange(0,num_years):\n",
    "    summary_emi = EPA_emis_kt.iloc[0,iyear+1]   \n",
    "    calc_emi = 0\n",
    "    for igroup in np.arange(0,len(proxy_comp_map)):\n",
    "        calc_emi +=  np.sum(vars()['State_'+proxy_comp_map.loc[igroup,'GHGI_Emi_Group']][:,iyear])+\\\n",
    "            vars()['NonState_'+proxy_comp_map.loc[igroup,'GHGI_Emi_Group']][iyear] #np.sum(Emissions[:,iyear]) + Emissions_nongrid[iyear] + Emissions_nonstate[iyear]\n",
    "    if DEBUG ==1:\n",
    "        print(summary_emi)\n",
    "        print(calc_emi)\n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0002:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.3 Allocate emissions to the CONUS region (0.1 x0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate State-Level emissions (kt) onto a 0.1x0.1 grid using gridcell level 'Proxy_Groups'\n",
    "\n",
    "DEBUG = 1\n",
    "\n",
    "#Define emission arrays\n",
    "#Emissions_array = np.zeros([area_map.shape[0],area_map.shape[1],num_years,num_months])\n",
    "Emissions_array_01 = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Emissions_nongrid = np.zeros([num_years])\n",
    "\n",
    "# For each year, (2a) distribute state-level emissions onto a grid using proxies defined above ....\n",
    "# To speed up the code, masks are used rather than looping individually through each lat/lon. \n",
    "# In this case, a mask of 1's is made for the grid cells that match the ANSI values for a given state\n",
    "# The masked values are set to zero, remaining values = 1. \n",
    "# AK and HI and territories are removed from the analysis at this stage. \n",
    "# The emissions allocated to each state are at 0.01x0.01 degree resolution, as required to calculate accurate 'mask'\n",
    "# arrays for each state. Emission arrays are re-gridded to 0.1x0.1 degrees as looping through monthly high-resolution\n",
    "# grids was prohibitively slow\n",
    "# (2b - not applicable here) For emission groups that were not first allocated to states, national emissions for those groups are gridded\n",
    "# based on the relevant gridded proxy arrays (0.1x0.1 resolution). These emissions are at 0.1x0.1 degrees resolution. \n",
    "# (2c - not applicable here) - record 'not mapped' emission groups in the 'non-grid' array\n",
    "\n",
    "print('**QA/QC Check: Sum of national gridded emissions vs. GHGI national emissions')\n",
    "for igroup in np.arange(len(proxy_comp_map)):\n",
    "    vars()['Ext_'+proxy_comp_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "    \n",
    "for iyear in np.arange(0,num_years):\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "        #month_days = month_day_leap\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        #month_days = month_day_nonleap \n",
    "    running_count = 0\n",
    "    calc_emi = 0\n",
    "    \n",
    "    #1. Step through each gridding group\n",
    "    for igroup in np.arange(0,len(proxy_comp_map)):\n",
    "        ## 1. weight proxy by the number of days in each month (depending on whether proxy has month res or not)\n",
    "        print(igroup,'of',len(proxy_comp_map))\n",
    "        proxy_temp = vars()[proxy_comp_map.loc[igroup,'Proxy_Group']]\n",
    "        proxy_temp_nongrid = vars()[proxy_comp_map.loc[igroup,'Proxy_Group']+'_nongrid']\n",
    "        \n",
    "        #2a. Step through each state (if group was previously allocated to state level)\n",
    "        if proxy_comp_map.loc[igroup,'State_Proxy_Group'] != '-' and proxy_comp_map.loc[igroup,'State_Proxy_Group'] != 'state_not_mapped':\n",
    "            for istate in np.arange(0,len(State_ANSI)):\n",
    "                mask_state = np.ma.ones(np.shape(state_ANSI_map))\n",
    "                mask_state = np.ma.masked_where(state_ANSI_map != State_ANSI['ansi'][istate], mask_state)\n",
    "                mask_state = np.ma.filled(mask_state,0)   \n",
    "                ##DEBUG## print(\"state \" + str(istate) +' of '+ str(len(State_ANSI)))\n",
    "                if np.sum(mask_state*proxy_temp[:,:,iyear]) > 0 and State_ANSI['abbr'][istate] not in {'AK','HI'} and istate < 51: \n",
    "                    weighted_array = data_fn.safe_div(mask_state*proxy_temp[:,:,iyear], np.sum(mask_state*proxy_temp[:,:,iyear]))\n",
    "                    weighted_array_01 = data_fn.regrid001_to_01(weighted_array, Lat_01, Lon_01)\n",
    "                    #for imonth in np.arange(0,num_months):\n",
    "                    emi_temp = vars()['State_'+proxy_comp_map.loc[igroup,'GHGI_Emi_Group']][istate,iyear]*weighted_array_01\n",
    "                    Emissions_array_01[:,:,iyear] += emi_temp\n",
    "                    vars()['Ext_'+proxy_comp_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += emi_temp\n",
    "                        \n",
    "                else: \n",
    "                    #for imonth in np.arange(0,num_months):\n",
    "                    Emissions_nongrid[iyear] += vars()['State_'+proxy_comp_map.loc[igroup,'GHGI_Emi_Group']][istate,iyear]\n",
    "                ##DEBUG## running_count += np.sum(vars()['State_'+proxy_comp_map.loc[igroup,'GHGI_Emi_Group']][istate,iyear,:])\n",
    "                \n",
    "                ##DEBUG## print(running_count)\n",
    "                ##DEBUG## print(np.sum(Emissions_array_01[:,:,iyear,:]) +np.sum(Emissions_nongrid[iyear,:]))\n",
    "         \n",
    "\n",
    "    #Emissions_array_01[:,:,iyear,:] += data_fn.regrid001_to_01(Emissions_array[:,:,iyear,:], Lat_01, Lon_01) #covert to 10x10km\n",
    "    for igroup in np.arange(0,len(proxy_comp_map)):\n",
    "        calc_emi += np.sum(vars()['Ext_'+proxy_comp_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])\n",
    "    calc_emi += np.sum(Emissions_nongrid[iyear]) \n",
    "    #calc_emi = np.sum(Emissions_array_01[:,:,iyear]) + np.sum(Emissions_nongrid[iyear]) \n",
    "    summary_emi = EPA_emis_kt.iloc[0,iyear+1] \n",
    "    emi_diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if DEBUG ==1:\n",
    "        print(calc_emi)\n",
    "        print(summary_emi)\n",
    "    if abs(emi_diff) < 0.0001:\n",
    "        print('Year '+ year_range_str[iyear]+': Difference < 0.01%: PASS')\n",
    "    else: \n",
    "        print('Year '+ year_range_str[iyear]+': Difference > 0.01%: FAIL, diff: '+str(emi_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1.4 Save gridded emissions (kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save gridded emissions for each gridding group - for extension\n",
    "\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(grid_emi_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "unique_groups = np.unique(proxy_comp_map['GHGI_Emi_Group'])\n",
    "unique_groups = unique_groups[unique_groups != 'Emi_not_mapped']\n",
    "\n",
    "nc_out = Dataset(grid_emi_outputfile, 'r+', format='NETCDF4')\n",
    "#nc_out.createDimension('state', len(State_ANSI))\n",
    "\n",
    "for igroup in np.arange(0,len(unique_groups)):\n",
    "    print('Ext_'+unique_groups[igroup])\n",
    "    if len(np.shape(vars()['Ext_'+unique_groups[igroup]])) ==4:\n",
    "        ghgi_temp = np.sum(vars()[unique_groups[igroup]],axis=3) #sum month data if data is monthly\n",
    "    else:\n",
    "        ghgi_temp = vars()['Ext_'+unique_groups[igroup]]\n",
    "\n",
    "    # Write data to netCDF\n",
    "    data_out = nc_out.createVariable('Ext_'+unique_groups[igroup], 'f8', ('lat', 'lon','year'), zlib=True)\n",
    "    data_out[:,:,:] = ghgi_temp[:,:,:]\n",
    "\n",
    "#save nongrid data to calculate non-grid fraction extension\n",
    "data_out = nc_out.createVariable('Emissions_nongrid', 'f8', ('year'), zlib=True)  \n",
    "data_out[:] = Emissions_nongrid[:]\n",
    "nc_out.close()\n",
    "\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded emissions (kt) written to file: {}\" .format(os.getcwd())+grid_emi_outputfile)\n",
    "print(' ')\n",
    "\n",
    "del data_out, ghgi_temp, nc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Calculate Gridded Emission Fluxes (molec./cm2/s) (0.1x0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert emissions to emission flux\n",
    "# conversion: kt emissions to molec/cm2/s flux\n",
    "\n",
    "\n",
    "DEBUG =1\n",
    "\n",
    "Flux_array_01_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "print('**QA/QC Check: Sum of national gridded emissions vs. GHGI national emissions')\n",
    "  \n",
    "for iyear in np.arange(0,num_years):\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        \n",
    "    conversion_factor_01 = 10**9 * Avogadro / float(Molarch4 *year_days * 24 * 60 *60) / area_matrix_01\n",
    "    Flux_array_01_annual[:,:,iyear] += Emissions_array_01[:,:,iyear]*conversion_factor_01\n",
    "    \n",
    "    #convert back to mass to check\n",
    "    conversion_factor_annual = 10**9 * Avogadro / float(Molarch4 *year_days * 24 * 60 *60) / area_matrix_01\n",
    "    calc_emi = np.sum(Flux_array_01_annual[:,:,iyear]/conversion_factor_annual)+np.sum(Emissions_nongrid[iyear])\n",
    "    summary_emi = EPA_emis_kt.iloc[0,iyear+1] \n",
    "    emi_diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if DEBUG ==1:\n",
    "        print(calc_emi)\n",
    "        print(summary_emi)\n",
    "    if abs(emi_diff) < 0.0001:\n",
    "        print('Year '+ year_range_str[iyear]+': Difference < 0.01%: PASS')\n",
    "    else: \n",
    "        print('Year '+ year_range_str[iyear]+': Difference > 0.01%: FAIL, diff: '+str(emi_diff))\n",
    "        \n",
    "Flux_Emissions_Total_annual = Flux_array_01_annual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 5. Write netCDF\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize netCDF file\n",
    "data_IO_fn.initialize_netCDF(gridded_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write data to netCDF\n",
    "nc_out = Dataset(gridded_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Total_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded composting fluxes written to file: {}\" .format(os.getcwd())+gridded_outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Step 6. Plot Gridded Data\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot Annual Data\n",
    "scale_max = 10\n",
    "save_flag =0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_str, scale_max,save_flag,save_outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot difference between last and first year\n",
    "save_flag =0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_diff_str, save_flag, save_outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = datetime.datetime.now() \n",
    "ft = ct.timestamp() \n",
    "time_elapsed = (ft-it)/(60*60)\n",
    "print('Time to run: '+str(time_elapsed)+' hours')\n",
    "print('** GEPA_5B1_Composting: COMPLETE **')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

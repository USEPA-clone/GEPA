{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded EPA Methane Inventory\n",
    "## Category: Post Meter Emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Authors: \n",
    "Erin E. McDuffie\n",
    "#### Date Last Updated: \n",
    "see Step 0\n",
    "#### Notebook Purpose\n",
    "This notebook calculates gridded (0.1⁰x0.1⁰) annual emission fluxes of methane (molecules CH4/cm2/s) from post meter activities in the CONUS region for the years 2012 - 2018. Emission fluxes are reported at an annual time resolution. \n",
    "Emissions are calculated from the 2022 version of the GHGI (which extends to 2020)\n",
    "#### Summary & Notes \n",
    "The national EPA GHGI emissions data are read in from the GHGI Post Meter workbook. Residential and commercial emissions use the same proxy datasets as other GEPA distribution segement residential and commercial customer meter emissions. Data are first allocated to each state using EIA customer counts and then to the grid using gridded population. For Industrial and EGU emissions, data are allocated using the same proxies as used for industrial and EGU stationary combustion sources. For industrial, emissions are allocated to the state using EIA SEDS data and then to the grid using GHGRP facility level information. For EGUs, emissions are allocated directly to the grid using EPA facility level Acid Rain Program data. For CNG vehicles, emissions are allocated to each state using CNG vehcile counts from MOVES and then to the grid using population. Total emissions are converted to annual emision fluxes (molec./cm2/s) and are written to final netCDFs in the '/code/Final_Gridded_Data/Supplement' folder. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Step 0. Set-Up Notebook Modules, Functions, and Local Parameters and Constants\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm working directory\n",
    "import os\n",
    "import time\n",
    "modtime = os.path.getmtime('./Ext_PostMeter.ipynb')\n",
    "modificationTime = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(modtime))\n",
    "print(\"This file was last modified on: \", modificationTime)\n",
    "print('')\n",
    "print(\"The directory we are working in is {}\" .format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include plots within notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from copy import copy\n",
    "\n",
    "# Import additional modules\n",
    "# Load plotting package Basemap \n",
    "# Must also specify project library path [unique to each user])\n",
    "#os.environ[\"PROJ_LIB\"] = \"C:/Users//Anaconda3/Library/share/proj\"\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# Load netCDF (for manipulating netCDF file types)\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Set up ticker\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#add path for the global function module (file)\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../../Global_Functions/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Load user-defined global functions (modules)\n",
    "import data_load_functions as data_load_fn\n",
    "import data_functions as data_fn\n",
    "import data_IO_functions as data_IO_fn\n",
    "import data_plot_functions as data_plot_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT Files\n",
    "# Assign global file names\n",
    "global_filenames = data_load_fn.load_global_file_names()\n",
    "State_ANSI_inputfile = global_filenames[0]\n",
    "#County_ANSI_inputfile = global_filenames[1]\n",
    "pop_map_inputfile = global_filenames[2]\n",
    "Grid_area01_inputfile = global_filenames[3]\n",
    "Grid_area001_inputfile = global_filenames[4]\n",
    "Grid_state001_ansi_inputfile = global_filenames[5]\n",
    "#Grid_county001_ansi_inputfile = global_filenames[6]\n",
    "\n",
    "# Specify names of inputs files used in this notebook\n",
    "EPA_post_meter_inputfile = \"../../Global_InputData/GHGI/Ch3_Energy/Copy of NG_Post-Meter_1990-2020.xlsx\" #EPA_Enteric_Cattle.csv\"\n",
    "\n",
    "#Activity Data\n",
    "EPA_ARP_inputfile = '../../GEPA_Combustion_Stationary/InputData/ARP_Data/EPA_ARP_FacilityEmissions.csv'\n",
    "EIA_SEDS_indconsump_inputfile = \"../../GEPA_Combustion_Stationary/InputData/EIA_SEDS/Industrial/sum_btu_ind_\"\n",
    "\n",
    "#GHGRP Data (reporting format changed in 2015)\n",
    "GHGRP_subCfacility_inputfile = \"../../GEPA_Combustion_Stationary/InputData/GHGRP/GHGRP_SubpartCEmissions.csv\" #subpart C facility IDs and emissions (locations not available)\n",
    "GHGRP_subDfacility_inputfile = \"../../GEPA_Combustion_Stationary/InputData/GHGRP/GHGRP_SubpartDEmissions.csv\" #subpart D facility IDs and emissions \n",
    "GHGRP_subDfacility_loc_inputfile = \"../../GEPA_Combustion_Stationary/InputData/GHGRP/GHGRP_FacilityInfo.csv\" #subpart D facility info (for all years, with ID & lat and lons)\n",
    "\n",
    "#EIA Data\n",
    "EIA_Residential_CC_inputfile = '../../GEPA_Gas_Distribution/InputData/EIA_CustomerCounts/NG_CONS_NUM_A_EPG0_VN3_COUNT_A.xls'\n",
    "EIA_Commercial_CC_inputfile = '../../GEPA_Gas_Distribution/InputData/EIA_CustomerCounts/NG_CONS_NUM_A_EPG0_VN5_COUNT_A.xls'\n",
    "\n",
    "#Moves Data\n",
    "moves_inputfile = './InputData/CNG_Vehicle_By_State.csv'\n",
    "\n",
    "#Proxy Data file\n",
    "PM_Mapping_inputfile = \"./InputData/PostMeter_ProxyMapping.xlsx\"\n",
    "\n",
    "#Specify names of gridded output files\n",
    "gridded_outputfile = '../../Final_Gridded_Data/Supplement/EPA_v2_Supp_PostMeter.nc'\n",
    "netCDF_description = 'Supplement to the Gridded EPA Inventory - Post Meter Emissions - IPCC Source Category 1B2b'\n",
    "title_str = \"EPA methane emissions from post meter\"\n",
    "title_diff_str = \"Emissions from post meter difference: 2018-2012\"\n",
    "\n",
    "#output gridded proxy data\n",
    "grid_emi_outputfile = '../../Final_Gridded_Data/Extension/v2_input_data/PostMeter_Grid_Emi.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local variables\n",
    "start_year = 2012  #First year in emission timeseries\n",
    "end_year = 2018    #Last year in emission timeseries\n",
    "year_range = [*range(start_year, end_year+1,1)] #List of emission years\n",
    "year_range_str=[str(i) for i in year_range]\n",
    "num_years = len(year_range)\n",
    "\n",
    "# Define constants\n",
    "Avogadro   = 6.02214129 * 10**(23)  #molecules/mol\n",
    "Molarch4   = 16.04                  #g/mol\n",
    "Res01      = 0.1                    # degrees\n",
    "Res_01     = 0.01                   # degrees\n",
    "tg_scale   = 0.001                  #Tg scale number [New file allows for the exclusion of the territories] \n",
    "\n",
    "\n",
    "# Continental US Lat/Lon Limits (for netCDF files)\n",
    "Lon_left = -130       #deg\n",
    "Lon_right = -60       #deg\n",
    "Lat_low  = 20         #deg\n",
    "Lat_up  = 55          #deg\n",
    "\n",
    "loc_dimensions = [Lat_low, Lat_up, Lon_left, Lon_right]\n",
    "ilat_start = int((90+Lat_low)/Res01) #1100:1450 (continental US range)\n",
    "ilat_end = int((90+Lat_up)/Res01)\n",
    "ilon_start = abs(int((-180-Lon_left)/Res01)) #500:1200 (continental US range)\n",
    "ilon_end = abs(int((-180-Lon_right)/Res01))\n",
    "\n",
    "# Number of days in each month\n",
    "month_day_leap  = [  31,  29,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_day_nonleap = [  31,  28,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "\n",
    "# Month arrays\n",
    "month_range_str = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "num_months = len(month_range_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track run time\n",
    "ct = datetime.datetime.now() \n",
    "it = ct.timestamp() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Step 1. Load in State and County ANSI data and Area Maps\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State-level ANSI Data\n",
    "#Read the state ANSI file array\n",
    "State_ANSI, name_dict, abbr_dict = data_load_fn.load_state_ansi('../'+State_ANSI_inputfile)[0:3]\n",
    "#QA: number of states\n",
    "print('Read input file: '+ f\"{State_ANSI_inputfile}\")\n",
    "print('Total \"States\" found: ' + '%.0f' % len(State_ANSI))\n",
    "print(' ')\n",
    "\n",
    "# 0.01 x0.01 degree Data\n",
    "# State ANSI IDs and grid cell area (m2) maps\n",
    "state_ANSI_map = data_load_fn.load_state_ansi_map('../'+Grid_state001_ansi_inputfile)\n",
    "area_map, lat001, lon001 = data_load_fn.load_area_map_001('../'+Grid_area001_inputfile)\n",
    "\n",
    "\n",
    "#County ANSI Data\n",
    "#Includes State ANSI number, county ANSI number, county name, and country area (square miles)\n",
    "#County_ANSI = pd.read_csv(County_ANSI_inputfile,encoding='latin-1')\n",
    "\n",
    "#QA: number of counties\n",
    "#print ('Read input file: ' + f\"{'../'+County_ANSI_inputfile}\")\n",
    "#print('Total \"Counties\" found (include PR): ' + '%.0f' % len(County_ANSI))\n",
    "#print(' ')\n",
    "\n",
    "#Create a placeholder array for county data\n",
    "#county_array = np.zeros([len(County_ANSI),3])\n",
    "\n",
    "#Populate array with State ANSI number (0), county ANSI number (1), and county area (2)\n",
    "#for icounty in np.arange(0,len(County_ANSI)):\n",
    "#    county_array[icounty,0] = int(County_ANSI.values[icounty,0])\n",
    "#    county_array[icounty,1] = int(County_ANSI.values[icounty,1])\n",
    "#    county_array[icounty,2] = County_ANSI.values[icounty,3]\n",
    "\n",
    "# 0.01 x0.01 degree Data\n",
    "# State ANSI IDs and grid cell area (m2) maps\n",
    "state_ANSI_map = data_load_fn.load_state_ansi_map('../'+Grid_state001_ansi_inputfile)\n",
    "state_ANSI_map = state_ANSI_map.astype('int32')\n",
    "#county_ANSI_map = data_load_fn.load_county_ansi_map(Grid_county001_ansi_inputfile)\n",
    "#county_ANSI_map = county_ANSI_map.astype('int32')\n",
    "area_map, lat001, lon001 = data_load_fn.load_area_map_001('../'+Grid_area001_inputfile)\n",
    "\n",
    "# 0.1 x0.1 degree data\n",
    "# grid cell area and state and county ANSI maps\n",
    "area_map01, Lat01, Lon01 = data_load_fn.load_area_map_01('../'+Grid_area01_inputfile)[0:3]\n",
    "#Select relevant Continental 0.1 x0.1 domain\n",
    "Lat_01 = Lat01[ilat_start:ilat_end]\n",
    "Lon_01 = Lon01[ilon_start:ilon_end]\n",
    "area_matrix_01 = data_fn.regrid001_to_01(area_map, Lat_01, Lon_01)\n",
    "area_matrix_01 *= 10000  #convert from m2 to cm2\n",
    "\n",
    "state_ANSI_map_01 = data_fn.regrid001_to_01(state_ANSI_map, Lat_01, Lon_01)\n",
    "\n",
    "# Print time\n",
    "ct = datetime.datetime.now() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------\n",
    "## Step 2. Read in and Format Proxy Data\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1 Read In Proxy Mapping File & Make Proxy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load GHGI Mapping Groups\n",
    "names = pd.read_excel(PM_Mapping_inputfile, sheet_name = \"GHGI Map - PM\", usecols = \"A:B\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "ghgi_pm_map = pd.read_excel(PM_Mapping_inputfile, sheet_name = \"GHGI Map - PM\", usecols = \"A:B\", skiprows = 1, names = colnames)\n",
    "#drop rows with no data, remove the parentheses and \"\"\n",
    "ghgi_pm_map = ghgi_pm_map[ghgi_pm_map['GHGI_Emi_Group'] != 'na']\n",
    "ghgi_pm_map = ghgi_pm_map[ghgi_pm_map['GHGI_Emi_Group'].notna()]\n",
    "ghgi_pm_map['GHGI_Source']= ghgi_pm_map['GHGI_Source'].str.replace(r\"\\(\",\"\")\n",
    "ghgi_pm_map['GHGI_Source']= ghgi_pm_map['GHGI_Source'].str.replace(r\"\\)\",\"\")\n",
    "ghgi_pm_map.reset_index(inplace=True, drop=True)\n",
    "display(ghgi_pm_map)\n",
    "\n",
    "#load emission group - proxy map\n",
    "names = pd.read_excel(PM_Mapping_inputfile, sheet_name = \"Proxy Map - PM\", usecols = \"A:G\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "proxy_pm_map = pd.read_excel(PM_Mapping_inputfile, sheet_name = \"Proxy Map - PM\", usecols = \"A:G\", skiprows = 1, names = colnames)\n",
    "display((proxy_pm_map))\n",
    "\n",
    "#create empty proxy and emission group arrays (add months for proxy variables that have monthly data)\n",
    "for igroup in np.arange(0,len(proxy_pm_map)):\n",
    "    if proxy_pm_map.loc[igroup, 'Grid_Month_Flag'] ==0:\n",
    "        vars()[proxy_pm_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        vars()[proxy_pm_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "    else:\n",
    "        vars()[proxy_pm_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "        vars()[proxy_pm_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years,num_months])\n",
    "        \n",
    "    vars()[proxy_pm_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])\n",
    "    \n",
    "    if proxy_pm_map.loc[igroup,'State_Proxy_Group'] != '-':\n",
    "        if proxy_pm_map.loc[igroup,'State_Month_Flag'] == 0:\n",
    "            vars()[proxy_pm_map.loc[igroup,'State_Proxy_Group']] = np.zeros([len(State_ANSI),num_years])\n",
    "        else:\n",
    "            vars()[proxy_pm_map.loc[igroup,'State_Proxy_Group']] = np.zeros([len(State_ANSI),num_years,num_months])\n",
    "    else:\n",
    "        continue # do not make state proxy variable if no variable assigned in mapping file\n",
    "        \n",
    "    if proxy_pm_map.loc[igroup,'County_Proxy_Group'] != '-':\n",
    "        if proxy_pm_map.loc[igroup,'County_Month_Flag'] == 0:\n",
    "            vars()[proxy_pm_map.loc[igroup,'County_Proxy_Group']] = np.zeros([len(State_ANSI),len(County_ANSI),num_years])\n",
    "        else:\n",
    "            vars()[proxy_pm_map.loc[igroup,'County_Proxy_Group']] = np.zeros([len(State_ANSI),len(County_ANSI),num_years,num_months])\n",
    "    else:\n",
    "        continue # do not make state proxy variable if no variable assigned in mapping file\n",
    "\n",
    "        \n",
    "emi_group_names = np.unique(ghgi_pm_map['GHGI_Emi_Group'])\n",
    "\n",
    "print('QA/QC: Is the number of emission groups the same for the proxy and emissions tabs?')\n",
    "if (len(emi_group_names) == len(np.unique(proxy_pm_map['GHGI_Emi_Group']))):\n",
    "    print('PASS')\n",
    "else:\n",
    "    print('FAIL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Read In and Format EPA (Acid Rain Program) Electric Power Emissions (Electric Energy Proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 Read in EPA power plant facility information and calculate facility-level emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read EPA ARP data for individual power plants. \n",
    "# Calculate emissions from the unit type and fuel type to calculate CH4 emission factor to apply to Heat Input.\n",
    "#https://ampd.epa.gov/ampd/\n",
    "\n",
    "fields = ['State', ' Year',' Month', ' Facility Name',' Facility Latitude',' Facility Longitude',' Unit Type', \\\n",
    "          ' Fuel Type (Primary)', ' Heat Input (MMBtu)']\n",
    "ARP_Raw = pd.read_csv(EPA_ARP_inputfile, usecols = fields, index_col=False, na_filter = False)\n",
    "\n",
    "# make a multidimensional dictionary that contains the data for each year\n",
    "# for calculations later, replace empty heat input values with NaNs and convert\n",
    "# to numeric (otherwise data in scientific notation are read in as strings)\n",
    "ARP_facilities = dict()\n",
    "for iyear in np.arange(num_years):\n",
    "    ARP_facilities[iyear] = ARP_Raw[ARP_Raw[' Year'] == year_range[iyear]]\n",
    "    ARP_facilities[iyear].fillna(0)#,inplace = True)\n",
    "    ARP_facilities[iyear].reset_index(inplace=True)\n",
    "    temp = pd.to_numeric(ARP_facilities[iyear].loc[:,' Heat Input (MMBtu)'], errors='coerce')\n",
    "    temp.fillna(0,inplace=True)\n",
    "    ARP_facilities[iyear].loc[:,' Heat Input (MMBtu)'] = temp\n",
    "\n",
    "\n",
    "# Clean up and standardize the Unit Type labels\n",
    "# Assign values to a temporary dataframe to avoid settingwithcopy warning\n",
    "\n",
    "# For each year, check the unit types and report a clean string version in a new column\n",
    "for iyear in np.arange(num_years):\n",
    "    temp = pd.DataFrame.from_dict(ARP_facilities[iyear])\n",
    "\n",
    "    for ifacility in np.arange(len(ARP_facilities[iyear])):\n",
    "        if re.search('combustion turbine',ARP_facilities[iyear].loc[ifacility,' Unit Type'].lower()) != None:\n",
    "            temp.loc[ifacility,'Unit_clean'] = 'combustion turbine'\n",
    "        elif re.search('combined cycle',ARP_facilities[iyear].loc[ifacility,' Unit Type'].lower()) != None:\n",
    "            temp.loc[ifacility,'Unit_clean'] = 'combined cycle'\n",
    "        elif re.search('wet bottom',ARP_facilities[iyear].loc[ifacility,' Unit Type'].lower()) != None:\n",
    "            temp.loc[ifacility,'Unit_clean'] = 'wet bottom'\n",
    "        elif re.search('dry bottom',ARP_facilities[iyear].loc[ifacility,' Unit Type'].lower()) != None:\n",
    "            temp.loc[ifacility,'Unit_clean'] = 'dry bottom'\n",
    "        elif re.search('bubbling',ARP_facilities[iyear].loc[ifacility,' Unit Type'].lower()) != None:\n",
    "            temp.loc[ifacility,'Unit_clean'] = 'bubbling'\n",
    "        else:\n",
    "            temp.loc[ifacility,'Unit_clean'] = ARP_facilities[iyear].loc[ifacility,' Unit Type'].lower()  \n",
    "\n",
    "        ARP_facilities[iyear] = temp.copy()\n",
    "\n",
    "#Clean up and standardize the fuel type labels\n",
    "# Assign values to a temporary dataframe to avoid settingwithcopy warning\n",
    "\n",
    "# For each year, check the primary fuel types and consolidate into Gas, Coal, Oil, and Wood fuel categories\n",
    "for iyear in np.arange(num_years):\n",
    "    \n",
    "    temp = pd.DataFrame.from_dict(ARP_facilities[iyear])\n",
    "    for ifacility in np.arange(len(ARP_facilities[iyear])):\n",
    "        if ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Pipeline Natural Gas' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Natural Gas' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Other Gas' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Other Gas, Pipeline Natural Gas' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Natural Gas, Pipeline Natural Gas' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Process Gas':\n",
    "            temp.loc[ifacility,'Fuel_clean'] = 'Gas'\n",
    "        elif ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Petroleum Coke' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Coal' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Coal, Pipeline Natural Gas' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Coal, Natural Gas' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Coal, Wood' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Coal, Coal Refuse' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Coal Refuse' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Other Solid Fuel':\n",
    "            temp.loc[ifacility,'Fuel_clean'] = 'Coal'\n",
    "        elif ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Other Oil' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Diesel Oil' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Diesel Oil, Residual Oil' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Diesel Oil, Pipeline Natural Gas' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Residual Oil' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Residual Oil, Pipeline Natural Gas':\n",
    "            temp.loc[ifacility,'Fuel_clean'] = 'Oil'\n",
    "        elif ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Wood' \\\n",
    "         or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Other Solid Fuel, Wood':\n",
    "            temp.loc[ifacility,'Fuel_clean'] = 'Wood'\n",
    "        else:\n",
    "            if ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] != '':\n",
    "                print(ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'])  \n",
    "                \n",
    "    ARP_facilities[iyear] = temp.copy()  \n",
    "\n",
    "del ARP_Raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2. Add the CH4 factor (kg gas/ TJ energy input) to the data dictionary, then calculate methane flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CH4 factor is based on the unit type and fuel used\n",
    "# From 'Acid Rain Prog - Unit-level Fuel+Technology' file in InputData Folder\n",
    "\n",
    "for iyear in np.arange(num_years):\n",
    "    ARP_facilities[iyear].loc[:,'CH4_f'] = 0.0\n",
    "\n",
    "    for ifacility in np.arange(len(ARP_facilities[iyear])):\n",
    "        # Gas: combined cycle or turbine= 3.7, \n",
    "        # Gas: others (Assume stoker, tangentially-fired, dry bottom, & wet bottom are boilers) = 1\n",
    "        if ARP_facilities[iyear].loc[ifacility,'Fuel_clean'] == 'Gas':\n",
    "            if ARP_facilities[iyear].loc[ifacility,'Unit_clean'] == 'combined cycle' \\\n",
    "             or re.search('turbine',ARP_facilities[iyear].loc[ifacility,'Unit_clean'].lower()) != None:\n",
    "                ARP_facilities[iyear].loc[ifacility,'CH4_f'] = 3.7     \n",
    "            else:\n",
    "                ARP_facilities[iyear].loc[ifacility,'CH4_f'] = 1\n",
    "        # Coal: tangentially-fired, dry bottom = 0.7\n",
    "        # Coal: wet bottom = 0.9\n",
    "        # Coal: Cyclone boiler = 0.2\n",
    "        # Coal: others (boilers, combined cycle) = 1\n",
    "        elif ARP_facilities[iyear].loc[ifacility,'Fuel_clean'] == 'Coal':\n",
    "            if ARP_facilities[iyear].loc[ifacility,'Unit_clean'] == 'tangentially-fired' \\\n",
    "             or ARP_facilities[iyear].loc[ifacility,'Unit_clean'] == 'dry bottom':\n",
    "                ARP_facilities[iyear].loc[ifacility,'CH4_f'] = 0.7\n",
    "            elif ARP_facilities[iyear].loc[ifacility,'Unit_clean'] == 'wet bottom':   \n",
    "                ARP_facilities[iyear].loc[ifacility,'CH4_f'] = 0.9\n",
    "            elif ARP_facilities[iyear].loc[ifacility,'Unit_clean'] == 'cyclone boiler':   \n",
    "                ARP_facilities[iyear].loc[ifacility,'CH4_f'] = 0.2\n",
    "            else:\n",
    "                ARP_facilities[iyear].loc[ifacility,'CH4_f'] = 1   \n",
    "        # Wood: assume all are recover boilders = 1\n",
    "        elif ARP_facilities[iyear].loc[ifacility,'Fuel_clean'] == 'Wood':\n",
    "            ARP_facilities[iyear].loc[ifacility,'CH4_f'] = 1   \n",
    "        # Oil: Reisdual oil, pipeline natural gas = 0.8\n",
    "        # Oil: Others = 0.9\n",
    "        elif ARP_facilities[iyear].loc[ifacility,'Fuel_clean'] == 'Oil':\n",
    "            if ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Residual Oil' \\\n",
    "             or ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] == 'Residual Oil, Pipeline Natural Gas':\n",
    "                ARP_facilities[iyear].loc[ifacility,'CH4_f'] = 0.8\n",
    "            else:\n",
    "                ARP_facilities[iyear].loc[ifacility,'CH4_f'] = 0.9\n",
    "        else:\n",
    "            if ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'] != '':\n",
    "                print(ARP_facilities[iyear].loc[ifacility,' Fuel Type (Primary)'])\n",
    "    \n",
    "\n",
    "#Calculate the methane flux at each facility and the flux by fuel at each facility relative to the national total. \n",
    "for iyear in np.arange(num_years):\n",
    "    # Calculate fluxes\n",
    "    ARP_facilities[iyear]['CH4_flux'] = 0.0\n",
    "    ARP_facilities[iyear]['CH4_flux'] = ARP_facilities[iyear]['CH4_f'] * ARP_facilities[iyear][' Heat Input (MMBtu)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2.3 Allocate flux data (as a function gas fuel type only) to grid arrays and to the state level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arp_gas_array_mon = np.zeros([len(Lat_01),len(Lon_01),num_years, num_months])\n",
    "arp_gas_array_mon_nongrid = np.zeros([num_years,num_months])\n",
    "arp_gas_array = np.zeros([len(Lat_01),len(Lon_01),num_years,])\n",
    "arp_gas_array_nongrid = np.zeros([num_years])\n",
    "\n",
    "\n",
    "for iyear in np.arange(num_years):\n",
    "    #count=0\n",
    "    var = 'CH4_flux'\n",
    "    for ifacility in np.arange(len(ARP_facilities[iyear])):\n",
    "        imon = int(ARP_facilities[iyear].loc[ifacility,' Month'] - 1)\n",
    "        istate = np.where(ARP_facilities[iyear].loc[ifacility,'State'] == State_ANSI['abbr'])[0][0]\n",
    "        #Filter inside Continental US domain\n",
    "        if ARP_facilities[iyear].loc[ifacility,' Facility Longitude'] > Lon_left \\\n",
    "         and ARP_facilities[iyear].loc[ifacility,' Facility Longitude'] < Lon_right \\\n",
    "         and ARP_facilities[iyear].loc[ifacility,' Facility Latitude'] > Lat_low \\\n",
    "         and ARP_facilities[iyear].loc[ifacility,' Facility Latitude'] < Lat_up:\n",
    "            #Find the index values of each facility lat and lon within the Continental US grid \n",
    "            ilat = int((ARP_facilities[iyear].loc[ifacility,' Facility Latitude'] - Lat_low)/Res01)\n",
    "            ilon = int((ARP_facilities[iyear].loc[ifacility,' Facility Longitude'] - Lon_left)/Res01)\n",
    "            if ARP_facilities[iyear].loc[ifacility, 'Fuel_clean'] == 'Gas':\n",
    "                arp_gas_array_mon[ilat,ilon,iyear,imon] += ARP_facilities[iyear].loc[ifacility,var]\n",
    "        else:    \n",
    "            if ARP_facilities[iyear].loc[ifacility, 'Fuel_clean'] == 'Gas':\n",
    "                arp_gas_array_mon_nongrid[iyear,imon] += ARP_facilities[iyear].loc[ifacility,var] \n",
    "                \n",
    "for iyear in np.arange(num_years):\n",
    "    arp_gas_array[:,:,iyear] = np.sum(arp_gas_array_mon[:,:,iyear,:],axis=2)\n",
    "    arp_gas_array_nongrid[iyear] = np.sum(arp_gas_array_mon_nongrid[iyear,:])\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3 Read In and Format EIA SEDS (State Engery Data System) Energy Consumption Data (Commercial, Residential, Industrial Proxies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.1 Read In EIA SEDS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Read state-level energy consumption data (Industrial)\n",
    "SEDS_ind = dict()\n",
    "for iyear in np.arange(0, num_years):\n",
    "    SEDS_ind[iyear] = pd.read_csv(EIA_SEDS_indconsump_inputfile+year_range_str[iyear]+'.csv',nrows=51)\n",
    "    SEDS_ind[iyear]['ASCI'] = 0\n",
    "    for istate in np.arange(len(SEDS_ind[iyear])):\n",
    "        SEDS_ind[iyear].loc[istate,'ASCI'] = name_dict[SEDS_ind[iyear].loc[istate,'State'].strip()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.2 Allocate BTUs to the state level (industrial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcualte relative state level BTU levels for industrial SEDS data, by state and fuel type\n",
    "\n",
    "sedsind_gas_state = np.zeros([len(State_ANSI), num_years])\n",
    "\n",
    "\n",
    "#print('**QA/QC: Check allocated state-level commercial emissions against GHGI')\n",
    "#print('')\n",
    "for iyear in np.arange(num_years):\n",
    "    #Calculate emissions\n",
    "    \n",
    "    for istate in np.arange(len(SEDS_ind[iyear])):\n",
    "        state_str = SEDS_ind[iyear].loc[istate,'State']\n",
    "        state_str = state_str.strip()\n",
    "        matchstate = np.where(state_str == State_ANSI['name'])[0][0]\n",
    "        sedsind_gas_state[matchstate,iyear] += SEDS_ind[iyear].loc[istate,'Natural Gas'] #/NGas_sum[iyear]      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.4. Read In GHGRP Subpart C and D Data (Industrial Proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.4.1 Read in Subpart C and D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Subpart D and C facility lists, find the subpart C facilities that were not in subpart D list\n",
    "# and then merge with subpart D list to create a complete facility information array\n",
    "\n",
    "#facility level data for Subpart C\n",
    "GHGRP_all = pd.read_csv(GHGRP_subCfacility_inputfile) \n",
    "#filter for methane emissions only\n",
    "GHGRP_all = GHGRP_all[GHGRP_all['C_SUBPART_LEVEL_INFORMATION.GHG_GAS_NAME'] == 'Methane']\n",
    "GHGRP_all = GHGRP_all.drop(columns = ['C_SUBPART_LEVEL_INFORMATION.FACILITY_NAME'])\n",
    "GHGRP_all.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#facility level data for subpart D\n",
    "GHGRP_elec = pd.read_csv(GHGRP_subDfacility_inputfile) \n",
    "#filter for methane emissions only\n",
    "GHGRP_elec = GHGRP_elec[GHGRP_elec['D_SUBPART_LEVEL_INFORMATION.GHG_NAME'] == 'Methane']\n",
    "GHGRP_elec = GHGRP_elec.drop(columns = ['D_SUBPART_LEVEL_INFORMATION.FACILITY_NAME'])\n",
    "GHGRP_elec.reset_index(inplace=True, drop=True)\n",
    "GHGRP_elec_fac = np.array(GHGRP_elec['D_SUBPART_LEVEL_INFORMATION.FACILITY_ID'])\n",
    "GHGRP_elec_fac = np.unique(GHGRP_elec_fac)\n",
    "\n",
    "GHGRP_comb_noloc = dict()\n",
    "\n",
    "#make a list of facilities that report to subpart C that are not in the subpart D facility list\n",
    "for iyear in np.arange(0,num_years):\n",
    "    temp = list()\n",
    "    for ifacility in np.arange(len(GHGRP_all)):       \n",
    "        if GHGRP_all.loc[ifacility,'C_SUBPART_LEVEL_INFORMATION.REPORTING_YEAR'] == year_range[iyear]:\n",
    "            if GHGRP_all.loc[ifacility,'C_SUBPART_LEVEL_INFORMATION.FACILITY_ID'] not in GHGRP_elec_fac:\n",
    "                temp.append(GHGRP_all.loc[ifacility])\n",
    "    GHGRP_comb_noloc[iyear] = pd.DataFrame(temp)\n",
    "\n",
    "GHGRP_comb_noloc[0].head(1)\n",
    "\n",
    "# Read Facility Info file that contains lat and lon for GHGRP facilities\n",
    "#extract the reporting facilities for the most recent year\n",
    "GHGRP_facloc = pd.read_csv(GHGRP_subDfacility_loc_inputfile)\n",
    "sort = GHGRP_facloc.sort_values(by=['V_GHG_EMITTER_FACILITIES.YEAR'])\n",
    "filter1 = sort.drop_duplicates(subset = 'V_GHG_EMITTER_FACILITIES.FACILITY_ID' , keep = 'last')\n",
    "filter2 = filter1.drop(columns=['V_GHG_EMITTER_FACILITIES.YEAR','V_GHG_EMITTER_FACILITIES.STATE'])\n",
    "Fac_rename = filter2.rename(columns={'V_GHG_EMITTER_FACILITIES.FACILITY_ID': 'FACILITY_ID'})\n",
    "\n",
    "#merge the missing subpart C facility list with the Subpart D facility list to get lat and lon values for all facilities\n",
    "GHGRP_combfac = dict()\n",
    "for iyear in np.arange(num_years):\n",
    "    Comb_rename = GHGRP_comb_noloc[iyear].rename(columns={'C_SUBPART_LEVEL_INFORMATION.FACILITY_ID': 'FACILITY_ID'})\n",
    "    temp = Comb_rename.merge(Fac_rename, on = 'FACILITY_ID')\n",
    "    GHGRP_combfac[iyear] = temp\n",
    "\n",
    "#Check that no repeats were added\n",
    "for iyear in np.arange(num_years):\n",
    "    if  GHGRP_combfac[iyear].shape[0] != GHGRP_comb_noloc[iyear].shape[0]:\n",
    "        print('Dataframe size discrepancy')\n",
    "\n",
    "display(GHGRP_combfac[0])#.head(1)\n",
    "del Fac_rename, GHGRP_all, GHGRP_elec, GHGRP_elec_fac,GHGRP_facloc,filter1,filter2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.4.2 Allocate facility level methane emissions to the CONUS grid (0.01x0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a 0.1x0.1 gridded map of GHGRP facility-level emissions\n",
    "# also record the emissions that are not within the CONUS grid \n",
    "\n",
    "#ghgrp_emi_array = np.zeros([num_years,area_map.shape[0],area_map.shape[1]])\n",
    "ghgrp_emi_array = np.zeros([area_map.shape[0],area_map.shape[1], num_years])\n",
    "ghgrp_emi_array_nongrid = np.zeros([num_years])\n",
    "\n",
    "for iyear in np.arange(num_years):\n",
    "    n_plants = 0\n",
    "    for ifacility in np.arange(len(GHGRP_combfac[iyear])):\n",
    "        #Filter inside domain\n",
    "        if GHGRP_combfac[iyear].loc[ifacility,'V_GHG_EMITTER_FACILITIES.LONGITUDE'] > Lon_left \\\n",
    "         and GHGRP_combfac[iyear].loc[ifacility,'V_GHG_EMITTER_FACILITIES.LONGITUDE'] < Lon_right \\\n",
    "         and GHGRP_combfac[iyear].loc[ifacility,'V_GHG_EMITTER_FACILITIES.LATITUDE'] > Lat_low \\\n",
    "         and GHGRP_combfac[iyear].loc[ifacility,'V_GHG_EMITTER_FACILITIES.LATITUDE'] < Lat_up:\n",
    "            #find the corresponding plant ilon and ilat, record the emissions at that location\n",
    "            ilat = int((GHGRP_combfac[iyear].loc[ifacility,'V_GHG_EMITTER_FACILITIES.LATITUDE'] - Lat_low)/Res_01)\n",
    "            ilon = int((GHGRP_combfac[iyear].loc[ifacility,'V_GHG_EMITTER_FACILITIES.LONGITUDE'] - Lon_left)/Res_01)\n",
    "            ghgrp_emi_array[ilat,ilon,iyear] += GHGRP_combfac[iyear].loc[ifacility,'C_SUBPART_LEVEL_INFORMATION.GHG_QUANTITY']\n",
    "            n_plants += 1\n",
    "        else:\n",
    "            ghgrp_emi_array_nongrid[iyear] += GHGRP_combfac[iyear].loc[ifacility,'C_SUBPART_LEVEL_INFORMATION.GHG_QUANTITY']\n",
    "    print (year_range_str[iyear]+' Facilities: ', n_plants)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.5 State-level EIA Customer Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read company level EIA data, but only load the totals for all companies across each state\n",
    "EIA_ResCounts = pd.read_excel(EIA_Residential_CC_inputfile, sheet_name = 'Data 1', skiprows=2)\n",
    "EIA_ComCounts = pd.read_excel(EIA_Commercial_CC_inputfile, sheet_name = 'Data 1', skiprows=2)\n",
    "    \n",
    "    #convert the time stamp to year\n",
    "EIA_ResCounts['Date'] = EIA_ResCounts['Date'].astype(str)\n",
    "EIA_ComCounts['Date'] = EIA_ComCounts['Date'].astype(str)\n",
    "EIA_ResCounts['Date'] = [EIA_ResCounts['Date'][i][0:4] for i in np.arange(len(EIA_ResCounts))]   #extract the year\n",
    "EIA_ComCounts['Date'] = [EIA_ComCounts['Date'][i][0:4] for i in np.arange(len(EIA_ComCounts))]   #extract the year\n",
    "#transpose and reset the column and row indexes\n",
    "EIA_ResCounts = EIA_ResCounts.T\n",
    "EIA_ComCounts = EIA_ComCounts.T\n",
    "EIA_ResCounts.columns = EIA_ResCounts.iloc[0]\n",
    "EIA_ComCounts.columns = EIA_ComCounts.iloc[0]\n",
    "EIA_ResCounts = EIA_ResCounts.drop(EIA_ResCounts.index[[0,1]])\n",
    "EIA_ComCounts = EIA_ComCounts.drop(EIA_ComCounts.index[[0,1]])\n",
    "#extract the state names and format as abbreviations\n",
    "Names_res = EIA_ResCounts.index.values.tolist()\n",
    "Names_com = EIA_ComCounts.index.values.tolist()\n",
    "EIA_ResCounts.reset_index(drop=True,inplace=True)\n",
    "EIA_ComCounts.reset_index(drop=True,inplace=True)\n",
    "EIA_ResCounts['State'] = Names_res\n",
    "EIA_ComCounts['State'] = Names_com\n",
    "    \n",
    "for istate in np.arange(0,len(State_ANSI)-6): #### -6\n",
    "    #print(State_ANSI['name'][istate])\n",
    "    match_state = np.where(EIA_ResCounts['State'].str.contains(State_ANSI['name'][istate]))[0][0]\n",
    "    EIA_ResCounts['State'][match_state] = State_ANSI['abbr'][istate]\n",
    "    #print(match_state)\n",
    "    match_state = np.where(EIA_ComCounts['State'].str.contains(State_ANSI['name'][istate]))[0][0]\n",
    "    EIA_ComCounts['State'][match_state] = State_ANSI['abbr'][istate]\n",
    "    #print(match_state)\n",
    "\n",
    "# Initialize state array of leak losses (State X year) \n",
    "res_counts = np.zeros((len(State_ANSI),num_years))\n",
    "com_counts = np.zeros((len(State_ANSI),num_years))\n",
    "\n",
    "    # To this array, add ANSI value for each state, then for each year,\n",
    "    # make a new leakvolume array that records the yearly state-level leak loss volume data\n",
    "    # Data only extend back to 1997, so use 1997 values for years 1990-1996\n",
    "\n",
    "for ifacility in np.arange(0,len(EIA_ResCounts)):\n",
    "    match_state1 = np.where(State_ANSI['abbr'] == EIA_ResCounts['State'][ifacility])[0][0]\n",
    "    match_state2 = np.where(State_ANSI['abbr'] == EIA_ComCounts['State'][ifacility])[0][0]\n",
    "\n",
    "    for iyear in np.arange(num_years):\n",
    "        res_counts[match_state1][iyear] += EIA_ResCounts[str(iyear+start_year)][ifacility]\n",
    "        com_counts[match_state2][iyear] += EIA_ComCounts[str(iyear+start_year)][ifacility]\n",
    "\n",
    "# Fill in data gaps (interpolate to fill zeros between years and extend most historical value back to 1990)\n",
    "res_counts = np.nan_to_num(res_counts)\n",
    "com_counts = np.nan_to_num(com_counts)\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    print(year_range_str[iyear]+' Total Counts: ')\n",
    "    print(' Residential Customers: '+str(res_counts.sum(axis=0)[iyear]))\n",
    "    print(' Commercial Customers: '+str(com_counts.sum(axis=0)[iyear]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.6. MOVES CNG Vehicle Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data were pulled from the MOVES model on CNG vehicle counts by state for the years 1990, 1999, and 2020. \n",
    "# Data are interpolated between years\n",
    "\n",
    "#Initialize array\n",
    "MOVES_cng_full = np.zeros([len(State_ANSI),2020-1990])\n",
    "MOVES_cng = np.zeros([len(State_ANSI),num_years])\n",
    "\n",
    "#post_meter_inputfile\n",
    "\n",
    "moves = pd.read_csv(moves_inputfile, sep=',')\n",
    "idx_1990 = 1990-start_year\n",
    "idx_1999 = 1999-start_year\n",
    "idx_2020 = 2020-start_year\n",
    "\n",
    "moves = moves[['yearID','stateAbbr','VehCount_ERG']]\n",
    "\n",
    "for istate in np.arange(0,len(State_ANSI)):\n",
    "    #print(istate)\n",
    "    subset = moves[moves['stateAbbr'] == State_ANSI['abbr'][istate]]\n",
    "    if len(subset) > 1:\n",
    "        #display(subset)\n",
    "        MOVES_cng_full[istate,idx_1990] = subset.loc[subset['yearID'] == 1990, 'VehCount_ERG']\n",
    "        MOVES_cng_full[istate,idx_1999] = subset.loc[subset['yearID'] == 1999, 'VehCount_ERG']\n",
    "        MOVES_cng_full[istate,idx_2020] = subset.loc[subset['yearID'] == 2020, 'VehCount_ERG']\n",
    "    \n",
    "# Fill in data gaps (interpolate to fill zeros between years and extend most historical value back to 1990)\n",
    "for istate in np.arange(0,len(MOVES_cng)):\n",
    "    temp = MOVES_cng_full[istate][:] \n",
    "    if MOVES_cng_full[istate][0] ==0:\n",
    "        index = np.argmax(temp > 0)    \n",
    "        temp[0:index] = temp[index]      #extend most historical value back to 1990 if no 1990 data available\n",
    "    temp = pd.Series(temp)       \n",
    "    temp.replace(0,np.NaN, inplace=True)\n",
    "    temp = temp.interpolate().values      #interpolate to fill missing years\n",
    "    MOVES_cng_full[istate][:] = temp            #reasaign to original array\n",
    "\n",
    "    MOVES_cng_full = np.nan_to_num(MOVES_cng_full)\n",
    "    #display(MOVES_cng[istate,:])\n",
    "    \n",
    "#extract desired timeseries\n",
    "MOVES_cng[:,:] = MOVES_cng_full[:,start_year-1990:end_year-1990+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.7 Read in gridded population density data and calculate state-level populations, regrid to 0.1x0.1 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read population density map\n",
    "pop_den_map = data_load_fn.load_pop_den_map('../'+pop_map_inputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "## Step 3. Read in and Format US EPA GHGI Emissions\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NG CH4 in units of metric tonnes, converted to kt\n",
    "names = pd.read_excel(EPA_post_meter_inputfile, sheet_name = \"Post-Meter Summary\", usecols = \"A:AD\", skiprows = 1, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "EPA_emi_postm_CH4 = pd.read_excel(EPA_post_meter_inputfile, sheet_name = \"Post-Meter Summary\", usecols = \"A:AD\", skiprows = 1, names = colnames, nrows = 6)\n",
    "EPA_emi_postm_CH4.rename(columns={'Year':'Source'},inplace=True)\n",
    "EPA_emi_postm_CH4 = EPA_emi_postm_CH4.drop(columns = [n for n in range(1990, start_year,1)])\n",
    "EPA_emi_postm_CH4.iloc[:,1:] = EPA_emi_postm_CH4.iloc[:,1:]/1000 #covert from metric tons to kt\n",
    "EPA_emi_postm_total = EPA_emi_postm_CH4[EPA_emi_postm_CH4['Source'] == 'Post-Meter Total']\n",
    "\n",
    "display(EPA_emi_postm_CH4)\n",
    "display(EPA_emi_postm_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Split Emissions into Gridding Groups (each Group will have the same proxy applied during the state allocation/gridding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in units of kt\n",
    "\n",
    "start_year_idx = EPA_emi_postm_CH4.columns.get_loc((start_year))\n",
    "end_year_idx = EPA_emi_postm_CH4.columns.get_loc((end_year))+1\n",
    "ghgi_pm_groups = ghgi_pm_map['GHGI_Emi_Group'].unique()\n",
    "sum_emi = np.zeros([num_years])\n",
    "\n",
    "DEBUG =1\n",
    "\n",
    "for igroup in np.arange(0,len(ghgi_pm_groups)): #loop through all groups, finding the GHGI sources in that group and summing emissions for that region, year\n",
    "    #print(igroup)\n",
    "    vars()[ghgi_pm_groups[igroup]] = np.zeros([num_years])\n",
    "    source_temp = ghgi_pm_map.loc[ghgi_pm_map['GHGI_Emi_Group'] == ghgi_pm_groups[igroup], 'GHGI_Source']\n",
    "    pattern_temp  = '|'.join(source_temp) \n",
    "    #print(pattern_temp)\n",
    "    emi_temp = EPA_emi_postm_CH4[EPA_emi_postm_CH4['Source'].str.contains(pattern_temp)]\n",
    "    vars()[ghgi_pm_groups[igroup]][:] = np.where(emi_temp.iloc[:,start_year_idx:] =='',[0],emi_temp.iloc[:,start_year_idx:]).sum(axis=0)\n",
    "    #print(emi_temp)    \n",
    "        \n",
    "#Check against total summary emissions \n",
    "print('QA/QC #1: Check Processing Emission Sum against GHGI Summary Emissions')\n",
    "for iyear in np.arange(0,num_years): \n",
    "    for igroup in np.arange(0,len(ghgi_pm_groups)):\n",
    "        sum_emi[iyear] += vars()[ghgi_pm_groups[igroup]][iyear]\n",
    "        \n",
    "    summary_emi = EPA_emi_postm_total.iloc[0,iyear+1]  \n",
    "    #Check 1 - make sure that the sums from all the regions equal the totals reported\n",
    "    diff1 = abs(sum_emi[iyear] - summary_emi)/((sum_emi[iyear] + summary_emi)/2)\n",
    "    if DEBUG ==1:\n",
    "        print(summary_emi)\n",
    "        print(sum_emi[iyear])\n",
    "    if diff1 < 0.0001:\n",
    "        print('Year ', year_range[iyear],': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear],': FAIL (check Production & summary tabs): ', diff1,'%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Step 4. Grid Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1. Allocate emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.1.1 Assign the Appropriate Proxy Variable Names (state & grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names on the *left* need to match the 'Stationary_ProxyMapping' 'State_Proxy_Group' names \n",
    "# (these are initialized in Step 2). \n",
    "# The names on the *right* are the variable names used to caluclate the proxies in this code.\n",
    "# Names on the right need to match those from the code in Step 2\n",
    "\n",
    "#national --> state proxies (state x year)\n",
    "state_indu_gas= sedsind_gas_state\n",
    "state_residential = res_counts\n",
    "state_commercial = com_counts\n",
    "state_cng_vehicles = MOVES_cng\n",
    "\n",
    "#national --> grid proxies (0.1x0.1)\n",
    "Map_elec_gas = arp_gas_array\n",
    "Map_elec_gas_nongrid = arp_gas_array_nongrid\n",
    "\n",
    "#state --> grid proxies (0.01x0.01)\n",
    "Map_indu = ghgrp_emi_array\n",
    "Map_indu_nongrid = ghgrp_emi_array_nongrid\n",
    "Map_population = np.zeros([area_map.shape[0], area_map.shape[1], num_years])\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    Map_population[:,:,iyear] = pop_den_map*area_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.1.2 Allocate National EPA Emissions to the State-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate state-level emissions for \n",
    "# Emissions in kt\n",
    "# State data = national GHGI emissions * state proxy/national total\n",
    "\n",
    "\n",
    "# Note that national emissions are retained for groups that do not have state proxies (identified in the mapping file)\n",
    "# and are gridded in the next step\n",
    "DEBUG =1\n",
    "\n",
    "# Make placeholder emission arrays for each group\n",
    "for igroup in np.arange(0,len(proxy_pm_map)):\n",
    "    #if proxy_pm_map.loc[igroup,'State_Month_Flag'] ==1:\n",
    "    vars()['State_'+proxy_pm_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(State_ANSI),num_years])\n",
    "    #else:\n",
    "    #    vars()['State_'+proxy_dist_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(State_ANSI),num_years])\n",
    "    vars()['NonState_'+proxy_pm_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])\n",
    "        \n",
    "#Loop over years\n",
    "for iyear in np.arange(num_years):\n",
    "    #Loop over states\n",
    "    for istate in np.arange(len(State_ANSI)):\n",
    "        for igroup in np.arange(0,len(proxy_pm_map)):    \n",
    "            if proxy_pm_map.loc[igroup,'State_Proxy_Group'] != '-' and proxy_pm_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "                vars()['State_'+proxy_pm_map.loc[igroup,'GHGI_Emi_Group']][istate,iyear] =  vars()[proxy_pm_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                    data_fn.safe_div(vars()[proxy_pm_map.loc[igroup,'State_Proxy_Group']][istate,iyear], np.sum(vars()[proxy_pm_map.loc[igroup,'State_Proxy_Group']][:,iyear]))\n",
    "            else:\n",
    "                vars()['NonState_'+proxy_pm_map.loc[igroup,'GHGI_Emi_Group']][iyear] = vars()[proxy_pm_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "                \n",
    "# Check sum of all gridded emissions + emissions not included in state allocation\n",
    "print('QA/QC #1: Check weighted emissions against GHGI')   \n",
    "for iyear in np.arange(0,num_years):\n",
    "    summary_emi = EPA_emi_postm_total.iloc[0,iyear+1] \n",
    "    calc_emi = 0\n",
    "    for igroup in np.arange(0,len(proxy_pm_map)):\n",
    "        calc_emi +=  np.sum(vars()['State_'+proxy_pm_map.loc[igroup,'GHGI_Emi_Group']][:,iyear])+\\\n",
    "            vars()['NonState_'+proxy_pm_map.loc[igroup,'GHGI_Emi_Group']][iyear] #np.sum(Emissions[:,iyear]) + Emissions_nongrid[iyear] + Emissions_nonstate[iyear]\n",
    "    if DEBUG ==1:\n",
    "        print(summary_emi)\n",
    "        print(calc_emi)\n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0001:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.3 Allocate emissions to the CONUS region (0.1x0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Allocate State-Level emissions (kt) onto a 0.1x0.1 grid using gridcell level 'Proxy_Groups'\n",
    "\n",
    "#Define emission arrays\n",
    "#Emissions_array = np.zeros([area_map.shape[0],area_map.shape[1],num_years,num_months])\n",
    "Emissions_array_01 = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Emissions_nongrid = np.zeros([num_years])\n",
    "\n",
    "DEBUG=1 \n",
    "# To speed up the code, masks are used rather than looping individually through each lat/lon. \n",
    "# In this case, a mask of 1's is made for the grid cells that match the ANSI values for a given county\n",
    "# The masked values are set to zero, remaining values = 1. \n",
    "# For each year, (2a), if emission groups have been previously allocated to the state-level, then allocate to grid\n",
    "# AK and HI and territories are removed from the analysis at this stage. \n",
    "# The final emissions allocated to the grid are at 0.01x0.01 degree resolution, as required to calculate accurate 'mask'\n",
    "# arrays for each state. \n",
    "######Emission arrays are re-gridded to 0.1x0.1 degrees as looping through monthly high-resolution\n",
    "# grids was prohibitively slow\n",
    "# (2b) For emission groups that were not first allocated to states, national emissions for those groups are gridded\n",
    "# based on the relevant gridded proxy arrays (0.1x0.1 resolution). These emissions are at 0.1x0.1 degrees resolution. \n",
    "# (2c ) - record 'not mapped' emission groups in the 'non-grid' array\n",
    "\n",
    "\n",
    "print('**QA/QC Check: Sum of national gridded emissions vs. GHGI national emissions')\n",
    "#make emission group array to save later (0.1x0.1 degrees)\n",
    "for igroup in np.arange(len(proxy_pm_map)):\n",
    "    vars()['Ext_'+proxy_pm_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    print(iyear, 'year of',num_years)\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "        month_days = month_day_leap\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        month_days = month_day_nonleap \n",
    "    running_count = 0\n",
    "    \n",
    "    #1. Step through each gridding group\n",
    "    for igroup in np.arange(0,len(proxy_pm_map)):\n",
    "        print(igroup, 'group of',len(proxy_pm_map))\n",
    "\n",
    "        # 1. weight proxy by the number of days in each month (depending on whether proxy has month res or not)\n",
    "        proxy_temp = vars()[proxy_pm_map.loc[igroup,'Proxy_Group']]\n",
    "        proxy_temp_nongrid = vars()[proxy_pm_map.loc[igroup,'Proxy_Group']+'_nongrid']\n",
    "        if proxy_pm_map.loc[igroup,'Grid_Month_Flag'] ==1:\n",
    "            for imonth in np.arange(0, num_months):\n",
    "                proxy_temp[:,:,iyear,imonth] *= month_days[imonth]\n",
    "                proxy_temp_nongrid[iyear,imonth] *= month_days[imonth]\n",
    "        else:\n",
    "            proxy_temp[:,:,iyear] *= np.sum(month_days)\n",
    "            proxy_temp_nongrid[iyear] *= np.sum(month_days)\n",
    "        ##DEBUG## print(\"group \" + str(igroup) +' of '+ str(len(proxy_pm_map)))\n",
    "                        \n",
    "        \n",
    "        #2a.if  allocated to state-level, Step through each state (if group was previously allocated to state level)\n",
    "        if proxy_pm_map.loc[igroup,'State_Proxy_Group'] != '-' and proxy_pm_map.loc[igroup,'State_Proxy_Group'] != 'state_not_mapped':\n",
    "            for istate in np.arange(0,len(State_ANSI)):\n",
    "                mask_state = np.ma.ones(np.shape(state_ANSI_map))\n",
    "                mask_state = np.ma.masked_where(state_ANSI_map != State_ANSI['ansi'][istate], mask_state)\n",
    "                mask_state = np.ma.filled(mask_state,0)   \n",
    "                if np.sum(mask_state*proxy_temp[:,:,iyear]) > 0 and State_ANSI['abbr'][istate] not in {'AK','HI'} and istate < 51: \n",
    "                    weighted_array = data_fn.safe_div(mask_state*proxy_temp[:,:,iyear], np.sum(mask_state*proxy_temp[:,:,iyear]))\n",
    "                    weighted_array_01 = data_fn.regrid001_to_01(weighted_array, Lat_01, Lon_01)\n",
    "                    #for imonth in np.arange(0,num_months):\n",
    "                    grid_emi = vars()['State_'+proxy_pm_map.loc[igroup,'GHGI_Emi_Group']][istate,iyear]*weighted_array_01\n",
    "                    Emissions_array_01[:,:,iyear] += grid_emi\n",
    "                    vars()['Ext_'+proxy_pm_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += grid_emi\n",
    "                else: \n",
    "                    #for imonth in np.arange(0,num_months):\n",
    "                    Emissions_nongrid[iyear] += vars()['State_'+proxy_pm_map.loc[igroup,'GHGI_Emi_Group']][istate,iyear]\n",
    "                ##DEBUG## running_count += np.sum(vars()['State_'+proxy_pm_map.loc[igroup,'GHGI_Emi_Group']][istate,iyear,:])\n",
    "                \n",
    "                ##DEBUG## print(running_count)\n",
    "                ##DEBUG## print(np.sum(Emissions_array_01[:,:,iyear,:]) +np.sum(Emissions_nongrid[iyear,:]))\n",
    "            #print(igroup, np.sum(Emissions_array_01[:,:,iyear]))\n",
    "            #print(Emissions_nongrid[iyear])\n",
    "                \n",
    "        #2b. if instead emissions are not allocated to state , allocate national total to grid here\n",
    "        elif proxy_pm_map.loc[igroup,'State_Proxy_Group'] == '-':\n",
    "            temp_sum = np.sum(vars()[proxy_pm_map.loc[igroup,'Proxy_Group']][:,:,iyear])+np.sum(vars()[proxy_pm_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear])\n",
    "            #for imonth in np.arange(0,num_months):\n",
    "            grid_emi = vars()[proxy_pm_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                data_fn.safe_div(vars()[proxy_pm_map.loc[igroup,'Proxy_Group']][:,:,iyear], temp_sum)\n",
    "            Emissions_array_01[:,:,iyear] += grid_emi\n",
    "            vars()['Ext_'+proxy_pm_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += grid_emi\n",
    "            Emissions_nongrid[iyear] += vars()[proxy_pm_map.loc[igroup,'GHGI_Emi_Group']][iyear] *\\\n",
    "                data_fn.safe_div(vars()[proxy_pm_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear], temp_sum)\n",
    "            ##DEBUG## running_count += vars()[proxy_pm_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "            #print(igroup, np.sum(Emissions_array_01[:,:,iyear]))\n",
    "            #print(Emissions_nongrid[iyear])\n",
    "                \n",
    "        #2d. this is the case that GHGI emissions are not mapped (e.g., specified outside of CONUS in the GHGI)\n",
    "        elif proxy_pm_map.loc[igroup,'Proxy_Group'] == 'Map_not_mapped':    \n",
    "            #for imonth in np.arange(0,num_months):\n",
    "            Emissions_nongrid[iyear] += vars()[proxy_pm_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "            ##DEBUG## running_count += vars()[proxy_pm_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "        ##DEBUG## print(running_count)\n",
    "        ##DEBUG## print(np.sum(Emissions_array_01[:,:,iyear,:]) +np.sum(Emissions_nongrid[iyear,:]))\n",
    "            #print(igroup, np.sum(Emissions_array_01[:,:,iyear]))\n",
    "            #print(Emissions_nongrid[iyear])\n",
    "            \n",
    "    #Emissions_array_01[:,:,iyear,:] += data_fn.regrid001_to_01(Emissions_array[:,:,iyear,:], Lat_01, Lon_01) #covert to 10x10km\n",
    "    calc_emi = np.sum(Emissions_array_01[:,:,iyear]) + np.sum(Emissions_nongrid[iyear]) \n",
    "    summary_emi = EPA_emi_postm_total.iloc[0,iyear+1] \n",
    "    emi_diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if DEBUG ==1:\n",
    "        print(calc_emi)\n",
    "        print(summary_emi)\n",
    "    if abs(emi_diff) < 0.0001:\n",
    "        print('Year '+ year_range_str[iyear]+': Difference < 0.01%: PASS')\n",
    "    else: \n",
    "        print('Year '+ year_range_str[iyear]+': Difference > 0.01%: FAIL, diff: '+str(emi_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1.4 Save gridded emissions (kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save gridded emissions for each gridding group - for extension\n",
    "\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(grid_emi_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "unique_groups = proxy_pm_map['GHGI_Emi_Group']\n",
    "unique_groups = unique_groups[unique_groups != 'Emi_not_mapped']\n",
    "\n",
    "nc_out = Dataset(grid_emi_outputfile, 'r+', format='NETCDF4')\n",
    "#nc_out.createDimension('state', len(State_ANSI))\n",
    "\n",
    "for igroup in np.arange(0,len(unique_groups)):\n",
    "    print('Ext_'+unique_groups[igroup])\n",
    "    #print(len(np.shape(vars()['Ext_'+unique_groups[igroup]])))\n",
    "    if len(np.shape(vars()['Ext_'+unique_groups[igroup]])) ==4:\n",
    "        ghgi_temp = np.sum(vars()[unique_groups[igroup]],axis=3) #sum month data\n",
    "    else:\n",
    "        ghgi_temp = vars()['Ext_'+unique_groups[igroup]][:,:,:]\n",
    "        print(np.shape(ghgi_temp))\n",
    "        print(np.sum(ghgi_temp[:,:,0]))\n",
    "\n",
    "    # Write data to netCDF\n",
    "    data_out = nc_out.createVariable('Ext_'+unique_groups[igroup], 'f8', ('lat', 'lon','year'), zlib=True)\n",
    "    data_out[:,:,:] = ghgi_temp[:,:,:]\n",
    "\n",
    "#save nongrid data to calculate non-grid fraction extension\n",
    "data_out = nc_out.createVariable('Emissions_nongrid', 'f8', ('year'), zlib=True)  \n",
    "data_out[:] = Emissions_nongrid[:]\n",
    "nc_out.close()\n",
    "\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded emissions (kt) written to file: {}\" .format(os.getcwd())+grid_emi_outputfile)\n",
    "print(' ')\n",
    "\n",
    "del data_out, ghgi_temp, nc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Calculate Gridded Emission Fluxes (molec./cm2/s) (0.1x0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert emissions to emission flux\n",
    "# conversion: kt emissions to molec/cm2/s flux\n",
    "\n",
    "Flux_array_01_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "print('**QA/QC Check: Sum of national gridded emissions vs. GHGI national emissions')\n",
    "  \n",
    "for iyear in np.arange(0,num_years):\n",
    "    calc_emi = 0\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "\n",
    "    conversion_factor_01 = 10**9 * Avogadro / float(Molarch4 *year_days * 24 * 60 *60) / area_matrix_01\n",
    "    Flux_array_01_annual[:,:,iyear] = Emissions_array_01[:,:,iyear]*conversion_factor_01\n",
    "    #convert back to mass to check\n",
    "    conversion_factor_annual = 10**9 * Avogadro / float(Molarch4 *year_days * 24 * 60 *60) / area_matrix_01\n",
    "    calc_emi = np.sum(Flux_array_01_annual[:,:,iyear]/conversion_factor_annual)+np.sum(Emissions_nongrid[iyear])\n",
    "    summary_emi = EPA_emi_postm_total.iloc[0,iyear+1]\n",
    "    emi_diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if DEBUG==1:\n",
    "        print(calc_emi)\n",
    "        print(summary_emi)\n",
    "    if abs(emi_diff) < 0.0001:\n",
    "        print('Year '+ year_range_str[iyear]+': Difference < 0.01%: PASS')\n",
    "    else: \n",
    "        print('Year '+ year_range_str[iyear]+': Difference > 0.01%: FAIL, diff: '+str(emi_diff))\n",
    "        \n",
    "Flux_Emissions_Total_annual = Flux_array_01_annual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 5. Write netCDF\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearly data\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(gridded_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write data to netCDF\n",
    "nc_out = Dataset(gridded_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Total_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded post meter emissions written to file: {}\" .format(os.getcwd())+gridded_outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Step 6. Plot Gridded Data\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.1. Plot Annual Emission Fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot Annual Data\n",
    "scale_max = 5\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_str,scale_max,save_flag,save_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.2 Plot Difference between first and last inventory year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot difference between last and first year\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_diff_str,save_flag,save_outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = datetime.datetime.now() \n",
    "ft = ct.timestamp() \n",
    "time_elapsed = (ft-it)/(60*60)\n",
    "print('Time to run: '+str(time_elapsed)+' hours')\n",
    "print('** GEPA_Post_Meter_Supplement: COMPLETE **')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

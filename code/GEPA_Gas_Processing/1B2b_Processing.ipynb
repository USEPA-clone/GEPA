{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded EPA Methane Inventory\n",
    "## Category: 1B2b Natural Gas Systems - Processing Segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Authors: \n",
    "Joannes D. Maasakkers, Erin E. McDuffie\n",
    "#### Date Last Updated: \n",
    "see Step 0\n",
    "#### Notebook Purpose: \n",
    "This Notebook calculates and reports annual gridded (0.1⁰x0.1⁰) methane emission fluxes (molec./cm2/s) from natural gas systems processing segment in the CONUS region between 2012-2018. \n",
    "#### Summary & Notes:\n",
    "EPA GHGI gas processing emissions are read in from the GHGI Natural Gas Systems workbook at the national level. Emissions are then distributed onto a 0.1x0.1 degree grid as a function of emission group. The activity/proxy data used to spatially distribute emissions from each group include Enverus Processing plant locations and GHGRP processing plant emissions. Emissions data are calculated as a function of year, using annual data from GHGRP and a single snapshot of current processing plant locations from Enverus. Annual emission fluxes (molec./cm2/s) are written to final netCDFs in the ‘/code/Final_Gridded_Data/’ folder. \n",
    "***\n",
    "\n",
    "**Potential update: Could alternatively read in O&G journal data, match with GHGRP plant-emissions by county. Then calculate national emissions/throughput ratio by county first, then taking national average. Could then calculate relative emissions based on O&G journal throughput and then distribute emissions by county (for all plants not matched to GHGRP data). Would require matching GHGRP & O&G journal based on county, then mapping based on county. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Step 0. Set-Up Notebook Modules, Functions, and Local Parameters and Constants\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm working directory\n",
    "import os\n",
    "import time\n",
    "modtime = os.path.getmtime('./1B2b_Processing.ipynb')\n",
    "modificationTime = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(modtime))\n",
    "print(\"This file was last modified on: \", modificationTime)\n",
    "print(\"The directory we are working in is {}\" .format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Include plots within notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import pyodbc\n",
    "import PyPDF2 as pypdf\n",
    "import tabula as tb\n",
    "from datetime import datetime\n",
    "from copy import copy\n",
    "\n",
    "# Import additional modules\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# Load netCDF (for manipulating netCDF file types)\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Set up ticker\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#add path for the global function module (file)\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../Global_Functions/'))\n",
    "#print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Load functions\n",
    "import data_load_functions as data_load_fn\n",
    "import data_functions as data_fn\n",
    "import data_IO_functions as data_IO_fn\n",
    "import data_plot_functions as data_plot_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT Files\n",
    "# Assign global file names\n",
    "global_filenames = data_load_fn.load_global_file_names()\n",
    "State_ANSI_inputfile = global_filenames[0]\n",
    "#County_ANSI_inputfile = global_filenames[1]\n",
    "pop_map_inputfile = global_filenames[2]\n",
    "Grid_area01_inputfile = global_filenames[3]\n",
    "Grid_area001_inputfile = global_filenames[4]\n",
    "Grid_state001_ansi_inputfile = global_filenames[5]\n",
    "#Grid_county001_ansi_inputfile = global_filenames[6]\n",
    "globalinputlocation = global_filenames[0][0:20]\n",
    "print(globalinputlocation)\n",
    "\n",
    "# EPA Inventory Data\n",
    "EPA_NG_inputfile = globalinputlocation+'GHGI/Ch3_Energy/NaturalGasSystems_1990-2018_GHGI_2020-04-11.xlsx'\n",
    "\n",
    "#proxy mapping file\n",
    "NG_Mapping_inputfile = './InputData/NaturalGas_Processing_ProxyMapping.xlsx'\n",
    "\n",
    "#Activity Data\n",
    "Enverus_NG_ProcPlant_inputfile = globalinputlocation+'Enverus/Midstream/Processing_Plants_CONUS_onshore_WGS84_01x01.xls'\n",
    "GHGRP_facility_inputfile = './InputData/GHGRP_Facility_Info.csv'\n",
    "GHGRP_subpartw_inputfile = './InputData/EF_W_EMISSIONS_SOURCE_GHG.xlsx'\n",
    "\n",
    "OGJ_ProcPlant_inputfile = './InputData/OGJ_2015_processing_plants.pdf'\n",
    "\n",
    "\n",
    "#OUTPUT FILES\n",
    "gridded_outputfile = '../Final_Gridded_Data/EPA_v2_1B2b_Natural_Gas_Processing.nc'\n",
    "netCDF_description = 'Gridded EPA Inventory - Natural Gas Systems Emissions - IPCC Source Category 1B2b - Processing'\n",
    "title_str = \"EPA methane emissions from gas processing\"\n",
    "title_diff_str = \"Emissions from gas processing difference: 2018-2012\"\n",
    "\n",
    "#output gridded proxy data\n",
    "grid_emi_outputfile = '../Final_Gridded_Data/Extension/v2_input_data/NG_Processing_Grid_Emi.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local variables\n",
    "start_year = 2012  #First year in emission timeseries\n",
    "end_year = 2018    #Last year in emission timeseries\n",
    "year_range = [*range(start_year, end_year+1,1)] #List of emission years\n",
    "year_range_str=[str(i) for i in year_range]\n",
    "num_years = len(year_range)\n",
    "\n",
    "# Define constants\n",
    "Avogadro   = 6.02214129 * 10**(23)  #molecules/mol\n",
    "Molarch4   = 16.04                  #g/mol\n",
    "Res01      = 0.1                    # degrees\n",
    "\n",
    "# Continental US Lat/Lon Limits (for netCDF files)\n",
    "Lon_left = -130       #deg\n",
    "Lon_right = -60       #deg\n",
    "Lat_low  = 20         #deg\n",
    "Lat_up  = 55          #deg\n",
    "loc_dimensions = [Lat_low, Lat_up, Lon_left, Lon_right]\n",
    "\n",
    "ilat_start = int((90+Lat_low)/Res01) #1100:1450 (continental US range)\n",
    "ilat_end = int((90+Lat_up)/Res01)\n",
    "ilon_start = abs(int((-180-Lon_left)/Res01)) #500:1200 (continental US range)\n",
    "ilon_end = abs(int((-180-Lon_right)/Res01))\n",
    "\n",
    "# Number of days in each month\n",
    "month_day_leap  = [  31,  29,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_day_nonleap = [  31,  28,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_tag = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "month_dict = {'January':1, 'February':2,'March':3,'April':4,'May':5,'June':6, 'July':7,'August':8,'September':9,'October':10,\\\n",
    "             'November':11,'December':12}\n",
    "\n",
    "# Month arrays\n",
    "month_range_str = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "num_months = len(month_range_str)\n",
    "#num_regions = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;\n",
    "//prevent auto-scrolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track run time\n",
    "ct = datetime.now() \n",
    "it = ct.timestamp() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Step 1. Load in State ANSI data and Area Maps\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State-level ANSI Data\n",
    "#Read the state ANSI file array\n",
    "State_ANSI, name_dict, abbr_dict = data_load_fn.load_state_ansi(State_ANSI_inputfile)[0:3]\n",
    "#QA: number of states\n",
    "print('Read input file: '+ f\"{State_ANSI_inputfile}\")\n",
    "print('Total \"States\" found: ' + '%.0f' % len(State_ANSI))\n",
    "print(' ')\n",
    "\n",
    "# 0.01 x0.01 degree Data\n",
    "# State ANSI IDs and grid cell area (m2) maps\n",
    "state_ANSI_map = data_load_fn.load_state_ansi_map(Grid_state001_ansi_inputfile)\n",
    "area_map, lat001, lon001 = data_load_fn.load_area_map_001(Grid_area001_inputfile)\n",
    "\n",
    "# 0.1 x0.1 degree data\n",
    "# grid cell area and state ANSI maps\n",
    "Lat01, Lon01 = data_load_fn.load_area_map_01(Grid_area01_inputfile)[1:3]\n",
    "#Select relevant Continental 0.1 x0.1 domain\n",
    "Lat_01 = Lat01[ilat_start:ilat_end]\n",
    "Lon_01 = Lon01[ilon_start:ilon_end]\n",
    "area_matrix_01 = data_fn.regrid001_to_01(area_map, Lat_01, Lon_01)\n",
    "area_matrix_01 *= 10000  #convert from m2 to cm2\n",
    "state_ANSI_map_01 = data_fn.regrid001_to_01(state_ANSI_map, Lat_01, Lon_01)\n",
    "del area_map, lat001, lon001\n",
    "\n",
    "# Print time\n",
    "ct = datetime.now() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 2: Read-in and Format Proxy Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.0 Read in and Process O&G Journal Data - NOT USED (potential update)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# READ In and process O&J Journal Data (NOT USED< potential future update)\n",
    "\n",
    "#1) deal with first page of US data\n",
    "\n",
    "data_raw = tb.read_pdf(OGJ_ProcPlant_inputfile, pages=[33], output_format = 'dataframe', multiple_tables = True)\n",
    "display(data_raw[1])\n",
    "df = pd.DataFrame(data_raw[1])\n",
    "display(df)\n",
    "df.rename(columns={df.columns[0]:'Location', df.columns[1]:'Throughput, Capacity'}, inplace=True)\n",
    "colnames = df.columns.values\n",
    "display(colnames)\n",
    "fm_data= df.drop(columns = [2,3,4,5,6,7,8,9,10,11,12])\n",
    "#fm_data = fm_data.drop(fm_data['Location'])\n",
    "fm_data = fm_data[fm_data['Location'] != 'nan']\n",
    "fm_data = fm_data[fm_data['Location'].notna()]\n",
    "#fm_data = data_raw[data_raw['Location', 'Throughput, Capacity']]\n",
    "fm_data.reset_index(inplace=True, drop=True)\n",
    "display(fm_data)\n",
    "\n",
    "new_data = pd.DataFrame()\n",
    "new_data['Location']=''\n",
    "new_data['Throughput, Capacity'] = []\n",
    "icounter = 0\n",
    "istop = -1\n",
    "\n",
    "for i in np.arange(0,len(fm_data)):\n",
    "    if fm_data.loc[i, 'Location'] == 'Company, plant location':\n",
    "        continue\n",
    "    else:\n",
    "        print(i, istop)\n",
    "        if i > istop:\n",
    "            str_temp = fm_data.loc[i, 'Location']\n",
    "            print('str1',str_temp)\n",
    "            istop = i\n",
    "            #print(type(fm_data.loc[i, 'Throughput, Capacity']))\n",
    "            if pd.notnull(fm_data.loc[i, 'Throughput, Capacity']):\n",
    "                icounter +=1\n",
    "                data_temp = fm_data.loc[i, 'Throughput, Capacity']\n",
    "                print(data_temp)\n",
    "                #istop +=1\n",
    "                if i +1 < len(fm_data):\n",
    "                    if pd.isnull(fm_data.loc[i+1, 'Throughput, Capacity']):\n",
    "                        str_temp += fm_data.loc[i+1, 'Location']\n",
    "                        print('str2',str_temp)\n",
    "                        istop+=1\n",
    "                        if pd.isnull(fm_data.loc[i+2, 'Throughput, Capacity']):\n",
    "                            str_temp += fm_data.loc[i+2, 'Location']\n",
    "                            print('str3',str_temp)\n",
    "                            istop+=1\n",
    "                #else:\n",
    "                new_data.loc[icounter, 'Location'] = str_temp\n",
    "                new_data.loc[icounter, 'Throughput, Capacity'] = data_temp\n",
    "            else:\n",
    "                continue #continue to next row if capacity/throughput = NaN\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "display(new_data) \n",
    "\n",
    "# 2) Deal with remaining pages of US data\n",
    "page = [34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58]\n",
    "data_raw = tb.read_pdf(OGJ_ProcPlant_inputfile, pages=page, output_format = 'dataframe')\n",
    "data_raw.rename(columns={data_raw.columns[0]:'Location', data_raw.columns[1]:'Throughput, Capacity'}, inplace=True)\n",
    "colnames = data_raw.columns.values\n",
    "fm_data= data_raw.drop(columns = ['Unnamed: 2', 'Unnamed: 3', '———–———— Production 1,000 gpd (average based on the past 12 months) ——–—————'])\n",
    "fm_data = fm_data[fm_data['Location'] != 'nan']\n",
    "fm_data = fm_data[fm_data['Location'].notna()]\n",
    "fm_data.reset_index(inplace=True, drop=True)\n",
    "display(fm_data)\n",
    "\n",
    "new_data2 = pd.DataFrame()\n",
    "new_data2['Location']=''\n",
    "new_data2['Throughput, Capacity'] = []\n",
    "icounter = 0\n",
    "istop = 0\n",
    "\n",
    "for i in np.arange(0,len(fm_data)):\n",
    "    if fm_data.loc[i, 'Location'] == 'Company, plant location' or \\\n",
    "        fm_data.loc[i, 'Location'] == 'Total':\n",
    "        #skip rows if contain state totals or the head of the table\n",
    "        continue\n",
    "    elif 'Venezuela' in fm_data.loc[i, 'Location']: \n",
    "        #indicates the end of the US data\n",
    "        break\n",
    "    elif fm_data.loc[i, 'Location'].lower() in (string.lower() for string in State_ANSI['name']):\n",
    "        # record the same name, then continue\n",
    "        icounter+=1\n",
    "        new_data2.loc[icounter, 'Location'] = fm_data.loc[i, 'Location']\n",
    "        new_data2.loc[icounter, 'Throughput, Capacity'] = ''\n",
    "        #istop +=1\n",
    "    else:\n",
    "        #print(i, istop)\n",
    "        if i > istop:\n",
    "            str_temp = fm_data.loc[i, 'Location']\n",
    "            #print('str1',i, str_temp)\n",
    "            istop = i\n",
    "            #print(type(fm_data.loc[i, 'Throughput, Capacity']))\n",
    "            if pd.notnull(fm_data.loc[i, 'Throughput, Capacity']):\n",
    "                icounter +=1\n",
    "                data_temp = fm_data.loc[i, 'Throughput, Capacity']\n",
    "                #print(data_temp)\n",
    "                if i+1 <= len(fm_data):\n",
    "                    if pd.isnull(fm_data.loc[i+1, 'Throughput, Capacity']):\n",
    "                        str_temp += fm_data.loc[i+1, 'Location']\n",
    "                        #print('str2',str_temp)\n",
    "                        istop+=1\n",
    "                        if i+2 <= len(fm_data):\n",
    "                            if pd.isnull(fm_data.loc[i+2, 'Throughput, Capacity']):\n",
    "                                str_temp += fm_data.loc[i+2, 'Location']\n",
    "                                #print('str3',str_temp)\n",
    "                                istop+=1\n",
    "                #else:\n",
    "                new_data2.loc[icounter, 'Location'] = str_temp\n",
    "                new_data2.loc[icounter, 'Throughput, Capacity'] = data_temp\n",
    "            else:\n",
    "                continue #continue to next row if capacity/throughput = NaN\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "display(new_data2)  \n",
    "\n",
    "#3) Combine into a single dataframe\n",
    "new_data3 = pd.concat([new_data, new_data2])\n",
    "new_data3.loc[-1] = ['Alabama','']  # adding a row\n",
    "new_data3.index = new_data3.index + 1  # shifting index\n",
    "new_data3 = new_data3.sort_index()  # sorting by index\n",
    "new_data3.reset_index(inplace=True, drop=True)\n",
    "display(new_data3)\n",
    "\n",
    "new_data3.to_csv('./OGJ_plants.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#extract county data and add to new column\n",
    "new_data3['County']=''\n",
    "\n",
    "for i in np.arange(0,len(new_data3)):\n",
    "    str_temp = new_data3.loc[i,'Location']\n",
    "    if str_temp.lower() not in (string.lower() for string in State_ANSI['name']):\n",
    "        #start = str_temp.find(\", \")+len(\", \")\n",
    "        #print(str_temp)\n",
    "        if 'Borough' in str_temp:\n",
    "            end = str_temp.find(\"Borough\")\n",
    "            start = str_temp.find(\",\")+len(\",\")\n",
    "            new_data3.loc[i,'County'] = str_temp[start:end]\n",
    "            print(i,str_temp[start:end])\n",
    "        elif 'Par.' in str_temp:\n",
    "            end = str_temp.find(\"Par.\")\n",
    "            start = str_temp.find(\",\")+len(\",\")\n",
    "            new_data3.loc[i,'County'] = str_temp[start:end]\n",
    "            print(i,str_temp[start:end])\n",
    "        elif 'Umiat' in str_temp:\n",
    "            #end = str_temp.find(\"Par.\")\n",
    "            #start = str_temp.find(\",\")+len(\",\")\n",
    "            new_data3.loc[i,'County'] = 'Umiat'\n",
    "            #print(i,str_temp[start:end])\n",
    "        elif 'Mobile Bay' in str_temp:\n",
    "            #end = str_temp.find(\"Par.\")\n",
    "            #start = str_temp.find(\",\")+len(\",\")\n",
    "            new_data3.loc[i,'County'] = 'Mobile'\n",
    "        elif 'Ventura' in str_temp:\n",
    "            #end = str_temp.find(\"Par.\")\n",
    "            #start = str_temp.find(\",\")+len(\",\")\n",
    "            new_data3.loc[i,'County'] = 'Ventura'\n",
    "        elif 'Harbor City' in str_temp:\n",
    "            #print(str_temp)\n",
    "            #end = str_temp.find(\"Par.\")\n",
    "            #start = str_temp.find(\",\")+len(\",\")\n",
    "            new_data3.loc[i,'County'] = 'Los Angeles'\n",
    "        elif 'Fresno' in str_temp:\n",
    "            new_data3.loc[i,'County'] = 'Fresno'\n",
    "        elif 'Piceance Creek' in str_temp:\n",
    "            new_data3.loc[i,'County'] = 'Rio Blanco'\n",
    "        elif 'Channahon' in str_temp:\n",
    "            new_data3.loc[i,'County'] = 'Grundy'\n",
    "        elif 'Hugoton' in str_temp:\n",
    "            new_data3.loc[i,'County'] = 'Grant'\n",
    "        elif 'Langley' in str_temp:\n",
    "            new_data3.loc[i,'County'] = 'Floyd'\n",
    "        elif 'Fall Rock' in str_temp:\n",
    "            new_data3.loc[i,'County'] = 'Clay'\n",
    "        elif ', St. Mary' in str_temp:\n",
    "            new_data3.loc[i,'County'] = 'St. Mary'\n",
    "        elif 'Bienville' in str_temp:\n",
    "            new_data3.loc[i,'County'] = 'Bienville'\n",
    "        elif 'Port Allen' in str_temp:\n",
    "            new_data3.loc[i,'County'] = 'W. Baton Rouge'\n",
    "        elif 'Co.' in str_temp:\n",
    "            #print(str_temp)\n",
    "            end = str_temp.rfind(\"Co.\")\n",
    "            #if ',' in str_temp:\n",
    "            start = str_temp.find(\",\")+len(\",\")\n",
    "            start2 = str_temp.rfind(\"—\")+len(\"—\")\n",
    "            if start2 > start:\n",
    "                start = start2\n",
    "                #print(start2)\n",
    "            #if 'Baca' in str_temp:\n",
    "            #    new_data3.loc[i,'County'] ='Baca'\n",
    "            #else:\n",
    "            new_data3.loc[i,'County'] = str_temp[start:end]\n",
    "            print(i, str_temp[start:end], start, end)\n",
    "        else:\n",
    "            print(i, str_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1 Read In Proxy Mapping File & Make Proxy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1.1 Format Proxy Group Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load GHGI Mapping Groups\n",
    "names = pd.read_excel(NG_Mapping_inputfile, sheet_name = \"GHGI Map - Proc\", usecols = \"A:B\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "ghgi_proc_map = pd.read_excel(NG_Mapping_inputfile, sheet_name = \"GHGI Map - Proc\", usecols = \"A:B\", skiprows = 2, names = colnames)\n",
    "#drop rows with no data, remove the parentheses and \"\"\n",
    "ghgi_proc_map = ghgi_proc_map[ghgi_proc_map['GHGI_Emi_Group'] != 'na']\n",
    "ghgi_proc_map = ghgi_proc_map[ghgi_proc_map['GHGI_Emi_Group'].notna()]\n",
    "ghgi_proc_map['GHGI_Source']= ghgi_proc_map['GHGI_Source'].str.replace(r\"\\(\",\"\")\n",
    "ghgi_proc_map['GHGI_Source']= ghgi_proc_map['GHGI_Source'].str.replace(r\"\\)\",\"\")\n",
    "ghgi_proc_map.reset_index(inplace=True, drop=True)\n",
    "display(ghgi_proc_map)\n",
    "\n",
    "#load emission group - proxy map\n",
    "names = pd.read_excel(NG_Mapping_inputfile, sheet_name = \"Proxy Map - Proc\", usecols = \"A:D\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "proxy_proc_map = pd.read_excel(NG_Mapping_inputfile, sheet_name = \"Proxy Map - Proc\", usecols = \"A:D\", skiprows = 1, names = colnames)\n",
    "display((proxy_proc_map))\n",
    "\n",
    "#create empty proxy and emission group arrays (add months for proxy variables that have monthly data)\n",
    "for igroup in np.arange(0,len(proxy_proc_map)):\n",
    "    if proxy_proc_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "        vars()[proxy_proc_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "        vars()[proxy_proc_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years,num_months])\n",
    "        vars()[ghgi_proc_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years,num_months])\n",
    "    else:\n",
    "        vars()[proxy_proc_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        vars()[proxy_proc_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "        vars()[ghgi_proc_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])\n",
    "        \n",
    "emi_group_names = np.unique(ghgi_proc_map['GHGI_Emi_Group'])\n",
    "print('QA/QC: Is the number of emission groups the same for the proxy and emissions tabs?')\n",
    "if (len(emi_group_names) == len(np.unique(proxy_proc_map['GHGI_Emi_Group']))):\n",
    "    print('PASS')\n",
    "else:\n",
    "    print('FAIL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1.2 Read In Enverus Processing Plant locations (pre-processed in ArcMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Env_ProcPlant_loc = pd.read_excel(Enverus_NG_ProcPlant_inputfile, usecols= \"C,F,I,K,L,AE,AF,AI,AH,AI:AK\", header = 0)\n",
    "Map_EnvProcPlants = np.zeros([len(Lat_01),len(Lon_01)]) #data represent a snapshot in time that is applied to entire timeseries\n",
    "Map_EnvProcPlants_nongrid = np.zeros([1])\n",
    "\n",
    "for iplant in np.arange(0,len(Env_ProcPlant_loc)):\n",
    "    if Env_ProcPlant_loc['Lon'][iplant] > Lon_left and Env_ProcPlant_loc['Lon'][iplant] < Lon_right \\\n",
    "        and Env_ProcPlant_loc['Lat'][iplant] > Lat_low and Env_ProcPlant_loc['Lat'][iplant] < Lat_up:\n",
    "        ilat = int((Env_ProcPlant_loc['Lat'][iplant] - Lat_low)/Res01)\n",
    "        ilon = int((Env_ProcPlant_loc['Lon'][iplant] - Lon_left)/Res01)\n",
    "        #if Env_ProcPlant_loc['Throughput'][iplant] >0:\n",
    "        Map_EnvProcPlants[ilat,ilon] += 1\n",
    "    else:\n",
    "        Map_EnvProcPlants_nongrid += 1   \n",
    "print('Total Processing Plants on grid: ',np.sum(Map_EnvProcPlants[:,:]))\n",
    "print('Total Processing Plants off grid: ',np.sum(Map_EnvProcPlants_nongrid))\n",
    "\n",
    "#correct data so no thruput > 1000\n",
    "#Env_ProcPlant_loc.loc[Env_ProcPlant_loc['THROUGHPUT']>1000,'THROUGHPUT']=\\\n",
    "#Env_ProcPlant_loc.loc[Env_ProcPlant_loc['THROUGHPUT']>1000,'THROUGHPUT']/10\n",
    "\n",
    "\n",
    "#to be applied later to calculate fraction of AK emissions (no HI plants)\n",
    "AK_plant_fraction = 5/len(Env_ProcPlant_loc)\n",
    "#print(AK_plant_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1.3 Read In GHGRP Facility Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#a) Read in the GHGRP data\n",
    "facility_info = pd.read_csv(GHGRP_facility_inputfile)\n",
    "facility_emissions = pd.read_excel(GHGRP_subpartw_inputfile)\n",
    "facility_emissions = facility_emissions[facility_emissions['INDUSTRY_SEGMENT'] =='Onshore natural gas processing [98.230(a)(3)]']\n",
    "facility_emissions = facility_emissions[facility_emissions['TOTAL_REPORTED_CH4_EMISSIONS'] >0]\n",
    "facility_emissions = facility_emissions[facility_emissions['REPORTING_YEAR'] <2019]\n",
    "facility_emissions.reset_index(drop=True,inplace=True)\n",
    "\n",
    "facility_emissions['State'] = ''\n",
    "facility_emissions['County'] = ''\n",
    "facility_emissions['City'] = ''\n",
    "facility_emissions['Zip'] = 0\n",
    "facility_emissions['Lat'] = 0\n",
    "facility_emissions['Lon'] = 0\n",
    "\n",
    "#b) match GHGRP facility and emissions data\n",
    "# for each entry in the data file (each facility each year), match the facility ID to the ID in the\n",
    "# GHGRP facility info file, then append the corresponding location data to the emissions array\n",
    "for index in np.arange(len(facility_emissions)):\n",
    "    #print(index)\n",
    "    ilocation = np.where(facility_info['V_GHG_EMITTER_FACILITIES.FACILITY_ID'] == facility_emissions['FACILITY_ID'][index])[0][0]\n",
    "    #for iloc in len(ilocation)\n",
    "    facility_emissions.loc[index, 'State'] = facility_info['V_GHG_EMITTER_FACILITIES.STATE'][ilocation]\n",
    "    facility_emissions.loc[index, 'County'] = facility_info['V_GHG_EMITTER_FACILITIES.COUNTY'][ilocation]\n",
    "    facility_emissions.loc[index, 'City'] = facility_info['V_GHG_EMITTER_FACILITIES.CITY'][ilocation]\n",
    "    facility_emissions.loc[index, 'Zip'] = facility_info['V_GHG_EMITTER_FACILITIES.ZIP'][ilocation]\n",
    "    facility_emissions.loc[index, 'Lat'] = facility_info['V_GHG_EMITTER_FACILITIES.LATITUDE'][ilocation]\n",
    "    facility_emissions.loc[index, 'Lon'] = facility_info['V_GHG_EMITTER_FACILITIES.LONGITUDE'][ilocation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plant-specific arrays for each year\n",
    "\n",
    "print('QA/QC: Check that all GHGRP emissions are allocated to specific plants')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    facility_emissions_temp = facility_emissions[facility_emissions['REPORTING_YEAR'] ==year_range[iyear]]\n",
    "    facility_emissions_temp.reset_index(drop=True,inplace=True)\n",
    "    GHGRP_plants = pd.DataFrame({'FID':facility_emissions_temp['FACILITY_ID'].unique()})\n",
    "    GHGRP_plants['Name'] = ' '\n",
    "    GHGRP_plants['State'] = ' '\n",
    "    GHGRP_plants['County'] = ' '\n",
    "    GHGRP_plants['City'] = ' '\n",
    "    GHGRP_plants['Zip'] = 0\n",
    "    GHGRP_plants['Lat'] = 0.0\n",
    "    GHGRP_plants['Lon'] = 0.0\n",
    "    GHGRP_plants['TgCH4'] = 0.0\n",
    "\n",
    "    #Put everything in per-plant array\n",
    "    for idx in np.arange(len(facility_emissions_temp)):\n",
    "        iFID = np.where(GHGRP_plants['FID'] == facility_emissions_temp['FACILITY_ID'][idx])[0][0]\n",
    "        GHGRP_plants.loc[iFID,'Name']   = facility_emissions_temp['FACILITY_NAME'][idx]\n",
    "        GHGRP_plants.loc[iFID,'State']  = facility_emissions_temp['State'][idx]\n",
    "        GHGRP_plants.loc[iFID,'County'] = facility_emissions_temp['County'][idx]\n",
    "        GHGRP_plants.loc[iFID,'City'] = facility_emissions_temp['City'][idx]\n",
    "        GHGRP_plants.loc[iFID,'Zip']    = facility_emissions_temp['Zip'][idx]\n",
    "        GHGRP_plants.loc[iFID,'Lat']    = facility_emissions_temp['Lat'][idx]\n",
    "        GHGRP_plants.loc[iFID,'Lon']    = facility_emissions_temp['Lon'][idx]\n",
    "        GHGRP_plants.loc[iFID,'TgCH4'] += facility_emissions_temp['TOTAL_REPORTED_CH4_EMISSIONS'][idx]/1e6\n",
    "    \n",
    "    vars()['GHGRP_plants'+'_'+year_range_str[iyear]] = GHGRP_plants\n",
    "    diff1 = abs(facility_emissions_temp['TOTAL_REPORTED_CH4_EMISSIONS'].sum()/1e6 -GHGRP_plants['TgCH4'].sum())/ \\\n",
    "        ((facility_emissions_temp['TOTAL_REPORTED_CH4_EMISSIONS'].sum()/1e6 + GHGRP_plants['TgCH4'].sum())/2)\n",
    "    #print(summary_emi)\n",
    "    #print(sum_emi2[iyear])\n",
    "    if diff1 < 0.0001:\n",
    "        print('Year ', year_range[iyear],': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear],': FAIL: ', diff1,'%') \n",
    "    print('Number of GHGRP Plants: ', len(vars()['GHGRP_plants'+'_'+year_range_str[iyear]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#correct County data for GHGRP plants for each year data\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    GHGRP_temp_data = vars()['GHGRP_plants'+'_'+year_range_str[iyear]].copy()\n",
    "    for iplant in np.arange(0,len(GHGRP_temp_data)):\n",
    "        if (pd.isna(GHGRP_temp_data['County'][iplant])):\n",
    "            #DEBUG# print(iyear, iplant, GHGRP_temp_data.loc[iplant,'City'], GHGRP_temp_data.loc[iplant,'State'])\n",
    "            if GHGRP_temp_data.loc[iplant,'City'] in ['Carlsbad', 'MALAGA']:\n",
    "                GHGRP_temp_data.loc[iplant,'County'] = 'Eddy'\n",
    "            elif GHGRP_temp_data.loc[iplant,'City'] =='Crockett':\n",
    "                GHGRP_temp_data.loc[iplant,'County'] = 'Houston'\n",
    "            elif GHGRP_temp_data.loc[iplant,'City'] in ['pecos', 'Balmorhea', 'Orla','Toyah']:\n",
    "                GHGRP_temp_data.loc[iplant,'County'] = 'Reeves'\n",
    "            elif GHGRP_temp_data.loc[iplant,'City'] =='Kermit':\n",
    "                GHGRP_temp_data.loc[iplant,'County'] = 'Winkler'\n",
    "            elif GHGRP_temp_data.loc[iplant,'City'] =='Coyanosa':\n",
    "                GHGRP_temp_data.loc[iplant,'County'] = 'Pecos'\n",
    "            elif GHGRP_temp_data.loc[iplant,'City'] =='Rangely':\n",
    "                GHGRP_temp_data.loc[iplant,'County'] = 'Rio Blanco'\n",
    "            elif GHGRP_temp_data.loc[iplant,'City'] =='PITTSBURG':\n",
    "                GHGRP_temp_data.loc[iplant,'County'] = 'Camp'\n",
    "            elif GHGRP_temp_data.loc[iplant,'City'] =='Stanley':\n",
    "                GHGRP_temp_data.loc[iplant,'County'] = 'Mountrail'\n",
    "            elif GHGRP_temp_data.loc[iplant,'City'] =='Lindsay':\n",
    "                GHGRP_temp_data.loc[iplant,'County'] = 'Cooke'\n",
    "            elif GHGRP_temp_data.loc[iplant,'City'] =='Loving County':\n",
    "                GHGRP_temp_data.loc[iplant,'County'] = 'Loving'\n",
    "            elif GHGRP_temp_data.loc[iplant,'City'] =='Stanton':\n",
    "                GHGRP_temp_data.loc[iplant,'County'] = 'Martin'\n",
    "            elif GHGRP_temp_data.loc[iplant,'City'] =='McKittrick':\n",
    "                GHGRP_temp_data.loc[iplant,'County'] = 'Kern'\n",
    "            elif GHGRP_temp_data.loc[iplant,'City'] =='Briggsdale':\n",
    "                GHGRP_temp_data.loc[iplant,'County'] = 'Weld'\n",
    "            elif GHGRP_temp_data.loc[iplant,'City'] =='Old Ocean':\n",
    "                GHGRP_temp_data.loc[iplant,'County'] = 'Brazoria'\n",
    "            elif GHGRP_temp_data.loc[iplant,'City'] =='Coushatta':\n",
    "                GHGRP_temp_data.loc[iplant,'County'] = 'Red River'\n",
    "            elif GHGRP_temp_data.loc[iplant,'City'] =='Garrison':\n",
    "                GHGRP_temp_data.loc[iplant,'County'] = 'Nacogdoches'\n",
    "            elif GHGRP_temp_data.loc[iplant,'City'] =='Odessa':\n",
    "                GHGRP_temp_data.loc[iplant,'County'] = 'Ector'\n",
    "        \n",
    "    vars()['GHGRP_plants'+'_'+year_range_str[iyear]] = GHGRP_temp_data.copy()\n",
    "        \n",
    "    print('Year: ', year_range_str[iyear])\n",
    "    print('On grid (Tg): ',np.sum(vars()['GHGRP_plants'+'_'+year_range_str[iyear]]['TgCH4'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each year of GHGRP data, match GHGRP plants to Enverus data\n",
    "# note there is only one available year of Enverus data\n",
    "\n",
    "print('QA/QC: Number of GHGRP plants not in Enverus data set')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    GHGRP_temp_data = vars()['GHGRP_plants'+'_'+year_range_str[iyear]].copy()\n",
    "\n",
    "    GHGRP_temp_data['match_flag'] = 0\n",
    "    GHGRP_temp_data['Env_name'] = ''\n",
    "    GHGRP_temp_data['Env_county'] = ''\n",
    "    GHGRP_temp_data['Env_state'] = ''\n",
    "    GHGRP_temp_data['Env_throughput'] = 0\n",
    "    Env_ProcPlants_notmatched = Env_ProcPlant_loc.copy()\n",
    "    rows_to_delete = []\n",
    "\n",
    "    #First, find exact matching lat/lon facilities\n",
    "    for iplant in np.arange(0,len(GHGRP_temp_data)):\n",
    "        lat_temp = round(GHGRP_temp_data['Lat'][iplant], 2)\n",
    "        lon_temp = round(GHGRP_temp_data['Lon'][iplant], 2)\n",
    "        match_lat = np.where(round(Env_ProcPlant_loc['Lat'],2) == lat_temp)\n",
    "        match_lon = np.where(round(Env_ProcPlant_loc['Lon'],2) == lon_temp)\n",
    "        if np.size(match_lat) > 0 and np.size(match_lon) > 0:\n",
    "            if np.size(match_lat) ==1:\n",
    "                if match_lat[0][0] in match_lon[0]: #check whether the same index is found in the lat/lon lists\n",
    "                    GHGRP_temp_data.loc[iplant,'match_flag'] = 1\n",
    "                    GHGRP_temp_data.loc[iplant,'Env_name'] = Env_ProcPlant_loc.loc[match_lat[0][0], 'NAME']\n",
    "                    GHGRP_temp_data.loc[iplant,'Env_county'] = Env_ProcPlant_loc.loc[match_lat[0][0], 'CNTY_NAME']\n",
    "                    GHGRP_temp_data.loc[iplant,'Env_state'] = Env_ProcPlant_loc.loc[match_lat[0][0], 'STATE_NAME']\n",
    "                    GHGRP_temp_data.loc[iplant,'Env_throughput'] = Env_ProcPlant_loc.loc[match_lat[0][0], 'THROUGHPUT']\n",
    "                    rows_to_delete = np.append(rows_to_delete, match_lat[0][0])\n",
    "            else:\n",
    "                for idx in np.arange(0, np.size(match_lat)): #loop through the matching lat values\n",
    "                    if match_lat[0][idx] in match_lon[0] and \\\n",
    "                        Env_ProcPlant_loc.loc[match_lat[0][idx], 'THROUGHPUT'] > 0: #check whether the same index is found in the lat/lon lists\n",
    "                        GHGRP_temp_data.loc[iplant,'match_flag'] = 1\n",
    "                        GHGRP_temp_data.loc[iplant,'Env_name'] = Env_ProcPlant_loc.loc[match_lat[0][idx], 'NAME']\n",
    "                        GHGRP_temp_data.loc[iplant,'Env_county'] = Env_ProcPlant_loc.loc[match_lat[0][idx], 'CNTY_NAME']\n",
    "                        GHGRP_temp_data.loc[iplant,'Env_state'] = Env_ProcPlant_loc.loc[match_lat[0][idx], 'STATE_NAME']\n",
    "                        GHGRP_temp_data.loc[iplant,'Env_throughput'] = Env_ProcPlant_loc.loc[match_lat[0][idx], 'THROUGHPUT']\n",
    "                        rows_to_delete = np.append(rows_to_delete, match_lat[0][idx])\n",
    "            \n",
    "\n",
    "    rows_to_delete = rows_to_delete.astype(int)\n",
    "    Env_ProcPlants_notmatched = Env_ProcPlant_loc.drop(rows_to_delete)\n",
    "   \n",
    "\n",
    "    #SECOND, find all Enverus plants within each country that did not match and try to match based on proximity to GHGRP plants\n",
    "    for iplant in np.arange(0,len(GHGRP_temp_data)):\n",
    "    \n",
    "        if GHGRP_temp_data.loc[iplant,'match_flag'] !=1:\n",
    "            if pd.isna(GHGRP_temp_data['County'][iplant]):\n",
    "                continue #this should no longer trigger since couty data are corrected above\n",
    "            else:\n",
    "                if 'county' in GHGRP_temp_data['County'][iplant].lower():\n",
    "                    match = np.where(Env_ProcPlants_notmatched['CNTY_NAME'].str.contains(GHGRP_temp_data['County'][iplant][:-7], case=False))\n",
    "                elif 'parish' in GHGRP_temp_data['County'][iplant].lower():\n",
    "                    match = np.where(Env_ProcPlants_notmatched['CNTY_NAME'].str.contains(GHGRP_temp_data['County'][iplant][:-7], case=False))\n",
    "                elif 'borough' in GHGRP_temp_data['County'][iplant].lower():\n",
    "                    match = np.where(Env_ProcPlants_notmatched['CNTY_NAME'].str.contains(GHGRP_temp_data['County'][iplant][:-8], case=False))\n",
    "                else:\n",
    "                    match = np.where(Env_ProcPlants_notmatched['CNTY_NAME'].str.contains(GHGRP_temp_data['County'][iplant], case=False))\n",
    "                if np.size(match) >0 :\n",
    "                    \n",
    "                #match is the list of enverus plants within the given county (not already matched)\n",
    "                # if both lat and lon is closest to a single plant, assign that plant regardless of throughput\n",
    "                # if lat and lon are closest to two different plants, then filter the throughput for non-zeros\n",
    "                #      if all throughputs are zeros, assign based on whichever lat/lons are closest\n",
    "                #      if some throughputs are non-zero, then assign based on whichever of those are closest to GHGRP \n",
    "                #(to increase chance of non-zero throughoutput)\n",
    "                #MATCH IS NOT THE BEST SO COULD UPDATE TO MATCH BASED ON FACILITY NAME TOO\n",
    "                    lat_temp = round(GHGRP_temp_data['Lat'][iplant], 2)\n",
    "                    lon_temp = round(GHGRP_temp_data['Lon'][iplant], 2)\n",
    "                    list_envmat = Env_ProcPlants_notmatched.iloc[match]\n",
    "                    vallat, idxlat = min((val, idx) for (idx, val) in enumerate(abs(list_envmat.loc[:,'Lat']-lat_temp)))\n",
    "                    vallon, idxlon = min((val, idx) for (idx, val) in enumerate(abs(list_envmat.loc[:,'Lon']-lon_temp)))\n",
    "                    if idxlat == idxlon:\n",
    "                        GHGRP_temp_data.loc[iplant,'match_flag'] = 1\n",
    "                        GHGRP_temp_data.loc[iplant,'Env_name'] = Env_ProcPlants_notmatched.loc[list_envmat.index[idxlat], 'NAME']\n",
    "                        GHGRP_temp_data.loc[iplant,'Env_county'] = Env_ProcPlants_notmatched.loc[list_envmat.index[idxlat],'CNTY_NAME']\n",
    "                        GHGRP_temp_data.loc[iplant,'Env_state'] = Env_ProcPlants_notmatched.loc[list_envmat.index[idxlat], 'STATE_NAME']\n",
    "                        GHGRP_temp_data.loc[iplant,'Env_throughput'] = Env_ProcPlants_notmatched.loc[list_envmat.index[idxlat], 'THROUGHPUT']\n",
    "                        rows_to_delete = np.append(rows_to_delete, list_envmat.index[idxlat])\n",
    "                    else:\n",
    "                        list_envmat_filter = list_envmat[list_envmat['THROUGHPUT']>0]\n",
    "                        if np.size(list_envmat_filter) ==0:\n",
    "                            if vallat < vallon: #assign location based on whether the lat or lon value in enverus is closer to GHGRP loc data\n",
    "                                GHGRP_temp_data.loc[iplant,'match_flag'] = 1\n",
    "                                GHGRP_temp_data.loc[iplant,'Env_name'] = Env_ProcPlants_notmatched.loc[list_envmat.index[idxlat], 'NAME']\n",
    "                                GHGRP_temp_data.loc[iplant,'Env_county'] = Env_ProcPlants_notmatched.loc[list_envmat.index[idxlat],'CNTY_NAME']\n",
    "                                GHGRP_temp_data.loc[iplant,'Env_state'] = Env_ProcPlants_notmatched.loc[list_envmat.index[idxlat], 'STATE_NAME']\n",
    "                                GHGRP_temp_data.loc[iplant,'Env_throughput'] = Env_ProcPlants_notmatched.loc[list_envmat.index[idxlat], 'THROUGHPUT']\n",
    "                                rows_to_delete = np.append(rows_to_delete, list_envmat.index[idxlat])\n",
    "                            elif vallat > vallon:\n",
    "                                GHGRP_temp_data.loc[iplant,'match_flag'] = 1\n",
    "                                GHGRP_temp_data.loc[iplant,'Env_name'] = Env_ProcPlants_notmatched.loc[list_envmat.index[idxlon], 'NAME']\n",
    "                                GHGRP_temp_data.loc[iplant,'Env_county'] = Env_ProcPlants_notmatched.loc[list_envmat.index[idxlon],'CNTY_NAME']\n",
    "                                GHGRP_temp_data.loc[iplant,'Env_state'] = Env_ProcPlants_notmatched.loc[list_envmat.index[idxlon], 'STATE_NAME']\n",
    "                                GHGRP_temp_data.loc[iplant,'Env_throughput'] = Env_ProcPlants_notmatched.loc[list_envmat.index[idxlon], 'THROUGHPUT']\n",
    "                                rows_to_delete = np.append(rows_to_delete, list_envmat.index[idxlon])\n",
    "                        else:\n",
    "                            vallat, idxlat = min((val, idx) for (idx, val) in enumerate(abs(list_envmat_filter.loc[:,'Lat']-lat_temp)))\n",
    "                            vallon, idxlon = min((val, idx) for (idx, val) in enumerate(abs(list_envmat_filter.loc[:,'Lon']-lon_temp)))\n",
    "                            if vallat < vallon: #assign location based on whether the lat or lon value in enverus is closer to GHGRP loc data\n",
    "                                GHGRP_temp_data.loc[iplant,'match_flag'] = 1\n",
    "                                GHGRP_temp_data.loc[iplant,'Env_name'] = Env_ProcPlants_notmatched.loc[list_envmat_filter.index[idxlat], 'NAME']\n",
    "                                GHGRP_temp_data.loc[iplant,'Env_county'] = Env_ProcPlants_notmatched.loc[list_envmat_filter.index[idxlat],'CNTY_NAME']\n",
    "                                GHGRP_temp_data.loc[iplant,'Env_state'] = Env_ProcPlants_notmatched.loc[list_envmat_filter.index[idxlat], 'STATE_NAME']\n",
    "                                GHGRP_temp_data.loc[iplant,'Env_throughput'] = Env_ProcPlants_notmatched.loc[list_envmat_filter.index[idxlat], 'THROUGHPUT']\n",
    "                                rows_to_delete = np.append(rows_to_delete, list_envmat_filter.index[idxlat])\n",
    "                            elif vallat > vallon:\n",
    "                                GHGRP_temp_data.loc[iplant,'match_flag'] = 1\n",
    "                                GHGRP_temp_data.loc[iplant,'Env_name'] = Env_ProcPlants_notmatched.loc[list_envmat_filter.index[idxlon], 'NAME']\n",
    "                                GHGRP_temp_data.loc[iplant,'Env_county'] = Env_ProcPlants_notmatched.loc[list_envmat_filter.index[idxlon],'CNTY_NAME']\n",
    "                                GHGRP_temp_data.loc[iplant,'Env_state'] = Env_ProcPlants_notmatched.loc[list_envmat_filter.index[idxlon], 'STATE_NAME']\n",
    "                                GHGRP_temp_data.loc[iplant,'Env_throughput'] = Env_ProcPlants_notmatched.loc[list_envmat_filter.index[idxlon], 'THROUGHPUT']\n",
    "                                rows_to_delete = np.append(rows_to_delete, list_envmat_filter.index[idxlon])\n",
    "                                \n",
    "\n",
    "                    #continue\n",
    "                    #look for matching name, of throughput >0 for points where lat is closer to one and lon closer to another\n",
    "                    \n",
    "    rows_to_delete = rows_to_delete.astype(int)\n",
    "    Env_ProcPlants_notmatched = Env_ProcPlant_loc.drop(rows_to_delete)\n",
    "    Env_ProcPlants_notmatched.reset_index(inplace=True, drop=True)   \n",
    "    \n",
    "    vars()['Env_ProcPlants_notmatched'+'_'+year_range_str[iyear]] = Env_ProcPlants_notmatched.copy()\n",
    "    vars()['GHGRP_plants'+'_'+year_range_str[iyear]] = GHGRP_temp_data.copy()\n",
    "    \n",
    "    print('Year ', year_range_str[iyear],': ', len(GHGRP_temp_data[GHGRP_temp_data['match_flag']==0]), ' of ', len(GHGRP_temp_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1.4. Calculate the average emissions/throuput ratio for matched plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_emis_throughput_ratio = np.zeros([num_years])\n",
    "\n",
    "print('QA/QC: Average Emissions to Throughput Ratio')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    GHGRP_temp_data = vars()['GHGRP_plants'+'_'+year_range_str[iyear]].copy()\n",
    "\n",
    "    GHGRP_temp_data['Emis_thru_ratio']=0\n",
    "    for iplant in np.arange(0,len(GHGRP_temp_data)):\n",
    "        GHGRP_temp_data.loc[iplant, 'Emis_thru_ratio'] = data_fn.safe_div(GHGRP_temp_data.loc[iplant, 'TgCH4'], \\\n",
    "                                                                        GHGRP_temp_data.loc[iplant, 'Env_throughput'])\n",
    "    GHGRP_temp_data['Emis_thru_ratio'] = GHGRP_temp_data['Emis_thru_ratio'].replace({0:np.nan})\n",
    "    avg_emis_throughput_ratio[iyear] = np.nanmedian(GHGRP_temp_data['Emis_thru_ratio'])\n",
    "    \n",
    "    vars()['GHGRP_plants'+'_'+year_range_str[iyear]] = GHGRP_temp_data.copy()\n",
    "    print('Year ', year_range_str[iyear],': ', avg_emis_throughput_ratio[iyear])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2. Make map of 'emissions' from each plant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map = GHGRP emissions for each plant that matched, then take all the enverus plants that didnt match, calculate emissions and then add to map\n",
    "Map_CombProcPlants = np.zeros([len(Lat_01),len(Lon_01),num_years]) #data represent a snapshot in time that is applied to entire timeseries\n",
    "Map_CombProcPlants_nongrid = np.zeros([num_years])\n",
    "\n",
    "print('QA/QC: Processing Plant Emissions Gridded:')\n",
    "for iyear in np.arange(0, num_years):\n",
    "    #first add GHGRP emissions for matched plants\n",
    "    GHGRP_temp_data = vars()['GHGRP_plants'+'_'+year_range_str[iyear]].copy()\n",
    "    for iplant in np.arange(0,len(GHGRP_temp_data)):\n",
    "        if GHGRP_temp_data.loc[iplant,'match_flag']==1:\n",
    "            if GHGRP_temp_data['Lon'][iplant] > Lon_left and GHGRP_temp_data['Lon'][iplant] < Lon_right \\\n",
    "                and GHGRP_temp_data['Lat'][iplant] > Lat_low and GHGRP_temp_data['Lat'][iplant] < Lat_up:\n",
    "                ilat = int((GHGRP_temp_data['Lat'][iplant] - Lat_low)/Res01)\n",
    "                ilon = int((GHGRP_temp_data['Lon'][iplant] - Lon_left)/Res01)\n",
    "                #if Env_ProcPlant_loc['Throughput'][iplant] >0:\n",
    "                Map_CombProcPlants[ilat,ilon,iyear] += GHGRP_temp_data.loc[iplant, 'TgCH4']\n",
    "            else:\n",
    "                Map_CombProcPlants_nongrid[iyear] += GHGRP_temp_data.loc[iplant, 'TgCH4']  \n",
    "\n",
    "    #then add calculated enverus emissions for all non-matched plants\n",
    "    Env_temp_data = vars()['Env_ProcPlants_notmatched'+'_'+year_range_str[iyear]].copy()\n",
    "    for iplant in np.arange(0, len(Env_temp_data)):\n",
    "        \n",
    "        if Env_temp_data['Lon'][iplant] > Lon_left and Env_temp_data['Lon'][iplant] < Lon_right \\\n",
    "            and Env_temp_data['Lat'][iplant] > Lat_low and Env_temp_data['Lat'][iplant] < Lat_up:\n",
    "            if Env_temp_data['NAME'][iplant]=='Sherwood I-XIII' and iyear ==0:\n",
    "                print('here!')\n",
    "                continue #skip the sherwood plant in 2012 (not porducing yet)\n",
    "                \n",
    "            else:\n",
    "                ilat = int((Env_temp_data['Lat'][iplant] - Lat_low)/Res01)\n",
    "                ilon = int((Env_temp_data['Lon'][iplant] - Lon_left)/Res01)\n",
    "                Map_CombProcPlants[ilat,ilon,iyear] += Env_temp_data.loc[iplant, 'THROUGHPUT']*avg_emis_throughput_ratio[iyear]\n",
    "        else:\n",
    "            Map_CombProcPlants_nongrid[iyear] += Env_temp_data.loc[iplant, 'THROUGHPUT']*avg_emis_throughput_ratio[iyear]\n",
    "    \n",
    "    vars()['GHGRP_plants'+'_'+year_range_str[iyear]] = GHGRP_temp_data.copy()\n",
    "    vars()['Env_ProcPlants_notmatched'+'_'+year_range_str[iyear]] = Env_temp_data.copy()\n",
    "    \n",
    "    print('Year: ', year_range_str[iyear])\n",
    "    print('On grid (Tg): ',np.sum(Map_CombProcPlants[:,:, iyear]))\n",
    "    print('Off grid (Tg): ',np.sum(Map_CombProcPlants_nongrid[iyear]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Step 3. Read In EPA GHGI Data\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1. Processing Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Emissions are in units of MG (= 1x10-6 Tg)\n",
    "\n",
    "# METHANE\n",
    "names = pd.read_excel(EPA_NG_inputfile, sheet_name = \"Inventory Emissions\", usecols = \"A:AG\", skiprows = 5, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "EPA_emi_proc_NG = pd.read_excel(EPA_NG_inputfile, sheet_name = \"Inventory Emissions\", usecols = \"A:AG\", skiprows = 132, names = colnames, nrows = 15)\n",
    "EPA_emi_proc_NG= EPA_emi_proc_NG.drop(columns = ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 3'])\n",
    "EPA_emi_proc_NG['Source']= EPA_emi_proc_NG['Source'].str.replace(r\"\\(\",\"\")\n",
    "EPA_emi_proc_NG['Source']= EPA_emi_proc_NG['Source'].str.replace(r\"\\)\",\"\")\n",
    "EPA_emi_proc_NG = EPA_emi_proc_NG.fillna('')\n",
    "EPA_emi_proc_NG = EPA_emi_proc_NG.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_emi_proc_NG.reset_index(inplace=True, drop=True)\n",
    "display(EPA_emi_proc_NG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1.2. Read in Total Processing Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in total processing emissions (with methane reductions accounted for)\n",
    "# data are in kt\n",
    "\n",
    "names = pd.read_excel(EPA_NG_inputfile, sheet_name = \"SUMMARY CH4\", usecols = \"A:AD\", skiprows = 10, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "EPA_emi_total_NG_CH4 = pd.read_excel(EPA_NG_inputfile, sheet_name = \"SUMMARY CH4\", usecols = \"A:AD\", skiprows = 17, names = colnames, nrows = 5)\n",
    "EPA_emi_total_NG_CH4.rename(columns={EPA_emi_total_NG_CH4.columns[0]:'Source'}, inplace=True)\n",
    "EPA_emi_total_NG_CH4 = EPA_emi_total_NG_CH4.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_emi_total_NG_CH4.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\"EPA GHGI Emissions with Reductions (kt)\")\n",
    "display(EPA_emi_total_NG_CH4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.3. Split Emissions into Gridding Groups (each Group will have the same proxy applied during the gridding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Emissions in Units of kt\n",
    "# Use mapping proxy and source files to split the GHGI emissions\n",
    "\n",
    "start_year_idx = EPA_emi_proc_NG.columns.get_loc(start_year)\n",
    "end_year_idx = EPA_emi_proc_NG.columns.get_loc(end_year)+1\n",
    "sum_emi = np.zeros(num_years)\n",
    "\n",
    "DEBUG=1\n",
    "\n",
    "ghgi_proc_groups = ghgi_proc_map['GHGI_Emi_Group'].unique()\n",
    "\n",
    "for igroup in np.arange(0,len(ghgi_proc_groups)): #loop through all groups, finding the GHGI sources in that group and summing emissions for that region, year\n",
    "        vars()[ghgi_proc_groups[igroup]] = np.zeros([num_years])\n",
    "        source_temp = ghgi_proc_map.loc[ghgi_proc_map['GHGI_Emi_Group'] == ghgi_proc_groups[igroup], 'GHGI_Source']\n",
    "        pattern_temp  = '|'.join(source_temp)\n",
    "        emi_temp = EPA_emi_proc_NG[EPA_emi_proc_NG['Source'].str.contains(pattern_temp)]\n",
    "        vars()[ghgi_proc_groups[igroup]][:] = np.where(emi_temp.iloc[:,start_year_idx:] =='',[0],emi_temp.iloc[:,start_year_idx:]).sum(axis=0)/float(1000) #convert Mg to kt\n",
    "\n",
    "#Check against total summary emissions \n",
    "print('QA/QC #1: Check Processing Emission Sum against GHGI Summary Emissions')\n",
    "for iyear in np.arange(0,num_years): \n",
    "    for igroup in np.arange(0,len(ghgi_proc_groups)):\n",
    "        sum_emi[iyear] += vars()[ghgi_proc_groups[igroup]][iyear]\n",
    "        \n",
    "    summary_emi = EPA_emi_total_NG_CH4.iloc[2,iyear+1]  \n",
    "    #Check 1 - make sure that the sums from all the regions equal the totals reported\n",
    "    diff1 = abs(sum_emi[iyear] - summary_emi)/((sum_emi[iyear] + summary_emi)/2)\n",
    "    if DEBUG==1:\n",
    "        print(summary_emi)\n",
    "        print(sum_emi[iyear])\n",
    "    if diff1 < 0.0001:\n",
    "        print('Year ', year_range[iyear],': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear],': FAIL (check Production & summary tabs): ', diff1,'%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Step 4. Grid Data (using spatial proxies)\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step. 4.1. Calculate the monthly and regional weighted arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1.1 Assign the Appropriate Proxy Variable Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names on the *left* need to match the 'NaturalGas_Processing_ProxyMapping' 'Proxy_Group' names \n",
    "# (these are initialized in Step 2). \n",
    "# The names on the right are the variable names used to caluclate the proxies in this code.\n",
    "# Names on the *right* need to match those from the code in Step 2.5\n",
    "\n",
    "#These represent the calculated emissions at each processing plant (Tg CH4)\n",
    "Map_Plants = Map_CombProcPlants\n",
    "Map_Plants_nongrid = Map_CombProcPlants_nongrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1.2 Calculate the fractional proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighting arrays\n",
    "# Find the fraction of processing plants in each grid cell, relative to the total counts (on and off grid)\n",
    "# also weight by the number of days in each year\n",
    "\n",
    "proxy_proc_map_unique = np.unique(proxy_proc_map['Proxy_Group'])\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "        month_days = month_day_leap\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        month_days = month_day_nonleap  \n",
    "    \n",
    "    #Step 1a: weighted proxy ongrid = ongrid proxy * days each year\n",
    "    #Step 1b: weighted proxy offgrid = offgrid proxy * days each year\n",
    "    #Step 2a: noramlized weighted proxy ongrid = weighted proxy in each grid cell / (sum weighted proxy ongrid + weighted proxy offgrid)\n",
    "    #Step 2b: noramlized weighted proxy offgrid = weighted proxy offgrid / (sum weighted proxy ongrid + weighted proxy offgrid)\n",
    "    print('Check Sum of Proc. Proxy Arrays = 1 for: ', year_range[iyear])\n",
    "    for iproxy in np.arange(0,len(proxy_proc_map_unique)):\n",
    "        vars()[proxy_proc_map.loc[iproxy,'Proxy_Group']][:,:,iyear] *= np.sum(month_days)\n",
    "        vars()[proxy_proc_map.loc[iproxy,'Proxy_Group']+'_nongrid'][iyear] *= np.sum(month_days)\n",
    "        temp_sum = float(np.sum(vars()[proxy_proc_map.loc[iproxy,'Proxy_Group']][:,:,iyear]) + \\\n",
    "                    np.sum(vars()[proxy_proc_map.loc[iproxy,'Proxy_Group']+'_nongrid'][iyear]))\n",
    "        vars()[proxy_proc_map.loc[iproxy,'Proxy_Group']][:,:,iyear] = \\\n",
    "                    data_fn.safe_div(vars()[proxy_proc_map.loc[iproxy,'Proxy_Group']][:,:,iyear], temp_sum)\n",
    "        vars()[proxy_proc_map.loc[iproxy,'Proxy_Group']+'_nongrid'][iyear] = \\\n",
    "                    data_fn.safe_div(vars()[proxy_proc_map.loc[iproxy,'Proxy_Group']+'_nongrid'][iyear], temp_sum)\n",
    "        proxy_sum = np.sum(vars()[proxy_proc_map.loc[iproxy,'Proxy_Group']][:,:,iyear])+np.sum(vars()[proxy_proc_map.loc[iproxy,'Proxy_Group']+'_nongrid'][iyear])\n",
    "        if proxy_sum >1.0001 or proxy_sum <0.9999:\n",
    "            print('CHECK ', proxy_proc_map.loc[iproxy,'Proxy_Group'],': ', proxy_sum)   \n",
    "        else:\n",
    "            print('PASS:', proxy_proc_map.loc[iproxy,'Proxy_Group'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step. 4.2. Grid the National Emissions Data, then Calculate 0.1x0.1 degree flux maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For the processing segment...\n",
    "# 1) make flux array with correct dimensions\n",
    "# 2) weight monthly data by days in month (or year)\n",
    "# 3) caluclate flux as Flux = GHGI emissions * Proxy Map\n",
    "\n",
    "Emissions = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Emissions_nongrid = np.zeros([num_years])\n",
    "Emi_not_mapped_sum = np.zeros(num_years)\n",
    "\n",
    "DEBUG =1\n",
    "\n",
    "#loop through each emission group, where: Gridded emissions = National emissions * proxy map\n",
    "for igroup in np.arange(0,len(proxy_proc_map)):\n",
    "    vars()['Ext_'+proxy_proc_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "    vars()['Ext_'+proxy_proc_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "    for iyear in np.arange(0,num_years):\n",
    "        vars()['Ext_'+proxy_proc_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += \\\n",
    "            vars()[proxy_proc_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "            vars()[proxy_proc_map.loc[igroup,'Proxy_Group']][:,:,iyear]\n",
    "        vars()['Ext_'+proxy_proc_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear] += \\\n",
    "            vars()[proxy_proc_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "            vars()[proxy_proc_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear]\n",
    "        Emissions[:,:,iyear] += vars()['Ext_'+proxy_proc_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]\n",
    "        Emissions_nongrid[iyear] += vars()['Ext_'+proxy_proc_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear]\n",
    "\n",
    "#Subtract out AK emissions fraction (based on plant count ratio)\n",
    "for iyear in np.arange(0,num_years):\n",
    "    for igroup in np.arange(len(proxy_proc_map)):\n",
    "        vars()['Ext_'+proxy_proc_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] -=  \\\n",
    "            vars()['Ext_'+proxy_proc_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]* AK_plant_fraction\n",
    "    Emi_not_mapped_sum[iyear] = np.sum(Emissions[:,:,iyear])* AK_plant_fraction\n",
    "    Emissions_nongrid[iyear] += Emi_not_mapped_sum[iyear]\n",
    "    Emissions[:,:,iyear] -= Emissions[:,:,iyear]* AK_plant_fraction\n",
    "\n",
    "    \n",
    "# QA/QC gridded emissions\n",
    "# Check sum of all gridded emissions + emissions not included in gridding (e.g., AK), and other non-gridded areas\n",
    "print('QA/QC #1: Check weighted emissions against GHGI')   \n",
    "for iyear in np.arange(0,num_years):\n",
    "    calc_emi=0\n",
    "    summary_emi = EPA_emi_total_NG_CH4.iloc[2,iyear+1]\n",
    "    for igroup in np.arange(0,len(proxy_proc_map)):\n",
    "        calc_emi +=  np.sum(vars()['Ext_'+proxy_proc_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]) \n",
    "    calc_emi += Emissions_nongrid[iyear]\n",
    "    if DEBUG==1:\n",
    "        print(summary_emi)\n",
    "        print(calc_emi)\n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0002:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1.4 Save gridded emissions (kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save gridded emissions for each gridding group - for extension\n",
    "\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(grid_emi_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "unique_groups = np.unique(proxy_proc_map['GHGI_Emi_Group'])\n",
    "unique_groups = unique_groups[unique_groups != 'Emi_not_mapped']\n",
    "\n",
    "nc_out = Dataset(grid_emi_outputfile, 'r+', format='NETCDF4')\n",
    "\n",
    "for igroup in np.arange(0,len(unique_groups)):\n",
    "    print('Ext_'+unique_groups[igroup])\n",
    "    if len(np.shape(vars()['Ext_'+unique_groups[igroup]])) ==4:\n",
    "        ghgi_temp = np.sum(vars()[unique_groups[igroup]],axis=3) #sum month data if data is monthly\n",
    "    else:\n",
    "        ghgi_temp = vars()['Ext_'+unique_groups[igroup]]\n",
    "\n",
    "    # Write data to netCDF\n",
    "    data_out = nc_out.createVariable('Ext_'+unique_groups[igroup], 'f8', ('lat', 'lon','year'), zlib=True)\n",
    "    data_out[:,:,:] = ghgi_temp[:,:,:]\n",
    "\n",
    "#save nongrid data to calculate non-grid fraction extension\n",
    "data_out = nc_out.createVariable('Emissions_nongrid', 'f8', ('year'), zlib=True)  \n",
    "data_out[:] = Emissions_nongrid[:]\n",
    "nc_out.close()\n",
    "\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded emissions (kt) written to file: {}\" .format(os.getcwd())+grid_emi_outputfile)\n",
    "print(' ')\n",
    "\n",
    "del data_out, ghgi_temp, nc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3 Calculate Gridded Fluxes (molec/s/cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 2 -- Calculate fluxes (molec./s/cm2)\n",
    "\n",
    "#Initialize arrays\n",
    "check_sum_annual = np.zeros([num_years])\n",
    "Flux_Emissions_Total_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "for igroup in np.arange(0,len(proxy_proc_map)):\n",
    "    vars()['Flux_'+proxy_proc_map.loc[igroup,'GHGI_Emi_Group']+'_annual'] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "\n",
    "\n",
    "#Calculate fluxes\n",
    "for iyear in np.arange(0,num_years):\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "        month_days = month_day_leap\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        month_days = month_day_nonleap \n",
    "    \n",
    "    # calculate fluxes for annual data  (=kt * grams/kt *molec/mol *mol/g *s^-1 * cm^-2)\n",
    "    conversion_factor_annual = 10**9 * Avogadro / float(Molarch4 * np.sum(month_days) * 24 * 60 *60) / area_matrix_01\n",
    "    for igroup in np.arange(0,len(proxy_proc_map)):\n",
    "        vars()['Ext_'+proxy_proc_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] *= conversion_factor_annual\n",
    "        vars()['Flux_'+proxy_proc_map.loc[igroup,'GHGI_Emi_Group']+'_annual'][:,:,iyear] = vars()['Ext_'+proxy_proc_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]\n",
    "        Flux_Emissions_Total_annual[:,:,iyear] = Emissions[:,:,iyear]*conversion_factor_annual\n",
    "    check_sum_annual[iyear] += np.sum(Flux_Emissions_Total_annual[:,:,iyear]/conversion_factor_annual) #convert back to emissions to check at end\n",
    "\n",
    "print(' ')\n",
    "print('QA/QC #2: Check final gridded fluxes against GHGI')  \n",
    "# for the sum, check the converted annual emissions (convert back from flux) plus all the non-gridded emissions\n",
    "for iyear in np.arange(0,num_years):\n",
    "    calc_emi = check_sum_annual[iyear] + Emi_not_mapped_sum[iyear]\n",
    "    summary_emi = EPA_emi_total_NG_CH4.iloc[2,iyear+1]\n",
    "    if DEBUG==1:\n",
    "        print(calc_emi)\n",
    "        print(summary_emi)\n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0001:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 5. Write gridded (0.1⁰x0.1⁰) data to netCDF files.\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize netCDF files\n",
    "data_IO_fn.initialize_netCDF(gridded_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write the Data to netCDF\n",
    "nc_out = Dataset(gridded_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Total_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded annual natural gas processing fluxes written to file: {}\" .format(os.getcwd())+gridded_outputfile)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 6. Plot Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Plot Annual Emission Fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot annual emissions for each year\n",
    "scale_max = 10\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_str, scale_max,save_flag,save_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Plot Difference Between First and Last Inventory Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot difference between last and first year\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_diff_str,save_flag,save_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Plot Activity Data Heat Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Map_Plants\n",
    "\n",
    "# Activity_Map = 0.1x0.1 map of activity data (counts or absolute units)\n",
    "# Plot_Frac    = 0 or 1 (0= plot activity data in absolute counts, 1= plot fractional activity data)\n",
    "# Lat          = 0.1 degree Lat values (select range)\n",
    "# Lon          = 0.1 degree Lon values (select range)\n",
    "# year_range   = array of inventory years\n",
    "# title_str    = title of map\n",
    "# legend_str   = title of legend\n",
    "# scale_max    = maximum of color scale\n",
    "\n",
    "Activity_Map = Map_CombProcPlants#Map_Plants\n",
    "Plot_Frac = 1\n",
    "Lat = Lat_01\n",
    "Lon = Lon_01\n",
    "year_range = year_range\n",
    "title_str2 = \"Proxy - Processing Plant Emissions\"\n",
    "legend_str = \"Annual Fraction of National Processing Plant Emissions\"\n",
    "scale_max = 0.05\n",
    "\n",
    "for iyear in np.arange(0,len(year_range)): \n",
    "    my_cmap = copy(plt.cm.get_cmap('rainbow',lut=3000))\n",
    "    my_cmap._init()\n",
    "    slopen = 200\n",
    "    alphas_slope = np.abs(np.linspace(0, 1.0, slopen))\n",
    "    alphas_stable = np.ones(3003-slopen)\n",
    "    alphas = np.concatenate((alphas_slope, alphas_stable))\n",
    "    my_cmap._lut[:,-1] = alphas\n",
    "    my_cmap.set_under('gray', alpha=0)\n",
    "    \n",
    "    Lon_cor = Lon[50:632]-0.05\n",
    "    Lat_cor = Lat[43:300]-0.05\n",
    "    \n",
    "    xpoints = Lon_cor\n",
    "    ypoints = Lat_cor\n",
    "    yp,xp = np.meshgrid(ypoints,xpoints)\n",
    "    \n",
    "    if np.shape(Activity_Map)[0] == len(year_range):\n",
    "        if Plot_Frac ==1:\n",
    "            zp = Activity_Map[iyear,43:300,50:632]/np.sum(Activity_Map[iyear,:,:])\n",
    "        else:\n",
    "            zp = Activity_Map[iyear,43:300,50:632]\n",
    "    elif np.shape(Activity_Map)[2] == len(year_range):\n",
    "        if Plot_Frac ==1:\n",
    "            zp = Activity_Map[43:300,50:632,iyear]/np.sum(Activity_Map[:,:,iyear])\n",
    "        else: \n",
    "            zp = Activity_Map[43:300,50:632,iyear]\n",
    "    #zp = zp/float(10**6 * Avogadro) * (year_days * 24 * 60 * 60) * Molarch4 * float(1e10)\n",
    "    \n",
    "    fig, ax = plt.subplots(dpi=300)\n",
    "    m = Basemap(llcrnrlon=xp.min(), llcrnrlat=yp.min(), urcrnrlon=xp.max(),\n",
    "                urcrnrlat=yp.max(), projection='merc', resolution='h', area_thresh=5000)\n",
    "    m.drawmapboundary(fill_color='Azure')\n",
    "    m.fillcontinents(color='FloralWhite', lake_color='Azure',zorder=1)\n",
    "    m.drawcoastlines(linewidth=0.5,zorder=3)\n",
    "    m.drawstates(linewidth=0.25,zorder=3)\n",
    "    m.drawcountries(linewidth=0.5,zorder=3)\n",
    "        \n",
    "        #if Plot_Frac == 1:\n",
    "        #    scale_max \n",
    "    \n",
    "    xpi,ypi = m(xp,yp)\n",
    "    #plot = m.pcolor(xpi,ypi,zp.transpose(), cmap=my_cmap, vmin=10**-15, vmax=scale_max, snap=True,zorder=2)\n",
    "    plot = m.scatter(xpi,ypi,s=20,c=zp.transpose(),cmap=my_cmap,zorder=2,vmin = 10**-15,snap = True,vmax = scale_max)\n",
    "    cb = m.colorbar(plot, location = \"bottom\", pad = \"1%\")        \n",
    "    tick_locator = ticker.MaxNLocator(nbins=5)\n",
    "    cb.locator = tick_locator\n",
    "    cb.update_ticks()\n",
    "    \n",
    "    cb.ax.set_xlabel(legend_str,fontsize=10)\n",
    "    cb.ax.tick_params(labelsize=10)\n",
    "    Titlestring = str(year_range[iyear])+' '+title_str2\n",
    "    plt.title(Titlestring, fontsize=14);\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = datetime.now() \n",
    "ft = ct.timestamp() \n",
    "time_elapsed = (ft-it)/(60*60)\n",
    "print('Time to run: '+str(time_elapsed)+' hours')\n",
    "print('** GEPA_1B2b_Natural_Gas_Systems_Processing: COMPLETE **')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

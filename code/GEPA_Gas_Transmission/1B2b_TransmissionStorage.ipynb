{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded EPA Methane Inventory\n",
    "## Category: 1B2b Natural Gas Transmission and Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Authors: \n",
    "Erin E. McDuffie\n",
    "#### Date Last Updated: \n",
    "see Step 0\n",
    "#### Notebook Purpose: \n",
    "This Notebook calculates and reports annual gridded (0.1°x0.1°) methane emission fluxes (molec./cm2/s) from Transmission and Storage from Natural Gas Systems in the CONUS region between 2012-2018. \n",
    "#### Summary & Notes:\n",
    "EPA GHGI Transmission and Storage emissions are read in at the national level from the GHGI workbook. Emissions are split into contributions from Transmission and Storage Compressor Stations, Transmission pipelines, Import and Export Terminals, Storage Wells, M&R stations on farm land, and LNG storage stations. The activity/proxy data used to allocate emissions from each group include DOE import and export terminal locations, Enverus midstream data for transmission pipeline locations, Enverus non-associated gas well locations, and a combination of EIA, PHMSA, Enverus, and GHGRP data for LNG storage stations, Transmission, and Storage Compressor Stations. National emissions are spatially distributed onto a 0.1°x0.1° grid based on the emissions/locations of wells, pipeline, stations, and storage facilities.  Emissions are converted to emission flux. Annual emission fluxes (molec./cm2/s) are written to final netCDFs in the ‘/code/Final_Gridded_Data/’ folder.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Step 0. Set-Up Notebook Modules, Functions, and Local Parameters and Constants\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm working directory & print last update time\n",
    "import os\n",
    "import time\n",
    "modtime = os.path.getmtime('./1B2b_TransmissionStorage.ipynb')\n",
    "modificationTime = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(modtime))\n",
    "print(\"This file was last modified on: \", modificationTime)\n",
    "print('')\n",
    "print(\"The directory we are working in is {}\" .format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Include plots within notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import pyodbc\n",
    "import PyPDF2 as pypdf\n",
    "import tabula as tb\n",
    "import shapefile as shp\n",
    "from datetime import datetime\n",
    "from copy import copy\n",
    "\n",
    "# Import additional modules\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# Load netCDF (for manipulating netCDF file types)\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Set up ticker\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#add path for the global function module (file)\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../Global_Functions/'))\n",
    "#print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Load functions\n",
    "import data_load_functions as data_load_fn\n",
    "import data_functions as data_fn\n",
    "import data_IO_functions as data_IO_fn\n",
    "import data_plot_functions as data_plot_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT Files\n",
    "# Assign global file names\n",
    "global_filenames = data_load_fn.load_global_file_names()\n",
    "State_ANSI_inputfile = global_filenames[0]\n",
    "#County_ANSI_inputfile = global_filenames[1]\n",
    "#pop_map_inputfile = global_filenames[2]\n",
    "Grid_area01_inputfile = global_filenames[3]\n",
    "Grid_area001_inputfile = global_filenames[4]\n",
    "#Grid_state001_ansi_inputfile = global_filenames[5]\n",
    "#Grid_county001_ansi_inputfile = global_filenames[6]\n",
    "globalinputlocation = global_filenames[0][0:20]\n",
    "print(globalinputlocation)\n",
    "\n",
    "# EPA Inventory Data\n",
    "EPA_NG_inputfile = globalinputlocation+'GHGI/Ch3_Energy/NaturalGasSystems_1990-2018_GHGI_2020-04-11.xlsx'\n",
    "\n",
    "#proxy mapping file\n",
    "NG_Mapping_inputfile = './InputData/NaturalGas_TransmissionStorage_ProxyMapping.xlsx'\n",
    "\n",
    "#Activity Data\n",
    "#LNG Import/Export terminals\n",
    "LNGTerminal_inputfile = './InputData/LNG_ImportExport_Terminals.xlsx'\n",
    "\n",
    "#Active non-associated gas wells\n",
    "NAgaswell_ongrid_inputfile = './InputData/Map_Enverus_NAGasWellLocations_ongrid.nc'\n",
    "NAgaswell_offgrid_inputfile = './InputData/Map_Enverus_NAGasWellLocations_offgrid.csv'\n",
    "\n",
    "\n",
    "#LNG storage facilities\n",
    "LNG_storage_inputfile = './InputData/annual-liquefied-natural-gas-2010-present/annual_liquefied_natural_gas_'\n",
    "LNG_storage_Enverus_inputfile = globalinputlocation +'Enverus/Midstream/LNG_Terminals_AllUS_WGS84.xls'\n",
    "FracTracker_inputfile = './InputData/FracTracker_PeakShaving_WGS84.xls'\n",
    "\n",
    "#Transmission Pipelines\n",
    "Enverus_NG_Transpipeline_inputfile = globalinputlocation+ 'Enverus/Midstream/Transmission_Pipelines_CONUS_WGS84.xls'\n",
    "AKHI_pipelines_shp = globalinputlocation+ 'Enverus/Midstream/Transmission_pipelines_AKHI_wgs84.shp'\n",
    "CONUS_pipelines_shp = globalinputlocation+ 'Enverus/Midstream/Transmission_pipelines_CONUS_wgs84.shp'\n",
    "\n",
    "#Crop Land\n",
    "Cropland_001_inputfile = globalinputlocation + 'Gridded/AllCrops_'\n",
    "\n",
    "#Transmission Compressor Stations\n",
    "Enverus_NG_TransStations_inputfile = globalinputlocation+ 'Enverus/Midstream/Transmission_CompressorStations_CONUS_onshore_WGS84.xls'\n",
    "AKHI_transstat_shp = globalinputlocation+ 'Enverus/Midstream/Transmission_CompressorStations_AKHI_WGS84.shp'\n",
    "CONUS_transstat_shp = globalinputlocation+ 'Enverus/Midstream/Transmission_CompressorStations_CONUS_onshore_WGS84.shp'\n",
    "\n",
    "#GHGRP Data\n",
    "GHGRP_facility_inputfile = './InputData/GHGRP_Facility_Info.csv'\n",
    "GHGRP_subpartw_inputfile = './InputData/ef_w_emissions_source_ghg.xlsx'\n",
    "\n",
    "#Storage Compressor Stations\n",
    "Enverus_NG_StorStations_inputfile = globalinputlocation+ 'Enverus/Midstream/Storage_CompressorStations_AllUS_WGS84.xls'\n",
    "Enverus_NG_StorFields_inputfile = globalinputlocation+ 'Enverus/Midstream/GasStorage_AllUS_WGS84.xls'\n",
    "EIA_StorFields_inputfile = './InputData/191 Field Level Storage Data (Annual).xlsx'\n",
    "EIA_StorFields_locs_inputfile = './InputData/EIA_Natural_Gas_Underground_Storage.xlsx'\n",
    "\n",
    "#OUTPUT FILES\n",
    "gridded_outputfile = '../Final_Gridded_Data/EPA_v2_1B2b_Natural_Gas_TransmissionStorage.nc'\n",
    "netCDF_description = 'Gridded EPA Inventory - Natural Gas Systems Emissions - IPCC Source Category 1B2b - Transmission and Storage'\n",
    "title_str = \"EPA methane emissions from gas transmission and storage\"\n",
    "title_diff_str = \"Emissions from gas transmission and storage difference: 2018-2012\"\n",
    "\n",
    "#output gridded proxy data\n",
    "grid_emi_outputfile = '../Final_Gridded_Data/Extension/v2_input_data/NG_Transmission_Grid_Emi.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIFY RECALCS\n",
    "\n",
    "# ReCalculate Cropland array = 1, load from previous file = 0\n",
    "ReCalc_Cropland = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local variables\n",
    "start_year = 2012  #First year in emission timeseries\n",
    "end_year = 2018    #Last year in emission timeseries\n",
    "year_range = [*range(start_year, end_year+1,1)] #List of emission years\n",
    "year_range_str=[str(i) for i in year_range]\n",
    "num_years = len(year_range)\n",
    "\n",
    "# Define constants\n",
    "Avogadro   = 6.02214129 * 10**(23)  #molecules/mol\n",
    "Molarch4   = 16.04                  #g/mol\n",
    "Res01      = 0.1                    # degrees\n",
    "\n",
    "# Continental US Lat/Lon Limits (for netCDF files)\n",
    "Lon_left = -130       #deg\n",
    "Lon_right = -60       #deg\n",
    "Lat_low  = 20         #deg\n",
    "Lat_up  = 55          #deg\n",
    "loc_dimensions = [Lat_low, Lat_up, Lon_left, Lon_right]\n",
    "\n",
    "ilat_start = int((90+Lat_low)/Res01) #1100:1450 (continental US range)\n",
    "ilat_end = int((90+Lat_up)/Res01)\n",
    "ilon_start = abs(int((-180-Lon_left)/Res01)) #500:1200 (continental US range)\n",
    "ilon_end = abs(int((-180-Lon_right)/Res01))\n",
    "\n",
    "# Number of days in each month\n",
    "month_day_leap  = [  31,  29,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_day_nonleap = [  31,  28,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_tag = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "month_dict = {'January':1, 'February':2,'March':3,'April':4,'May':5,'June':6, 'July':7,'August':8,'September':9,'October':10,\\\n",
    "             'November':11,'December':12}\n",
    "\n",
    "# Month arrays\n",
    "month_range_str = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "num_months = len(month_range_str)\n",
    "num_regions = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;\n",
    "//prevent auto-scrolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track run time\n",
    "ct = datetime.now() \n",
    "it = ct.timestamp() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Step 1. Load in State ANSI data, and Area Maps\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State-level ANSI Data\n",
    "#Read the state ANSI file array\n",
    "State_ANSI, name_dict, abbr_dict = data_load_fn.load_state_ansi(State_ANSI_inputfile)[0:3]\n",
    "#QA: number of states\n",
    "print('Read input file: '+ f\"{State_ANSI_inputfile}\")\n",
    "print('Total \"States\" found: ' + '%.0f' % len(State_ANSI))\n",
    "print(' ')\n",
    "\n",
    "# 0.01 x0.01 degree Data\n",
    "# State ANSI IDs and grid cell area (m2) maps\n",
    "#state_ANSI_map = data_load_fn.load_state_ansi_map(Grid_state001_ansi_inputfile)\n",
    "area_map, lat001, lon001 = data_load_fn.load_area_map_001(Grid_area001_inputfile)\n",
    "\n",
    "# 0.1 x0.1 degree data\n",
    "# grid cell area and state ANSI maps\n",
    "Lat01, Lon01 = data_load_fn.load_area_map_01(Grid_area01_inputfile)[1:3]\n",
    "#Select relevant Continental 0.1 x0.1 domain\n",
    "Lat_01 = Lat01[ilat_start:ilat_end]\n",
    "Lon_01 = Lon01[ilon_start:ilon_end]\n",
    "area_matrix_01 = data_fn.regrid001_to_01(area_map, Lat_01, Lon_01)\n",
    "area_matrix_01 *= 10000  #convert from m2 to cm2\n",
    "#state_ANSI_map_01 = data_fn.regrid001_to_01(state_ANSI_map, Lat_01, Lon_01)\n",
    "del area_map, lat001, lon001, global_filenames\n",
    "\n",
    "# Print time\n",
    "ct = datetime.now() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 2: Read-in and Format Proxy Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 Read In Proxy Mapping File & Make Proxy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1.1 Format Proxy Group Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load GHGI Mapping Groups\n",
    "names = pd.read_excel(NG_Mapping_inputfile, sheet_name = \"GHGI Map - T&S\", usecols = \"A:B\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "ghgi_ts_map = pd.read_excel(NG_Mapping_inputfile, sheet_name = \"GHGI Map - T&S\", usecols = \"A:B\", skiprows = 2, names = colnames)\n",
    "#drop rows with no data, remove the parentheses and \"\"\n",
    "ghgi_ts_map = ghgi_ts_map[ghgi_ts_map['GHGI_Emi_Group'] != 'na']\n",
    "ghgi_ts_map = ghgi_ts_map[ghgi_ts_map['GHGI_Emi_Group'].notna()]\n",
    "ghgi_ts_map = ghgi_ts_map[ghgi_ts_map['GHGI_Emi_Group'] != '-']\n",
    "ghgi_ts_map['GHGI_Source']= ghgi_ts_map['GHGI_Source'].str.replace(r\"\\(\",\"\")\n",
    "ghgi_ts_map['GHGI_Source']= ghgi_ts_map['GHGI_Source'].str.replace(r\"\\)\",\"\")\n",
    "ghgi_ts_map['GHGI_Source']= ghgi_ts_map['GHGI_Source'].str.replace(r\"+\",\"\")\n",
    "ghgi_ts_map.reset_index(inplace=True, drop=True)\n",
    "display(ghgi_ts_map)\n",
    "\n",
    "#load emission group - proxy map\n",
    "names = pd.read_excel(NG_Mapping_inputfile, sheet_name = \"Proxy Map - T&S\", usecols = \"A:D\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "proxy_ts_map = pd.read_excel(NG_Mapping_inputfile, sheet_name = \"Proxy Map - T&S\", usecols = \"A:D\", skiprows = 1, names = colnames)\n",
    "display((proxy_ts_map))\n",
    "\n",
    "#create empty proxy and emission group arrays (add months for proxy variables that have monthly data)\n",
    "for igroup in np.arange(0,len(proxy_ts_map)):\n",
    "    if proxy_ts_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "        vars()[proxy_ts_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "        vars()[proxy_ts_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years,num_months])\n",
    "        vars()[proxy_ts_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years,num_months])\n",
    "    else:\n",
    "        vars()[proxy_ts_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        vars()[proxy_ts_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "        vars()[proxy_ts_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])\n",
    "        \n",
    "emi_group_names = np.unique(ghgi_ts_map['GHGI_Emi_Group'])\n",
    "print('QA/QC: Is the number of emission groups the same for the proxy and emissions tabs?')\n",
    "if (len(emi_group_names) == len(np.unique(proxy_ts_map['GHGI_Emi_Group']))):\n",
    "    print('PASS')\n",
    "else:\n",
    "    print('FAIL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2 Read In EIA Import/Export Terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read in the DOE Data (from pre-processed input file, see Data and Assumptions document for more details)\n",
    "names = pd.read_excel(LNGTerminal_inputfile, sheet_name = 'Sheet1', usecols = \"A:F\",skiprows = 5, header = 0)\n",
    "colnames = names.columns.values\n",
    "DOE_LNGTerminals = pd.read_excel(LNGTerminal_inputfile, sheet_name = 'Sheet1',usecols = \"A:F\", skiprows = 5, names = colnames)\n",
    "#select only years within range\n",
    "DOE_LNGTerminals = DOE_LNGTerminals[DOE_LNGTerminals['Year'] <= year_range[-1]]\n",
    "#display(DOE_LNGTerminals)\n",
    "\n",
    "#initialize the map arrays\n",
    "map_InputTerminals = np.zeros([len(Lat_01),len(Lon_01), num_years]) \n",
    "map_InputTerminals_nongrid = np.zeros([num_years])\n",
    "map_ExportTerminals = np.zeros([len(Lat_01),len(Lon_01),num_years]) \n",
    "map_ExportTerminals_nongrid = np.zeros([num_years])\n",
    "\n",
    "# Place the DOE data onto the map arrays\n",
    "for iterminal in np.arange(0,len(DOE_LNGTerminals)):\n",
    "    if type(DOE_LNGTerminals['Terminal Latitude'][iterminal]) is str:\n",
    "        term_lat = float(DOE_LNGTerminals['Terminal Latitude'][iterminal].strip('\\u200e'))\n",
    "    else:\n",
    "        term_lat = DOE_LNGTerminals['Terminal Latitude'][iterminal]\n",
    "    \n",
    "    if type(DOE_LNGTerminals['Terminal Longitude'][iterminal]) is str:\n",
    "        term_lon = float(DOE_LNGTerminals['Terminal Longitude'][iterminal].strip('\\u200e'))\n",
    "    else:\n",
    "        term_lon = (DOE_LNGTerminals['Terminal Longitude'][iterminal])\n",
    "    \n",
    "    if term_lon > Lon_left and term_lon < Lon_right \\\n",
    "        and term_lat > Lat_low and term_lat < Lat_up:\n",
    "        ilat = int((term_lat - Lat_low)/Res01)\n",
    "        ilon = int((term_lon - Lon_left)/Res01)\n",
    "        iyear = np.where(year_range == DOE_LNGTerminals['Year'][iterminal])[0][0]\n",
    "        if DOE_LNGTerminals['Terminal Type'][iterminal] =='Import':\n",
    "            map_InputTerminals[ilat,ilon,iyear] += 1\n",
    "        elif DOE_LNGTerminals['Terminal Type'][iterminal] =='Export':\n",
    "            map_ExportTerminals[ilat,ilon,iyear] += 1\n",
    "    else: \n",
    "        #print(iterminal)\n",
    "        iyear = np.where(year_range == DOE_LNGTerminals['Year'][iterminal])[0][0]\n",
    "        if DOE_LNGTerminals['Terminal Type'][iterminal] =='Import':\n",
    "            map_InputTerminals_nongrid[iyear] += 1\n",
    "        elif DOE_LNGTerminals['Terminal Type'][iterminal] =='Export':\n",
    "            map_ExportTerminals_nongrid[iyear] += 1 \n",
    "\n",
    "###NOTE: CORRECT FOR YEARS WITH NO EXPORT TERMINAL DATA\n",
    "map_ExportTerminals[:,:,0] = map_ExportTerminals[:,:,1]\n",
    "map_ExportTerminals[:,:,2] = map_ExportTerminals[:,:,4]\n",
    "map_ExportTerminals[:,:,3] = map_ExportTerminals[:,:,4]\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    print('Year: ',year_range_str[iyear])\n",
    "    print('Total Import Terminals on grid: ',np.sum(map_InputTerminals[:,:,iyear]))\n",
    "    print('Total Import Terminals off grid: ',np.sum(map_InputTerminals_nongrid[iyear]))\n",
    "    print('Total Export Terminals on grid: ',np.sum(map_ExportTerminals[:,:,iyear]))\n",
    "    print('Total Export Terminals off grid: ',np.sum(map_ExportTerminals_nongrid[iyear]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3 Read In Enverus Non-Associated Gas Wells (pre-processed in GEPA Production script)\n",
    "New proxy now formatted in Step 2.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.4 LNG Storage Station Proxy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.4.1 Read In PHMSA Data (master list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read in the PHMSA Data (see Data and Assumptions document for more details)\n",
    "PHMSA_LNGstorage = np.zeros(0)\n",
    "\n",
    "print(\"Number of In Service LNG Stations:\")\n",
    "for iyear in np.arange(0,num_years):\n",
    "    names = pd.read_excel(LNG_storage_inputfile+year_range_str[iyear]+'.xlsx', sheet_name = 'LNG AR Part B', skiprows = 2, header = 0)\n",
    "    colnames = names.columns.values\n",
    "    temp = pd.read_excel(LNG_storage_inputfile+year_range_str[iyear]+'.xlsx', sheet_name = 'LNG AR Part B',skiprows = 2, names = colnames)\n",
    "    #temp = PHMSA_LNGstorage\n",
    "    temp = temp[['REPORT_YEAR','FACILITY_NAME','PARTA2NAMEOFCOMP','TOTAL_CAPACITY_BBLS','TOTAL_CAPACITY_MMCF','FACILITY_STATE','FACILITY_ZIP_CODE','FACILITY_STATUS','TYPE_OF_FACILITY','FUNCTION_OF_FACILITY']]\n",
    "    temp = temp[temp['FUNCTION_OF_FACILITY'].isin(['Storage w/ Liquefaction','Storage w/o Liquefaction','Storage w/ Both'])]\n",
    "    temp = temp[temp['FACILITY_STATUS'] == 'In Service']\n",
    "    temp = temp.fillna('')\n",
    "    if iyear ==0:\n",
    "        PHMSA_LNGstorage = temp\n",
    "    else:\n",
    "        PHMSA_LNGstorage = PHMSA_LNGstorage.append(temp)\n",
    "    print('Year',year_range_str[iyear],': ', len(temp))\n",
    "\n",
    "PHMSA_LNGstorage['Lat']=0\n",
    "PHMSA_LNGstorage['Lon']=0\n",
    "PHMSA_LNGstorage.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.4.2 Read In Enverus Midstream LNG station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read in Enverus Midstream LNG station data (not complete and includes non-peak shaving facilities as well)\n",
    "LNG_storage_Enverus_inputfile\n",
    "names = pd.read_excel(LNG_storage_Enverus_inputfile, skiprows = 0, header = 0)\n",
    "colnames = names.columns.values\n",
    "Enverus_LNGstations = pd.read_excel(LNG_storage_Enverus_inputfile, skiprows = 0, names = colnames)\n",
    "Enverus_LNGstations = Enverus_LNGstations[['NAME','OPERATOR','TYPE','CAP_STO','STATE_NAME','CNTY_NAME','Longitude','Latitude']]\n",
    "Enverus_LNGstations = Enverus_LNGstations[~Enverus_LNGstations['STATE_NAME'].isin(['Alaska','Hawaii'])]\n",
    "Enverus_LNGstations.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.4.3 Find matching plant locations (PHMSA/Enverus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Find matching plant locations between PHMSA and Enverus and add Lat/Lons\n",
    "for istation in np.arange(0,len(PHMSA_LNGstorage)):\n",
    "    temp_state = PHMSA_LNGstorage.loc[istation,'FACILITY_STATE']\n",
    "    temp_name = PHMSA_LNGstorage.loc[istation,'FACILITY_NAME'].lower()\n",
    "    temp_name = temp_name.replace(\"lng\",\"\")\n",
    "    #print(temp_state)\n",
    "    state_name = State_ANSI.loc[State_ANSI['abbr']==temp_state,'name'].values[0]\n",
    "    #select matching state values in Enverus dataset\n",
    "    temp_enverus = Enverus_LNGstations[Enverus_LNGstations['STATE_NAME']== state_name]\n",
    "    temp_enverus.reset_index(drop=True,inplace=True)\n",
    "    splitname1 = temp_name.split()\n",
    "    num_match = 0\n",
    "    for ienverus in np.arange(0,len(temp_enverus)):\n",
    "        temp_name2 = temp_enverus.loc[ienverus,'NAME'].lower()\n",
    "        test = temp_enverus.loc[ienverus,'CNTY_NAME'].lower()\n",
    "        temp_name2 = temp_name2.replace(\"lng\",\"\")\n",
    "        splitname2 = temp_name2.split()\n",
    "        splitname2.append(test)\n",
    "        #print(splitname2)\n",
    "        if bool(set(splitname1) & set(splitname2)):\n",
    "            num_match +=1\n",
    "            PHMSA_LNGstorage.loc[istation,'Lat'] = temp_enverus.loc[ienverus,'Latitude']\n",
    "            PHMSA_LNGstorage.loc[istation,'Lon'] = temp_enverus.loc[ienverus,'Longitude']\n",
    "            #print('match found, ', temp_name, ',',splitname2)\n",
    "        if 'chatanooga' in splitname2 and 'chattanooga' in splitname1:\n",
    "            num_match +=1\n",
    "            PHMSA_LNGstorage.loc[istation,'Lat'] = temp_enverus.loc[ienverus,'Latitude']\n",
    "            PHMSA_LNGstorage.loc[istation,'Lon'] = temp_enverus.loc[ienverus,'Longitude']\n",
    "            #print('match found, ', temp_name, ',',splitname2)\n",
    "        if num_match > 1:\n",
    "            print('STOP')\n",
    "            print(splitname1)\n",
    "            print(temp_enverus)\n",
    "            break\n",
    "    if num_match ==0:\n",
    "        continue\n",
    "\n",
    "#display(PHMSA_LNGstorage)\n",
    "print('QA/QC: Number of PHMSA LNG storage stations Missing Lat/Lons:', len(PHMSA_LNGstorage[PHMSA_LNGstorage['Lon']==0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.4.4 Read In Frac Tracker Peak Shavers Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read in FracTracker Peak Shaving station data (see Data dn Assumptions for further details)\n",
    "names = pd.read_excel(FracTracker_inputfile, skiprows = 0, header = 0)\n",
    "colnames = names.columns.values\n",
    "FracTracker_PeakShavers = pd.read_excel(FracTracker_inputfile, skiprows = 0, names = colnames)\n",
    "FracTracker_PeakShavers = FracTracker_PeakShavers[['Company','City','State','Zip','Longitude','Latitude']]\n",
    "FracTracker_PeakShavers = FracTracker_PeakShavers[~FracTracker_PeakShavers['State'].isin(['Alaska','Hawaii'])]\n",
    "FracTracker_PeakShavers.reset_index(drop=True,inplace=True)\n",
    "\n",
    "for iplant in np.arange(0,len(FracTracker_PeakShavers)):\n",
    "    temp_zip = FracTracker_PeakShavers.loc[iplant,'Zip']\n",
    "    temp_zip = temp_zip.replace('.','0')\n",
    "    temp_zip = temp_zip.zfill(5)\n",
    "    FracTracker_PeakShavers.loc[iplant,'Zip'] = temp_zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.4.5 Find remaining matching plant locations (PHMSA/Frack Tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Match PHMSA data based on zip code first, then any matching words in the operator and city names\n",
    "\n",
    "#Find matching plant locations between PHMSA and Enverus and add Lat/Lons\n",
    "for istation in np.arange(0,len(PHMSA_LNGstorage)):\n",
    "    if PHMSA_LNGstorage.loc[istation,'Lat'] ==0:\n",
    "        #print(istation)\n",
    "        temp_state = PHMSA_LNGstorage.loc[istation,'FACILITY_STATE']\n",
    "        temp_zip = PHMSA_LNGstorage.loc[istation,'FACILITY_ZIP_CODE']\n",
    "        match = np.where(FracTracker_PeakShavers['Zip'] == temp_zip)\n",
    "        if match[0].size:\n",
    "            if len(match[0]==1):\n",
    "                #continue\n",
    "                #num_match +=1\n",
    "                PHMSA_LNGstorage.loc[istation,'Lat'] = FracTracker_PeakShavers.loc[match[0][0],'Latitude']\n",
    "                PHMSA_LNGstorage.loc[istation,'Lon'] = FracTracker_PeakShavers.loc[match[0][0],'Longitude']\n",
    "            else:\n",
    "                print('error', istation)\n",
    "        elif temp_state =='AK' or temp_state =='HI':\n",
    "            continue\n",
    "            #don't need to find lat/lon for AK or HI plants\n",
    "        else:\n",
    "            #print(istation, temp_state)\n",
    "            #try matching based on company name, state, and city (also hand-correct stations that can't be found)\n",
    "            temp_state = PHMSA_LNGstorage.loc[istation,'FACILITY_STATE']\n",
    "            #make list of company and facility name to try to match later\n",
    "            temp_name = PHMSA_LNGstorage.loc[istation,'PARTA2NAMEOFCOMP'].lower()\n",
    "            temp_name = temp_name.replace(\"lng\",\"\")\n",
    "            test = PHMSA_LNGstorage.loc[istation,'FACILITY_NAME']\n",
    "            temp_name = temp_name+' '+test.lower()\n",
    "            splitname1 = temp_name.split()\n",
    "            state_name = State_ANSI.loc[State_ANSI['abbr']==temp_state,'name'].values[0]\n",
    "            #select FracTracker facilities within the current state\n",
    "            temp_frac = FracTracker_PeakShavers[FracTracker_PeakShavers['State']== state_name]\n",
    "            temp_frac.reset_index(drop=True,inplace=True)\n",
    "            num_match = 0\n",
    "            ifrac_list = []\n",
    "            match_list = []\n",
    "            for ifrac in np.arange(0,len(temp_frac)):\n",
    "                #capture all the fractracker data within a given state that has matching components of the\n",
    "                # company and city name\n",
    "                #make list of company, city name to compare to the PHMSA list\n",
    "                temp_name2 = temp_frac.loc[ifrac,'Company'].lower()\n",
    "                test = temp_frac.loc[ifrac,'City']\n",
    "                temp_name2 = temp_name2.replace(\"lng\",\"\")\n",
    "                temp_name2 = temp_name2+' '+test.lower()\n",
    "                splitname2 = temp_name2.split()\n",
    "                if bool(set(splitname1) & set(splitname2)):\n",
    "                    #if there are words that match, record that datapoint from FracTracker\n",
    "                    num_match +=1\n",
    "                    match_len = len(set(splitname1) & set(splitname2))\n",
    "                    ifrac_list = ifrac_list+[ifrac]\n",
    "                    match_list = match_list+[match_len]\n",
    "            if num_match == 1:\n",
    "                #if only one match - assign value\n",
    "                PHMSA_LNGstorage.loc[istation,'Lat'] = temp_frac.loc[ifrac_list[0],'Latitude']\n",
    "                PHMSA_LNGstorage.loc[istation,'Lon'] = temp_frac.loc[ifrac_list[0],'Longitude']\n",
    "            elif num_match > 1:\n",
    "                #if more than one match, assign best possible guess (and hand correct select stations)\n",
    "                if min(match_list) != max(match_list):\n",
    "                    # assign based on which entry has the largest number of matching words\n",
    "                    imax = match_list.index(max(match_list))\n",
    "                    PHMSA_LNGstorage.loc[istation,'Lat'] = temp_frac.loc[ifrac_list[imax],'Latitude']\n",
    "                    PHMSA_LNGstorage.loc[istation,'Lon'] = temp_frac.loc[ifrac_list[imax],'Longitude']\n",
    "                else:\n",
    "                    #of assign by hand if same number of matching words\n",
    "                    if set(splitname1) == set(['alabama', 'gas', 'corporation', 'east', 'lauderdale', 'lng', 'facility']) or \\\n",
    "                        set(splitname1) == set(['spire', 'alabama', 'inc.', 'east', 'lauderdale', 'lng', 'facility']):\n",
    "                        #print('here', istation, splitname1)\n",
    "                        #use Anderson location\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lat'] = 34.928418\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lon'] = -87.266407\n",
    "                    elif set(splitname1) == set(['essex', 'county', 'gas', 'co', 'lng', 'plant', 'haverhill-ma']):\n",
    "                        #havermill lat/lon\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lat'] = 42.785666\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lon'] = -71.088676\n",
    "                    elif set(splitname1) == set(['northwest', 'natural', 'gas', 'co', 'gasco', 'lng', 'plant']):\n",
    "                        #portland loc\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lat'] = 45.525211\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lon'] = -122.672080\n",
    "                    elif set(splitname1) == set(['philadelphia', 'gas', 'works', 'passyunk_lng']):\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lat'] = 39.978667\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lon'] = -75.148777\n",
    "                    elif set(splitname1) == set(['philadelphia', 'gas', 'works', 'richmond_lng']):\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lat'] = 39.984250\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lon'] = -75.088662\n",
    "                    elif set(splitname1) == set(['energy', 'north', 'natural', 'gas', 'inc', 'lng', 'plant', 'manchester-nh']):\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lat'] = 42.995640\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lon'] = -71.454789\n",
    "                    elif set(splitname1) == set(['energy', 'north', 'natural', 'gas', 'inc', 'lng', 'plant', 'tilton-nh']):\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lat'] = 43.456485\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lon'] = -71.565167\n",
    "                    elif set(splitname1) == set(['energy', 'north', 'natural', 'gas', 'inc', 'lng', 'plant', 'concord-nh']):\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lat'] = 43.198320\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lon'] = -71.540134\n",
    "                    elif set(splitname1) == set(['midamerican', 'energy', 'company', 'wat', 'lng', 'plant'])  :\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lat'] = 42.508137\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lon'] = -92.347521\n",
    "                    elif set(splitname1) == set(['midamerican', 'energy', 'company', 'bet', 'lng', 'plant'])  :\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lat'] = 41.564233\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lon'] = -90.476182\n",
    "                    elif set(splitname1) == set(['ugi', 'energy', 'services', 'temple', 'lng', 'plant'])  :\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lat'] = 40.421699\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lon'] =  -75.927026\n",
    "                    elif set(splitname1) == set(['ugi', 'energy', 'services', 'steelton', 'lng'])  :\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lat'] = 40.237811\n",
    "                        PHMSA_LNGstorage.loc[istation,'Lon'] =  -76.851624   \n",
    "                    else:\n",
    "                        print('Review', istation, 'here1')\n",
    "                        print(PHMSA_LNGstorage.iloc[istation,:])\n",
    "                        print(splitname1)\n",
    "                        print(temp_frac)\n",
    "            elif num_match ==0:\n",
    "                #if there were no matching words, then assign by hand\n",
    "                if set(splitname1) == set(['puget', 'sound', 'energy', 'lng', 'mobile', 'system']) or \\\n",
    "                          set(splitname1) == set(['puget', 'sound', 'energy', 'gig', 'harbor', 'satellite'])  :\n",
    "                    PHMSA_LNGstorage.loc[istation,'Lat'] = 47.327118\n",
    "                    PHMSA_LNGstorage.loc[istation,'Lon'] = -122.579219\n",
    "                elif set(splitname1) == set(['north', 'dakota', 'llc', 'tioga', 'plant']):\n",
    "                    PHMSA_LNGstorage.loc[istation,'Lat'] = 48.402600\n",
    "                    PHMSA_LNGstorage.loc[istation,'Lon'] = -102.918507\n",
    "                elif set(splitname1) == set(['hr', 'nu', 'blu', 'energy,', 'llc', 'hr', 'nu', 'blu', 'energy']):\n",
    "                    PHMSA_LNGstorage.loc[istation,'Lat'] = 30.496414\n",
    "                    PHMSA_LNGstorage.loc[istation,'Lon'] = -91.223587\n",
    "                elif set(splitname1) == set(['indy,', 'llc', 'lng', 'south'])  :\n",
    "                    PHMSA_LNGstorage.loc[istation,'Lat'] = 39.717421\n",
    "                    PHMSA_LNGstorage.loc[istation,'Lon'] =  -86.068010\n",
    "                elif set(splitname1) == set(['indy,', 'llc', 'lng', 'north'])  :\n",
    "                    PHMSA_LNGstorage.loc[istation,'Lat'] = 39.915464\n",
    "                    PHMSA_LNGstorage.loc[istation,'Lon'] =  -86.239457\n",
    "                elif set(splitname1) == set(['centerpoint', 'energy', 'resources', 'corp.,', 'dba', 'centerpoint', 'energy', 'minnesota', 'gas', 'waterbath', 'portable'])  :\n",
    "                    PHMSA_LNGstorage.loc[istation,'Lat'] = 30.031979\n",
    "                    PHMSA_LNGstorage.loc[istation,'Lon'] =  -95.441862 \n",
    "                else:\n",
    "                    print('Review', istation, 'here2')\n",
    "                    print(PHMSA_LNGstorage.iloc[istation,:])\n",
    "                    print(splitname1)\n",
    "\n",
    "#display(PHMSA_LNGstorage)\n",
    "print('QA/QC: Number of PHMSA LNG storage stations Missing Lat/Lons (should be at least 2 AK plants per year):', len(PHMSA_LNGstorage[PHMSA_LNGstorage['Lon']==0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.4.6 Put LNG Storage Station (Storage Capacity) onto CONUS Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Put LNG data onto Grid\n",
    "# This will include any AK & HI data in the nongrid data\n",
    "\n",
    "#initialize the map arrays\n",
    "Map_LNGstations = np.zeros([len(Lat_01),len(Lon_01), num_years]) \n",
    "Map_LNGstations_nongrid = np.zeros([num_years])\n",
    "\n",
    "# Place the DOE data onto the map arrays\n",
    "for istation in np.arange(0,len(PHMSA_LNGstorage)):\n",
    "    term_lat = PHMSA_LNGstorage['Lat'][istation]\n",
    "    term_lon = (PHMSA_LNGstorage['Lon'][istation])\n",
    "    #print(iterminal, (term_lat), (term_lon))\n",
    "    \n",
    "    if term_lon > Lon_left and term_lon < Lon_right \\\n",
    "        and term_lat > Lat_low and term_lat < Lat_up:\n",
    "        ilat = int((term_lat - Lat_low)/Res01)\n",
    "        ilon = int((term_lon - Lon_left)/Res01)\n",
    "        iyear = np.where(year_range == PHMSA_LNGstorage['REPORT_YEAR'][istation])[0][0]\n",
    "        Map_LNGstations[ilat,ilon,iyear] += PHMSA_LNGstorage['TOTAL_CAPACITY_BBLS'][istation]\n",
    "    else:\n",
    "        #print(iterminal)\n",
    "        iyear = np.where(year_range == PHMSA_LNGstorage['REPORT_YEAR'][istation])[0][0]\n",
    "        Map_LNGstations_nongrid[iyear] += PHMSA_LNGstorage['TOTAL_CAPACITY_BBLS'][istation]\n",
    "\n",
    "print('QA/QC: Total Storage Capacity (bbl)')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    print('YEAR ',year_range_str[iyear])\n",
    "    print('on grid: ',np.sum(Map_LNGstations[:,:,iyear]))\n",
    "    print('off grid: ',np.sum(Map_LNGstations_nongrid[iyear]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.5 Read In Transmission Pipeline Miles & Transmission Miles Over Cropland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1) Read In Enverus Gathering Pipeline Data (pre-processed in ArcMap), only contains CONUS info\n",
    "# 2) Read In Cropland data (and re-grid to 0.1x0.1 resolution)\n",
    "# 3) Make a proxy map with the length of pipeline in each grid cell. For grid cells where the cropland area\n",
    "#    in that grid cell is > 0, count those miles towards the sum of pipeline miles intersecting crop land. \n",
    "# 4) Calculate the ratio of GB infrastructure in AK & HI compared to the national onshore total\n",
    "#    (assume all M&R Farm tap emissions occur in the CONUS region - alternatively, could also apply the same trans.\n",
    "#    pipeline AKHI fraction to this emissions group in Step 4)\n",
    "\n",
    "if ReCalc_Cropland ==1:\n",
    "    #Step 1)\n",
    "    Env_TransPipelines_loc = pd.read_excel(Enverus_NG_Transpipeline_inputfile, usecols= \"C:G\", header = 0)\n",
    "    Map_EnvTrans_pipelines = np.zeros([len(Lat_01),len(Lon_01),num_years]) #data represent a snapshot in time that is applied to entire timeseries\n",
    "    Map_EnvTrans_pipelines_nongrid = np.zeros([num_years])\n",
    "    Map_Farm_pipelines = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "    Map_Farm_pipelines_nongrid = np.zeros([num_years]) \n",
    "    Cropland = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "    Cropland_nongrid = np.zeros([num_years])\n",
    "    Map_Farm_pipelines = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "    Map_Farm_pipelines_nongrid = np.zeros([num_years])\n",
    "    #display(Env_GathPipelines_loc)\n",
    "\n",
    "    #allocation is based on the relative pipeline length in each grid cell (pre-processed in ArcGIS)\n",
    "    # Note that the sum mileage in each grid cell != original dataset mileage due to changes when data was projected\n",
    "\n",
    "    # Step 2) \n",
    "    for iyear in np.arange(0,num_years):\n",
    "        Cropland_001 = pd.read_csv(Cropland_001_inputfile+year_range_str[iyear]+'_001x001.csv')\n",
    "        #re-grid from 0.01 to 0.1 resolution\n",
    "        for idx in np.arange(0,len(Cropland_001)):\n",
    "            if Cropland_001['FIRST_Longitude'][idx] > Lon_left and Cropland_001['FIRST_Longitude'][idx] < Lon_right and \\\n",
    "                Cropland_001['FIRST_Latitude'][idx] > Lat_low and Cropland_001['FIRST_Latitude'][idx] < Lat_up:\n",
    "                #Set ilon and ilat\n",
    "                ilat = int((Cropland_001['FIRST_Latitude'][idx]  - Lat_low)/Res01)\n",
    "                ilon = int((Cropland_001['FIRST_Longitude'][idx] - Lon_left)/Res01)\n",
    "                Cropland[ilat,ilon,iyear] += Cropland_001['SUM_Area_AllCrops'][idx]\n",
    "            else:\n",
    "                Cropland_nongrid[iyear] += Cropland_001['SUM_Area_AllCrops'][idx]\n",
    "        del Cropland_001\n",
    "\n",
    "        # Step 3)\n",
    "        for iloc in np.arange(0,len(Env_TransPipelines_loc)):\n",
    "            if Env_TransPipelines_loc['Longitude'][iloc] > Lon_left and Env_TransPipelines_loc['Longitude'][iloc] < Lon_right \\\n",
    "                and Env_TransPipelines_loc['Latitude'][iloc] > Lat_low and Env_TransPipelines_loc['Latitude'][iloc] < Lat_up:\n",
    "                ilat = int((Env_TransPipelines_loc['Latitude'][iloc] - Lat_low)/Res01)\n",
    "                ilon = int((Env_TransPipelines_loc['Longitude'][iloc] - Lon_left)/Res01)\n",
    "                Map_EnvTrans_pipelines[ilat,ilon,iyear] += Env_TransPipelines_loc['SUM_Shape_'][iloc]\n",
    "                if Cropland[ilat,ilon,iyear] > 0:\n",
    "                    Map_Farm_pipelines[ilat,ilon,iyear] += Env_TransPipelines_loc['SUM_Shape_'][iloc]\n",
    "                #else:\n",
    "                #    Map_Farm_pipelines_nongrid[iyear] += Env_TransPipelines_loc['SUM_Shape_'][iloc]\n",
    "            else:\n",
    "                Map_EnvTrans_pipelines_nongrid[iyear] += Env_TransPipelines_loc['SUM_Shape_'][iloc]\n",
    "                #Map_Farm_pipelines_nongrid[iyear] += Env_TransPipelines_loc['SUM_Shape_'][iloc]\n",
    "            \n",
    "        print('Year: ',year_range[iyear])    \n",
    "        print('Total Transmission Pipeline length on grid: ',np.sum(Map_EnvTrans_pipelines[:,:,iyear]))\n",
    "        print('Total Transmission Pipeline length off grid: ',np.sum(Map_EnvTrans_pipelines_nongrid[iyear]))\n",
    "        print('Total Transmission Pipeline length on cropland: ',np.sum(Map_Farm_pipelines[:,:,iyear]))\n",
    "        print('Total Transmission Pipeline length not on cropland: ',np.sum(Map_Farm_pipelines_nongrid[iyear]))\n",
    "\n",
    "    \n",
    "    #Step 4)\n",
    "    #1. Open Transmission_pipelines_wgs.shp\n",
    "    #2. sum the miles field\n",
    "    #3. Open Transmission_pipelines_AKHI_wgs84.shp\n",
    "    #4. sum the miles field\n",
    "    #5. Ratio the AKHI miles / (conus + AKHI miles)\n",
    "    #6. Apply this ratio and subtract from the GB pipeline fields (save this fraction as 'not_mapped')\n",
    "    # Assume all M&R tap emissions occur in the CONUS region. \n",
    "\n",
    "    shape = shp.Reader(AKHI_pipelines_shp)\n",
    "    AKHI_miles = 0\n",
    "    for rec in shape.iterRecords():\n",
    "        AKHI_miles += rec['MILES']\n",
    "    print('Miles in AK & HI: ', AKHI_miles)\n",
    "    \n",
    "    shape = shp.Reader(CONUS_pipelines_shp)\n",
    "    CONUS_miles = 0\n",
    "    for rec in shape.iterRecords():\n",
    "        CONUS_miles += rec[\"MILES\"]\n",
    "    print('Miles in CONUS: ', CONUS_miles)\n",
    "\n",
    "    #apply this fraction and subtract from the national pipeline emissions in step 4 (the non-grid data is zero)\n",
    "    CONUS_transpipe_ratio = AKHI_miles/CONUS_miles\n",
    "    print('Fraction of Miles Outside CONUS: ', CONUS_transpipe_ratio)\n",
    "\n",
    "    np.savez('./IntermediateOutputs/Pipelines_Transmission', x=Map_EnvTrans_pipelines, y=Map_EnvTrans_pipelines_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Pipelines_Farms', x=Map_Farm_pipelines, y=Map_Farm_pipelines_nongrid)\n",
    "    np.save('./IntermediateOutputs/CONUS_TransPipeline_Ratio', CONUS_transpipe_ratio)\n",
    "else:\n",
    "    npzfile = np.load('./IntermediateOutputs/Pipelines_Transmission.npz')\n",
    "    Map_EnvTrans_pipelines = npzfile['x']\n",
    "    Map_EnvTrans_pipelines_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Pipelines_Farms.npz')\n",
    "    Map_Farm_pipelines = npzfile['x']\n",
    "    Map_Farm_pipelines_nongrid = npzfile['y']\n",
    "    CONUS_transpipe_ratio = np.load('./IntermediateOutputs/CONUS_TransPipeline_Ratio.npy')\n",
    "    \n",
    "    for iyear in np.arange(0, num_years):\n",
    "        print('Year: ',year_range[iyear])    \n",
    "        print('Total Transmission Pipeline length on grid: ',np.sum(Map_EnvTrans_pipelines[:,:,iyear]))\n",
    "        print('Total Transmission Pipeline length off grid: ',np.sum(Map_EnvTrans_pipelines_nongrid[iyear]))\n",
    "        print('Total Transmission Pipeline length on cropland: ',np.sum(Map_Farm_pipelines[:,:,iyear]))\n",
    "        print('Total Transmission Pipeline length not on cropland: ',np.sum(Map_Farm_pipelines_nongrid[iyear]))\n",
    "    print('Fraction of Miles Outside CONUS: ', CONUS_transpipe_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.6 Transmission Compressor Stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.6.1 Read in Enverus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in Enverus Midstream Transmission Compressor station data (onshore only)\n",
    "# Need to read in AK/HI data and CONUS onshore data\n",
    "\n",
    "#Step 1)\n",
    "# Read in CONUS onshore counts\n",
    "names = pd.read_excel(Enverus_NG_TransStations_inputfile, skiprows = 0, header = 0)\n",
    "colnames = names.columns.values\n",
    "Enverus_Trans_CompStations = pd.read_excel(Enverus_NG_TransStations_inputfile, skiprows = 0, names = colnames)\n",
    "Enverus_Trans_CompStations = Enverus_Trans_CompStations[['NAME','OPERATOR','TYPE','FUEL_MCFD','HP','STATE_NAME','CNTY_NAME','Longitude','Latitude']]\n",
    "#Enverus_Trans_CompStations = Enverus_Trans_CompStations[~Enverus_Trans_CompStations['STATE_NAME'].isin(['Alaska','Hawaii'])]\n",
    "Enverus_Trans_CompStations.reset_index(drop=True,inplace=True)\n",
    "#print(colnames)\n",
    "\n",
    "#Step 2)\n",
    "# Calculate average Fuel Useage to HP ratio and then fill in where possible\n",
    "#Estimate Fuel Useage based on HorsePower, we'll then use Fuel Useage to allocate emissions below\n",
    "Fuel_HP_ratio = np.mean(Enverus_Trans_CompStations['FUEL_MCFD']\\\n",
    "                        [(Enverus_Trans_CompStations['FUEL_MCFD']>0) & (Enverus_Trans_CompStations['HP']>0)] \\\n",
    "                        / Enverus_Trans_CompStations['HP'][(Enverus_Trans_CompStations['FUEL_MCFD']>0) & (Enverus_Trans_CompStations['HP']>0)])\n",
    "for index in np.arange(len(Enverus_Trans_CompStations)):\n",
    "    if Enverus_Trans_CompStations['FUEL_MCFD'][index] == 0:\n",
    "        Enverus_Trans_CompStations.loc[index,'FUEL_MCFD'] = Fuel_HP_ratio * Enverus_Trans_CompStations['HP'][index]\n",
    "#if Fuel_MCFD is still zero, fill with median value\n",
    "median_mcfd = np.median(Enverus_Trans_CompStations.loc[Enverus_Trans_CompStations['FUEL_MCFD'] > 0,'FUEL_MCFD'])\n",
    "#print(median_mcfd)\n",
    "for index in np.arange(len(Enverus_Trans_CompStations)):\n",
    "    if Enverus_Trans_CompStations['FUEL_MCFD'][index] == 0:\n",
    "        Enverus_Trans_CompStations.loc[index,'FUEL_MCFD'] = median_mcfd\n",
    "display(Enverus_Trans_CompStations)\n",
    "\n",
    "#Step 3)\n",
    "#1. Sum CONUS onshore transmission compressor stations\n",
    "#2. Sum AK/HI transmission compressor stations\n",
    "#4. Ratio the AKHI miles / (conus + AKHI miles)\n",
    "#6. Apply this ratio and subtract from the Transmission Compressor Stations emi group (save this fraction as 'not_mapped')\n",
    "\n",
    "shape = shp.Reader(AKHI_transstat_shp)\n",
    "AKHI_counts = 0\n",
    "#for rec in shape.iterRecords():\n",
    "AKHI_counts = len(shape)\n",
    "print('Stations in AK & HI: ', AKHI_counts)\n",
    "\n",
    "shape = shp.Reader(CONUS_transstat_shp)\n",
    "CONUS_counts = 0\n",
    "#for rec in shape.iterRecords():\n",
    "CONUS_counts = len(shape)\n",
    "print('Stations in CONUS: ', CONUS_counts)\n",
    "\n",
    "#apply this fraction and subtract from the national pipeline emissions in step 4 (the non-grid data is zero)\n",
    "#CONUS_transstat_ratio = AKHI_counts/(CONUS_counts + AKHI_counts)\n",
    "#print('Fraction of Stations Outside CONUS: ', CONUS_transstat_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.6.2 Read In GHGRP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a) Read in the GHGRP data\n",
    "# emissions of methane reported in metric ton\n",
    "facility_info = pd.read_csv(GHGRP_facility_inputfile)\n",
    "facility_emissions = pd.read_excel(GHGRP_subpartw_inputfile,sheet_name = 'Export Worksheet')\n",
    "facility_emissions = facility_emissions[facility_emissions['INDUSTRY_SEGMENT'] =='Onshore natural gas transmission compression [98.230(a)(4)]']\n",
    "facility_emissions = facility_emissions[facility_emissions['TOTAL_REPORTED_CH4_EMISSIONS'] >0]\n",
    "facility_emissions = facility_emissions[facility_emissions['REPORTING_YEAR'] <= year_range[-1]]\n",
    "facility_emissions.reset_index(drop=True,inplace=True)\n",
    "#print(facility_emissions)\n",
    "\n",
    "facility_emissions['State'] = ''\n",
    "facility_emissions['County'] = ''\n",
    "facility_emissions['City'] = ''\n",
    "facility_emissions['Zip'] = 0\n",
    "facility_emissions['Lat'] = 0\n",
    "facility_emissions['Lon'] = 0\n",
    "\n",
    "#b) match GHGRP facility and emissions data\n",
    "# for each entry in the data file (each facility each year), match the facility ID to the ID in the\n",
    "# GHGRP facility info file, then append the corresponding location data to the emissions array\n",
    "for index in np.arange(len(facility_emissions)):\n",
    "    #print(index)\n",
    "    ilocation = np.where(facility_info['V_GHG_EMITTER_FACILITIES.FACILITY_ID'] == facility_emissions['FACILITY_ID'][index])[0][0]\n",
    "    #for iloc in len(ilocation)\n",
    "    facility_emissions.loc[index, 'State'] = facility_info['V_GHG_EMITTER_FACILITIES.STATE'][ilocation]\n",
    "    facility_emissions.loc[index, 'County'] = facility_info['V_GHG_EMITTER_FACILITIES.COUNTY'][ilocation]\n",
    "    facility_emissions.loc[index, 'City'] = facility_info['V_GHG_EMITTER_FACILITIES.CITY'][ilocation]\n",
    "    facility_emissions.loc[index, 'Zip'] = facility_info['V_GHG_EMITTER_FACILITIES.ZIP'][ilocation]\n",
    "    facility_emissions.loc[index, 'Lat'] = facility_info['V_GHG_EMITTER_FACILITIES.LATITUDE'][ilocation]\n",
    "    facility_emissions.loc[index, 'Lon'] = facility_info['V_GHG_EMITTER_FACILITIES.LONGITUDE'][ilocation]\n",
    "\n",
    "\n",
    "    # make station-specific arrays for each year (with emissions in Tg)\n",
    "print('QA/QC: Check that all GHGRP emissions are allocated to specific stations')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    facility_emissions_temp = facility_emissions[facility_emissions['REPORTING_YEAR'] ==year_range[iyear]]\n",
    "    facility_emissions_temp.reset_index(drop=True,inplace=True)\n",
    "    GHGRP_transstations = pd.DataFrame({'FID':facility_emissions_temp['FACILITY_ID'].unique()})\n",
    "    GHGRP_transstations['Name'] = ' '\n",
    "    GHGRP_transstations['State'] = ' '\n",
    "    GHGRP_transstations['County'] = ' '\n",
    "    GHGRP_transstations['City'] = ' '\n",
    "    GHGRP_transstations['Zip'] = 0\n",
    "    GHGRP_transstations['Lat'] = 0.0\n",
    "    GHGRP_transstations['Lon'] = 0.0\n",
    "    GHGRP_transstations['TgCH4'] = 0.0\n",
    "\n",
    "    #Put everything in per-station array\n",
    "    for idx in np.arange(len(facility_emissions_temp)):\n",
    "        iFID = np.where(GHGRP_transstations['FID'] == facility_emissions_temp['FACILITY_ID'][idx])[0][0]\n",
    "        GHGRP_transstations.loc[iFID,'Name']   = facility_emissions_temp['FACILITY_NAME'][idx]\n",
    "        GHGRP_transstations.loc[iFID,'State']  = facility_emissions_temp['State'][idx]\n",
    "        GHGRP_transstations.loc[iFID,'County'] = facility_emissions_temp['County'][idx]\n",
    "        GHGRP_transstations.loc[iFID,'City'] = facility_emissions_temp['City'][idx]\n",
    "        GHGRP_transstations.loc[iFID,'Zip']    = facility_emissions_temp['Zip'][idx]\n",
    "        GHGRP_transstations.loc[iFID,'Lat']    = facility_emissions_temp['Lat'][idx]\n",
    "        GHGRP_transstations.loc[iFID,'Lon']    = facility_emissions_temp['Lon'][idx]\n",
    "        GHGRP_transstations.loc[iFID,'TgCH4'] += facility_emissions_temp['TOTAL_REPORTED_CH4_EMISSIONS'][idx]/1e6\n",
    "    \n",
    "    vars()['GHGRP_transstations'+'_'+year_range_str[iyear]] = GHGRP_transstations\n",
    "    diff1 = abs(facility_emissions_temp['TOTAL_REPORTED_CH4_EMISSIONS'].sum()/1e6 -GHGRP_transstations['TgCH4'].sum())/ \\\n",
    "        ((facility_emissions_temp['TOTAL_REPORTED_CH4_EMISSIONS'].sum()/1e6 + GHGRP_transstations['TgCH4'].sum())/2)\n",
    "    #print(summary_emi)\n",
    "    #print(sum_emi2[iyear])\n",
    "    if diff1 < 0.0001:\n",
    "        print('Year ', year_range[iyear],': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear],': FAIL: ', diff1,'%') \n",
    "    print('Number of GHGRP Transmission Stations: ', len(vars()['GHGRP_transstations'+'_'+year_range_str[iyear]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.6.3. Match Transmission Compressor Stations between Enverus and GHGRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each year of GHGRP data, match GHGRP transmission compressor stations to Enverus data \n",
    "# (based on nearest location, not name)\n",
    "# note there is only one available year of Enverus data\n",
    "# also record the station daily fuel usage for later calculations\n",
    "\n",
    "found = 0\n",
    "DEBUG=0\n",
    "\n",
    "print('QA/QC: Number of GHGRP Trans. Compressor Stations not in Enverus dataset')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    GHGRP_temp_data = vars()['GHGRP_transstations'+'_'+year_range_str[iyear]].copy()\n",
    "\n",
    "    GHGRP_temp_data['match_flag'] = 0\n",
    "    GHGRP_temp_data['Env_name'] = ''\n",
    "    GHGRP_temp_data['Env_county'] = ''\n",
    "    GHGRP_temp_data['Env_state'] = ''\n",
    "    GHGRP_temp_data['Env_FuelUseage'] = 0\n",
    "    Enverus_Trans_CompStations['match_flag'] = 0\n",
    "\n",
    "    #First, find exact matching lat/lon facilities\n",
    "    for istation in np.arange(0,len(GHGRP_temp_data)):\n",
    "        matched = np.where((np.abs(Enverus_Trans_CompStations['Latitude']-GHGRP_temp_data['Lat'][istation]) < 0.2) & \\\n",
    "                              (np.abs(Enverus_Trans_CompStations['Longitude']-GHGRP_temp_data['Lon'][istation]) < 0.2))[0]\n",
    "        #print(np.size(matched))\n",
    "        if np.size(matched)==1: #if exactly one station within 0.1 degrees\n",
    "            #print('HERE')\n",
    "            Enverus_Trans_CompStations.loc[matched[0],'match_flag'] = 1\n",
    "            GHGRP_temp_data.loc[istation,'match_flag'] = 1\n",
    "            GHGRP_temp_data.loc[istation,'Env_name'] = Enverus_Trans_CompStations.loc[matched[0], 'NAME']\n",
    "            GHGRP_temp_data.loc[istation,'Env_county'] = Enverus_Trans_CompStations.loc[matched[0], 'CNTY_NAME']\n",
    "            GHGRP_temp_data.loc[istation,'Env_state'] = Enverus_Trans_CompStations.loc[matched[0], 'STATE_NAME']\n",
    "            GHGRP_temp_data.loc[istation,'Env_FuelUseage'] = Enverus_Trans_CompStations.loc[matched[0], 'FUEL_MCFD']\n",
    "\n",
    "        elif np.size(matched) > 1: #if more than one station within <0.1 degrees, find nearest match\n",
    "            dist_calc = np.zeros(len(matched))\n",
    "            GHGRP_temp_data.loc[istation,'match_flag'] = 1\n",
    "            for imatch in np.arange(len(dist_calc)): #loop through the matching stations to find the closest match\n",
    "                dist_calc[imatch] = np.abs(GHGRP_temp_data.loc[istation,'Lat'] - \\\n",
    "                                           Enverus_Trans_CompStations.loc[matched[imatch],'Latitude'])**2 + \\\n",
    "                               np.abs(GHGRP_temp_data.loc[istation,'Lon'] - Enverus_Trans_CompStations.loc[matched[imatch],'Longitude'])**2\n",
    "            bestpick = np.where(dist_calc == dist_calc.min())[0][0]\n",
    "\n",
    "            if len(np.where(dist_calc == dist_calc.min())[0]) == 1: #if there is only one closest match, assign the correct data\n",
    "                Enverus_Trans_CompStations.loc[matched[bestpick],'match_flag'] = 1\n",
    "                GHGRP_temp_data.loc[istation,'match_flag'] = 1\n",
    "                GHGRP_temp_data.loc[istation,'Env_name'] = Enverus_Trans_CompStations.loc[matched[bestpick], 'NAME']\n",
    "                GHGRP_temp_data.loc[istation,'Env_county'] = Enverus_Trans_CompStations.loc[matched[bestpick], 'CNTY_NAME']\n",
    "                GHGRP_temp_data.loc[istation,'Env_state'] = Enverus_Trans_CompStations.loc[matched[bestpick], 'STATE_NAME']\n",
    "                GHGRP_temp_data.loc[istation,'Env_FuelUseage'] = Enverus_Trans_CompStations.loc[matched[bestpick], 'FUEL_MCFD']\n",
    "            else: #if there is more than one match, sum the fuel usage from all matching stations and assign average to GHGRP array\n",
    "                best_array = np.where(dist_calc == dist_calc.min())[0]\n",
    "                total_use = 0.0\n",
    "                nonzero_use = 0\n",
    "                for ibest in np.arange(len(best_array)):\n",
    "                    if Enverus_Trans_CompStations.loc[matched[best_array[ibest]], 'FUEL_MCFD'] > 0:\n",
    "                        total_use += Enverus_Trans_CompStations.loc[matched[best_array[ibest]], 'FUEL_MCFD']\n",
    "                        nonzero_use += 1\n",
    "                    Enverus_Trans_CompStations.loc[matched[best_array[ibest]],'match_flag'] = 1\n",
    "                GHGRP_temp_data.loc[istation,'match_flag'] = 1\n",
    "                #GHGRP_temp_data.loc[istation,'Env_name'] = Enverus_Trans_CompStations.loc[matched[bestpick], 'NAME']\n",
    "                GHGRP_temp_data.loc[istation,'Env_county'] = Enverus_Trans_CompStations.loc[best_array[0], 'CNTY_NAME']\n",
    "                GHGRP_temp_data.loc[istation,'Env_state'] = Enverus_Trans_CompStations.loc[best_array[0], 'STATE_NAME']\n",
    "                GHGRP_temp_data.loc[istation,'Env_FuelUseage'] = data_fn.safe_div(total_use,nonzero_use)\n",
    "        \n",
    "        else: #match stations by hand\n",
    "            found = 1\n",
    "            if GHGRP_temp_data.loc[istation,'FID'] == 1008158:\n",
    "                matched = np.where((Enverus_Trans_CompStations['OPERATOR'] == 'Iroquois Gas Transmission System, LP') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Connecticut'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1004777:\n",
    "                matched = np.where((Enverus_Trans_CompStations['NAME'] == 'Shevlin - 3') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Minnesota'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1007536:\n",
    "                matched = np.where((Enverus_Trans_CompStations['NAME'] == 'Tionesta') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'California'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1007450:\n",
    "                matched = np.where((Enverus_Trans_CompStations['NAME'] == 'Kemmerer') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Wyoming'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1003191:\n",
    "                matched = np.where((Enverus_Trans_CompStations['NAME'] == 'CS - 10') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Mississippi'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1006232:\n",
    "                matched = np.where((Enverus_Trans_CompStations['NAME'] == 'Elberta') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Utah'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1006523:\n",
    "                matched = np.where((Enverus_Trans_CompStations['CNTY_NAME'] == 'Ellis') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Texas'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1004932:\n",
    "                matched = np.where((Enverus_Trans_CompStations['CNTY_NAME'] == 'Hansford') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Texas'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1010481:\n",
    "                matched = np.where((Enverus_Trans_CompStations['CNTY_NAME'] == 'Elko') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Nevada'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1002763:\n",
    "                matched = np.where((Enverus_Trans_CompStations['NAME'] == 'Dry Lake') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Nevada'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1002762:\n",
    "                matched = np.where((Enverus_Trans_CompStations['NAME'] == 'Goodsprings') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Nevada'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1005806:\n",
    "                matched = np.where((Enverus_Trans_CompStations['NAME'] == 'Anshutz') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Wyoming'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1008941:\n",
    "                matched = np.where((Enverus_Trans_CompStations['NAME'] == 'Green River B') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Wyoming'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1002761:\n",
    "                matched = np.where((Enverus_Trans_CompStations['NAME'] == 'Muddy Creek') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Wyoming'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1009608:\n",
    "                matched = np.where((Enverus_Trans_CompStations['NAME'] == 'CS - 159') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Oklahoma'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1008080:\n",
    "                matched = np.where((Enverus_Trans_CompStations['NAME'] == 'CS - 194') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Kansas'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1006497:\n",
    "                matched = np.where((Enverus_Trans_CompStations['NAME'] == 'Plymouth') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Washington'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1003308:\n",
    "                matched = np.where((Enverus_Trans_CompStations['NAME'] == 'Searcy') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Arkansas'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1012158:\n",
    "                matched = np.where((Enverus_Trans_CompStations['NAME'] == 'CS - 310') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Pennsylvania'))[0]\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1011897:\n",
    "                matched = np.where((Enverus_Trans_CompStations['NAME'] == 'Webb County') & \\\n",
    "                                   (Enverus_Trans_CompStations['STATE_NAME'] == 'Texas'))[0]\n",
    "            else:\n",
    "                found = 0\n",
    "                if DEBUG ==1:\n",
    "                    print(istation, matched)\n",
    "                    display(GHGRP_temp_data.loc[istation,:])\n",
    "            if found==1:\n",
    "                Enverus_Trans_CompStations.loc[matched[0],'match_flag'] = 1\n",
    "                GHGRP_temp_data.loc[istation,'match_flag'] = 1\n",
    "                GHGRP_temp_data.loc[istation,'Env_name'] = Enverus_Trans_CompStations.loc[matched[0], 'NAME']\n",
    "                GHGRP_temp_data.loc[istation,'Env_county'] = Enverus_Trans_CompStations.loc[matched[0], 'CNTY_NAME']\n",
    "                GHGRP_temp_data.loc[istation,'Env_state'] = Enverus_Trans_CompStations.loc[matched[0], 'STATE_NAME']\n",
    "                GHGRP_temp_data.loc[istation,'Env_FuelUseage'] = Enverus_Trans_CompStations.loc[matched[0], 'FUEL_MCFD']\n",
    "\n",
    "    GHGRP_notmatched = GHGRP_temp_data[GHGRP_temp_data['match_flag'] == 0]\n",
    "    vars()['GHGRP_transstations'+'_'+year_range_str[iyear]] = GHGRP_temp_data.copy()   \n",
    "    Env_notmatched = Enverus_Trans_CompStations[Enverus_Trans_CompStations['match_flag'] == 0]\n",
    "    Env_notmatched.reset_index(inplace=True, drop=True)\n",
    "    vars()['Env_Trans_CompStations_notmatched'+'_'+year_range_str[iyear]] = Env_notmatched.copy()                          \n",
    "    \n",
    "    print('Year ', year_range_str[iyear],': ', len(GHGRP_notmatched), ' of ', len(GHGRP_temp_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.6.4. Calculate the average Emission to fuel Useage Ratio for matched plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_emis_fueluse_ratio = np.zeros([num_years])\n",
    "GHGRP_station_emi_median = np.zeros([num_years]) #in Tg\n",
    "\n",
    "print('QA/QC: Average Emissions to Fuel Usage Ratio')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    GHGRP_temp_data = vars()['GHGRP_transstations'+'_'+year_range_str[iyear]].copy()\n",
    "\n",
    "    GHGRP_temp_data['Emis_fuel_ratio']=0\n",
    "    for iplant in np.arange(0,len(GHGRP_temp_data)):\n",
    "        GHGRP_temp_data.loc[iplant, 'Emis_fuel_ratio'] = data_fn.safe_div(GHGRP_temp_data.loc[iplant, 'TgCH4'], \\\n",
    "                                                                        GHGRP_temp_data.loc[iplant, 'Env_FuelUseage'])\n",
    "    GHGRP_temp_data['Emis_fuel_ratio'] = GHGRP_temp_data['Emis_fuel_ratio'].replace({0:np.nan})\n",
    "    avg_emis_fueluse_ratio[iyear] = np.mean(GHGRP_temp_data['Emis_fuel_ratio'])\n",
    "    #GHGRP_station_emi_median[iyear] = np.median(GHGRP_temp_data.loc[:, 'TgCH4'])\n",
    "    \n",
    "    vars()['GHGRP_transstations'+'_'+year_range_str[iyear]] = GHGRP_temp_data.copy()\n",
    "    print('Year ', year_range_str[iyear],': ', 'avg ratio', avg_emis_fueluse_ratio[iyear])# ',','median emi (Tg)', GHGRP_station_emi_median[iyear])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.6.5. Map Emissions to CONUS grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Map Emissions to Grid (for GHGRP matched stations), Calculate emissions from non-matched stations, \n",
    "# apply median plant level emissions to those Enverus stations with zero-calculated emissions\n",
    "# AK/HI Note: Note that AK/HI compressor transmission stations are not in the Enverus datasets. \n",
    "# Therefore, AK/HI emissions will be split from the CONUS region using the relative station counts on and off-grid. \n",
    "# The Enverus AK/HI station counts are processed from shapefiles above in 2.6.1. GHGRP counts of off-grid stations\n",
    "# are then added to this total each year and the ratio of AK/HI vs CONUS stations is used to remove AK/HI fraction\n",
    "# of national emissions in Step 4 below. \n",
    "\n",
    "map_TransCompStations = np.zeros([len(Lat_01),len(Lon_01),num_years]) #data represent a snapshot in time that is applied to entire timeseries\n",
    "map_TransCompStations_nongrid = np.zeros([num_years])\n",
    "CONUS_transstat_ratio = np.zeros([num_years])\n",
    "\n",
    "print('QA/QC: Transmission Station Emissions Gridded:')\n",
    "for iyear in np.arange(0, num_years):\n",
    "    stations_ongrid = 0\n",
    "    stations_nongrid = 0\n",
    "    #first add GHGRP emissions (for all plants in GHGRP) \n",
    "    GHGRP_temp_data = vars()['GHGRP_transstations'+'_'+year_range_str[iyear]].copy()\n",
    "    for istation in np.arange(0,len(GHGRP_temp_data)):\n",
    "        #if GHGRP_temp_data.loc[istation,'match_flag']==1:\n",
    "        if GHGRP_temp_data['Lon'][istation] > Lon_left and GHGRP_temp_data['Lon'][istation] < Lon_right \\\n",
    "            and GHGRP_temp_data['Lat'][istation] > Lat_low and GHGRP_temp_data['Lat'][istation] < Lat_up:\n",
    "            ilat = int((GHGRP_temp_data['Lat'][istation] - Lat_low)/Res01)\n",
    "            ilon = int((GHGRP_temp_data['Lon'][istation] - Lon_left)/Res01)\n",
    "            #if Env_ProcPlant_loc['Throughput'][istation] >0:\n",
    "            map_TransCompStations[ilat,ilon,iyear] += GHGRP_temp_data.loc[istation, 'TgCH4']\n",
    "            stations_ongrid +=1\n",
    "        else:\n",
    "            map_TransCompStations_nongrid[iyear] += GHGRP_temp_data.loc[istation, 'TgCH4']  \n",
    "            stations_nongrid +=1\n",
    "\n",
    "    #then add calculated enverus emissions for all non-matched plants\n",
    "    Env_temp_data = vars()['Env_Trans_CompStations_notmatched'+'_'+year_range_str[iyear]].copy()\n",
    "    #display(Env_temp_data)\n",
    "    for istation in np.arange(0, len(Env_temp_data)):\n",
    "        #print(Env_temp_data['Longitude'][0])\n",
    "        if Env_temp_data['Longitude'][istation] > Lon_left and Env_temp_data['Longitude'][istation] < Lon_right \\\n",
    "            and Env_temp_data['Latitude'][istation] > Lat_low and Env_temp_data['Latitude'][istation] < Lat_up:\n",
    "            ilat = int((Env_temp_data['Latitude'][istation] - Lat_low)/Res01)\n",
    "            ilon = int((Env_temp_data['Longitude'][istation] - Lon_left)/Res01)\n",
    "            if Env_temp_data.loc[istation, 'FUEL_MCFD']  > 0 :\n",
    "                map_TransCompStations[ilat,ilon,iyear] += Env_temp_data.loc[istation,'FUEL_MCFD']*avg_emis_fueluse_ratio[iyear]\n",
    "            stations_ongrid +=1\n",
    "        else:\n",
    "            if Env_temp_data.loc[istation, 'FUEL_MCFD']  > 0 :\n",
    "                map_TransCompStations_nongrid[iyear] += Env_temp_data.loc[istation, 'FUEL_MCFD']*avg_emis_fueluse_ratio[iyear]\n",
    "            stations_nongrid +=1\n",
    "            \n",
    "    vars()['GHGRP_plants'+'_'+year_range_str[iyear]] = GHGRP_temp_data.copy()\n",
    "    vars()['Env_Trans_CompStations_notmatched'+'_'+year_range_str[iyear]] = Env_temp_data.copy()\n",
    "    \n",
    "    # Deal with AK/HI emissions (allocate based on off-grid station counts)\n",
    "    stations_nongrid += AKHI_counts #Add Enverus counts of AK/HI stations to GHGRP counts (hold constant each year)\n",
    "    #apply this fraction and subtract from the national pipeline emissions in step 4 (the non-grid data is zero)\n",
    "    CONUS_transstat_ratio[iyear] = stations_nongrid/(stations_ongrid + stations_nongrid)\n",
    "    #print('Fraction of Stations Outside CONUS: ', CONUS_transstat_ratio)\n",
    "    \n",
    "    print('Year: ', year_range_str[iyear])\n",
    "    print('On grid (Tg):',np.sum(map_TransCompStations[:,:, iyear]), ', stations:',stations_ongrid)\n",
    "    print('Off grid (Tg):',np.sum(map_TransCompStations_nongrid[iyear]),', stations:', stations_nongrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.7  Storage Compressor Stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.7.1 Read In EIA Storage Field Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#EIA Storage Field Capacities\n",
    "\n",
    "names = pd.read_excel(EIA_StorFields_inputfile, skiprows = 0, header = 0)\n",
    "colnames = names.columns.values\n",
    "EIA_StorFields = pd.read_excel(EIA_StorFields_inputfile, skiprows = 0, names = colnames)\n",
    "EIA_StorFields = EIA_StorFields[['Year','Report State ', 'Company Name','Field Name','Reservoir Name','County Name',\\\n",
    "                                 'Status','Total Field Capacity(Mcf)']]\n",
    "# filter for active storage fields only\n",
    "EIA_StorFields = EIA_StorFields[EIA_StorFields['Status']== 'Active']\n",
    "EIA_StorFields.reset_index(drop=True,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.7.2 Read In Enverus Gas Storage Field Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enverus_NG_StorFields_inputfile\n",
    "names = pd.read_excel(Enverus_NG_StorFields_inputfile, skiprows = 0, header = 0)\n",
    "colnames = names.columns.values\n",
    "Env_StorFields = pd.read_excel(Enverus_NG_StorFields_inputfile, skiprows = 0, names = colnames)\n",
    "#print(colnames)\n",
    "Env_StorFields = Env_StorFields[['STATUS','RESERVOIR', 'NAME','OPERATOR','STATE_NAME','CNTY_NAME','FLDCAPMMCF',\\\n",
    "                                'Latitude','Longitude']]\n",
    "Env_StorFields = Env_StorFields[Env_StorFields['STATUS']=='Operational']\n",
    "\n",
    "Env_StorFields.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.7.3 Read In Enverus Storage Compressor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read In Enverus Compressor Station Data (then use this to find the fields with compressor stations)\n",
    "names = pd.read_excel(Enverus_NG_StorStations_inputfile, skiprows = 0, header = 0)\n",
    "colnames = names.columns.values\n",
    "Enverus_Storage_CompStations = pd.read_excel(Enverus_NG_StorStations_inputfile, skiprows = 0, names = colnames)\n",
    "Enverus_Storage_CompStations = Enverus_Storage_CompStations[['NAME','OPERATOR','TYPE','STATE_NAME','CNTY_NAME','Longitude','Latitude']]\n",
    "Enverus_Storage_CompStations.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.7.4. Find Which Enverus Storage Fields have Storage Compressor Stations, save location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loop through each Enverus storage field to find where there is also a storage compressor station\n",
    "# First try matching on name, then operator to narrow down\n",
    "# If no match on name, then try matching based on location (within 0.01 degrees)\n",
    "# If no match on name, or location (within 0.01 degrees), try matching on 0.05 degrees and county name\n",
    "# If none of these criteria are met, there is no compressor station at that field. \n",
    "\n",
    "# Mark where there is a match and the lat/lon values of the compressor stations. \n",
    "\n",
    "Env_StorFields['Comp_flag'] = 0\n",
    "Env_StorFields['Comp_lat'] = 0\n",
    "Env_StorFields['Comp_lon'] = 0\n",
    "Env_StorFields['State'] = ''\n",
    "\n",
    "nomatch = 0\n",
    "for ifield in np.arange(0,len(Env_StorFields)):\n",
    "    matched_state = np.where((Env_StorFields['STATE_NAME'][ifield] == State_ANSI['name']))[0]\n",
    "    Env_StorFields.loc[ifield,'State'] = State_ANSI.loc[matched_state[0],'abbr']\n",
    "    matched = np.where((Env_StorFields['NAME'][ifield] == Enverus_Storage_CompStations['NAME']))[0]\n",
    "    if np.size(matched) >1:\n",
    "        best_match = np.where(Env_StorFields['OPERATOR'][ifield] == Enverus_Storage_CompStations.loc[matched,'OPERATOR'])[0]\n",
    "        if np.size(best_match) ==1:\n",
    "            Env_StorFields.loc[ifield,'Comp_flag'] = 1\n",
    "            Env_StorFields.loc[ifield,'Comp_lat'] = Enverus_Storage_CompStations.loc[matched[best_match[0]],'Latitude']\n",
    "            Env_StorFields.loc[ifield,'Comp_lon'] = Enverus_Storage_CompStations.loc[matched[best_match[0]],'Longitude']\n",
    "        elif np.size(best_match) >1:\n",
    "            # This is occuring when there is a double count of the compressor stations (entries are identical except for Enverus ID)\n",
    "            # In this case, assign one compressor station to the field\n",
    "            Env_StorFields.loc[ifield,'Comp_flag'] = 1 #could alternatively set this to the number of matches (if actually >1 station per field)\n",
    "            Env_StorFields.loc[ifield,'Comp_lat'] = Enverus_Storage_CompStations.loc[matched[best_match[0]],'Latitude']\n",
    "            Env_StorFields.loc[ifield,'Comp_lon'] = Enverus_Storage_CompStations.loc[matched[best_match[0]],'Longitude']\n",
    "        else:\n",
    "            #more than one case identified where name matches, but operator does not\n",
    "            nomatch +=1\n",
    "    elif np.size(matched) ==1:\n",
    "        Env_StorFields.loc[ifield,'Comp_flag'] = 1\n",
    "        Env_StorFields.loc[ifield,'Comp_lat'] = Enverus_Storage_CompStations.loc[matched[0],'Latitude']\n",
    "        Env_StorFields.loc[ifield,'Comp_lon'] = Enverus_Storage_CompStations.loc[matched[0],'Longitude']\n",
    "        \n",
    "    elif np.size(matched) <1:\n",
    "        #if they don't match based on name, then match based on location (likely due to slight spelling differences)\n",
    "        best_match = np.where((np.abs(Env_StorFields['Latitude'][ifield]-Enverus_Storage_CompStations['Latitude']) < 0.01) & \\\n",
    "                              (np.abs(Env_StorFields['Longitude'][ifield]-Enverus_Storage_CompStations['Longitude']) < 0.01))[0]\n",
    "        if np.size(best_match) ==1:\n",
    "            Env_StorFields.loc[ifield,'Comp_flag'] = 1\n",
    "            Env_StorFields.loc[ifield,'Comp_lat'] = Enverus_Storage_CompStations.loc[best_match[0],'Latitude']\n",
    "            Env_StorFields.loc[ifield,'Comp_lon'] = Enverus_Storage_CompStations.loc[best_match[0],'Longitude']\n",
    "        elif np.size(best_match) >1:\n",
    "            # This is occuring when there is a double count of the compressor stations (entries are identical except for Enverus ID)\n",
    "            # In this case, assign one compressor station to the field\n",
    "            Env_StorFields.loc[ifield,'Comp_flag'] = 1 #could alternatively set this to the number of matches (if actually >1 station per field)\n",
    "            Env_StorFields.loc[ifield,'Comp_lat'] = Enverus_Storage_CompStations.loc[best_match[0],'Latitude']\n",
    "            Env_StorFields.loc[ifield,'Comp_lon'] = Enverus_Storage_CompStations.loc[best_match[0],'Longitude']\n",
    "        else:\n",
    "            best_match = np.where((np.abs(Env_StorFields['Latitude'][ifield]-Enverus_Storage_CompStations['Latitude']) < 0.05) & \\\n",
    "                              (np.abs(Env_StorFields['Longitude'][ifield]-Enverus_Storage_CompStations['Longitude']) < 0.05) &\n",
    "                                 (Env_StorFields['CNTY_NAME'][ifield] == Enverus_Storage_CompStations.loc[:,'CNTY_NAME']))[0]\n",
    "            if np.size(best_match) >=1: \n",
    "                Env_StorFields.loc[ifield,'Comp_flag'] = 1 #could alternatively set this to the number of matches (if actually >1 station per field)\n",
    "                Env_StorFields.loc[ifield,'Comp_lat'] = Enverus_Storage_CompStations.loc[best_match[0],'Latitude']\n",
    "                Env_StorFields.loc[ifield,'Comp_lon'] = Enverus_Storage_CompStations.loc[best_match[0],'Longitude']\n",
    "            else:\n",
    "                nomatch +=1\n",
    "#print('NO MATCH', nomatch)\n",
    "\n",
    "print('Number of Enverus Fields w/ Compressor Stations: ',len(Env_StorFields[Env_StorFields['Comp_flag']==1]))\n",
    "print('Number of Enverus Fields w/out Compressor Stations: ',len(Env_StorFields[Env_StorFields['Comp_flag']==0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.7.5 Match the EIA and Enverus Storage Field/Station Data, record associated Stor. Compressor Station locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First clean up/correct mistakes in arrays\n",
    "Env_StorFields['CNTY_NAME'] = Env_StorFields['CNTY_NAME'].str.lower()\n",
    "Env_StorFields['RESERVOIR'] = Env_StorFields['RESERVOIR'].str.lower()\n",
    "Env_StorFields['NAME'] = Env_StorFields['NAME'].str.lower()\n",
    "Env_StorFields['OPERATOR'] = Env_StorFields['OPERATOR'].str.lower()\n",
    "EIA_StorFields['Reservoir Name'] = EIA_StorFields['Reservoir Name'].replace({np.nan:'NaN'})\n",
    "EIA_StorFields['County Name'] = EIA_StorFields['County Name'].replace({np.nan:'NaN'})\n",
    "Env_StorFields['RESERVOIR'] = Env_StorFields['RESERVOIR'].str.replace(r\"-\",\"\")\n",
    "EIA_StorFields['Reservoir Name'] = EIA_StorFields['Reservoir Name'].str.replace(r\"-\",\"\")\n",
    "Env_StorFields['RESERVOIR'] = Env_StorFields['RESERVOIR'].str.replace(r\".\",\"\")\n",
    "EIA_StorFields['Reservoir Name'] = EIA_StorFields['Reservoir Name'].str.replace(r\"\\(\",\"\")\n",
    "EIA_StorFields['Reservoir Name'] = EIA_StorFields['Reservoir Name'].str.replace(r\"\\)\",\"\")\n",
    "Env_StorFields['NAME'] = Env_StorFields['NAME'].str.replace(r\".\",\"\")\n",
    "EIA_StorFields['Reservoir Name'] = EIA_StorFields['Reservoir Name'].str.replace(r\".\",\"\")\n",
    "EIA_StorFields['Field Name'] = EIA_StorFields['Field Name'].str.rstrip()\n",
    "EIA_StorFields['Reservoir Name'] = EIA_StorFields['Reservoir Name'].str.rstrip()\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'CA') & (EIA_StorFields['County Name'] == 'Butte')),'County Name'] = 'colusa'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'IA') & (EIA_StorFields['County Name'] == 'Winnebago')),'County Name'] = 'washington'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'IL') & (EIA_StorFields['County Name'] == 'La Salle')),'County Name'] = 'lasalle'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'IL') & (EIA_StorFields['County Name'] == 'Coles') & (EIA_StorFields['Reservoir Name'] == 'NIAGARIAN')),'County Name'] = 'peoria'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'IL') & (EIA_StorFields['County Name'] == 'Coles') & (EIA_StorFields['Reservoir Name'] == 'NIAGARAN')),'County Name'] = 'peoria'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'IL') & (EIA_StorFields['County Name'] == 'Coles') & (EIA_StorFields['Reservoir Name'] == 'GLASFORD')),'County Name'] = 'peoria'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'IL') & (EIA_StorFields['County Name'] == 'Logan') & (EIA_StorFields['Reservoir Name'] == 'GALESVILLE')),'County Name'] = 'warren'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'IL') & (EIA_StorFields['County Name'] == 'Coles') & (EIA_StorFields['Reservoir Name'] == 'BENOIST')),'County Name'] = 'bond'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'IL') & (EIA_StorFields['County Name'] == 'Douglas') & (EIA_StorFields['Reservoir Name'] == 'CYPRESS  ROSICL')),'County Name'] = 'moultrie'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'IL') & (EIA_StorFields['County Name'] == 'Douglas') & (EIA_StorFields['Reservoir Name'] == 'CYPRESS ROSICLARE')),'County Name'] = 'moultrie'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'IL') & (EIA_StorFields['County Name'] == 'Mclean') & (EIA_StorFields['Field Name'] == 'PECATONICA')),'County Name'] = 'winnebago'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'IL') & (EIA_StorFields['County Name'] == 'Montgomery') & (EIA_StorFields['Field Name'] == 'HILLSBORO')),'County Name'] = 'st. clair'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'IN') & (EIA_StorFields['County Name'] == 'Daviess') & (EIA_StorFields['Field Name'] == 'WHITE RIVER')),'County Name'] = 'pike'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'IN') & (EIA_StorFields['County Name'] == 'Clark') & (EIA_StorFields['Field Name'] == 'WOLCOTT')),'County Name'] = 'white'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'KS') & (EIA_StorFields['County Name'] == 'Woodson') & (EIA_StorFields['Field Name'] == 'PIQUA')),'County Name'] = 'allen'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'KS') & (EIA_StorFields['County Name'] == 'NaN') & (EIA_StorFields['Field Name'] == 'PIQUA')),'County Name'] = 'allen'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'KS') & (EIA_StorFields['County Name'] == 'Morris') & (EIA_StorFields['Field Name'] == 'BOEHM')),'County Name'] = 'morton'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'KY') & (EIA_StorFields['County Name'] == 'Hart') & (EIA_StorFields['Field Name'] == 'MAGNOLIA UPPER')),'County Name'] = 'larue'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'KY') & (EIA_StorFields['County Name'] == 'Hart') & (EIA_StorFields['Field Name'] == 'MAGNOLIA DEEP')),'County Name'] = 'larue'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'KY') & (EIA_StorFields['County Name'] == 'Meade') & (EIA_StorFields['Field Name'] == 'DOE RUN')),'County Name'] = 'hardin'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'KY') & (EIA_StorFields['County Name'] == 'Daviess') & (EIA_StorFields['Field Name'] == 'EAST DIAMOND')),'County Name'] = 'hopkins'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'KY') & (EIA_StorFields['County Name'] == 'Christian') & (EIA_StorFields['Field Name'] == 'CROFTON EAST')),'County Name'] = 'hopkins'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'LA') & (EIA_StorFields['County Name'] == 'Ascension') & (EIA_StorFields['Field Name'] == 'NAPOLEON')),'County Name'] = 'assumption parish'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'LA') & (EIA_StorFields['County Name'] == 'Ascension') & (EIA_StorFields['Field Name'] == 'NAPOLEONVILLE')),'County Name'] = 'assumption parish'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'LA') & (EIA_StorFields['County Name'] == 'East Carroll') & (EIA_StorFields['Field Name'] == 'EPPS')),'County Name'] = 'west carroll parish'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'LA') & (EIA_StorFields['County Name'] == 'W. Carroll') & (EIA_StorFields['Field Name'] == 'EPPS')),'County Name'] = 'west carroll parish'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'LA') & (EIA_StorFields['County Name'] == 'Iberia') & (EIA_StorFields['Field Name'] == 'JEFFERSON ISLAN')),'County Name'] = 'vermilion parish'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'LA') & (EIA_StorFields['County Name'] == 'Iberia') & (EIA_StorFields['Field Name'] == 'JEFFERSON ISLAND')),'County Name'] = 'vermilion parish'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MI') & (EIA_StorFields['County Name'] == 'Oakland') & (EIA_StorFields['Field Name'] == 'LYON 29')),'County Name'] = 'washtenaw'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MI') & (EIA_StorFields['County Name'] == 'St. Clair') & (EIA_StorFields['Field Name'] == 'MARYSVILLE STORAGE')),'Reservoir Name'] = 'morton 16'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MI') & (EIA_StorFields['County Name'] == 'St. Clair') & (EIA_StorFields['Field Name'] == 'MARYSVILLE STOR')),'Reservoir Name'] = 'morton 16'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MI') & (EIA_StorFields['County Name'] == 'St. Clair') & (EIA_StorFields['Field Name'] == 'LEE 2')),'County Name'] = 'calhoun'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MI') & (EIA_StorFields['County Name'] == 'St. Clair') & (EIA_StorFields['Field Name'] == 'LEE 11')),'County Name'] = 'calhoun'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MI') & (EIA_StorFields['County Name'] == 'NaN') & (EIA_StorFields['Field Name'] == 'WINTERFIELD')),'County Name'] = 'clare'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MI') & (EIA_StorFields['County Name'] == 'NaN') & (EIA_StorFields['Field Name'] == 'CRANBERRY LAKE')),'County Name'] = 'clare'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MI') & (EIA_StorFields['County Name'] == 'NaN') & (EIA_StorFields['Field Name'] == 'HESSEN')),'County Name'] = 'st. clair'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MI') & (EIA_StorFields['County Name'] == 'NaN') & (EIA_StorFields['Field Name'] == 'IRA')),'County Name'] = 'st. clair'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MI') & (EIA_StorFields['County Name'] == 'NaN') & (EIA_StorFields['Field Name'] == 'FOUR CORNERS')),'County Name'] = 'st. clair'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MI') & (EIA_StorFields['County Name'] == 'NaN') & (EIA_StorFields['Field Name'] == 'SWAN CREEK')),'County Name'] = 'st. clair'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MI') & (EIA_StorFields['County Name'] == 'NaN') & (EIA_StorFields['Field Name'] == 'PUTTYGUT')),'County Name'] = 'st. clair'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MI') & (EIA_StorFields['County Name'] == 'NaN') & (EIA_StorFields['Field Name'] == 'WINFIELD')),'County Name'] = 'montcalm'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MI') & (EIA_StorFields['County Name'] == 'St. Clair') & (EIA_StorFields['Field Name'] == 'TAGGART')),'County Name'] = 'montcalm'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MI') & (EIA_StorFields['County Name'] == 'NaN') & (EIA_StorFields['Field Name'] == 'LOREED')),'County Name'] = 'osceola'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MN') & (EIA_StorFields['County Name'] == 'Waseca') & (EIA_StorFields['Field Name'] == 'WATERVILLE')),'County Name'] = 'steele'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MS') & (EIA_StorFields['County Name'] == 'Adams') & (EIA_StorFields['Field Name'] == 'NEW HOME DOME')),'County Name'] = 'smith'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MS') & (EIA_StorFields['County Name'] == 'Jasper') & (EIA_StorFields['Field Name'] == 'NEW HONE DOME')),'County Name'] = 'smith'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MS') & (EIA_StorFields['County Name'] == 'Monroe') & (EIA_StorFields['Field Name'] == 'GOODWIN')),'County Name'] = 'itawamba'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MS') & (EIA_StorFields['County Name'] == 'Monroe') & (EIA_StorFields['Field Name'] == 'GOODWIN STORAGE')),'County Name'] = 'itawamba'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MS') & (EIA_StorFields['County Name'] == 'NaN') & (EIA_StorFields['Field Name'] == 'HATTIESBURG')),'County Name'] = 'forrest'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MS') & (EIA_StorFields['County Name'] == 'Montgomery') & (EIA_StorFields['Field Name'] == 'SOUTHERN PINES')),'County Name'] = 'greene'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'MT') & (EIA_StorFields['County Name'] == 'Blaine') & (EIA_StorFields['Field Name'] == 'DRY CREEK')),'County Name'] = 'carbon'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'NY') & (EIA_StorFields['County Name'] == 'Medina') & (EIA_StorFields['Field Name'] == 'BENNINGTON STOR')),'County Name'] = 'wyoming'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'NY') & (EIA_StorFields['County Name'] == 'Erie') & (EIA_StorFields['Field Name'] == 'BENNINGTON STOR')),'County Name'] = 'wyoming'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'NY') & (EIA_StorFields['County Name'] == 'Erie') & (EIA_StorFields['Field Name'] == 'BENNINGTON STORAGE')),'County Name'] = 'wyoming'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'NY') & (EIA_StorFields['County Name'] == 'Kings') & (EIA_StorFields['Field Name'] == 'BEECH HILL STORAGE')),'County Name'] = 'allegany'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'NY') & (EIA_StorFields['County Name'] == 'Putnam') & (EIA_StorFields['Field Name'] == 'SENECA LAKE STORAGE')),'County Name'] = 'schuyler'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'NY') & (EIA_StorFields['County Name'] == 'Putnam') & (EIA_StorFields['Field Name'] == 'DUNDEE')),'County Name'] = 'schuyler'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'OH') & (EIA_StorFields['County Name'] == 'Hocking') & (EIA_StorFields['Field Name'] == 'CRAWFORD')),'County Name'] = 'fairfield'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'OH') & (EIA_StorFields['County Name'] == 'NaN') & (EIA_StorFields['Field Name'] == 'BRINKER')),'County Name'] = 'columbiana'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'OH') & (EIA_StorFields['County Name'] == 'Wayne') & (EIA_StorFields['Field Name'] == 'GABOR WERTZ')),'County Name'] = 'summit'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'OH') & (EIA_StorFields['County Name'] == 'Hancock') & (EIA_StorFields['Field Name'] == 'BENTON')),'County Name'] = 'hocking'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'OH') & (EIA_StorFields['County Name'] == 'Wayne') & (EIA_StorFields['Field Name'] == 'HOLMES')),'County Name'] = 'holmes'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'OK') & (EIA_StorFields['County Name'] == 'Grady') & (EIA_StorFields['Field Name'] == 'SALT PLAINS STO')),'County Name'] = 'grant'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'PA') & (EIA_StorFields['County Name'] == 'Warren') & (EIA_StorFields['Field Name'] == 'EAST BRANCH STO')),'County Name'] = 'mckean'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'PA') & (EIA_StorFields['County Name'] == 'Warren') & (EIA_StorFields['Field Name'] == 'EAST BRANCH STORAGE')),'County Name'] = 'mckean'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'PA') & (EIA_StorFields['County Name'] == 'Potter') & (EIA_StorFields['Field Name'] == 'LEIDY TAMARACK')),'County Name'] = 'clinton'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'PA') & (EIA_StorFields['County Name'] == 'NaN') & (EIA_StorFields['Field Name'] == 'LEIDY TAMARACK')),'County Name'] = 'clinton'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'PA') & (EIA_StorFields['County Name'] == 'Allegheny') & (EIA_StorFields['Field Name'] == 'WEBSTER')),'County Name'] = 'westmoreland'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'PA') & (EIA_StorFields['County Name'] == 'Allegheny') & (EIA_StorFields['Field Name'] == 'RAGER MOUNTAIN')),'County Name'] = 'cambria'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'PA') & (EIA_StorFields['County Name'] == 'Mercer') & (EIA_StorFields['Field Name'] == 'HENDERSON STORA')),'County Name'] = 'venango'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'PA') & (EIA_StorFields['County Name'] == 'Mercer') & (EIA_StorFields['Field Name'] == 'HENDERSON STORAGE')),'County Name'] = 'venango'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'TX') & (EIA_StorFields['County Name'] == 'Fort Bend') & (EIA_StorFields['Field Name'] == 'KATY HUB & STOR')),'County Name'] = 'waller'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'TX') & (EIA_StorFields['County Name'] == 'Fort Bend') & (EIA_StorFields['Field Name'] == 'KATY HUB & STORA')),'County Name'] = 'waller'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'WV') & (EIA_StorFields['County Name'] == 'Doddridge') & (EIA_StorFields['Field Name'] == 'SHIRLEY')),'County Name'] = 'tyler'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'WV') & (EIA_StorFields['County Name'] == 'Raleigh') & (EIA_StorFields['Field Name'] == 'RALEIGH CITY')),'County Name'] = 'wyoming'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'WV') & (EIA_StorFields['County Name'] == 'Kanawha') & (EIA_StorFields['Field Name'] == 'RALEIGH CITY')),'County Name'] = 'wyoming'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'WY') & (EIA_StorFields['County Name'] == 'Fremont') & (EIA_StorFields['Field Name'] == 'BUNKER HILL')),'County Name'] = 'carbon'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'WV') & (EIA_StorFields['County Name'] == 'Wirt') & (EIA_StorFields['Field Name'] == 'ROCKPORT')),'County Name'] = 'wood'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'WV') & (EIA_StorFields['County Name'] == 'Ritchie') & (EIA_StorFields['Field Name'] == 'RACKET  NEW BER')),'County Name'] = 'gilmer'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'WV') & (EIA_StorFields['County Name'] == 'Ritchie') & (EIA_StorFields['Field Name'] == 'RACHET-NEWBERNE')),'County Name'] = 'gilmer'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'TX') & (EIA_StorFields['County Name'] == 'NaN') & (EIA_StorFields['Field Name'] == 'WEST CLEAR LAKE')),'County Name'] = 'harris'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'TX') & (EIA_StorFields['County Name'] == 'Bastrop') & (EIA_StorFields['Field Name'] == 'PIERCE JUNCTION')),'County Name'] = 'harris'\n",
    "Env_StorFields.loc[((Env_StorFields['State'] == 'KS') & (Env_StorFields['NAME'] == 'welda (north)')), 'NAME'] = 'north welda'\n",
    "Env_StorFields.loc[((Env_StorFields['State'] == 'KS') & (Env_StorFields['NAME'] == 'welda (south)')), 'NAME'] = 'south welda'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'TX') & (EIA_StorFields['Reservoir Name'] == 'DW69')), 'Reservoir Name'] = 'dw 6'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'TX') & (EIA_StorFields['Reservoir Name'] == 'DW 69')), 'Reservoir Name'] = 'dw 6'\n",
    "EIA_StorFields.loc[((EIA_StorFields['Report State '] == 'PA') & (EIA_StorFields['Field Name'] == 'ST  MARYS STORAGE')), 'Field Name'] = 'ST MARYS STORAGE'\n",
    "\n",
    "# Second, loop through each EIA storage field, find the matching field in Enverus (by county, state, reservoir,\n",
    "# company, operator, etc) and record the associated storage compressor station location (if the field has one)\n",
    "# NOTE: As of March 2022, there are ~15 EIA fields that could not be matched to the Enverus dataset. In this\n",
    "# case, these fields are assumed to have zero compressor stations and are not accounted for in the national\n",
    "# or CONUS total capacity calculations (used to calculate emissions based on ratio to GHGRP data)\n",
    "\n",
    "EIA_StorFields['Lat'] = 0\n",
    "EIA_StorFields['Lon'] = 0\n",
    "EIA_StorFields['Comp_flag'] = 0\n",
    "DEBUG = 0\n",
    "\n",
    "print('QA/QC: The following EIA fields could not be matched to Enverus Gas Storage Fields')\n",
    "print('Assume that these fields have 0 storage compressor stations')\n",
    "\n",
    "for ifield in np.arange(0,len(EIA_StorFields)):\n",
    "    #first match based on state and county, then match either reservoir or name\n",
    "    matched = np.where((EIA_StorFields['Report State '][ifield] == Env_StorFields['State']) & \\\n",
    "                           (Env_StorFields['CNTY_NAME'].str.contains(EIA_StorFields['County Name'][ifield][0:5].lower())))[0]\n",
    "    #print(ifield)\n",
    "    if EIA_StorFields['Reservoir Name'][ifield][0:10] != 'NaN':\n",
    "        #for all the fields in the same state and county, choose the field that is in the same reservoir\n",
    "        best_match =  np.where(Env_StorFields['RESERVOIR'][matched].str.contains(EIA_StorFields['Reservoir Name'][ifield][0:10].lower()))[0]\n",
    "        \n",
    "        if np.size(best_match) == 1:\n",
    "            loc = matched[best_match[0]]\n",
    "            EIA_StorFields.loc[ifield,'Lat'] = Env_StorFields.loc[loc,'Comp_lat']\n",
    "            EIA_StorFields.loc[ifield,'Lon'] = Env_StorFields.loc[loc,'Comp_lon']\n",
    "            EIA_StorFields.loc[ifield,'Comp_flag'] = Env_StorFields.loc[loc,'Comp_flag']\n",
    "        elif np.size(best_match) >1:\n",
    "            # there is more than one field in the state, county, and reservoir - assign based on either matching company or field name, if\n",
    "            # still more than one match, assign manually\n",
    "            #print('>1 match')\n",
    "            if 'liberty north' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                better_match = np.where(Env_StorFields['NAME'][matched[best_match]].str.contains('liberty north'))[0]\n",
    "                loc = matched[best_match[better_match[0]]]\n",
    "            elif 'liberty south' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                better_match = np.where(Env_StorFields['NAME'][matched[best_match]].str.contains('liberty south'))[0]\n",
    "                loc = matched[best_match[better_match[0]]]\n",
    "            elif 'st  charles' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                better_match = np.where(Env_StorFields['NAME'][matched[best_match]].str.contains('st charles'))[0]\n",
    "                loc = matched[best_match[better_match[0]]] \n",
    "            elif 'east diamond' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                better_match = np.where(Env_StorFields['NAME'][matched].str.contains('east diamond'))[0]\n",
    "                loc = matched[better_match[0]]\n",
    "            elif 'crofton east' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                better_match = np.where(Env_StorFields['NAME'][matched[best_match]].str.contains('kirkwood springs'))[0]\n",
    "                loc = matched[best_match[better_match[0]]] \n",
    "            elif 'cold springs' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                better_match = np.where(Env_StorFields['NAME'][matched][0:14].str.contains(EIA_StorFields.loc[ifield,'Field Name'].lower()))[0]\n",
    "                loc = matched[better_match[0]]\n",
    "            elif 'niagaran' in EIA_StorFields.loc[ifield,'Reservoir Name'].lower() :\n",
    "                if 'belle river' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                    better_match = np.where(Env_StorFields['NAME'][matched].str.contains(EIA_StorFields.loc[ifield,'Field Name'].lower()))[0]\n",
    "                    loc = matched[better_match[0]]\n",
    "                else:\n",
    "                    better_match = np.where(Env_StorFields['NAME'][matched[best_match]].str.contains(EIA_StorFields.loc[ifield,'Field Name'][0:12].lower()))[0]\n",
    "                    loc = matched[best_match[better_match[0]]]\n",
    "            elif EIA_StorFields.loc[ifield,'Reservoir Name'].lower() == 'niagaran':\n",
    "                better_match = np.where(Env_StorFields['NAME'][matched].str.contains(EIA_StorFields.loc[ifield,'Field Name'].lower()))[0]\n",
    "                loc = matched[better_match[0]]\n",
    "            elif 'washington ' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                better_match = np.where(Env_StorFields['NAME'][matched][0:13].str.contains(EIA_StorFields.loc[ifield,'Field Name'].lower()))[0]\n",
    "                loc = matched[better_match[0]]\n",
    "            elif 'greenwood' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                better_match = np.where(Env_StorFields['NAME'][matched][0:13].str.contains(EIA_StorFields.loc[ifield,'Field Name'].lower()))[0]\n",
    "                loc = matched[better_match[0]]\n",
    "            elif 'amory storage' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                better_match = np.where(Env_StorFields['NAME'][matched].str.contains(EIA_StorFields.loc[ifield,'Field Name'].lower()[0:5]))[0]\n",
    "                loc = matched[better_match[0]]\n",
    "            elif 'derby storage' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                better_match = np.where(Env_StorFields['NAME'][matched].str.contains(EIA_StorFields.loc[ifield,'Field Name'].lower()[0:5]))[0]\n",
    "                loc = matched[better_match[0]]\n",
    "            elif 'zane storage' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                better_match = np.where(Env_StorFields['NAME'][matched[best_match]].str.contains('zane'))[0]\n",
    "                loc = matched[best_match[better_match[0]]]\n",
    "            elif 'artemas ' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                better_match = np.where(Env_StorFields['NAME'][matched][0:11].str.contains(EIA_StorFields.loc[ifield,'Field Name'].lower()))[0]\n",
    "                loc = matched[better_match[0]]\n",
    "            elif 'ellisburg' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                better_match = np.where(Env_StorFields['OPERATOR'][matched[best_match]].str.contains('berkshire'))[0]\n",
    "                loc = matched[best_match[better_match[0]]]\n",
    "            elif 'stratton ridge' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                better_match = np.where(Env_StorFields['OPERATOR'][matched[best_match]].str.contains('freeport'))[0]\n",
    "                loc = matched[best_match[better_match[0]]]\n",
    "            elif 'spindletop' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                better_match = np.where(Env_StorFields['OPERATOR'][matched].str.contains(EIA_StorFields.loc[ifield,'Company Name'].lower()[0:6]))[0]\n",
    "                loc = matched[better_match[0]]\n",
    "            elif 'ambassador' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                better_match = np.where(Env_StorFields['NAME'][matched[best_match]].str.contains('la-pan'))[0]\n",
    "                loc = matched[best_match[better_match[0]]]\n",
    "            elif 'terra alta' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                better_match = np.where(Env_StorFields['NAME'][matched][0:15].str.contains(EIA_StorFields.loc[ifield,'Field Name'].lower()))[0]\n",
    "                loc = matched[better_match[0]]\n",
    "            else:\n",
    "                better_match =  np.where(Env_StorFields['NAME'][matched[best_match]].str.contains(EIA_StorFields['Company Name'][ifield][0:6].lower()) | \\\n",
    "                               (Env_StorFields['NAME'][matched[best_match]].str.contains(EIA_StorFields['Field Name'][ifield][0:6].lower())))[0]\n",
    "                loc = matched[best_match[better_match[0]]]\n",
    "                if np.size(better_match) != 1:\n",
    "                    print('> 1 match - Check Manually')\n",
    "                    print(EIA_StorFields.loc[ifield,:])\n",
    "                    display(Env_StorFields.loc[matched[best_match[better_match]],:])\n",
    "            \n",
    "            #Assign lat/Lon, whether there is compressor station there\n",
    "            EIA_StorFields.loc[ifield,'Lat'] = Env_StorFields.loc[loc,'Comp_lat']\n",
    "            EIA_StorFields.loc[ifield,'Lon'] = Env_StorFields.loc[loc,'Comp_lon']\n",
    "            EIA_StorFields.loc[ifield,'Comp_flag'] = Env_StorFields.loc[loc,'Comp_flag']\n",
    "        else:\n",
    "            #print('NO match')\n",
    "            #this will occur if there is a match based on county/state, but not on reservoir\n",
    "            # (or some cases where no match on county/state)\n",
    "            # in this case, look at company and field name and assign mannually if needed\n",
    "            if 'totem storage' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                best_match = np.where(Env_StorFields['NAME'][matched].str.contains('totem'))[0]\n",
    "            elif 'lincoln storage' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                best_match = np.where(Env_StorFields['NAME'][matched].str.contains('lincoln'))[0]\n",
    "            elif 'cecilia storage' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                best_match = np.where(Env_StorFields['NAME'][matched].str.contains('cecilia'))[0]\n",
    "            elif 'egan storage do' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                best_match = np.where(Env_StorFields['NAME'][matched].str.contains('egan'))[0]\n",
    "            elif 'washington ' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                best_match = np.where(Env_StorFields['NAME'][matched].str.contains(EIA_StorFields.loc[ifield,'Field Name'].lower()[0:13]))[0]\n",
    "            elif 'petal' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                best_match = np.where(Env_StorFields['NAME'][matched][0:5].str.contains('hattiesburg'))[0]\n",
    "            elif 'zoar storage' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                best_match = np.where(Env_StorFields['NAME'][matched].str.contains('zoar'))[0]\n",
    "            elif 'love storage' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                best_match = np.where(Env_StorFields['NAME'][matched][0:5].str.contains('perry'))[0]\n",
    "            elif 'swarts and swar' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                best_match = np.where(Env_StorFields['NAME'][matched].str.contains('swarts'))[0]\n",
    "            elif 'clemens  n.e.' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                best_match = np.where(Env_StorFields['NAME'][matched][0:5].str.contains('clemens'))[0]\n",
    "            elif 'worsham steed' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                best_match = np.where(Env_StorFields['NAME'][matched][0:6].str.contains('worsham-steed'))[0]\n",
    "            elif 'early grove' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                best_match = np.where(Env_StorFields['NAME'][matched][0:6].str.contains('early grove'))[0]\n",
    "            elif 'terra alta' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                if 'south' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                    best_match = np.where(Env_StorFields['NAME'][matched].str.contains('south'))[0]\n",
    "                else:\n",
    "                    best_match = np.where(~Env_StorFields['NAME'][matched].str.contains('south'))[0]           \n",
    "            elif 'racket ' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                best_match = np.where(Env_StorFields['NAME'][matched][0:15].str.contains('rachet-newberne'))[0]\n",
    "            elif 'rachet' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                best_match = np.where(Env_StorFields['NAME'][matched][0:15].str.contains('rachet-newberne'))[0]\n",
    "            elif 'ryckman creek' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                best_match = np.where(Env_StorFields['NAME'][matched][0:15].str.contains('belle butte'))[0]\n",
    "            elif 'east mahoney' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                best_match = np.where(Env_StorFields['NAME'][matched][0:15].str.contains('oil springs'))[0]\n",
    "            else:\n",
    "                best_match =  np.where(Env_StorFields['NAME'][matched][0:8].str.contains(EIA_StorFields['Company Name'][ifield][0:8].lower()) | \\\n",
    "                               (Env_StorFields['NAME'][matched][0:8].str.contains(EIA_StorFields['Field Name'][ifield][0:8].lower())))[0]\n",
    "            if np.size(best_match) != 1:\n",
    "                #continue\n",
    "                if DEBUG ==1:\n",
    "                    print('No best match - Check Mannually')\n",
    "                    print(EIA_StorFields.loc[ifield,:])\n",
    "                    display(Env_StorFields.loc[matched[best_match],:])\n",
    "            else:\n",
    "                loc = matched[best_match[0]]\n",
    "                #Assign lat/Lon, whether there is compressor station there\n",
    "                EIA_StorFields.loc[ifield,'Lat'] = Env_StorFields.loc[loc,'Comp_lat']\n",
    "                EIA_StorFields.loc[ifield,'Lon'] = Env_StorFields.loc[loc,'Comp_lon']\n",
    "                EIA_StorFields.loc[ifield,'Comp_flag'] = Env_StorFields.loc[loc,'Comp_flag']\n",
    "\n",
    "    else:\n",
    "        #in this case, the EIA data has no reservoir information and need to match based on company/field name\n",
    "        #print(ifield)\n",
    "        #print('NO res')\n",
    "        best_match =  np.where((Env_StorFields['NAME'][matched][0:6].str.contains(EIA_StorFields['Company Name'][ifield][0:6].lower())) | \\\n",
    "                               (Env_StorFields['NAME'][matched][0:6].str.contains(EIA_StorFields['Field Name'][ifield][0:6].lower())))[0]\n",
    "        if np.size(best_match) ==1:\n",
    "            loc = matched[best_match[0]]\n",
    "            EIA_StorFields.loc[ifield,'Lat'] = Env_StorFields.loc[loc,'Comp_lat']\n",
    "            EIA_StorFields.loc[ifield,'Lon'] = Env_StorFields.loc[loc,'Comp_lon']\n",
    "            EIA_StorFields.loc[ifield,'Comp_flag'] = Env_StorFields.loc[loc,'Comp_flag']\n",
    "        elif np.size(best_match) >1:\n",
    "            #if more than one match based on county/state, and no reservoir data...look at operator\n",
    "            better_match =  np.where((Env_StorFields['OPERATOR'][matched[best_match]][0:6].str.contains(EIA_StorFields['Company Name'][ifield][0:6].lower())))[0]\n",
    "            if np.size(better_match) ==1:\n",
    "                # if one operator match...\n",
    "                loc = matched[best_match[better_match[0]]]\n",
    "                EIA_StorFields.loc[ifield,'Lat'] = Env_StorFields.loc[loc,'Comp_lat']\n",
    "                EIA_StorFields.loc[ifield,'Lon'] = Env_StorFields.loc[loc,'Comp_lon']\n",
    "                EIA_StorFields.loc[ifield,'Comp_flag'] = Env_StorFields.loc[loc,'Comp_flag']\n",
    "            elif np.size(better_match) >1:\n",
    "                # if more than one operator match...\n",
    "                if EIA_StorFields.loc[ifield,'Field Name'].lower() == 'kirby hills wagenet':\n",
    "                    finalmatch = np.where(Env_StorFields['RESERVOIR'][matched[best_match[better_match]]].str.contains('wagenet'))[0]\n",
    "                    loc = matched[best_match[better_match[finalmatch[0]]]]\n",
    "                elif 'early grove' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                    finalmatch = np.where(Env_StorFields['NAME'][matched[best_match[better_match]]].str.contains('early grove'))[0] \n",
    "                    loc = matched[best_match[better_match[finalmatch[0]]]]\n",
    "                else:\n",
    "                    print('HERE STOP*****************************')\n",
    "                    loc = -99\n",
    "            else:\n",
    "                # if no operator match\n",
    "                if EIA_StorFields.loc[ifield,'Field Name'].lower() == 'markham':\n",
    "                    finalmatch = np.where(Env_StorFields['NAME'][matched[best_match]].str.contains('markham'))[0]\n",
    "                    loc = matched[best_match[finalmatch[0]]]\n",
    "                else:\n",
    "                    print('STOP HERE2*******************************')\n",
    "                    loc = -99\n",
    "            # Assign lat/lon values\n",
    "            EIA_StorFields.loc[ifield,'Lat'] = Env_StorFields.loc[loc,'Comp_lat']\n",
    "            EIA_StorFields.loc[ifield,'Lon'] = Env_StorFields.loc[loc,'Comp_lon']\n",
    "            EIA_StorFields.loc[ifield,'Comp_flag'] = Env_StorFields.loc[loc,'Comp_flag']     \n",
    "    \n",
    "        else:\n",
    "            #if no reservoir, or company or field match\n",
    "            if 'egan storage dome' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                best_match = np.where(Env_StorFields['NAME'][matched].str.contains('egan'))[0]\n",
    "                loc = matched[best_match[0]]\n",
    "            elif 'new home dome' in EIA_StorFields.loc[ifield,'Field Name'].lower():\n",
    "                best_match = np.where(Env_StorFields['NAME'].str.contains('new home dome'))[0]\n",
    "                loc =best_match[0]\n",
    "            else:\n",
    "                #print(best_match)\n",
    "                print('No Res Data - Check Mannually')\n",
    "                print(EIA_StorFields.loc[ifield,:])\n",
    "                display(Env_StorFields.loc[matched,:])\n",
    "                loc =-99\n",
    "            if loc > 0:\n",
    "                # Assign lat/lon values\n",
    "                EIA_StorFields.loc[ifield,'Lat'] = Env_StorFields.loc[loc,'Comp_lat']\n",
    "                EIA_StorFields.loc[ifield,'Lon'] = Env_StorFields.loc[loc,'Comp_lon']\n",
    "                EIA_StorFields.loc[ifield,'Comp_flag'] = Env_StorFields.loc[loc,'Comp_flag']  \n",
    "\n",
    "print('QA/QC: Report the number of EIA fields and Enverus storage compressor stations')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    total_stations = len(EIA_StorFields[(EIA_StorFields['Year']==year_range[iyear]) & (EIA_StorFields['Comp_flag']==1)])\n",
    "    total_fields = len(EIA_StorFields[EIA_StorFields['Year']==year_range[iyear]])\n",
    "    print('Year: ', year_range_str[iyear])\n",
    "    print('Fields, stations, (%):', total_fields,',', total_stations,',',round((total_stations/total_fields)*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.7.6 Read In GHGRP Storage Compressor Station Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a) Read in the GHGRP data\n",
    "# emissions of methane reported in metric ton\n",
    "facility_info = pd.read_csv(GHGRP_facility_inputfile)\n",
    "facility_emissions = pd.read_excel(GHGRP_subpartw_inputfile,sheet_name = 'Export Worksheet')\n",
    "facility_emissions = facility_emissions[facility_emissions['INDUSTRY_SEGMENT'] =='Underground natural gas storage [98.230(a)(5)]']\n",
    "facility_emissions = facility_emissions[facility_emissions['TOTAL_REPORTED_CH4_EMISSIONS'] >0]\n",
    "facility_emissions = facility_emissions[facility_emissions['REPORTING_YEAR'] <= year_range[-1]]\n",
    "facility_emissions.reset_index(drop=True,inplace=True)\n",
    "#print(facility_emissions)\n",
    "\n",
    "facility_emissions['State'] = ''\n",
    "facility_emissions['County'] = ''\n",
    "facility_emissions['City'] = ''\n",
    "facility_emissions['Zip'] = 0\n",
    "facility_emissions['Lat'] = 0\n",
    "facility_emissions['Lon'] = 0\n",
    "\n",
    "#b) match GHGRP facility and emissions data\n",
    "# for each entry in the data file (each facility each year), match the facility ID to the ID in the\n",
    "# GHGRP facility info file, then append the corresponding location data to the emissions array\n",
    "for index in np.arange(len(facility_emissions)):\n",
    "    #print(index)\n",
    "    ilocation = np.where(facility_info['V_GHG_EMITTER_FACILITIES.FACILITY_ID'] == facility_emissions['FACILITY_ID'][index])[0][0]\n",
    "    #for iloc in len(ilocation)\n",
    "    facility_emissions.loc[index, 'State'] = facility_info['V_GHG_EMITTER_FACILITIES.STATE'][ilocation]\n",
    "    facility_emissions.loc[index, 'County'] = facility_info['V_GHG_EMITTER_FACILITIES.COUNTY'][ilocation]\n",
    "    facility_emissions.loc[index, 'City'] = facility_info['V_GHG_EMITTER_FACILITIES.CITY'][ilocation]\n",
    "    facility_emissions.loc[index, 'Zip'] = facility_info['V_GHG_EMITTER_FACILITIES.ZIP'][ilocation]\n",
    "    facility_emissions.loc[index, 'Lat'] = facility_info['V_GHG_EMITTER_FACILITIES.LATITUDE'][ilocation]\n",
    "    facility_emissions.loc[index, 'Lon'] = facility_info['V_GHG_EMITTER_FACILITIES.LONGITUDE'][ilocation]\n",
    "    \n",
    "#b) make station-specific arrays for each year (with emissions in Tg)\n",
    "print('QA/QC: Check that all GHGRP emissions are allocated to specific plants')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    facility_emissions_temp = facility_emissions[facility_emissions['REPORTING_YEAR'] ==year_range[iyear]]\n",
    "    facility_emissions_temp.reset_index(drop=True,inplace=True)\n",
    "    GHGRP_storstations = pd.DataFrame({'FID':facility_emissions_temp['FACILITY_ID'].unique()})\n",
    "    GHGRP_storstations['Name'] = ' '\n",
    "    GHGRP_storstations['State'] = ' '\n",
    "    GHGRP_storstations['County'] = ' '\n",
    "    GHGRP_storstations['City'] = ' '\n",
    "    GHGRP_storstations['Zip'] = 0\n",
    "    GHGRP_storstations['Lat'] = 0.0\n",
    "    GHGRP_storstations['Lon'] = 0.0\n",
    "    GHGRP_storstations['TgCH4'] = 0.0\n",
    "\n",
    "    #Put everything in per-plant array\n",
    "    for idx in np.arange(len(facility_emissions_temp)):\n",
    "        iFID = np.where(GHGRP_storstations['FID'] == facility_emissions_temp['FACILITY_ID'][idx])[0][0]\n",
    "        GHGRP_storstations.loc[iFID,'Name']   = facility_emissions_temp['FACILITY_NAME'][idx]\n",
    "        GHGRP_storstations.loc[iFID,'State']  = facility_emissions_temp['State'][idx]\n",
    "        GHGRP_storstations.loc[iFID,'County'] = facility_emissions_temp['County'][idx]\n",
    "        GHGRP_storstations.loc[iFID,'City'] = facility_emissions_temp['City'][idx]\n",
    "        GHGRP_storstations.loc[iFID,'Zip']    = facility_emissions_temp['Zip'][idx]\n",
    "        GHGRP_storstations.loc[iFID,'Lat']    = facility_emissions_temp['Lat'][idx]\n",
    "        GHGRP_storstations.loc[iFID,'Lon']    = facility_emissions_temp['Lon'][idx]\n",
    "        GHGRP_storstations.loc[iFID,'TgCH4'] += facility_emissions_temp['TOTAL_REPORTED_CH4_EMISSIONS'][idx]/1e6\n",
    "    \n",
    "    vars()['GHGRP_storstations'+'_'+year_range_str[iyear]] = GHGRP_storstations\n",
    "    diff1 = abs(facility_emissions_temp['TOTAL_REPORTED_CH4_EMISSIONS'].sum()/1e6 -GHGRP_storstations['TgCH4'].sum())/ \\\n",
    "        ((facility_emissions_temp['TOTAL_REPORTED_CH4_EMISSIONS'].sum()/1e6 + GHGRP_storstations['TgCH4'].sum())/2)\n",
    "    #print(summary_emi)\n",
    "    #print(sum_emi2[iyear])\n",
    "    if diff1 < 0.0001:\n",
    "        print('Year ', year_range[iyear],': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear],': FAIL: ', diff1,'%') \n",
    "    print('Number of GHGRP Storage Stations: ', len(vars()['GHGRP_storstations'+'_'+year_range_str[iyear]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.7.7 Match EIA storage compressor stations to GHGRP based on location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each year of GHGRP data, match GHGRP stroage stations to EIA data (based on nearest location, not name)\n",
    "\n",
    "print('QA/QC: Number of GHGRP plants not in Enverus data set')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    # use the correct year of data and filter only for fields with storage compressor stations\n",
    "    GHGRP_temp_data = vars()['GHGRP_storstations'+'_'+year_range_str[iyear]].copy()\n",
    "    EIA_StorFields_temp = EIA_StorFields[(EIA_StorFields['Year']==year_range[iyear])]# & (EIA_StorFields['Comp_flag'] ==1)]\n",
    "    EIA_StorFields_temp.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    GHGRP_temp_data.loc[:,'match_flag'] = 0\n",
    "    GHGRP_temp_data.loc[:,'EIA_name'] = ''\n",
    "    GHGRP_temp_data.loc[:,'EIA_county'] = ''\n",
    "    GHGRP_temp_data.loc[:,'EIA_state'] = ''\n",
    "    #GHGRP_temp_data['Env_HP'] = 0\n",
    "    GHGRP_temp_data.loc[:,'EIA_fieldcap'] = 0\n",
    "    EIA_StorFields_temp.loc[:,'GHGRP_match'] = 0\n",
    "\n",
    "    #First, find exact matching lat/lon facilities\n",
    "    for istation in np.arange(0,len(GHGRP_temp_data)):\n",
    "        matched = np.where((np.abs(EIA_StorFields_temp['Lat']-GHGRP_temp_data['Lat'][istation]) < 0.12) & \\\n",
    "                              (np.abs(EIA_StorFields_temp['Lon']-GHGRP_temp_data['Lon'][istation]) < 0.12))[0]\n",
    "        if np.size(matched)==1:\n",
    "            EIA_StorFields_temp.loc[matched[0],'GHGRP_match'] = 1\n",
    "            GHGRP_temp_data.loc[istation,'match_flag'] = 1\n",
    "            GHGRP_temp_data.loc[istation,'EIA_name'] = EIA_StorFields_temp.loc[matched[0], 'Company Name']\n",
    "            GHGRP_temp_data.loc[istation,'EIA_county'] = EIA_StorFields_temp.loc[matched[0], 'County Name']\n",
    "            GHGRP_temp_data.loc[istation,'EIA_state'] = EIA_StorFields_temp.loc[matched[0], 'Report State ']\n",
    "            GHGRP_temp_data.loc[istation,'EIA_fieldcap'] = EIA_StorFields_temp.loc[matched[0], 'Total Field Capacity(Mcf)']\n",
    "        elif np.size(matched) > 1:\n",
    "            dist_calc = np.zeros(len(matched))\n",
    "            GHGRP_temp_data.loc[istation,'match_flag'] = 1\n",
    "            #print(dist_calc)\n",
    "            for imatch in np.arange(len(dist_calc)): #loop through the matching stations to find the closest match\n",
    "                dist_calc[imatch] = np.abs(GHGRP_temp_data.loc[istation,'Lat'] - \\\n",
    "                                           EIA_StorFields_temp.loc[matched[imatch],'Lat'])**2 + \\\n",
    "                               np.abs(GHGRP_temp_data.loc[istation,'Lon'] - EIA_StorFields_temp.loc[matched[imatch],'Lon'])**2\n",
    "            bestpick = np.where(dist_calc == dist_calc.min())[0][0]\n",
    "            #print(bestpick)\n",
    "            #print(np.size(np.where(dist_calc == dist_calc.min())[0]))\n",
    "            if np.size(np.where(dist_calc == dist_calc.min())[0]) == 1: #if there is only one match, assign the correct data\n",
    "                #print(matched[bestpick])\n",
    "                EIA_StorFields_temp.loc[matched[bestpick],'GHGRP_match'] = 1\n",
    "                GHGRP_temp_data.loc[istation,'match_flag'] = 1\n",
    "                GHGRP_temp_data.loc[istation,'EIA_name'] = EIA_StorFields_temp.loc[matched[bestpick], 'Company Name']\n",
    "                GHGRP_temp_data.loc[istation,'EIA_county'] = EIA_StorFields_temp.loc[matched[bestpick], 'County Name']\n",
    "                GHGRP_temp_data.loc[istation,'EIA_state'] = EIA_StorFields_temp.loc[matched[bestpick], 'Report State ']\n",
    "                GHGRP_temp_data.loc[istation,'EIA_fieldcap'] = EIA_StorFields_temp.loc[matched[bestpick], 'Total Field Capacity(Mcf)']\n",
    "                #GHGRP_temp_data.loc[istation,'Env_HP'] = Enverus_Trans_CompStations.loc[matched[bestpick], 'HP']\n",
    "            else: #if there is more than one match, sum the field capacity from all matching stations and assign average to GHGRP array\n",
    "                best_array = np.where(dist_calc == dist_calc.min())[0]\n",
    "                #print(matched[best_array])\n",
    "                #print(len(best_array))\n",
    "                total_stor = 0.0\n",
    "                nonzero_stor = 0\n",
    "                for ibest in np.arange(0,len(best_array)):\n",
    "                    if EIA_StorFields_temp.loc[matched[best_array[ibest]], 'Total Field Capacity(Mcf)'] > 0:\n",
    "                        total_stor += EIA_StorFields_temp.loc[matched[best_array[ibest]], 'Total Field Capacity(Mcf)']\n",
    "                        nonzero_stor += 1\n",
    "                    EIA_StorFields_temp.loc[matched[best_array[ibest]],'GHGRP_match'] = 1\n",
    "                GHGRP_temp_data.loc[istation,'match_flag'] = 1\n",
    "                GHGRP_temp_data.loc[istation,'EIA_county'] = EIA_StorFields_temp.loc[matched[best_array[0]], 'County Name']\n",
    "                GHGRP_temp_data.loc[istation,'EIA_state'] = EIA_StorFields_temp.loc[matched[best_array[0]], 'Report State ']\n",
    "                GHGRP_temp_data.loc[istation,'EIA_fieldcap'] = data_fn.safe_div(total_stor,nonzero_stor)\n",
    "        else:\n",
    "            if GHGRP_temp_data.loc[istation,'Name'] == 'SNG Station 4020 Bear Creek Storage, LA':\n",
    "                matched = np.where((EIA_StorFields_temp['Company Name'] == 'BEAR CREEK STORAGE COMPANY') & \\\n",
    "                                   (EIA_StorFields_temp['Report State '] == 'LA'))[0]\n",
    "                EIA_StorFields_temp.loc[matched[0],'GHGRP_match'] = 1\n",
    "                GHGRP_temp_data.loc[istation,'match_flag'] = 1\n",
    "                GHGRP_temp_data.loc[istation,'EIA_name'] = EIA_StorFields_temp.loc[matched[0], 'Company Name']\n",
    "                GHGRP_temp_data.loc[istation,'EIA_county'] = EIA_StorFields_temp.loc[matched[0], 'County Name']\n",
    "                GHGRP_temp_data.loc[istation,'EIA_state'] = EIA_StorFields_temp.loc[matched[0], 'Report State ']\n",
    "                GHGRP_temp_data.loc[istation,'EIA_fieldcap'] = EIA_StorFields_temp.loc[matched[0], 'Total Field Capacity(Mcf)']\n",
    "\n",
    "            elif GHGRP_temp_data.loc[istation,'FID'] == 1009849:\n",
    "                matched = np.where((EIA_StorFields_temp['Field Name'] == 'BOLING') & \\\n",
    "                                   (EIA_StorFields_temp['Report State '] == 'TX'))[0]\n",
    "                #print(matched, istation)\n",
    "                EIA_StorFields_temp.loc[matched[0],'GHGRP_match'] = 1\n",
    "                GHGRP_temp_data.loc[istation,'match_flag'] = 1\n",
    "                GHGRP_temp_data.loc[istation,'EIA_name'] = EIA_StorFields_temp.loc[matched[0], 'Company Name']\n",
    "                GHGRP_temp_data.loc[istation,'EIA_county'] = EIA_StorFields_temp.loc[matched[0], 'County Name']\n",
    "                GHGRP_temp_data.loc[istation,'EIA_state'] = EIA_StorFields_temp.loc[matched[0], 'Report State ']\n",
    "                GHGRP_temp_data.loc[istation,'EIA_fieldcap'] = EIA_StorFields_temp.loc[matched[0], 'Total Field Capacity(Mcf)']\n",
    "\n",
    "                \n",
    "    GHGRP_notmatched = GHGRP_temp_data[GHGRP_temp_data['match_flag'] == 0]\n",
    "    vars()['GHGRP_storstations'+'_'+year_range_str[iyear]] = GHGRP_temp_data.copy()   \n",
    "    #save a list of all the EIA fields with storage compressor stations that are not in the GHGRP dataset\n",
    "    EIA_notmatched = EIA_StorFields_temp[(EIA_StorFields_temp['GHGRP_match'] == 0) & (EIA_StorFields_temp['Comp_flag'] == 1)]\n",
    "    EIA_notmatched.reset_index(inplace=True, drop=True)\n",
    "    vars()['EIA_StorCompStations_notmatched'+'_'+year_range_str[iyear]] = EIA_notmatched.copy()                          \n",
    "            \n",
    "    \n",
    "    print('Year ', year_range_str[iyear],': ', len(GHGRP_notmatched), ' of ', len(GHGRP_temp_data))\n",
    "    #display(GHGRP_notmatched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.7.8 Calculate average ratio of emissions per total field capacity (for fields with compressor stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_emis_cap_ratio = np.zeros([num_years])\n",
    "\n",
    "print('QA/QC: Average Emissions to Field Capacity Ratio')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    GHGRP_temp_data = vars()['GHGRP_storstations'+'_'+year_range_str[iyear]].copy()\n",
    "\n",
    "    GHGRP_temp_data['Emis_cap_ratio']=0\n",
    "    for istation in np.arange(0,len(GHGRP_temp_data)):\n",
    "        GHGRP_temp_data.loc[istation, 'Emis_cap_ratio'] = data_fn.safe_div(GHGRP_temp_data.loc[istation, 'TgCH4'], \\\n",
    "                                                                        GHGRP_temp_data.loc[istation, 'EIA_fieldcap'])\n",
    "    GHGRP_temp_data['Emis_cap_ratio'] = GHGRP_temp_data['Emis_cap_ratio'].replace({0:np.nan})\n",
    "    avg_emis_cap_ratio[iyear] = np.mean(GHGRP_temp_data['Emis_cap_ratio'])\n",
    "    \n",
    "    vars()['GHGRP_storstations'+'_'+year_range_str[iyear]] = GHGRP_temp_data.copy()\n",
    "    print('Year ', year_range_str[iyear],': ', avg_emis_cap_ratio[iyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.7.9 Assign emissions to grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map = GHGRP emissions for each station that matched, then take all the EIA compressor stations that didnt match,\n",
    "# calculate emissions and then add to map\n",
    "# AK/HI Note: Note that AK/HI compressor stations and storage fields are included in the Enverus, EIA, and GHGRP datasets. \n",
    "# Therefore, no further action is required to split out AK/HI emissions from the CONUS region, other than the code below, \n",
    "# which filters these emissions based on the locations of each compressor station\n",
    "\n",
    "map_StorStations = np.zeros([len(Lat_01),len(Lon_01),num_years]) #data represent a snapshot in time that is applied to entire timeseries\n",
    "map_StorStations_nongrid = np.zeros([num_years])\n",
    "\n",
    "print('QA/QC: Storage Compressor Station Emissions Gridded:')\n",
    "for iyear in np.arange(0, num_years):\n",
    "    stations_ongrid = 0\n",
    "    stations_nongrid = 0\n",
    "    #first add GHGRP emissions for matched stations\n",
    "    GHGRP_temp_data = vars()['GHGRP_storstations'+'_'+year_range_str[iyear]].copy()\n",
    "    for istation in np.arange(0,len(GHGRP_temp_data)):\n",
    "        if GHGRP_temp_data.loc[istation,'match_flag']==1:\n",
    "            if GHGRP_temp_data['Lon'][istation] > Lon_left and GHGRP_temp_data['Lon'][istation] < Lon_right \\\n",
    "                and GHGRP_temp_data['Lat'][istation] > Lat_low and GHGRP_temp_data['Lat'][istation] < Lat_up:\n",
    "                ilat = int((GHGRP_temp_data['Lat'][istation] - Lat_low)/Res01)\n",
    "                ilon = int((GHGRP_temp_data['Lon'][istation] - Lon_left)/Res01)\n",
    "                #if Env_ProcPlant_loc['Throughput'][iplant] >0:\n",
    "                map_StorStations[ilat,ilon,iyear] += GHGRP_temp_data.loc[istation, 'TgCH4']\n",
    "                stations_ongrid +=1\n",
    "            else:\n",
    "                map_StorStations_nongrid[iyear] += GHGRP_temp_data.loc[istation, 'TgCH4']  \n",
    "                stations_nongrid +=1\n",
    "\n",
    "    #then add calculated EIA emissions for all non-matched storage compressor stations\n",
    "    EIA_temp_data = vars()['EIA_StorCompStations_notmatched'+'_'+year_range_str[iyear]].copy()\n",
    "    for istation in np.arange(0, len(EIA_temp_data)):\n",
    "        if EIA_temp_data['Lon'][istation] > Lon_left and EIA_temp_data['Lon'][istation] < Lon_right \\\n",
    "            and EIA_temp_data['Lat'][istation] > Lat_low and EIA_temp_data['Lat'][istation] < Lat_up:\n",
    "            ilat = int((EIA_temp_data['Lat'][istation] - Lat_low)/Res01)\n",
    "            ilon = int((EIA_temp_data['Lon'][istation] - Lon_left)/Res01)\n",
    "            map_StorStations[ilat,ilon,iyear] += EIA_temp_data.loc[istation,'Total Field Capacity(Mcf)']*avg_emis_cap_ratio[iyear]\n",
    "            stations_ongrid +=1\n",
    "        else:\n",
    "            map_StorStations_nongrid[iyear] += EIA_temp_data.loc[istation,'Total Field Capacity(Mcf)']*avg_emis_cap_ratio[iyear]\n",
    "            stations_nongrid +=1\n",
    "            \n",
    "    vars()['GHGRP_storstations'+'_'+year_range_str[iyear]] = GHGRP_temp_data.copy()\n",
    "    vars()['EIA_StorCompStations_notmatched'+'_'+year_range_str[iyear]] = EIA_temp_data.copy()\n",
    "    \n",
    "    print('Year: ', year_range_str[iyear])\n",
    "    print('On grid (Tg): ',np.sum(map_StorStations[:,:, iyear]), ', stations:', stations_ongrid)\n",
    "    print('Off grid (Tg): ',np.sum(map_StorStations_nongrid[iyear]), ', stations:', stations_nongrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.8 - Make Storage Well Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in EIA storage field timeseries data\n",
    "# Read in EIA storage field location data\n",
    "# assign lat/lon to timeseries data based on matching the gas field code\n",
    "\n",
    "#place data onto grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make array of gas storage capacities at EIA underground gas storage facilities\n",
    "# Get gas capacities at each field overtime from the full 191 survey data (EIA)\n",
    "# Get lat/lons of storage facilities (for the current year only) from the EIA data explorer dataset\n",
    "# Match the fields based on EIA field code to get the locations of all fields over time\n",
    "\n",
    "Map_Storage = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Map_Storage_nongrid = np.zeros([num_years])\n",
    "\n",
    "#Re-load EIA Storage Field Capacities\n",
    "names = pd.read_excel(EIA_StorFields_inputfile, skiprows = 0, header = 0)\n",
    "colnames = names.columns.values\n",
    "EIA_StorFields = pd.read_excel(EIA_StorFields_inputfile, skiprows = 0, names = colnames)\n",
    "EIA_StorFields = EIA_StorFields[['Year','Gas Field Code','Report State ','Status','Reservoir Code','Total Field Capacity(Mcf)']]\n",
    "# filter for active storage fields only\n",
    "EIA_StorFields = EIA_StorFields[EIA_StorFields['Status']== 'Active']\n",
    "EIA_StorFields.reset_index(drop=True,inplace=True)\n",
    "#display(EIA_StorFields)\n",
    "\n",
    "#load field locations\n",
    "names = pd.read_excel(EIA_StorFields_locs_inputfile, skiprows = 0, header = 0)\n",
    "colnames = names.columns.values\n",
    "EIA_StorFields_locs = pd.read_excel(EIA_StorFields_locs_inputfile, skiprows = 0, names = colnames)\n",
    "EIA_StorFields_locs = EIA_StorFields_locs[['fld_code','res_code','Longitude','Latitude']]\n",
    "EIA_StorFields_locs.reset_index(drop=True,inplace=True)\n",
    "#display(EIA_StorFields_locs)\n",
    "\n",
    "#find lat/lon values from the locations array (matching based on gas field code)\n",
    "EIA_StorFields['Lat'] = 0\n",
    "EIA_StorFields['Lon'] = 0\n",
    "\n",
    "for ifield in np.arange(0, len(EIA_StorFields)):\n",
    "    imatch = np.where(EIA_StorFields_locs['fld_code'] == EIA_StorFields.loc[ifield,'Gas Field Code'])[0]\n",
    "    if len(imatch) == 1:\n",
    "        EIA_StorFields.loc[ifield,'Lat'] = EIA_StorFields_locs.loc[imatch[0],'Latitude']\n",
    "        EIA_StorFields.loc[ifield,'Lon'] = EIA_StorFields_locs.loc[imatch[0],'Longitude']                    \n",
    "    elif len(imatch) > 1: \n",
    "        new_match = np.where((EIA_StorFields_locs['fld_code'] == EIA_StorFields.loc[ifield,'Gas Field Code']) &\\\n",
    "                             (EIA_StorFields_locs['res_code'] == EIA_StorFields.loc[ifield,'Reservoir Code']))[0]\n",
    "        #print(new_match)\n",
    "        #display(EIA_StorFields.iloc[imatch,:])\n",
    "        if len(new_match) >0:\n",
    "            EIA_StorFields.loc[ifield,'Lat'] = EIA_StorFields_locs.loc[new_match[0],'Latitude']\n",
    "            EIA_StorFields.loc[ifield,'Lon'] = EIA_StorFields_locs.loc[new_match[0],'Longitude']\n",
    "        #if len(new_match) ==0:\n",
    "        #    display(EIA_StorFields.iloc[ifield,:])\n",
    "for iyear in np.arange(0, num_years):\n",
    "    fields_ongrid = 0\n",
    "    fields_nongrid = 0\n",
    "    temp_data = EIA_StorFields[EIA_StorFields['Year'] == year_range[iyear]]\n",
    "    temp_data.reset_index(drop=True,inplace=True)               \n",
    "    for ifield in np.arange(0,len(temp_data)):\n",
    "        if temp_data['Lon'][ifield] > Lon_left and temp_data['Lon'][ifield] < Lon_right \\\n",
    "            and temp_data['Lat'][ifield] > Lat_low and temp_data['Lat'][ifield] < Lat_up:\n",
    "            ilat = int((temp_data['Lat'][ifield] - Lat_low)/Res01)\n",
    "            ilon = int((temp_data['Lon'][ifield] - Lon_left)/Res01)\n",
    "            Map_Storage[ilat,ilon,iyear] += temp_data.loc[ifield, 'Total Field Capacity(Mcf)']\n",
    "            fields_ongrid +=1\n",
    "        else:\n",
    "            if temp_data.loc[ifield, 'Report State '] in ('|'.join(['AK','HI'])): \n",
    "                #only include AK/HI fields in the 'non-grid' caetgory. This essentially ignores any fields where locations were not found\n",
    "                Map_Storage_nongrid[iyear] += temp_data.loc[ifield, 'Total Field Capacity(Mcf)']  \n",
    "                #display(temp_data.iloc[ifield,:])\n",
    "                fields_nongrid +=1\n",
    "    print('Year',year_range[iyear])\n",
    "    print('Total Gas Capacity (mcf) ongrid: ', np.sum(Map_Storage[:,:,iyear]), 'fields:',fields_ongrid)\n",
    "    print('Total Gas Capacity (mcf) offgrid:', np.sum(Map_Storage_nongrid[iyear]),'fields:',fields_nongrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Step 3. Read In EPA GHGI Data\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1. Transmission & Storage Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Emissions are in units of MG (= 1x10-6 Tg)\n",
    "\n",
    "names = pd.read_excel(EPA_NG_inputfile, sheet_name = \"Inventory Emissions\", usecols = \"A:AG\", skiprows = 5, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "EPA_emi_ts_NG = pd.read_excel(EPA_NG_inputfile, sheet_name = \"Inventory Emissions\", usecols = \"A:AG\", skiprows = 149, names = colnames, nrows = 54)\n",
    "EPA_emi_ts_NG= EPA_emi_ts_NG.drop(columns = ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 3'])\n",
    "EPA_emi_ts_NG['Source']= EPA_emi_ts_NG['Source'].str.replace(r\"\\(\",\"\")\n",
    "EPA_emi_ts_NG['Source']= EPA_emi_ts_NG['Source'].str.replace(r\"\\)\",\"\")\n",
    "EPA_emi_ts_NG['Source']= EPA_emi_ts_NG['Source'].str.replace(r\"+\",\"\")\n",
    "EPA_emi_ts_NG = EPA_emi_ts_NG.fillna('')\n",
    "EPA_emi_ts_NG = EPA_emi_ts_NG.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_emi_ts_NG.reset_index(inplace=True, drop=True)\n",
    "display(EPA_emi_ts_NG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1.2. Read in Total Transmission and Storage Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in total Transmission and Storage emissions (with methane reductions accounted for)\n",
    "# data are in kt\n",
    "\n",
    "names = pd.read_excel(EPA_NG_inputfile, sheet_name = \"SUMMARY CH4\", usecols = \"A:AD\", skiprows = 10, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "EPA_emi_total_NG_CH4 = pd.read_excel(EPA_NG_inputfile, sheet_name = \"SUMMARY CH4\", usecols = \"A:AD\", skiprows = 17, names = colnames, nrows = 5)\n",
    "EPA_emi_total_NG_CH4.rename(columns={EPA_emi_total_NG_CH4.columns[0]:'Source'}, inplace=True)\n",
    "EPA_emi_total_NG_CH4 = EPA_emi_total_NG_CH4.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_emi_total_NG_CH4.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\"EPA GHGI Emissions with Reductions (kt)\")\n",
    "display(EPA_emi_total_NG_CH4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.1.3 Read in and Format NG GasSTAR Reductions (kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in and format Gas STAR reductions data (units of Mg, converted here to kt)\n",
    "# For NG CH4, current reductions include those for Gas Engines, Compressor Starts, and 'Other'\n",
    "\n",
    "# get column names from top of spreadsheet\n",
    "col_range = 'A:AG'\n",
    "names = pd.read_excel(EPA_NG_inputfile, sheet_name = \"Gas STAR Reductions\", usecols = col_range, skiprows = 5, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "\n",
    "# Load full Gas STAR page and save required reductions\n",
    "EPA_Gas_STAR_NG_CH4 = pd.read_excel(EPA_NG_inputfile, sheet_name = \"Gas STAR Reductions\", usecols = col_range, skiprows = 76, names = colnames, nrows = 57)\n",
    "EPA_Gas_STAR_NG_CH4 = EPA_Gas_STAR_NG_CH4.fillna('')\n",
    "EPA_Gas_STAR_NG_CH4['Source']= EPA_Gas_STAR_NG_CH4['Source'].str.replace(r\"\\(\",\"\")\n",
    "EPA_Gas_STAR_NG_CH4['Source']= EPA_Gas_STAR_NG_CH4['Source'].str.replace(r\"\\)\",\"\")\n",
    "EPA_Gas_STAR_NG_CH4['Source']= EPA_Gas_STAR_NG_CH4['Source'].str.replace(r\"+\",\"\")\n",
    "EPA_Gas_STAR_NG_CH4 = EPA_Gas_STAR_NG_CH4[EPA_Gas_STAR_NG_CH4['Unnamed: 0'].str.contains('Engines|Other')]\n",
    "EPA_Gas_STAR_NG_CH4= EPA_Gas_STAR_NG_CH4.drop(columns = ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 3'])\n",
    "EPA_Gas_STAR_NG_CH4 = EPA_Gas_STAR_NG_CH4.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_Gas_STAR_NG_CH4.reset_index(inplace=True, drop = True)\n",
    "EPA_Gas_STAR_NG_CH4.loc[1,'Source'] = 'Other'\n",
    "print('EPA GHGI Gas STAR Reductions (row 0-1 in Mg):')\n",
    "display(EPA_Gas_STAR_NG_CH4)\n",
    "\n",
    "#Last row (OTHER REDUCTIONS) apply to pipelines, dehydrator vents, and transmission station venting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Need to break down 'Other' reductions into their subcateogries (2013 data held constant moving forward)\n",
    "# NOTE: This code can be changed in future years where this 'other' category is broken down in the Gas STAR tab\n",
    "# NOTE: Current code has hardcoded rows (not ideal and should be changed in the future)\n",
    "\n",
    "# get column names from top of spreadsheet\n",
    "col_range = 'A:AF'\n",
    "names = pd.read_excel(EPA_NG_inputfile, sheet_name = \"Data Input\", usecols = col_range, skiprows = 5, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "\n",
    "# Load full Gas STAR page and save required reductions\n",
    "EPA_input_gasstar = pd.read_excel(EPA_NG_inputfile, sheet_name = \"Data Input\", usecols = col_range, skiprows = 86, names = colnames, nrows = 16)\n",
    "EPA_input_gasstar = EPA_input_gasstar.fillna('')\n",
    "EPA_input_gasstar['Input']= EPA_input_gasstar['Input'].str.replace(r\"\\(\",\"\")\n",
    "EPA_input_gasstar['Input']= EPA_input_gasstar['Input'].str.replace(r\"\\)\",\"\")\n",
    "EPA_input_gasstar['Input']= EPA_input_gasstar['Input'].str.replace(r\"+\",\"\")\n",
    "EPA_input_gasstar= EPA_input_gasstar.drop(columns = ['Input No.', 'Units'])\n",
    "EPA_input_gasstar.reset_index(inplace=True, drop = True)\n",
    "#display(EPA_input_gasstar)\n",
    "\n",
    "pipeline_red = EPA_input_gasstar[EPA_input_gasstar['Input'].str.contains('Transmission: Pipeline Leaks')]\n",
    "pipeline_red = pipeline_red.drop(columns = [*range(1990, start_year,1)])\n",
    "pipeline_red = pipeline_red.iloc[:,1:].sum(axis=0)\n",
    "\n",
    "dehy_vents_red = EPA_input_gasstar[EPA_input_gasstar['Input'].str.contains('Dehy Vents 1 Year|Dehy Vents Ongoing')]\n",
    "dehy_vents_red.reset_index(inplace=True, drop = True)\n",
    "dehy_vents_red.iloc[1,1:] = dehy_vents_red.iloc[1,1:].cumsum()\n",
    "dehy_vents_red = dehy_vents_red.drop(columns = [*range(1990, start_year,1)])\n",
    "dehy_vents_red = dehy_vents_red.iloc[:,1:].sum(axis=0)\n",
    "\n",
    "stat_vent_red = EPA_input_gasstar[EPA_input_gasstar['Input'].str.contains('Stat Vent 1 Year|Stat Vent Ongoing')]\n",
    "stat_vent_red.reset_index(inplace=True, drop = True)\n",
    "stat_vent_red.iloc[1,1:] = stat_vent_red.iloc[1,1:].cumsum()\n",
    "stat_vent_red = stat_vent_red.drop(columns = [*range(1990, start_year,1)])\n",
    "stat_vent_red = stat_vent_red.iloc[:,1:].sum(axis=0)\n",
    "\n",
    "#set all later years to 2013 values\n",
    "for iyear in np.arange(2,num_years):\n",
    "    pipeline_red[year_range[iyear]] = pipeline_red[2013]\n",
    "    dehy_vents_red[year_range[iyear]] = dehy_vents_red[2013]\n",
    "    stat_vent_red[year_range[iyear]] = stat_vent_red[2013]\n",
    "    \n",
    "#covert values to correct units & reset index\n",
    "pipeline_red = pipeline_red*(19.26/1000)\n",
    "dehy_vents_red = dehy_vents_red*(19.26/1000)\n",
    "stat_vent_red = stat_vent_red*(19.26/1000)\n",
    "pipeline_red.reset_index(inplace=True, drop = True)\n",
    "dehy_vents_red.reset_index(inplace=True, drop = True)\n",
    "stat_vent_red.reset_index(inplace=True, drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply GasSTAR reductions to emissions from \n",
    "# Gas Engines, Pipelines, transmission station venting, and Dehydrator Vents. \n",
    "## NOTE: negative values are a known issue by ERG (updates are currently being made to the 2022 GHGI) \n",
    "# units in Mg\n",
    "\n",
    "print('Net Corrected National Emissions (Mg)')\n",
    "#correct gas engine emissions (subtract the reduction from national totals)\n",
    "emi_temp = EPA_emi_ts_NG[EPA_emi_ts_NG['Source'] == 'Engines Transmission']\n",
    "emi_temp.reset_index(inplace=True, drop = True)\n",
    "red_temp = EPA_Gas_STAR_NG_CH4[EPA_Gas_STAR_NG_CH4['Source'].str.contains('Engines Transmission')]\n",
    "for iyear in np.arange(0,num_years):\n",
    "    EPA_emi_ts_NG.loc[EPA_emi_ts_NG['Source']=='Engines Transmission',year_range[iyear]] = emi_temp.loc[0,year_range[iyear]]- red_temp.loc[0,year_range[iyear]]\n",
    "display(EPA_emi_ts_NG.loc[EPA_emi_ts_NG['Source']=='Engines Transmission', :])\n",
    "\n",
    "#correct pipeline leak emissions\n",
    "emi_temp = EPA_emi_ts_NG[EPA_emi_ts_NG['Source'] == 'Pipeline Leaks']\n",
    "emi_temp.reset_index(inplace=True, drop = True)\n",
    "red_temp = pipeline_red\n",
    "for iyear in np.arange(0,num_years):\n",
    "    EPA_emi_ts_NG.loc[EPA_emi_ts_NG['Source']=='Pipeline Leaks',year_range[iyear]] = emi_temp.loc[0,year_range[iyear]]- red_temp[iyear] \n",
    "display(EPA_emi_ts_NG[EPA_emi_ts_NG['Source'] == 'Pipeline Leaks'])\n",
    "\n",
    "#correct Dehydrator vent emissions\n",
    "emi_temp = EPA_emi_ts_NG[EPA_emi_ts_NG['Source'] == 'Dehydrator vents Transmission']\n",
    "emi_temp.reset_index(inplace=True, drop = True)\n",
    "red_temp = dehy_vents_red\n",
    "for iyear in np.arange(0,num_years):\n",
    "    EPA_emi_ts_NG.loc[EPA_emi_ts_NG['Source']=='Dehydrator vents Transmission',year_range[iyear]] = emi_temp.loc[0,year_range[iyear]]- red_temp[iyear] \n",
    "display(EPA_emi_ts_NG[EPA_emi_ts_NG['Source'] == 'Dehydrator vents Transmission'])\n",
    "\n",
    "#correct Dehydrator vent emissions\n",
    "emi_temp = EPA_emi_ts_NG[EPA_emi_ts_NG['Source'] == 'Station Venting Transmission']\n",
    "emi_temp.reset_index(inplace=True, drop = True)\n",
    "red_temp = stat_vent_red\n",
    "for iyear in np.arange(0,num_years):\n",
    "    EPA_emi_ts_NG.loc[EPA_emi_ts_NG['Source']=='Station Venting Transmission',year_range[iyear]] = emi_temp.loc[0,year_range[iyear]]- red_temp[iyear] \n",
    "display(EPA_emi_ts_NG[EPA_emi_ts_NG['Source'] == 'Station Venting Transmission'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Code that can be adapted for future inventory versions\n",
    "\n",
    "EPA_emi_red_NG_CH4 = EPA_Gas_STAR_NG_CH4[EPA_Gas_STAR_NG_CH4['Unnamed: 0'].str.contains('Gas Engines|Compressor Starts|Reduction: Other|Scaling factor')]\n",
    "EPA_emi_red_NG_CH4= EPA_emi_red_NG_CH4.drop(columns = ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 3'])\n",
    "EPA_emi_red_NG_CH4 = EPA_emi_red_NG_CH4.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_emi_red_NG_CH4.reset_index(inplace=True, drop = True)\n",
    "print('EPA GHGI Gas STAR Reductions (row 0-1 in Mg):')\n",
    "display(EPA_emi_red_NG_CH4)\n",
    "\n",
    "#Calculate Reductions for Non-Associated Gas production sources\n",
    "Emi_red_NonAssoc_NG_CH4 = EPA_emi_red_NG_CH4[EPA_emi_red_NG_CH4['Source'].str.contains('Gas Engines|Compressor Starts')]\n",
    "start_year_idx = Emi_red_NonAssoc_NG_CH4.columns.get_loc(start_year)\n",
    "Emi_red_NonAssoc_NG_CH4 = Emi_red_NonAssoc_NG_CH4.iloc[:,start_year_idx:].sum(axis=0)/float(1000) #convert to kt\n",
    "Emi_red_NonAssoc_total_NG_CH4 = Emi_red_NonAssoc_NG_CH4\n",
    "print('EPA GHGI Non-Assoc. Reductions (kt):')\n",
    "display(Emi_red_NonAssoc_total_NG_CH4)\n",
    "\n",
    "#Calculate reduction for 'Other' production sources\n",
    "Emi_red_Other_NG_CH4 = EPA_emi_red_NG_CH4.iloc[2,start_year_idx:]/float(1000) \n",
    "Emi_red_scale_NG_CH4 = EPA_emi_red_NG_CH4.iloc[3,start_year_idx:] #read in scaling factor for 'Other' reductions\n",
    "Emi_red_Other_total_NG_CH4 = Emi_red_Other_NG_CH4*Emi_red_scale_NG_CH4   #scale 'Other' reductions\n",
    "print('EPA GHGI Other Reductions (kt):')\n",
    "display(Emi_red_Other_total_NG_CH4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.1.4. Read In and Format NG Regulation Reductions (kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no transmission and storage regulatory emission reductions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Split Generator Emissions into Storage vs. Transmission contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the fraction of generator emissions that occur at transmission vs. storage compressor stations\n",
    "# Calculate as the average ratio of the horsepower of engines and turbines at transmission stations relative to at storage stations\n",
    "# In the GHGI: Horsepower data is calcualted for 1992 based on GRI study. Factors relative to 1992 are applied to 1992 values to complete the timeseries\n",
    "\n",
    "names = pd.read_excel(EPA_NG_inputfile, sheet_name = \"Activity Factors\", usecols = \"B:AI\", skiprows = 6, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "EPA_Gen_AD = pd.read_excel(EPA_NG_inputfile, sheet_name = \"Activity Factors\", usecols = \"B:AI\", skiprows = 47, names = colnames, nrows = 4)\n",
    "EPA_Gen_AD['Source']= EPA_Gen_AD['Source'].str.replace(r\"\\(\",\"\")\n",
    "EPA_Gen_AD['Source']= EPA_Gen_AD['Source'].str.replace(r\"\\)\",\"\")\n",
    "EPA_Gen_AD = EPA_Gen_AD.drop(columns = ['Unnamed: 1','Unnamed: 2', 'Unnamed: 3', 'Units'])\n",
    "EPA_Gen_AD = EPA_Gen_AD.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_Gen_AD.reset_index(inplace=True, drop=True)\n",
    "\n",
    "frac_gen_trans = np.zeros([num_years])\n",
    " \n",
    "eng_trans = EPA_Gen_AD.loc[EPA_Gen_AD['Source'].str.contains('Engines Transmission')]\n",
    "turb_trans = EPA_Gen_AD.loc[EPA_Gen_AD['Source'].str.contains('Turbines Transmission')]\n",
    "eng_stor = EPA_Gen_AD.loc[EPA_Gen_AD['Source'].str.contains('Engines Storage')]\n",
    "turb_stor = EPA_Gen_AD.loc[EPA_Gen_AD['Source'].str.contains('Turbines Storage')]\n",
    "\n",
    "print('Fraction of Generator Emissions from Transmission Stations (relative to Storage Stations):')\n",
    "for iyear in np.arange(0, num_years):\n",
    "    frac_gen_trans[iyear] = ((eng_trans.iloc[0,iyear+1]/(eng_trans.iloc[0,iyear+1] + eng_stor.iloc[0,iyear+1])) +\\\n",
    "                            (turb_trans.iloc[0,iyear+1]/(turb_trans.iloc[0,iyear+1] + turb_stor.iloc[0,iyear+1]))) / 2\n",
    "    \n",
    "    print('Year', year_range_str[iyear], ':', frac_gen_trans[iyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Split Emissions into Gridding Groups (each Group will have the same proxy applied during the gridding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Final Emissions in Units of kt\n",
    "# Use mapping proxy and source files to split the GHGI emissions\n",
    "DEBUG=1\n",
    "\n",
    "start_year_idx = EPA_emi_ts_NG.columns.get_loc(start_year)\n",
    "end_year_idx = EPA_emi_ts_NG.columns.get_loc(end_year)+1\n",
    "sum_emi = np.zeros(num_years)\n",
    "\n",
    "ghgi_ts_groups = ghgi_ts_map['GHGI_Emi_Group'].unique()\n",
    "\n",
    "for igroup in np.arange(0,len(ghgi_ts_groups)): #loop through all groups, finding the GHGI sources in that group and summing emissions for that region, year\n",
    "        vars()[ghgi_ts_groups[igroup]] = np.zeros([num_years])\n",
    "        source_temp = ghgi_ts_map.loc[ghgi_ts_map['GHGI_Emi_Group'] == ghgi_ts_groups[igroup], 'GHGI_Source']\n",
    "        pattern_temp  = '|'.join(source_temp)\n",
    "        ##DEBUG## display(pattern_temp)\n",
    "        emi_temp = EPA_emi_ts_NG[EPA_emi_ts_NG['Source'].str.contains(pattern_temp)]\n",
    "        # make sure to use the correct transmission and storage station data for the correct GHGI group\n",
    "        if 'Station Total Emissions' in pattern_temp:\n",
    "            if 'Trans' in ghgi_ts_groups[igroup]:\n",
    "                emi_temp = emi_temp.drop(emi_temp.index[1])\n",
    "            elif 'Stat' in ghgi_ts_groups[igroup]:\n",
    "                emi_temp = emi_temp.drop(emi_temp.index[0])\n",
    "        ##DEBUG## display(emi_temp)\n",
    "        vars()[ghgi_ts_groups[igroup]][:] = np.where(emi_temp.iloc[:,start_year_idx:] =='',[0],emi_temp.iloc[:,start_year_idx:]).sum(axis=0)/float(1000) #convert Mg to kt\n",
    "\n",
    "#Check against total summary emissions \n",
    "print('QA/QC #1: Check Transmission and Storage Emission Sum against GHGI Summary Emissions')\n",
    "for iyear in np.arange(0,num_years): \n",
    "    for igroup in np.arange(0,len(ghgi_ts_groups)):\n",
    "        sum_emi[iyear] += vars()[ghgi_ts_groups[igroup]][iyear]       \n",
    "    summary_emi = EPA_emi_total_NG_CH4.iloc[3,iyear+1]  \n",
    "    #Check 1 - make sure that the sums from all the regions equal the totals reported\n",
    "    diff1 = abs(sum_emi[iyear] - summary_emi)/((sum_emi[iyear] + summary_emi)/2)\n",
    "    if DEBUG==1:\n",
    "        print(summary_emi)\n",
    "        print(sum_emi[iyear])\n",
    "    if diff1 < 0.0001:\n",
    "        print('Year ', year_range[iyear],': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear],': FAIL (check Production & summary tabs): ', diff1,'%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Step 4. Grid Data (using spatial proxies)\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step. 4.1. Calculate the monthly and regional weighted arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1.1 Assign the Appropriate Proxy Variable Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names on the *left* need to match the 'NaturalGas_TransmissionStorage_ProxyMapping' 'Proxy_Group' names \n",
    "# (these are initialized in Step 2). \n",
    "# The names on the right are the variable names used to caluclate the proxies in this code.\n",
    "# Names on the *right* need to match those from the code in Step 2.\n",
    "\n",
    "#\n",
    "Map_TransCompStations = map_TransCompStations\n",
    "Map_StorageCompStations = map_StorStations\n",
    "Map_TransPipelines = Map_EnvTrans_pipelines\n",
    "Map_InputTerminals = map_InputTerminals\n",
    "Map_ExportTerminals = map_ExportTerminals\n",
    "Map_StorageWells = Map_Storage\n",
    "Map_FarmPipelines = Map_Farm_pipelines#Map_EnvTrans_pipelines \n",
    "Map_LNGStorage = Map_LNGstations\n",
    "Map_Generators = np.ones([len(Lat_01),len(Lon_01),num_years])  ## National Emissions split into Transmission and Storage contributions below\n",
    "\n",
    "Map_TransCompStations_nongrid = map_TransCompStations_nongrid\n",
    "Map_StorageCompStations_nongrid = map_StorStations_nongrid\n",
    "Map_TransPipelines_nongrid = Map_EnvTrans_pipelines_nongrid\n",
    "Map_InputTerminals_nongrid = map_InputTerminals_nongrid\n",
    "Map_ExportTerminals_nongrid = map_ExportTerminals_nongrid\n",
    "Map_StorageWells_nongrid = Map_Storage_nongrid\n",
    "Map_FarmPipelines_nongrid = Map_Farm_pipelines_nongrid#Map_EnvTrans_pipelines_nongrid gg\n",
    "Map_LNGStorage_nongrid = Map_LNGstations_nongrid\n",
    "Map_Generators_nongrid = np.ones([num_years]) ## National Emissions split into Transmission and Storage contributions below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1.2 Calculate the fractional proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate weighting arrays\n",
    "# Find the fraction of processing plants in each grid cell, relative to the total counts (on and off grid)\n",
    "# also weight by the number of days in each year\n",
    "\n",
    "proxy_ts_map_unique = np.unique(proxy_ts_map['Proxy_Group'])\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "        month_days = month_day_leap\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        month_days = month_day_nonleap  \n",
    "    \n",
    "    #Step 1a: weighted proxy ongrid = ongrid proxy * days each year\n",
    "    #Step 1b: weighted proxy offgrid = offgrid proxy * days each year\n",
    "    #Step 2a: noramlized weighted proxy ongrid = weighted proxy in each grid cell / (sum weighted proxy ongrid + weighted proxy offgrid)\n",
    "    #Step 2b: noramlized weighted proxy offgrid = weighted proxy offgrid / (sum weighted proxy ongrid + weighted proxy offgrid)\n",
    "    print('Check Sum of T & S Proxy Arrays = 1 for: ', year_range[iyear])\n",
    "    for iproxy in np.arange(0,len(proxy_ts_map_unique)):\n",
    "        vars()[proxy_ts_map.loc[iproxy,'Proxy_Group']][:,:,iyear] *= np.sum(month_days)\n",
    "        vars()[proxy_ts_map.loc[iproxy,'Proxy_Group']+'_nongrid'][iyear] *= np.sum(month_days)\n",
    "        temp_sum = float(np.sum(vars()[proxy_ts_map.loc[iproxy,'Proxy_Group']][:,:,iyear]) + \\\n",
    "                    np.sum(vars()[proxy_ts_map.loc[iproxy,'Proxy_Group']+'_nongrid'][iyear]))\n",
    "        vars()[proxy_ts_map.loc[iproxy,'Proxy_Group']][:,:,iyear] = \\\n",
    "                    data_fn.safe_div(vars()[proxy_ts_map.loc[iproxy,'Proxy_Group']][:,:,iyear], temp_sum)\n",
    "        vars()[proxy_ts_map.loc[iproxy,'Proxy_Group']+'_nongrid'][iyear] = \\\n",
    "                    data_fn.safe_div(vars()[proxy_ts_map.loc[iproxy,'Proxy_Group']+'_nongrid'][iyear], temp_sum)\n",
    "        proxy_sum = np.sum(vars()[proxy_ts_map.loc[iproxy,'Proxy_Group']][:,:,iyear])+np.sum(vars()[proxy_ts_map.loc[iproxy,'Proxy_Group']+'_nongrid'][iyear])\n",
    "        if proxy_sum >1.0001 or proxy_sum <0.9999:\n",
    "            print('CHECK ', proxy_ts_map.loc[iproxy,'Proxy_Group'],': ', proxy_sum)   \n",
    "        else:\n",
    "            print('PASS:', proxy_ts_map.loc[iproxy,'Proxy_Group'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step. 4.2. Grid the National Emissions Data, then Calculate 0.1x0.1 degree flux maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate emissions as Emissions = GHGI emissions * Proxy Map\n",
    "\n",
    "# For transmission pipelines and Transmission compressor stations, AK/HI fraction of National emissions needs to be removed\n",
    "# prior to gridding\n",
    "# For national generator emissions, emissions need to be split between the transmission and storage station categories\n",
    "# (based on relative activity data from the National GHGI) and then gridded to each compressor station category accordingly\n",
    "\n",
    "Emissions = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Emissions_nongrid = np.zeros([num_years])\n",
    "Emi_not_mapped_sum = np.zeros(num_years)\n",
    "CONUS_red= np.zeros(num_years)\n",
    "DEBUG =1\n",
    "\n",
    "#loop through each emission group, where: Gridded emissions = National emissions * proxy map\n",
    "for igroup in np.arange(0,len(proxy_ts_map)):\n",
    "    vars()['Ext_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "    vars()['Ext_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "    for iyear in np.arange(0,num_years):\n",
    "        if proxy_ts_map.loc[igroup,'Proxy_Group'] == 'Map_TransCompStations': \n",
    "            #Remove AK/HI fraction before gridding\n",
    "            vars()['Ext_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += \\\n",
    "                     (vars()[proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][iyear] -\\\n",
    "                     (vars()[proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][iyear]* CONUS_transstat_ratio[iyear])) * \\\n",
    "                     vars()[proxy_ts_map.loc[igroup,'Proxy_Group']][:,:,iyear]\n",
    "            vars()['Ext_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear] += vars()[proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][iyear]* CONUS_transstat_ratio[iyear]\n",
    "        elif proxy_ts_map.loc[igroup,'Proxy_Group'] == 'Map_TransPipelines': \n",
    "            #Remove AK/HI fraction before gridding\n",
    "            vars()['Ext_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += \\\n",
    "                     (vars()[proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][iyear] -\\\n",
    "                     (vars()[proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][iyear]* CONUS_transpipe_ratio)) * \\\n",
    "                     vars()[proxy_ts_map.loc[igroup,'Proxy_Group']][:,:,iyear]\n",
    "            vars()['Ext_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear] += vars()[proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][iyear]* CONUS_transpipe_ratio\n",
    "        elif proxy_ts_map.loc[igroup,'Proxy_Group'] == 'Map_Generators': \n",
    "            #need to split between Storage & Trans stations\n",
    "            # = GHGI * Trans frac * Map_TransCompStations + GHGI * Storage frac * Map_StatCompStations\n",
    "            vars()['Ext_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += \\\n",
    "                (vars()[proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][iyear] * frac_gen_trans[iyear] * Map_TransCompStations[:,:,iyear]) +\\\n",
    "                (vars()[proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][iyear] * (1-frac_gen_trans[iyear]) * Map_StorageCompStations[:,:,iyear])\n",
    "            vars()['Ext_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear] += \\\n",
    "                (vars()[proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][iyear] * frac_gen_trans[iyear] * Map_TransCompStations_nongrid[iyear]) +\\\n",
    "                (vars()[proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][iyear] * (1-frac_gen_trans[iyear]) * Map_StorageCompStations_nongrid[iyear])\n",
    "\n",
    "        elif proxy_ts_map.loc[igroup,'Proxy_Group'] == 'Map_StorageWells':\n",
    "            #deal with Aliso Canyon storage event (one time event added to 'Inventory Emissions' tab in GHGI workbook)\n",
    "            #allocate Aliso Canyon emissions to the appropriate grid cell\n",
    "            #lat/lon of ALiso Canyon storage facility (from google maps)\n",
    "            ilat = int((34.31307 - Lat_low)/Res01)\n",
    "            ilon = int((-118.56462 - Lon_left)/Res01)\n",
    "            aliso_red = 0\n",
    "            if year_range[iyear] == 2015:\n",
    "                aliso_red = 78.350 #kt\n",
    "            elif year_range[iyear]==2016:\n",
    "                aliso_red = 21.288 #kt\n",
    "            ghgi_temp = vars()[proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "            ghgi_temp -= aliso_red\n",
    "            vars()['Ext_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][ilat,ilon,iyear] += aliso_red\n",
    "            #allocate remaining emissions based on storage well proxy\n",
    "            vars()['Ext_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += \\\n",
    "                ghgi_temp * vars()[proxy_ts_map.loc[igroup,'Proxy_Group']][:,:,iyear]\n",
    "            vars()['Ext_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear] += \\\n",
    "                ghgi_temp * vars()[proxy_ts_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear]\n",
    "        else:\n",
    "            vars()['Ext_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += \\\n",
    "                vars()[proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                vars()[proxy_ts_map.loc[igroup,'Proxy_Group']][:,:,iyear]\n",
    "            vars()['Ext_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear] += \\\n",
    "                vars()[proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                vars()[proxy_ts_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear]\n",
    "        \n",
    "        #DEBUG# print(igroup)\n",
    "        #DEBU# print(vars()[proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][iyear])\n",
    "        #DEBUG# print(np.sum(vars()['Flux_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])+\\\n",
    "        #DEBUG #        vars()['Flux_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear])\n",
    "        #if proxy_ts_map.loc[igroup,'Proxy_Group'] == 'Map_StorageWells':\n",
    "\n",
    "        Emissions[:,:,iyear] += vars()['Ext_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]\n",
    "        Emissions_nongrid[iyear] += vars()['Ext_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear]\n",
    "\n",
    "    \n",
    "# QA/QC gridded emissions\n",
    "# Check sum of all gridded emissions + emissions not included in gridding (e.g., AK), and other non-gridded areas\n",
    "print('QA/QC #1: Check weighted emissions against GHGI')   \n",
    "for iyear in np.arange(0,num_years):\n",
    "    calc_emi = 0\n",
    "    summary_emi = EPA_emi_total_NG_CH4.iloc[3,iyear+1]\n",
    "    calc_emi =  np.sum(Emissions[:,:,iyear]) + Emissions_nongrid[iyear] # +Emi_not_mapped_sum[iyear]# \n",
    "    if DEBUG ==1:\n",
    "        print(summary_emi)\n",
    "        print(calc_emi)\n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0001:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2.2 Save gridded emissions (kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save gridded emissions for each gridding group - for extension\n",
    "\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(grid_emi_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "unique_groups = np.unique(proxy_ts_map['GHGI_Emi_Group'])\n",
    "unique_groups = unique_groups[unique_groups != 'Emi_not_mapped']\n",
    "\n",
    "nc_out = Dataset(grid_emi_outputfile, 'r+', format='NETCDF4')\n",
    "\n",
    "for igroup in np.arange(0,len(unique_groups)):\n",
    "    print('Ext_'+unique_groups[igroup])\n",
    "    if len(np.shape(vars()['Ext_'+unique_groups[igroup]])) ==4:\n",
    "        ghgi_temp = np.sum(vars()[unique_groups[igroup]],axis=3) #sum month data if data is monthly\n",
    "    else:\n",
    "        ghgi_temp = vars()['Ext_'+unique_groups[igroup]]\n",
    "\n",
    "    # Write data to netCDF\n",
    "    data_out = nc_out.createVariable('Ext_'+unique_groups[igroup], 'f8', ('lat', 'lon','year'), zlib=True)\n",
    "    data_out[:,:,:] = ghgi_temp[:,:,:]\n",
    "\n",
    "#save nongrid data to calculate non-grid fraction extension\n",
    "data_out = nc_out.createVariable('Emissions_nongrid', 'f8', ('year'), zlib=True)  \n",
    "data_out[:] = Emissions_nongrid[:]\n",
    "nc_out.close()\n",
    "\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded emissions (kt) written to file: {}\" .format(os.getcwd())+grid_emi_outputfile)\n",
    "print(' ')\n",
    "\n",
    "del data_out, ghgi_temp, nc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3 Calculate Gridded Fluxes (molec/s/cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 -- Calculate fluxes (molec./s/cm2)\n",
    "DEBUG =1\n",
    "\n",
    "#Initialize arrays\n",
    "check_sum_annual = np.zeros([num_years])\n",
    "Flux_Emissions_Total_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "for igroup in np.arange(0,len(proxy_ts_map)):\n",
    "    vars()['Flux_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']+'_annual'] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "\n",
    "\n",
    "#Calculate fluxes\n",
    "for iyear in np.arange(0,num_years):\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "        month_days = month_day_leap\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        month_days = month_day_nonleap \n",
    "    \n",
    "    # calculate fluxes for annual data  (=kt * grams/kt *molec/mol *mol/g *s^-1 * cm^-2)\n",
    "    conversion_factor_annual = 10**9 * Avogadro / float(Molarch4 * np.sum(month_days) * 24 * 60 *60) / area_matrix_01\n",
    "    for igroup in np.arange(0,len(proxy_ts_map)):\n",
    "        vars()['Ext_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] *= conversion_factor_annual\n",
    "        vars()['Flux_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']+'_annual'][:,:,iyear] = vars()['Ext_'+proxy_ts_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]\n",
    "        Flux_Emissions_Total_annual[:,:,iyear] = Emissions[:,:,iyear]*conversion_factor_annual\n",
    "    check_sum_annual[iyear] += np.sum(Flux_Emissions_Total_annual[:,:,iyear]/conversion_factor_annual) #convert back to emissions to check at end\n",
    "\n",
    "print(' ')\n",
    "print('QA/QC #2: Check final gridded fluxes against GHGI')  \n",
    "# for the sum, check the converted annual emissions (convert back from flux) plus all the non-gridded emissions\n",
    "for iyear in np.arange(0,num_years):\n",
    "    calc_emi = check_sum_annual[iyear] + Emissions_nongrid[iyear]\n",
    "    summary_emi = EPA_emi_total_NG_CH4.iloc[3,iyear+1]\n",
    "    if DEBUG ==1:\n",
    "        print(calc_emi)\n",
    "        print(summary_emi)\n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0001:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 5. Write gridded (0.1⁰x0.1⁰) data to netCDF files.\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize netCDF files\n",
    "data_IO_fn.initialize_netCDF(gridded_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write the Data to netCDF\n",
    "nc_out = Dataset(gridded_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Total_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded annual natural gas transmission & storage fluxes written to file: {}\" .format(os.getcwd())+gridded_outputfile)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 6. Plot Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Plot Annual Emission Fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot annual emissions for each year\n",
    "scale_max = 10\n",
    "save_fig = 0\n",
    "save_file = ''\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_str, scale_max, save_fig, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Plot Difference Between First and Last Inventory Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_diff_str,save_flag,save_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Plot Activity Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Location Points\n",
    "\n",
    "# Activity_Map = 0.1x0.1 map of activity data (counts or absolute units)\n",
    "# Plot_Frac    = 0 or 1 (0= plot activity data in absolute counts, 1= plot fractional activity data)\n",
    "# Lat          = 0.1 degree Lat values (select range)\n",
    "# Lon          = 0.1 degree Lon values (select range)\n",
    "# year_range   = array of inventory years\n",
    "# title_str    = title of map\n",
    "# legend_str   = title of legend\n",
    "# scale_max    = maximum of color scale\n",
    "\n",
    "Activity_Map = Map_LNGstations#Map_Plants\n",
    "Plot_Frac = 1\n",
    "Lat = Lat_01\n",
    "Lon = Lon_01\n",
    "year_range = year_range\n",
    "title_str2 = \"Proxy - LNG Storage Station Capacity\"\n",
    "legend_str = \"Annual Fraction of National LNG Storage Station Capacity\"\n",
    "scale_max = 0.05\n",
    "\n",
    "for iyear in np.arange(0,len(year_range)): \n",
    "    my_cmap = copy(plt.cm.get_cmap('rainbow',lut=3000))\n",
    "    my_cmap._init()\n",
    "    slopen = 200\n",
    "    alphas_slope = np.abs(np.linspace(0, 1.0, slopen))\n",
    "    alphas_stable = np.ones(3003-slopen)\n",
    "    alphas = np.concatenate((alphas_slope, alphas_stable))\n",
    "    my_cmap._lut[:,-1] = alphas\n",
    "    my_cmap.set_under('gray', alpha=0)\n",
    "    \n",
    "    Lon_cor = Lon[50:632]-0.05\n",
    "    Lat_cor = Lat[43:300]-0.05\n",
    "    \n",
    "    xpoints = Lon_cor\n",
    "    ypoints = Lat_cor\n",
    "    yp,xp = np.meshgrid(ypoints,xpoints)\n",
    "    \n",
    "    if np.shape(Activity_Map)[0] == len(year_range):\n",
    "        if Plot_Frac ==1:\n",
    "            zp = Activity_Map[iyear,43:300,50:632]/np.sum(Activity_Map[iyear,:,:])\n",
    "        else:\n",
    "            zp = Activity_Map[iyear,43:300,50:632]\n",
    "    elif np.shape(Activity_Map)[2] == len(year_range):\n",
    "        if Plot_Frac ==1:\n",
    "            zp = Activity_Map[43:300,50:632,iyear]/np.sum(Activity_Map[:,:,iyear])\n",
    "        else: \n",
    "            zp = Activity_Map[43:300,50:632,iyear]\n",
    "    #zp = zp/float(10**6 * Avogadro) * (year_days * 24 * 60 * 60) * Molarch4 * float(1e10)\n",
    "    \n",
    "    fig, ax = plt.subplots(dpi=300)\n",
    "    m = Basemap(llcrnrlon=xp.min(), llcrnrlat=yp.min(), urcrnrlon=xp.max(),\n",
    "                urcrnrlat=yp.max(), projection='merc', resolution='h', area_thresh=5000)\n",
    "    m.drawmapboundary(fill_color='Azure')\n",
    "    m.fillcontinents(color='FloralWhite', lake_color='Azure',zorder=1)\n",
    "    m.drawcoastlines(linewidth=0.5,zorder=3)\n",
    "    m.drawstates(linewidth=0.25,zorder=3)\n",
    "    m.drawcountries(linewidth=0.5,zorder=3)\n",
    "        \n",
    "        #if Plot_Frac == 1:\n",
    "        #    scale_max \n",
    "    \n",
    "    xpi,ypi = m(xp,yp)\n",
    "    #plot = m.pcolor(xpi,ypi,zp.transpose(), cmap=my_cmap, vmin=10**-15, vmax=scale_max, snap=True,zorder=2)\n",
    "    plot = m.scatter(xpi,ypi,s=20,c=zp.transpose(),cmap=my_cmap,zorder=2,vmin = 10**-15,snap = True,vmax = scale_max)\n",
    "    cb = m.colorbar(plot, location = \"bottom\", pad = \"1%\")        \n",
    "    tick_locator = ticker.MaxNLocator(nbins=5)\n",
    "    cb.locator = tick_locator\n",
    "    cb.update_ticks()\n",
    "    \n",
    "    cb.ax.set_xlabel(legend_str,fontsize=10)\n",
    "    cb.ax.tick_params(labelsize=10)\n",
    "    Titlestring = str(year_range[iyear])+' '+title_str2\n",
    "    plt.title(Titlestring, fontsize=14);\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Heat Map\n",
    "\n",
    "#Map_TransPipelines\n",
    "\n",
    "# Activity_Map = 0.1x0.1 map of activity data (counts or absolute units)\n",
    "# Plot_Frac    = 0 or 1 (0= plot activity data in absolute counts, 1= plot fractional activity data)\n",
    "# Lat          = 0.1 degree Lat values (select range)\n",
    "# Lon          = 0.1 degree Lon values (select range)\n",
    "# year_range   = array of inventory years\n",
    "# title_str    = title of map\n",
    "# legend_str   = title of legend\n",
    "# scale_max    = maximum of color scale\n",
    "\n",
    "Activity_Map = Map_TransPipelines\n",
    "Plot_Frac = 1\n",
    "Lat = Lat_01\n",
    "Lon = Lon_01\n",
    "year_range = year_range\n",
    "title_str2 = \"Proxy - Transmission Pipeline Mileage\"\n",
    "legend_str = \"Annual Fraction of National Transmission Pipeline Mileage\"\n",
    "scale_max = 0.005\n",
    "\n",
    "for iyear in np.arange(0,len(year_range)): \n",
    "    my_cmap = copy(plt.cm.get_cmap('rainbow',lut=3000))\n",
    "    my_cmap._init()\n",
    "    slopen = 200\n",
    "    alphas_slope = np.abs(np.linspace(0, 1.0, slopen))\n",
    "    alphas_stable = np.ones(3003-slopen)\n",
    "    alphas = np.concatenate((alphas_slope, alphas_stable))\n",
    "    my_cmap._lut[:,-1] = alphas\n",
    "    my_cmap.set_under('gray', alpha=0)\n",
    "    \n",
    "    Lon_cor = Lon[50:632]-0.05\n",
    "    Lat_cor = Lat[43:300]-0.05\n",
    "    \n",
    "    xpoints = Lon_cor\n",
    "    ypoints = Lat_cor\n",
    "    yp,xp = np.meshgrid(ypoints,xpoints)\n",
    "    \n",
    "    if np.shape(Activity_Map)[0] == len(year_range):\n",
    "        if Plot_Frac ==1:\n",
    "            zp = Activity_Map[iyear,43:300,50:632]/np.sum(Activity_Map[iyear,:,:])\n",
    "        else:\n",
    "            zp = Activity_Map[iyear,43:300,50:632]\n",
    "    elif np.shape(Activity_Map)[2] == len(year_range):\n",
    "        if Plot_Frac ==1:\n",
    "            zp = Activity_Map[43:300,50:632,iyear]/np.sum(Activity_Map[:,:,iyear])\n",
    "        else: \n",
    "            zp = Activity_Map[43:300,50:632,iyear]\n",
    "    #zp = zp/float(10**6 * Avogadro) * (year_days * 24 * 60 * 60) * Molarch4 * float(1e10)\n",
    "    \n",
    "    fig, ax = plt.subplots(dpi=300)\n",
    "    m = Basemap(llcrnrlon=xp.min(), llcrnrlat=yp.min(), urcrnrlon=xp.max(),\n",
    "                urcrnrlat=yp.max(), projection='merc', resolution='h', area_thresh=5000)\n",
    "    m.drawmapboundary(fill_color='Azure')\n",
    "    m.fillcontinents(color='FloralWhite', lake_color='Azure',zorder=1)\n",
    "    m.drawcoastlines(linewidth=0.5,zorder=3)\n",
    "    m.drawstates(linewidth=0.25,zorder=3)\n",
    "    m.drawcountries(linewidth=0.5,zorder=3)\n",
    "        \n",
    "        #if Plot_Frac == 1:\n",
    "        #    scale_max \n",
    "    \n",
    "    xpi,ypi = m(xp,yp)\n",
    "    plot = m.pcolor(xpi,ypi,zp.transpose(), cmap=my_cmap, vmin=10**-15, vmax=scale_max, snap=True,zorder=2)\n",
    "    #plot = m.scatter(xpi,ypi,s=20,c=zp.transpose(),cmap=my_cmap,zorder=2,vmin = 10**-15,snap = True,vmax = scale_max)\n",
    "    cb = m.colorbar(plot, location = \"bottom\", pad = \"1%\")        \n",
    "    tick_locator = ticker.MaxNLocator(nbins=5)\n",
    "    cb.locator = tick_locator\n",
    "    cb.update_ticks()\n",
    "    \n",
    "    cb.ax.set_xlabel(legend_str,fontsize=10)\n",
    "    cb.ax.tick_params(labelsize=10)\n",
    "    Titlestring = str(year_range[iyear])+' '+title_str2\n",
    "    plt.title(Titlestring, fontsize=14);\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = datetime.now() \n",
    "ft = ct.timestamp() \n",
    "time_elapsed = (ft-it)/(60*60)\n",
    "print('Time to run: '+str(time_elapsed)+' hours')\n",
    "print('** GEPA_1B2b_TransmissionStorage: COMPLETE **')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

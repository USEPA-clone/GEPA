{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded EPA Methane Inventory\n",
    "## Category: 1A Mobile Combustion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Authors: \n",
    "Joannes D. Maasakkers, Erin E. McDuffie\n",
    "#### Date Last Updated: \n",
    "see Step 0\n",
    "#### Notebook Purpose: \n",
    "This Notebook calculates and reports annual gridded (0.1°x0.1°) methane emission fluxes (molec./cm2/s) from mobile combustion sources in the CONUS region between 2012-2018.    \n",
    "#### Summary & Notes:\n",
    "EPA GHGI mobile combustion emissions from on-road and non-highway emission sources are read in at the national level from the GHGI (file via personal communication). For on-road sources, national emissions for each vehicle type (e.g., passenger, light, heavy duty, diesel) are allocated to states based on vehicle miles traveled, as a function of vehicle type, road type (e.g., primary, secondary, other), and whether the road types are in rural or urban regions. The vehicle miles traveled as a function of vehicle type, road type, and region are derived from state-level datasets from the U.S. Department of Transportation, Federal Highway Administration. State-level on-road emissions (as a function of vehicle type, road type, and region) are then allocated to a 0.01°x0.01° grid using high resolution maps of urban and rural roads, as a function of road type, derived from U.S. Census and U.S. DOT Highway Performance Monitoring System data. National-level non-highway emissions are allocated to the 0.1°x0.1° grid using gridded source-specific proxies, including maps of navigable waterways, railroads, mine locations, crop areas, and population. All emissions are re-gridded to a 0.1°x0.1° grid. Emissions are converted to emission flux. Annual emission fluxes (molec./cm2/s) are written to final netCDFs in the ‘/code/Final_Gridded_Data/’ folder. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Step 0. Set-Up Notebook Modules, Functions, and Local Parameters and Constants\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm working directory\n",
    "import os\n",
    "import time\n",
    "modtime = os.path.getmtime('./1A_Combustion_Mobile.ipynb')\n",
    "modificationTime = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(modtime))\n",
    "print(\"This file was last modified on: \", modificationTime)\n",
    "print('')\n",
    "print(\"The directory we are working in is {}\" .format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Include plots within notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from copy import copy\n",
    "\n",
    "# Import additional modules\n",
    "# Load plotting package Basemap \n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# Load netCDF (for manipulating netCDF file types)\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Set up ticker\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#add path for the global function module (file)\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../Global_Functions/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Load Tabula (for reading tables from PDFs)\n",
    "import tabula as tb   \n",
    "    \n",
    "# Load user-defined global functions (modules)\n",
    "import data_load_functions as data_load_fn\n",
    "import data_functions as data_fn\n",
    "import data_IO_functions as data_IO_fn\n",
    "import data_plot_functions as data_plot_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT Files\n",
    "# Assign global file names\n",
    "global_filenames = data_load_fn.load_global_file_names()\n",
    "State_ANSI_inputfile = global_filenames[0]\n",
    "#County_ANSI_inputfile = global_filenames[1]\n",
    "pop_map_inputfile = global_filenames[2]\n",
    "Grid_area01_inputfile = global_filenames[3]\n",
    "Grid_area001_inputfile = global_filenames[4]\n",
    "Grid_state001_ansi_inputfile = global_filenames[5]\n",
    "#Grid_county001_ansi_inputfile = global_filenames[6]\n",
    "globalinputlocation = global_filenames[0][0:20]\n",
    "print(globalinputlocation)\n",
    "\n",
    "# Specify names of inputs files used in this notebook\n",
    "#EPA Data\n",
    "EPA_comb_inputfile = '../Global_InputData/GHGI/Ch3_Energy/Transport non-CO2.csv'\n",
    "\n",
    "#Proxy Data file\n",
    "MobComb_Mapping_inputfile = \"./InputData/MobileCombustion_ProxyMapping.xlsx\"\n",
    "\n",
    "\n",
    "#Activity Data\n",
    "#US DOT Federal Highway Statistics\n",
    "State_vmt_file = \"./InputData/vm2/vm2_\"\n",
    "State_vdf_file = \"./InputData/vm4/vm4_\"\n",
    "Primary_roads_file = \"./InputData/High_Resolution_Data/PrimaryRoads_\"\n",
    "Secondary_roads_file = \"./InputData/High_Resolution_Data/PrimarySecondaryRoads_\"\n",
    "CONUS_rural_roads_file = \"./InputData/High_Resolution_Data/CONUS_HPMS_Rural_Roads_001x001.csv\"\n",
    "CONUS_urban_roads_file = \"./InputData/High_Resolution_Data/CONUS_HPMS_Urban_Roads_001x001.csv\"\n",
    "Urban_area_file = \"./InputData/High_Resolution_Data/UrbanAreas_\"\n",
    "\n",
    "Waterways_file = './InputData/High_Resolution_Data/Navigable_Waterways_001x001.csv'\n",
    "Mines_file = '../Global_InputData/MSHA/Mines.txt'\n",
    "Railroad_file = './InputData/High_Resolution_Data/Railroads_'\n",
    "Crop_file = globalinputlocation+'Gridded/AllCrops_'\n",
    "\n",
    "#OUTPUT FILES\n",
    "gridded_outputfile = '../Final_Gridded_Data/EPA_v2_1A_Combustion_Mobile.nc'\n",
    "netCDF_description = 'Gridded EPA Inventory - Mobile Combustion Emissions - IPCC Source Category 1A'\n",
    "title_str = \"EPA methane emissions from mobile combustion\"\n",
    "title_diff_str = \"Emissions from mobile combustion difference: 2018-2012\"\n",
    "\n",
    "#output gridded proxy data\n",
    "grid_emi_outputfile = '../Final_Gridded_Data/Extension/v2_input_data/Combustion_Mobile_Grid_Emi.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPECIFY RECALCS\n",
    "\n",
    "#0 = don't recalcuate, 1 = re-calculate\n",
    "ReCalc_Crop =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local variables\n",
    "start_year = 2012  #First year in emission timeseries\n",
    "end_year = 2018    #Last year in emission timeseries\n",
    "year_range = [*range(start_year, end_year+1,1)] #List of emission years\n",
    "year_range_str=[str(i) for i in year_range]\n",
    "num_years = len(year_range)\n",
    "\n",
    "# Define constants\n",
    "Avogadro   = 6.02214129 * 10**(23)  #molecules/mol\n",
    "Molarch4   = 16.04                  #g/mol\n",
    "Res01      = 0.1                    # degrees\n",
    "Res_01     = 0.01\n",
    "tg_scale   = 0.001                  #Tg scale number [New file allows for the exclusion of the territories] \n",
    "\n",
    "# Continental US Lat/Lon Limits (for netCDF files)\n",
    "Lon_left = -130       #deg\n",
    "Lon_right = -60       #deg\n",
    "Lat_low  = 20         #deg\n",
    "Lat_up  = 55          #deg\n",
    "loc_dimensions = [Lat_low, Lat_up, Lon_left, Lon_right]\n",
    "\n",
    "ilat_start = int((90+Lat_low)/Res01) #1100:1450 (continental US range)\n",
    "ilat_end = int((90+Lat_up)/Res01)\n",
    "ilon_start = abs(int((-180-Lon_left)/Res01)) #500:1200 (continental US range)\n",
    "ilon_end = abs(int((-180-Lon_right)/Res01))\n",
    "\n",
    "# Number of days in each month\n",
    "month_day_leap  = [  31,  29,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_day_nonleap = [  31,  28,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "\n",
    "# Month arrays\n",
    "month_range_str = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "num_months = len(month_range_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track run time\n",
    "ct = datetime.datetime.now() \n",
    "it = ct.timestamp() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Step 1. Load in State ANSI data and Area Maps\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State-level ANSI Data\n",
    "#Read the state ANSI file array\n",
    "State_ANSI, name_dict = data_load_fn.load_state_ansi(State_ANSI_inputfile)[0:2]\n",
    "#QA: number of states\n",
    "print('Read input file: '+ f\"{State_ANSI_inputfile}\")\n",
    "print('Total \"States\" found: ' + '%.0f' % len(State_ANSI))\n",
    "print(' ')\n",
    "\n",
    "# 0.01 x0.01 degree Data\n",
    "# State ANSI IDs and grid cell area (m2) maps\n",
    "state_ANSI_map = data_load_fn.load_state_ansi_map(Grid_state001_ansi_inputfile)\n",
    "area_map, lat001, lon001 = data_load_fn.load_area_map_001(Grid_area001_inputfile)\n",
    "\n",
    "# 0.01 x0.01 degree Data\n",
    "# State ANSI IDs and grid cell area (m2) maps\n",
    "state_ANSI_map = data_load_fn.load_state_ansi_map(Grid_state001_ansi_inputfile)\n",
    "state_ANSI_map = state_ANSI_map.astype('int32')\n",
    "#county_ANSI_map = data_load_fn.load_county_ansi_map(Grid_county001_ansi_inputfile)\n",
    "#county_ANSI_map = county_ANSI_map.astype('int32')\n",
    "area_map, lat001, lon001 = data_load_fn.load_area_map_001(Grid_area001_inputfile)\n",
    "\n",
    "# 0.1 x0.1 degree data\n",
    "# grid cell area and state and county ANSI maps\n",
    "area_map01, Lat01, Lon01 = data_load_fn.load_area_map_01(Grid_area01_inputfile)[0:3]\n",
    "#Select relevant Continental 0.1 x0.1 domain\n",
    "Lat_01 = Lat01[ilat_start:ilat_end]\n",
    "Lon_01 = Lon01[ilon_start:ilon_end]\n",
    "area_matrix_01 = data_fn.regrid001_to_01(area_map, Lat_01, Lon_01)\n",
    "area_matrix_01 *= 10000  #convert from m2 to cm2\n",
    "\n",
    "state_ANSI_map_01 = data_fn.regrid001_to_01(state_ANSI_map, Lat_01, Lon_01)\n",
    "\n",
    "# Print time\n",
    "ct = datetime.datetime.now() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 2: Read-in and Format Proxy Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1 Read In Proxy Mapping File & Make Proxy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##NOTE: Mobile combustion uses an additional road type, and urban/rural flag, which means that the proxy data\n",
    "# have up to two added dimensions (road type and region)\n",
    "\n",
    "#load GHGI Mapping Groups\n",
    "names = pd.read_excel(MobComb_Mapping_inputfile, sheet_name = \"GHGI Map - Mob. Comb.\", usecols = \"A:B\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "ghgi_mob_map = pd.read_excel(MobComb_Mapping_inputfile, sheet_name = \"GHGI Map - Mob. Comb.\", usecols = \"A:B\", skiprows = 1, names = colnames)\n",
    "#drop rows with no data, remove the parentheses and \"\"\n",
    "ghgi_mob_map = ghgi_mob_map[ghgi_mob_map['GHGI_Emi_Group'] != 'na']\n",
    "ghgi_mob_map = ghgi_mob_map[ghgi_mob_map['GHGI_Emi_Group'] != '-']\n",
    "ghgi_mob_map = ghgi_mob_map[ghgi_mob_map['GHGI_Emi_Group'].notna()]\n",
    "ghgi_mob_map['GHGI_Source']= ghgi_mob_map['GHGI_Source'].str.replace(r\"\\(\",\"\")\n",
    "ghgi_mob_map['GHGI_Source']= ghgi_mob_map['GHGI_Source'].str.replace(r\"\\)\",\"\")\n",
    "ghgi_mob_map.reset_index(inplace=True, drop=True)\n",
    "display(ghgi_mob_map)\n",
    "\n",
    "#load emission group - proxy map\n",
    "names = pd.read_excel(MobComb_Mapping_inputfile, sheet_name = \"Proxy Map - Mob. Comb.\", usecols = \"A:F\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "proxy_mob_map = pd.read_excel(MobComb_Mapping_inputfile, sheet_name = \"Proxy Map - Mob. Comb.\", usecols = \"A:F\", skiprows = 1, names = colnames)\n",
    "display((proxy_mob_map))\n",
    "\n",
    "#create empty proxy and emission group arrays (add months for proxy variables that have monthly data)\n",
    "for igroup in np.arange(0,len(proxy_mob_map)):\n",
    "    if proxy_mob_map.loc[igroup, 'Grid_Month_Flag'] ==0:\n",
    "        if proxy_mob_map.loc[igroup, 'Grid_Urban_Rural_Flag'] >= 1:\n",
    "            vars()[proxy_mob_map.loc[igroup,'Proxy_Group']] = np.zeros([2,len(Lat_01),len(Lon_01),num_years])\n",
    "            vars()[proxy_mob_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([2,num_years])\n",
    "        else:\n",
    "            vars()[proxy_mob_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "            vars()[proxy_mob_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "    else:\n",
    "        vars()[proxy_mob_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "        vars()[proxy_mob_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years,num_months])\n",
    "        \n",
    "    vars()[proxy_mob_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])\n",
    "    \n",
    "    if proxy_mob_map.loc[igroup,'State_Proxy_Group'] != '-':\n",
    "        #if proxy_mob_map.loc[igroup,'State_Month_Flag'] == 0:\n",
    "        if proxy_mob_map.loc[igroup, 'Urban_Rural_Flag'] >= 1:\n",
    "            vars()[proxy_mob_map.loc[igroup,'State_Proxy_Group']] = np.zeros([2,len(State_ANSI),num_years])\n",
    "        else:\n",
    "            vars()[proxy_mob_map.loc[igroup,'State_Proxy_Group']] = np.zeros([len(State_ANSI),num_years])\n",
    "        #else:\n",
    "        #    vars()[proxy_mob_map.loc[igroup,'State_Proxy_Group']] = np.zeros([len(State_ANSI),num_years,num_months])\n",
    "    else:\n",
    "        continue # do not make state proxy variable if no variable assigned in mapping file\n",
    "        \n",
    "emi_group_names = np.unique(ghgi_mob_map['GHGI_Emi_Group'])\n",
    "\n",
    "print('QA/QC: Is the number of emission groups the same for the proxy and emissions tabs?')\n",
    "if (len(emi_group_names) == len(np.unique(proxy_mob_map['GHGI_Emi_Group']))):\n",
    "    print('PASS')\n",
    "else:\n",
    "    print('FAIL')\n",
    "    print(emi_group_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2. Read in Federal Highway Administration Data (vehcile miles traveled by state (vehicle & road type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2.1 Read In Vehicle Miles Traveled, by State and Functional Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in state-level vehicle miles travels by road type, from the Federal Highway Administration (e.g., road miles)\n",
    "\n",
    "#Available roads [Urban / Rural]\n",
    "#  - Interstate => INTERSTATE \n",
    "#  - Primary & Secondary => OTHER FREEWAYS AND EXPRESSWAYS / OTHER PRINCIPAL ARTERIAL / MINOR ARTERIAL\n",
    "#  - Other =>  MAJOR COLLECTOR / MINOR COLLECTOR / LOCAL\n",
    "    \n",
    "#Map roads to road table (3 categories x state x year)\n",
    "Miles_road_primary = np.zeros([2, len(State_ANSI), num_years])\n",
    "Miles_road_secondary = np.zeros([2, len(State_ANSI), num_years])\n",
    "Miles_road_other = np.zeros([2, len(State_ANSI), num_years])\n",
    "total = np.zeros(num_years)\n",
    "total2 = np.zeros(num_years)\n",
    "\n",
    "for iyear in np.arange(0, num_years):\n",
    "    names = pd.read_excel(State_vmt_file+year_range_str[iyear]+'.xls',  sheet_name = 'A', skiprows = 12, header = 0, nrows = 1)\n",
    "    colnames = names.columns.values\n",
    "    VMT_road = pd.read_excel(State_vmt_file+year_range_str[iyear]+'.xls', sheet_name = 'A', names = colnames, skiprows = 13, nrows = 51)\n",
    "    #print(type(VMT_road))\n",
    "\n",
    "    VMT_road.rename(columns = {'INTERSTATE':'RURAL - INTERSTATE', 'FREEWAYS  AND':'RURAL - FREEWAYS',\\\n",
    "                                     'PRINCIPAL':'RURAL - PRINCIPAL','MINOR':'RURAL - MINOR',\\\n",
    "                                     'MAJOR':'RURAL - MAJOR COLLECTOR','MINOR.1':'RURAL - MINOR COLLECTOR',\\\n",
    "                                     'LOCAL':'RURAL - LOCAL','TOTAL':'RURAL - TOTAL',\\\n",
    "                                     'INTERSTATE.1':'URBAN - INTERSTATE','FREEWAYS  AND.1':'URBAN - FREEWAYS',\\\n",
    "                                     'PRINCIPAL.1':'URBAN - PRINCIPAL','MINOR.2':'URBAN - MINOR',\n",
    "                                     'MAJOR.1':'URBAN - MAJOR COLLECTOR','MINOR.3':'URBAN - MINOR COLLECTOR',\n",
    "                                     'LOCAL.1':'URBAN - LOCAL','TOTAL.1':'URBAN - TOTAL',\n",
    "                                     'TOTAL.2':'TOTAL'}, inplace = True)\n",
    "\n",
    "    VMT_road['STATE'] = VMT_road['STATE'].str.replace(r\"\\(2\\)\",\"\") #fix state names\n",
    "    VMT_road['STATE'] = VMT_road['STATE'].str.replace(\"Dist. of Columbia\",\"District of Columbia\") #fix state names\n",
    "    #display(VMT_road)\n",
    "    \n",
    "    #Add state ID to dataframes\n",
    "    VMT_road['ANSI'] = 0\n",
    "    for idx in np.arange(len(VMT_road)):\n",
    "        VMT_road.loc[idx,'ANSI'] = name_dict[VMT_road.loc[idx,'STATE'].strip()]\n",
    "        istate = np.where(VMT_road.loc[idx,'ANSI'] == State_ANSI['ansi'])\n",
    "        Miles_road_primary[0,istate,iyear] = VMT_road.loc[idx,'URBAN - INTERSTATE']\n",
    "        Miles_road_primary[1,istate,iyear] = VMT_road.loc[idx,'RURAL - INTERSTATE']\n",
    "        Miles_road_secondary[0,istate,iyear] = VMT_road.loc[idx,'URBAN - FREEWAYS']+VMT_road.loc[idx,'URBAN - PRINCIPAL']+VMT_road.loc[idx,'URBAN - MINOR']\n",
    "        Miles_road_secondary[1,istate,iyear] = VMT_road.loc[idx,'RURAL - FREEWAYS']+VMT_road.loc[idx,'RURAL - PRINCIPAL']+VMT_road.loc[idx,'RURAL - MINOR']\n",
    "        Miles_road_other[0,istate,iyear] = VMT_road.loc[idx,'URBAN - MAJOR COLLECTOR']+VMT_road.loc[idx,'URBAN - MINOR COLLECTOR']+VMT_road.loc[idx,'URBAN - LOCAL']\n",
    "        Miles_road_other[1,istate,iyear] = VMT_road.loc[idx,'RURAL - MAJOR COLLECTOR']+VMT_road.loc[idx,'RURAL - MINOR COLLECTOR']+VMT_road.loc[idx,'RURAL - LOCAL']\n",
    "        total[iyear] += np.sum(Miles_road_primary[:,istate,iyear])+np.sum(Miles_road_secondary[:,istate,iyear])+\\\n",
    "                        np.sum(Miles_road_other[:,istate,iyear])\n",
    "        total2[iyear] += VMT_road.loc[idx,'TOTAL']\n",
    "    \n",
    "    #calc_emi += np.sum(Emissions_nongrid[iyear,:])\n",
    "    #summary_emi = EPA_statcom_total.iloc[0,iyear+1] \n",
    "    abs_diff = abs(total[iyear]-total2[iyear])/((total[iyear]+total2[iyear])/2)\n",
    "    #DEBUG## print(calc_emi)\n",
    "    #DEBUG## print(summary_emi)\n",
    "    if abs(abs_diff) < 0.0001:\n",
    "        print('Year '+ year_range_str[iyear]+': Difference < 0.01%: PASS')\n",
    "    else: \n",
    "        print('Year '+ year_range_str[iyear]+': Difference > 0.01%: FAIL, diff: '+str(abs_diff))\n",
    "        print(total[iyear])\n",
    "        print(total2[iyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2.2 Read In Fraction of Vehicle Miles Traveled, by State, Functional Type, and Vehicle Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read VMT per vehicle type & road-type (urban & rural)\n",
    "#http://www.fhwa.dot.gov/policyinformation/statistics/2013/vm4.cfm\n",
    "\n",
    " #Map percentages to emission categories\n",
    "#Interstate / P&S / Other __ #Passenger cars / Light-Duty Trucks / Medium- and Heavy-Duty Trucks and buses\n",
    "#0- urban, 1 - rural\n",
    "# 0- primary roads, 1- secondary roads, 3- other roads\n",
    "Per_vmt_mot = np.zeros([2,3, len(State_ANSI), num_years])\n",
    "Per_vmt_pas = np.zeros([2,3, len(State_ANSI), num_years])\n",
    "Per_vmt_lig = np.zeros([2,3, len(State_ANSI), num_years])\n",
    "Per_vmt_hea = np.zeros([2,3, len(State_ANSI), num_years])\n",
    "total_R = np.zeros(num_years)\n",
    "total_U = np.zeros(num_years)\n",
    "total = np.zeros(num_years)\n",
    "total2_U = np.zeros(num_years)\n",
    "total2 = np.zeros(num_years)\n",
    "total2_R = np.zeros(num_years)\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    if year_range[iyear] ==2012 or year_range[iyear]==2016:\n",
    "        continue #deal with missing data at the end\n",
    "    else:\n",
    "        #read in rural sheet\n",
    "        names = pd.read_excel(State_vdf_file+year_range_str[iyear]+'.xls',  sheet_name = 'A', skiprows = 12, header = 0, nrows = 1)\n",
    "        colnames = names.columns.values\n",
    "        VMT_type_R = pd.read_excel(State_vdf_file+year_range_str[iyear]+'.xls', na_values=['-'],sheet_name = 'A', names = colnames, skiprows = 13, nrows = 51)\n",
    "\n",
    "        VMT_type_R.rename(columns = {'MOTOR-':'INTERSTATE - MOTORCYCLES', 'PASSENGER':'INTERSTATE - PASSENGER CARS',\\\n",
    "                                     'LIGHT':'INTERSTATE - LIGHT TRUCKS','Unnamed: 4':'INTERSTATE - BUSES',\\\n",
    "                                     'SINGLE-UNIT':'INTERSTATE - SINGLE-UNIT TRUCKS','COMBINATION':'INTERSTATE - COMBINATION TRUCKS',\\\n",
    "                                     'Unnamed: 7':'INTERSTATE - TOTAL',\n",
    "                                     'MOTOR-.1':'ARTERIALS - MOTORCYCLES', 'PASSENGER.1':'ARTERIALS - PASSENGER CARS',\\\n",
    "                                     'LIGHT.1':'ARTERIALS - LIGHT TRUCKS','Unnamed: 11':'ARTERIALS - BUSES',\\\n",
    "                                     'SINGLE-UNIT.1':'ARTERIALS - SINGLE-UNIT TRUCKS','COMBINATION.1':'ARTERIALS - COMBINATION TRUCKS',\\\n",
    "                                     'Unnamed: 14':'ARTERIALS - TOTAL',\n",
    "                                     'MOTOR-.2':'OTHER - MOTORCYCLES', 'PASSENGER.2':'OTHER - PASSENGER CARS',\\\n",
    "                                     'LIGHT.2':'OTHER - LIGHT TRUCKS','Unnamed: 18':'OTHER - BUSES',\\\n",
    "                                     'SINGLE-UNIT.2':'OTHER - SINGLE-UNIT TRUCKS','COMBINATION.2':'OTHER - COMBINATION TRUCKS',\\\n",
    "                                     'Unnamed: 21':'OTHER - TOTAL'}, inplace = True)\n",
    "\n",
    "        VMT_type_R['STATE'] = VMT_type_R['STATE'].str.replace(r\"\\(2\\)\",\"\") #fix state names\n",
    "        VMT_type_R['STATE'] = VMT_type_R['STATE'].str.replace(\"Dist. of Columbia\",\"District of Columbia\") #fix state names\n",
    "        VMT_type_R = VMT_type_R.fillna(0)\n",
    "        \n",
    "        #read in urban sheet\n",
    "        names = pd.read_excel(State_vdf_file+year_range_str[iyear]+'.xls',  sheet_name = 'B', skiprows = 12, header = 0, nrows = 1)\n",
    "        colnames = names.columns.values\n",
    "        VMT_type_U = pd.read_excel(State_vdf_file+year_range_str[iyear]+'.xls', na_values=['-'],sheet_name = 'B', names = colnames, skiprows = 13, nrows = 51)\n",
    "\n",
    "\n",
    "        VMT_type_U.rename(columns = {'MOTOR-':'INTERSTATE - MOTORCYCLES', 'PASSENGER':'INTERSTATE - PASSENGER CARS',\\\n",
    "                                     'LIGHT':'INTERSTATE - LIGHT TRUCKS','Unnamed: 4':'INTERSTATE - BUSES',\\\n",
    "                                     'SINGLE-UNIT':'INTERSTATE - SINGLE-UNIT TRUCKS','COMBINATION':'INTERSTATE - COMBINATION TRUCKS',\\\n",
    "                                     'Unnamed: 7':'INTERSTATE - TOTAL',\n",
    "                                     'MOTOR-.1':'ARTERIALS - MOTORCYCLES', 'PASSENGER.1':'ARTERIALS - PASSENGER CARS',\\\n",
    "                                     'LIGHT.1':'ARTERIALS - LIGHT TRUCKS','Unnamed: 11':'ARTERIALS - BUSES',\\\n",
    "                                     'SINGLE-UNIT.1':'ARTERIALS - SINGLE-UNIT TRUCKS','COMBINATION.1':'ARTERIALS - COMBINATION TRUCKS',\\\n",
    "                                     'Unnamed: 14':'ARTERIALS - TOTAL',\n",
    "                                     'MOTOR-.2':'OTHER - MOTORCYCLES', 'PASSENGER.2':'OTHER - PASSENGER CARS',\\\n",
    "                                     'LIGHT.2':'OTHER - LIGHT TRUCKS','Unnamed: 18':'OTHER - BUSES',\\\n",
    "                                     'SINGLE-UNIT.2':'OTHER - SINGLE-UNIT TRUCKS','COMBINATION.2':'OTHER - COMBINATION TRUCKS',\\\n",
    "                                     'Unnamed: 21':'OTHER - TOTAL'}, inplace = True)\n",
    "\n",
    "        VMT_type_U['STATE'] = VMT_type_U['STATE'].str.replace(r\"\\(2\\)\",\"\") #fix state names\n",
    "        VMT_type_U['STATE'] = VMT_type_U['STATE'].str.replace(\"Dist. of Columbia\",\"District of Columbia\") #fix state names\n",
    "        VMT_type_U = VMT_type_U.fillna(0)\n",
    "        #display(VMT_type_U)\n",
    "        \n",
    "        \n",
    "        #Add state ID to dataframes\n",
    "        VMT_type_R['ANSI'] = 0\n",
    "        VMT_type_U['ANSI'] = 0\n",
    "        for idx in np.arange(len(VMT_type_R)):\n",
    "            VMT_type_R.loc[idx,'ANSI'] = name_dict[VMT_type_R.loc[idx,'STATE'].strip()]\n",
    "            istate_R = np.where(VMT_type_R.loc[idx,'ANSI'] == State_ANSI['ansi'])\n",
    "            Per_vmt_mot[1,0,istate_R,iyear] = VMT_type_R.loc[idx,'INTERSTATE - MOTORCYCLES']\n",
    "            Per_vmt_mot[1,1,istate_R,iyear] = VMT_type_R.loc[idx,'ARTERIALS - MOTORCYCLES']\n",
    "            Per_vmt_mot[1,2,istate_R,iyear] = VMT_type_R.loc[idx,'OTHER - MOTORCYCLES']\n",
    "            Per_vmt_pas[1,0,istate_R,iyear] = VMT_type_R.loc[idx,'INTERSTATE - PASSENGER CARS']\n",
    "            Per_vmt_pas[1,1,istate_R,iyear] = VMT_type_R.loc[idx,'ARTERIALS - PASSENGER CARS']\n",
    "            Per_vmt_pas[1,2,istate_R,iyear] = VMT_type_R.loc[idx,'OTHER - PASSENGER CARS']\n",
    "            Per_vmt_lig[1,0,istate_R,iyear] = VMT_type_R.loc[idx,'INTERSTATE - LIGHT TRUCKS']\n",
    "            Per_vmt_lig[1,1,istate_R,iyear] = VMT_type_R.loc[idx,'ARTERIALS - LIGHT TRUCKS']\n",
    "            Per_vmt_lig[1,2,istate_R,iyear] = VMT_type_R.loc[idx,'OTHER - LIGHT TRUCKS']\n",
    "            Per_vmt_hea[1,0,istate_R,iyear] = VMT_type_R.loc[idx,'INTERSTATE - BUSES']+VMT_type_R.loc[idx,'INTERSTATE - SINGLE-UNIT TRUCKS']+VMT_type_R.loc[idx,'INTERSTATE - COMBINATION TRUCKS']\n",
    "            Per_vmt_hea[1,1,istate_R,iyear] = VMT_type_R.loc[idx,'ARTERIALS - BUSES']+VMT_type_R.loc[idx,'ARTERIALS - SINGLE-UNIT TRUCKS']+VMT_type_R.loc[idx,'ARTERIALS - COMBINATION TRUCKS']\n",
    "            Per_vmt_hea[1,2,istate_R,iyear] = VMT_type_R.loc[idx,'OTHER - BUSES']+VMT_type_R.loc[idx,'OTHER - SINGLE-UNIT TRUCKS']+VMT_type_R.loc[idx,'OTHER - COMBINATION TRUCKS']\n",
    "            total_R[iyear] += np.sum(Per_vmt_mot[1,:,istate_R,iyear])+np.sum(Per_vmt_pas[1,:,istate_R,iyear])+\\\n",
    "                np.sum(Per_vmt_lig[1,:,istate_R,iyear])+np.sum(Per_vmt_hea[1,:,istate_R,iyear])\n",
    "            total2_R[iyear] += VMT_type_R.loc[idx,'INTERSTATE - TOTAL']+VMT_type_R.loc[idx,'ARTERIALS - TOTAL']+VMT_type_R.loc[idx,'OTHER - TOTAL']\n",
    "\n",
    "        for idx in np.arange(len(VMT_type_U)):\n",
    "            VMT_type_U.loc[idx,'ANSI'] = name_dict[VMT_type_U.loc[idx,'STATE'].strip()]            \n",
    "            istate_U = np.where(VMT_type_U.loc[idx,'ANSI'] == State_ANSI['ansi'])\n",
    "            Per_vmt_mot[0,0,istate_U,iyear] = VMT_type_U.loc[idx,'INTERSTATE - MOTORCYCLES']\n",
    "            Per_vmt_mot[0,1,istate_U,iyear] = VMT_type_U.loc[idx,'ARTERIALS - MOTORCYCLES']\n",
    "            Per_vmt_mot[0,2,istate_U,iyear] = VMT_type_U.loc[idx,'OTHER - MOTORCYCLES']\n",
    "            Per_vmt_pas[0,0,istate_U,iyear] = VMT_type_U.loc[idx,'INTERSTATE - PASSENGER CARS']\n",
    "            Per_vmt_pas[0,1,istate_U,iyear] = VMT_type_U.loc[idx,'ARTERIALS - PASSENGER CARS']\n",
    "            Per_vmt_pas[0,2,istate_U,iyear] = VMT_type_U.loc[idx,'OTHER - PASSENGER CARS']\n",
    "            Per_vmt_lig[0,0,istate_U,iyear] = VMT_type_U.loc[idx,'INTERSTATE - LIGHT TRUCKS']\n",
    "            Per_vmt_lig[0,1,istate_U,iyear] = VMT_type_U.loc[idx,'ARTERIALS - LIGHT TRUCKS']\n",
    "            Per_vmt_lig[0,2,istate_U,iyear] = VMT_type_U.loc[idx,'OTHER - LIGHT TRUCKS']\n",
    "            Per_vmt_hea[0,0,istate_U,iyear] = VMT_type_U.loc[idx,'INTERSTATE - BUSES']+VMT_type_U.loc[idx,'INTERSTATE - SINGLE-UNIT TRUCKS']+VMT_type_U.loc[idx,'INTERSTATE - COMBINATION TRUCKS']\n",
    "            Per_vmt_hea[0,1,istate_U,iyear] = VMT_type_U.loc[idx,'ARTERIALS - BUSES']+VMT_type_U.loc[idx,'ARTERIALS - SINGLE-UNIT TRUCKS']+VMT_type_U.loc[idx,'ARTERIALS - COMBINATION TRUCKS']\n",
    "            Per_vmt_hea[0,2,istate_U,iyear] = VMT_type_U.loc[idx,'OTHER - BUSES']+VMT_type_U.loc[idx,'OTHER - SINGLE-UNIT TRUCKS']+VMT_type_U.loc[idx,'OTHER - COMBINATION TRUCKS']\n",
    "            total_U[iyear] += np.sum(Per_vmt_mot[0,:,istate_U,iyear])+np.sum(Per_vmt_pas[0,:,istate_U,iyear])+\\\n",
    "                np.sum(Per_vmt_lig[0,:,istate_U,iyear])+np.sum(Per_vmt_hea[0,:,istate_U,iyear])\n",
    "            total2_U[iyear] += VMT_type_U.loc[idx,'INTERSTATE - TOTAL']+VMT_type_U.loc[idx,'ARTERIALS - TOTAL']+VMT_type_U.loc[idx,'OTHER - TOTAL']\n",
    "   \n",
    "        total[iyear] = total_U[iyear]+total_R[iyear]\n",
    "        total2[iyear] = total2_R[iyear]+total2_U[iyear]\n",
    "        abs_diff1 = abs(total[iyear]-total2[iyear])/((total[iyear]+total2[iyear])/2)\n",
    "        #abs_diff2 = abs(total_R[iyear]-total2_R[iyear])/((total_R[iyear]+total2_R[iyear])/2)\n",
    "        #DEBUG## print(calc_emi)\n",
    "        #DEBUG## print(summary_emi)\n",
    "        if abs(abs_diff1) < 0.0001:\n",
    "            print('Year '+ year_range_str[iyear]+': Urban Difference < 0.01%: PASS')\n",
    "        else: \n",
    "            print('Year '+ year_range_str[iyear]+': Urban Difference > 0.01%: FAIL, diff: '+str(abs_diff1))\n",
    "            print(total[iyear])\n",
    "            print(total2[iyear])\n",
    "\n",
    "#Correct Years (assign 2012 to 2013), assign 2016 as average of 2015 and 2017\n",
    "#ADD\n",
    "idx_2012 = (2012-start_year)\n",
    "idx_2016 = (2016-start_year)\n",
    "Per_vmt_mot[:,:,:,idx_2012] = Per_vmt_mot[:,:,:,idx_2012+1]\n",
    "Per_vmt_pas[:,:,:,idx_2012] = Per_vmt_pas[:,:,:,idx_2012+1]\n",
    "Per_vmt_lig[:,:,:,idx_2012] = Per_vmt_lig[:,:,:,idx_2012+1]\n",
    "Per_vmt_hea[:,:,:,idx_2012] = Per_vmt_hea[:,:,:,idx_2012+1]\n",
    "\n",
    "Per_vmt_mot[:,:,:,idx_2016] = 0.5*(Per_vmt_mot[:,:,:,idx_2016-1]+Per_vmt_mot[:,:,:,idx_2016+1])\n",
    "Per_vmt_pas[:,:,:,idx_2016] = 0.5*(Per_vmt_pas[:,:,:,idx_2016-1]+Per_vmt_pas[:,:,:,idx_2016+1])\n",
    "Per_vmt_lig[:,:,:,idx_2016] = 0.5*(Per_vmt_lig[:,:,:,idx_2016-1]+Per_vmt_lig[:,:,:,idx_2016+1])\n",
    "Per_vmt_hea[:,:,:,idx_2016] = 0.5*(Per_vmt_hea[:,:,:,idx_2016-1]+Per_vmt_hea[:,:,:,idx_2016+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2.3. Calculate State-Level Road Proxy - VMT per road/type, by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the vehicle miles traveled by state, year, vehcile type, and road type\n",
    "# VMT = fraction of miles traveled as a function of vehicle type and state * road miles by road type\n",
    "\n",
    "#initialize variables\n",
    "# urban/rural x road type x state x year\n",
    "#0 - urban, 1 - rural\n",
    "# road types\n",
    "# 0 - interstate, #1 - primary & secondary, #2 - other\n",
    "vmt_pas = np.zeros([2,3,len(State_ANSI), num_years])\n",
    "vmt_lig = np.zeros([2,3,len(State_ANSI), num_years])\n",
    "vmt_hea = np.zeros([2,3,len(State_ANSI), num_years])\n",
    "vmt_tot = np.zeros([2,len(State_ANSI), num_years])\n",
    "\n",
    "#calculate the absolute number of VMT by region, road type, vehicle type, and state, for each year\n",
    "# e.g. vmt_pas = VMT for passenger vehicles with dimensions = region (urban/rural), road type (primary, secondary,\n",
    "# other), state, and year\n",
    "# vmt_tot = region x state, year\n",
    "# road mile variable dimensions (urban/rural, state, year)\n",
    "# vmt percentage variable dimensions (urban/rural, road type, state, year)\n",
    "for iyear in np.arange(0, num_years):\n",
    "    vmt_pas[:,0,:,iyear] = Miles_road_primary[:,:,iyear] * Per_vmt_pas[:,0,:,iyear]\n",
    "    vmt_pas[:,1,:,iyear] = Miles_road_secondary[:,:,iyear] * Per_vmt_pas[:,1,:,iyear]\n",
    "    vmt_pas[:,2,:,iyear] = Miles_road_other[:,:,iyear] * Per_vmt_pas[:,2,:,iyear]\n",
    "    \n",
    "    vmt_lig[:,0,:,iyear] = Miles_road_primary[:,:,iyear] * Per_vmt_lig[:,0,:,iyear]\n",
    "    vmt_lig[:,1,:,iyear] = Miles_road_secondary[:,:,iyear] * Per_vmt_lig[:,1,:,iyear]\n",
    "    vmt_lig[:,2,:,iyear] = Miles_road_other[:,:,iyear] * Per_vmt_lig[:,2,:,iyear]\n",
    "    \n",
    "    vmt_hea[:,0,:,iyear] = Miles_road_primary[:,:,iyear] * Per_vmt_hea[:,0,:,iyear]\n",
    "    vmt_hea[:,1,:,iyear] = Miles_road_secondary[:,:,iyear] * Per_vmt_hea[:,1,:,iyear]\n",
    "    vmt_hea[:,2,:,iyear] = Miles_road_other[:,:,iyear] * Per_vmt_hea[:,2,:,iyear]\n",
    "    \n",
    "    vmt_tot[:,:,iyear] += Miles_road_primary[:,:,iyear]+Miles_road_secondary[:,:,iyear]+Miles_road_other[:,:,iyear]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.4. Read In and Format High Resolution Road Gridded Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make proxy maps for road types\n",
    "\n",
    "# Maps of road length in each grid cell, by urban and rural region, and type of road (primary, secondary, other),\n",
    "# are calculated as follows:\n",
    "# 1) high resolution maps of all road types in urban and rural areas (in the CONUS region) are read in. These\n",
    "#    files are only avialable for a single year and are used to calculation the 'other road' type later in the process\n",
    "# 2) high resolution maps of primary and primary+secondary roads are read in. These files have year-specific\n",
    "#    information and are used to calculate primary and secondary roads each year\n",
    "# 3) split each of the three road types between urban and rural contributions. A map of urban area in the given year\n",
    "#    is read in. This map is then used to calculate the urban roads as those in grid cells with urban area > 0.5.\n",
    "#    Rural areas are then calculated as the total roads minus the urban roads\n",
    "\n",
    "\n",
    " #Create gridded maps\n",
    "# urban/rural -> 0 - urban, #1 - rural\n",
    "# road type - > 0 - primary roads, 1 - secondary roads, 2 - other roads\n",
    "map_roads = np.zeros([2,3,len(lat001),len(lon001),num_years])\n",
    "map_roads_nongrid = np.zeros([2,3,num_years])\n",
    "map_urban_area = np.zeros([len(lat001),len(lon001)]) \n",
    "    \n",
    "# Step 1) Read in total urban and rural roads in the CONUS region and assign road lengths to grid  \n",
    "roads_other_urb = pd.read_csv(CONUS_urban_roads_file, usecols=[2,3,4])\n",
    "roads_other_rur = pd.read_csv(CONUS_rural_roads_file, usecols=[2,3,4])\n",
    "\n",
    "for idx in np.arange(len(roads_other_urb)):\n",
    "    if roads_other_urb['FIRST_Longitude'][idx] > Lon_left and roads_other_urb['FIRST_Longitude'][idx] < Lon_right and\\\n",
    "        roads_other_urb['FIRST_Latitude'][idx] > Lat_low and roads_other_urb['FIRST_Latitude'][idx] < Lat_up:\n",
    "        #Set ilon and ilat\n",
    "        ilat = int((roads_other_urb['FIRST_Latitude'][idx]  - Lat_low) /Res_01)\n",
    "        ilon = int((roads_other_urb['FIRST_Longitude'][idx] - Lon_left)/Res_01)\n",
    "        map_roads[0,2,ilat,ilon,0] += roads_other_urb['SUM_Shape_Length'][idx]        \n",
    "    else:\n",
    "        map_roads_nongrid[0,2,0] += roads_other_urb['SUM_Shape_Length'][idx]\n",
    "print('Finished Processing All Urban Roads')\n",
    "ct = datetime.datetime.now() \n",
    "print(\"current time:\", ct) \n",
    "\n",
    "for idx in np.arange(len(roads_other_rur)): #set all to 'urban' for now, split urban vs rural for all roads later\n",
    "    if roads_other_rur['FIRST_Longitude'][idx] > Lon_left and roads_other_rur['FIRST_Longitude'][idx] < Lon_right and \\\n",
    "        roads_other_rur['FIRST_Latitude'][idx] > Lat_low and roads_other_rur['FIRST_Latitude'][idx] < Lat_up:\n",
    "        ilat = int((roads_other_rur['FIRST_Latitude'][idx]  - Lat_low) /Res_01)\n",
    "        ilon = int((roads_other_rur['FIRST_Longitude'][idx] - Lon_left)/Res_01)\n",
    "        map_roads[0,2,ilat,ilon,0] += roads_other_rur['SUM_Shape_Length'][idx]  \n",
    "    else:\n",
    "        map_roads_nongrid[0,2,0] += roads_other_rur['SUM_Shape_Length'][idx]  \n",
    "del roads_other_urb, roads_other_rur\n",
    "print('Finished Processing All Rural Roads')\n",
    "ct = datetime.datetime.now() \n",
    "print(\"current time:\", ct) \n",
    "\n",
    "\n",
    "# Step 2) make yearly maps of primary, secondary, and other road types, split between urban and rural regions\n",
    "for iyear in np.arange(0, num_years):\n",
    "    #Assign set all years of 'other roads' as constant\n",
    "    map_roads[0,2,:,:,iyear] = map_roads[0,2,:,:,0]\n",
    "    map_roads[1,2,:,:,iyear] = map_roads[1,2,:,:,0]\n",
    "    \n",
    "    #Read primary and secondary road maps\n",
    "    roads_primary = pd.read_csv(Primary_roads_file+year_range_str[iyear]+'_001x001.csv', sep=',')\n",
    "    roads_primsec = pd.read_csv(Secondary_roads_file+year_range_str[iyear]+'_001x001.csv', sep=',')\n",
    "\n",
    "    for idx in np.arange(len(roads_primary)):\n",
    "        if roads_primary['Longitude'][idx] > Lon_left and roads_primary['Longitude'][idx] < Lon_right and \\\n",
    "            roads_primary['Latitude'][idx] > Lat_low and roads_primary['Latitude'][idx] < Lat_up:\n",
    "            #Set ilon and ilat\n",
    "            ilat = int((roads_primary['Latitude'][idx]  - Lat_low) /Res_01)\n",
    "            ilon = int((roads_primary['Longitude'][idx] - Lon_left)/Res_01)\n",
    "            map_roads[0,0,ilat,ilon,iyear] += roads_primary['SUM_Shape_Length'][idx]\n",
    "        else:\n",
    "            map_roads_nongrid[0,0,iyear] += roads_primary['SUM_Shape_Length'][idx] \n",
    "    \n",
    "    for idx in np.arange(len(roads_primsec)):\n",
    "        if roads_primsec['Longitude'][idx] > Lon_left and roads_primsec['Longitude'][idx] < Lon_right and \\\n",
    "            roads_primsec['Latitude'][idx] > Lat_low and roads_primsec['Latitude'][idx] < Lat_up:\n",
    "            #Set ilon and ilat\n",
    "            ilat = int((roads_primsec['Latitude'][idx]  - Lat_low) /Res_01)\n",
    "            ilon = int((roads_primsec['Longitude'][idx] - Lon_left)/Res_01)\n",
    "            map_roads[0,1,ilat,ilon,iyear] += roads_primsec['SUM_Shape_Length'][idx]\n",
    "        else:\n",
    "            map_roads_nongrid[0,1,iyear] += roads_primsec['SUM_Shape_Length'][idx]\n",
    "    \n",
    "    # Calculate Secondary Road lengths\n",
    "    # 1) secondary = secondary&primary - primary\n",
    "    # 2) remove negative values\n",
    "    # 3) repeat for non-grid arrays\n",
    "    map_roads[0,1,:,:,iyear] = map_roads[0,1,:,:,iyear] - map_roads[0,0,:,:,iyear]\n",
    "    map_roads[map_roads<0] = 0.0\n",
    "    map_roads_nongrid[0,1,iyear] = map_roads_nongrid[0,1,iyear] - map_roads_nongrid[0,0,iyear]\n",
    "    map_roads_nongrid[map_roads_nongrid<0] = 0.0\n",
    "\n",
    "    # Calculate Other road lengths\n",
    "    # 1) other = other - secondary - primary\n",
    "    # 2) replace negatives with zeros\n",
    "    map_roads[0,2,:,:,iyear] = map_roads[0,2,:,:,iyear] - map_roads[0,1,:,:,iyear] - map_roads[0,0,:,:,iyear]\n",
    "    map_roads[map_roads < 0] = 0.0\n",
    "    map_roads_nongrid[0,2,iyear] = map_roads_nongrid[0,2,iyear] - map_roads_nongrid[0,1,iyear] - map_roads_nongrid[0,0,iyear]\n",
    "    map_roads_nongrid[map_roads_nongrid < 0] = 0.0\n",
    "    \n",
    "    print ('Gridded  primary roads length: ', np.sum(map_roads[0,0,:,:,iyear]))\n",
    "    print ('Gridded  secondary roads length: ', np.sum(map_roads[0,1,:,:,iyear]))\n",
    "    print ('Gridded  other roads length:   ', np.sum(map_roads[0,2,:,:,iyear]))\n",
    "\n",
    "    # Step 3) Separate out urban vs rural portion\n",
    "    # Read in the urban area and normalize to the grid cell area\n",
    "    urban_area   = pd.read_csv(Urban_area_file+year_range_str[iyear]+'_001x001.csv', sep=',')\n",
    "    urban_area['SUM_Area_Urbanized'] = urban_area['SUM_Area_Urbanized']/np.max(urban_area['SUM_Area_Urbanized'])*np.max(area_map)\n",
    "    # create a map of urban areas\n",
    "    for idx in np.arange(len(urban_area)):\n",
    "        if urban_area['Longitude'][idx] > Lon_left and urban_area['Longitude'][idx] < Lon_right and\\\n",
    "            urban_area['Latitude'][idx] > Lat_low and urban_area['Latitude'][idx] < Lat_up:\n",
    "            ilat = int((urban_area['Latitude'][idx]  - Lat_low) /Res_01)\n",
    "            ilon = int((urban_area['Longitude'][idx] - Lon_left)/Res_01)\n",
    "            map_urban_area[ilat,ilon] = urban_area['SUM_Area_Urbanized'][idx] / float(area_map[ilat,ilon])\n",
    "    \n",
    "    # Split urban vs rural contributions for all three road types\n",
    "    # 1) make a temporary copy of all roads (for that road type) for the given year\n",
    "    # 2) urban region = all roads with urban area > 0.5\n",
    "    # 3) rural region = all roads - all urban roads\n",
    "    # 4) re-assign urban fraction to map_roads array\n",
    "    # 5) calculate the fraction of urban/rural roads and apply to off grid region\n",
    "    # 5) Repeat for all road types\n",
    "    map_temp = map_roads[0,0,:,:,iyear].copy()\n",
    "    map_temp[map_urban_area < 0.5] = 0.0\n",
    "    map_roads[1,0,:,:,iyear] = map_roads[0,0,:,:,iyear] - map_temp[:,:]\n",
    "    map_roads[0,0,:,:,iyear] = map_temp.copy()\n",
    "    map_roads_nongrid[0,0,iyear] = np.sum(map_roads[0,0,:,:,iyear])/np.sum(map_roads[1,0,:,:,iyear]+map_roads[0,0,:,:,iyear])\n",
    "    map_roads_nongrid[1,0,iyear] = np.sum(map_roads[1,0,:,:,iyear])/np.sum(map_roads[1,0,:,:,iyear]+map_roads[0,0,:,:,iyear])\n",
    "\n",
    "    #secondary\n",
    "    map_temp = map_roads[0,1,:,:,iyear].copy()\n",
    "    map_temp[map_urban_area < 0.5] = 0.0\n",
    "    map_roads[1,1,:,:,iyear] = map_roads[0,1,:,:,iyear] - map_temp[:,:]\n",
    "    map_roads[0,1,:,:,iyear] = map_temp.copy()\n",
    "    map_roads_nongrid[0,1,iyear] = np.sum(map_roads[0,1,:,:,iyear])/np.sum(map_roads[1,1,:,:,iyear]+map_roads[0,1,:,:,iyear])\n",
    "    map_roads_nongrid[1,1,iyear] = np.sum(map_roads[1,1,:,:,iyear])/np.sum(map_roads[1,1,:,:,iyear]+map_roads[0,1,:,:,iyear])\n",
    "\n",
    "    #other\n",
    "    map_temp = map_roads[0,2,:,:,iyear].copy()\n",
    "    map_temp[map_urban_area < 0.5] = 0.0\n",
    "    map_roads[1,2,:,:,iyear] = map_roads[0,2,:,:,iyear] - map_temp[:,:]\n",
    "    map_roads[0,2,:,:,iyear] = map_temp.copy()\n",
    "    map_roads_nongrid[0,2,iyear] = np.sum(map_roads[0,2,:,:,iyear])/np.sum(map_roads[1,2,:,:,iyear]+map_roads[0,2,:,:,iyear])\n",
    "    map_roads_nongrid[1,2,iyear] = np.sum(map_roads[1,2,:,:,iyear])/np.sum(map_roads[1,2,:,:,iyear]+map_roads[0,2,:,:,iyear])\n",
    "\n",
    "    print('Year:', year_range_str[iyear])\n",
    "    print ('Gridded  primary roads length: ', np.sum(map_roads[:,0,:,:,iyear]))\n",
    "    print ('Gridded  primary roads urban:  ', np.sum(map_roads[0,0,:,:,iyear]))\n",
    "    print ('Gridded  primary roads rural:  ', np.sum(map_roads[1,0,:,:,iyear]))\n",
    "    print ('Gridded  primsec roads length: ', np.sum(map_roads[:,1,:,:,iyear]))\n",
    "    print ('Gridded  primsec roads urban:  ', np.sum(map_roads[0,1,:,:,iyear]))\n",
    "    print ('Gridded  primsec roads rural:  ', np.sum(map_roads[1,1,:,:,iyear]))\n",
    "    print ('Gridded  other roads length:   ', np.sum(map_roads[:,2,:,:,iyear]))\n",
    "    print ('Gridded  other roads urban:    ', np.sum(map_roads[0,2,:,:,iyear]))\n",
    "    print ('Gridded  other roads rural:    ', np.sum(map_roads[1,2,:,:,iyear]))\n",
    "    ct = datetime.datetime.now() \n",
    "    print(\"current time:\", ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.5. Make Maps of Waterways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Laod Waterways data and allocate to 0.1x0.1 degree map\n",
    "# only include CONUS region\n",
    "\n",
    "#Initialize \n",
    "map_waterways = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "map_waterways_nongrid = np.zeros([num_years])\n",
    "\n",
    "##Load waterways\n",
    "waterways_loc = pd.read_csv(Waterways_file, sep=',')\n",
    "#waterways_loc.head(1)\n",
    "\n",
    "for idx in np.arange(len(waterways_loc)):\n",
    "    if waterways_loc['Longitude'][idx] > Lon_left and waterways_loc['Longitude'][idx] < Lon_right and \\\n",
    "        waterways_loc['Latitude'][idx] > Lat_low and waterways_loc['Latitude'][idx] < Lat_up:\n",
    "        ilat = int((waterways_loc['Latitude'][idx]  - Lat_low)/Res01)\n",
    "        ilon = int((waterways_loc['Longitude'][idx] - Lon_left)/Res01)\n",
    "        map_waterways[ilat,ilon,0] += waterways_loc['SUM_Shape_Length'][idx]\n",
    "    else:\n",
    "        map_waterways_nongrid[0] += waterways_loc['SUM_Shape_Length'][idx]\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    map_waterways[:,:,iyear] = map_waterways[:,:,0]\n",
    "    print('Year:', year_range_str[iyear])\n",
    "    print ('Database waterways length: ', np.sum(waterways_loc['SUM_Shape_Length']))\n",
    "    print ('Gridded  waterways length: ', np.sum(map_waterways[:,:,iyear]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.6. Make Maps of Mines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load mines and make gridded 0.1x0.1 degree map - only includes one year of data\n",
    "# includes mines outside of CONUS\n",
    "\n",
    "#Initialize map arrays\n",
    "map_mines = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "map_mines_nongrid = np.zeros(num_years)\n",
    "\n",
    "mine_loc = pd.read_csv(Mines_file, sep='|', encoding= 'unicode_escape')\n",
    "print ('Database mines: ', len(mine_loc))\n",
    "mine_loc = mine_loc[mine_loc['LATITUDE']>0]\n",
    "mine_loc = mine_loc[mine_loc['CURRENT_MINE_STATUS']=='Active']\n",
    "mine_loc['LONGITUDE'] = 1*mine_loc['LONGITUDE']\n",
    "mine_loc.reset_index(inplace=True, drop=True)\n",
    "print ('Active mines with location: ', len(mine_loc))\n",
    "\n",
    "for idx in np.arange(len(mine_loc)):\n",
    "    if mine_loc['LONGITUDE'][idx] > Lon_left and mine_loc['LONGITUDE'][idx] < Lon_right and \\\n",
    "        mine_loc['LATITUDE'][idx] > Lat_low and mine_loc['LATITUDE'][idx] < Lat_up:\n",
    "        ilat = int((mine_loc['LATITUDE'][idx] - Lat_low)/Res01)\n",
    "        ilon = int((mine_loc['LONGITUDE'][idx] - Lon_left)/Res01)\n",
    "        map_mines[ilat,ilon] += 1\n",
    "    else:\n",
    "        map_mines_nongrid += 1\n",
    "    \n",
    "for iyear in np.arange(0,num_years):\n",
    "    map_mines[:,:,iyear] = map_mines[:,:,0]\n",
    "    print('Year:', year_range_str[iyear])\n",
    "    print ('Gridded  mines: ', np.sum(map_mines[:,:,iyear]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.7. Make Maps of Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read population density map, convert to absolute population, regrid to 0.1 x0.1 degrees\n",
    "# only includes CONUS region\n",
    "\n",
    "pop_abs = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "map_abs_nongrid = np.zeros(num_years)\n",
    "\n",
    "pop_den_map = data_load_fn.load_pop_den_map(pop_map_inputfile)\n",
    "pop_abs_001 = pop_den_map*area_map\n",
    "pop_abs[:,:,0] = data_fn.regrid001_to_01(pop_abs_001, Lat_01, Lon_01)\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    pop_abs[:,:,iyear] = pop_abs[:,:,0]\n",
    "    print('Year:', year_range_str[iyear])\n",
    "    print ('Gridded  population: ', np.sum(pop_abs[:,:,iyear]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.8 Make Maps of Railroads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in railroad data and place on 0.1x0.1 degree map\n",
    "#only includes CONUS region\n",
    "\n",
    "map_rail = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "map_rail_nongrid = np.zeros(num_years)\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    rail_loc = pd.read_csv(Railroad_file+year_range_str[iyear]+'_01x01.csv', sep=',')\n",
    "\n",
    "    for idx in np.arange(len(rail_loc)):\n",
    "        if rail_loc['Longitude'][idx] > Lon_left and rail_loc['Longitude'][idx] < Lon_right and \\\n",
    "            rail_loc['Latitude'][idx] > Lat_low and rail_loc['Latitude'][idx] < Lat_up:\n",
    "            ilat = int((rail_loc['Latitude'][idx]  - Lat_low)/Res01)\n",
    "            ilon = int((rail_loc['Longitude'][idx] - Lon_left)/Res01)\n",
    "            map_rail[ilat,ilon,iyear] += rail_loc['SUM_Shape_Length'][idx]\n",
    "        else:\n",
    "            map_rail_nongrid[iyear] += rail_loc['SUM_Railroad_Length'][idx]\n",
    "    print('Year:', year_range_str[iyear])\n",
    "    print ('Database rail length: ', np.sum(rail_loc['SUM_Shape_Length']))\n",
    "    print ('Gridded  rail length: ', np.sum(map_rail[:,:,iyear]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.9 Make Maps of Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only includes CONUS region\n",
    "\n",
    "if ReCalc_Crop ==1:\n",
    "    map_crop = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "    map_crop_nongrid = np.zeros(num_years)\n",
    "\n",
    "    for iyear in np.arange(0,num_years):\n",
    "        crop_loc = pd.read_csv(Crop_file+year_range_str[iyear]+'_001x001.csv', sep=',')\n",
    "        for idx in np.arange(0,len(crop_loc)):\n",
    "            if crop_loc['FIRST_Longitude'][idx] > Lon_left and crop_loc['FIRST_Longitude'][idx] < Lon_right and \\\n",
    "                crop_loc['FIRST_Latitude'][idx] > Lat_low and crop_loc['FIRST_Latitude'][idx] < Lat_up:\n",
    "                ilat = int((crop_loc['FIRST_Latitude'][idx]  - Lat_low)/Res01)\n",
    "                ilon = int((crop_loc['FIRST_Longitude'][idx] - Lon_left)/Res01)\n",
    "                map_crop[ilat,ilon,iyear] += crop_loc['SUM_Area_AllCrops'][idx]\n",
    "            else:\n",
    "                map_crop_nongrid[iyear] += crop_loc['SUM_Area_AllCrops'][idx]\n",
    "        print('Year:', year_range_str[iyear])\n",
    "        print ('Database crop area: ', np.sum(crop_loc['SUM_Area_AllCrops']))\n",
    "        print ('Gridded  crop area: ', np.sum(map_crop[:,:,iyear]))\n",
    "        ct = datetime.datetime.now() \n",
    "        print(\"current time:\", ct)\n",
    "    np.save('./IntermediateOutputs/Crops_tempoutput', map_crop)\n",
    "    np.save('./IntermediateOutputs/Crops_nongrid_tempoutput', map_crop_nongrid)\n",
    "    \n",
    "else:\n",
    "    map_crop = np.load('./IntermediateOutputs/Crops_tempoutput.npy')\n",
    "    map_crop_nongrid = np.load('./IntermediateOutputs/Crops_nongrid_tempoutput.npy')\n",
    "    for iyear in np.arange(0,num_years):\n",
    "        print('Year:', year_range_str[iyear])\n",
    "        print ('Gridded  crop area: ', np.sum(map_crop[:,:,iyear]))\n",
    "        ct = datetime.datetime.now() \n",
    "        print(\"current time:\", ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "## Step 3. Read in and Format US EPA GHGI Emissions\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#We only map emissions which are reported as being substantial in the inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read mobile combustion emissions (units = Gg (==kt))\n",
    "\n",
    "names = pd.read_csv(EPA_comb_inputfile,  skiprows = 1, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "EPA_emi_mobcomb_CH4 = pd.read_csv(EPA_comb_inputfile, skiprows = 2, names = colnames, nrows = 18)\n",
    "EPA_emi_mobcomb_CH4 = EPA_emi_mobcomb_CH4.fillna('')\n",
    "EPA_emi_mobcomb_CH4 = EPA_emi_mobcomb_CH4.drop(columns = [str(n) for n in range(1990, start_year,1)])\n",
    "EPA_emi_mobcomb_CH4.reset_index(inplace=True, drop=True)\n",
    "EPA_emi_mobcomb_CH4['Fuel Type/Vehicle Type'] = EPA_emi_mobcomb_CH4['Fuel Type/Vehicle Type'].str.replace(\"*\",\"\")\n",
    "EPA_mobcomb_total = EPA_emi_mobcomb_CH4[EPA_emi_mobcomb_CH4['Fuel Type/Vehicle Type'] == 'Total']\n",
    "\n",
    "display(EPA_mobcomb_total)\n",
    "display(EPA_emi_mobcomb_CH4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Split Emissions into Gridding Groups (each Group will have the same proxy applied during the state allocation/gridding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split GHG emissions into gridding groups, based on Mobile Combustion Proxy Mapping file\n",
    "\n",
    "DEBUG =0\n",
    "start_year_idx = EPA_emi_mobcomb_CH4.columns.get_loc(str(start_year))\n",
    "end_year_idx = EPA_emi_mobcomb_CH4.columns.get_loc(str(end_year))+1\n",
    "ghgi_mob_groups = ghgi_mob_map['GHGI_Emi_Group'].unique()\n",
    "sum_emi = np.zeros([num_years])\n",
    "\n",
    "for igroup in np.arange(0,len(ghgi_mob_groups)): #loop through all groups, finding the GHGI sources in that group and summing emissions for that region, year        vars()[ghgi_prod_groups[igroup]] = np.zeros([num_regions-1,num_years])\n",
    "    ##DEBUG## print(ghgi_stat_groups[igroup])\n",
    "    vars()[ghgi_mob_groups[igroup]] = np.zeros([num_years])\n",
    "    source_temp = ghgi_mob_map.loc[ghgi_mob_map['GHGI_Emi_Group'] == ghgi_mob_groups[igroup], 'GHGI_Source']\n",
    "    pattern_temp  = '|'.join(source_temp) \n",
    "   # print(pattern_temp) \n",
    "    emi_temp =EPA_emi_mobcomb_CH4[EPA_emi_mobcomb_CH4['Fuel Type/Vehicle Type'].str.contains(pattern_temp)]\n",
    "    #display(emi_temp)\n",
    "    if 'Light-Duty' in pattern_temp :\n",
    "        vars()[ghgi_mob_groups[igroup]][:] = emi_temp.iloc[0,start_year_idx:] #only use the first value\n",
    "    elif 'Passenger' in pattern_temp or 'Heavy' in pattern_temp:\n",
    "        vars()[ghgi_mob_groups[igroup]][:] = emi_temp.iloc[0:2,start_year_idx:].sum()\n",
    "    else:\n",
    "        vars()[ghgi_mob_groups[igroup]][:] = emi_temp.iloc[:,start_year_idx:].sum()\n",
    "    #display(vars()[ghgi_mob_groups[igroup]][:])\n",
    "        \n",
    "        \n",
    "#Check against total summary emissions \n",
    "print('QA/QC #1: Check Processing Emission Sum against GHGI Summary Emissions')\n",
    "for iyear in np.arange(0,num_years): \n",
    "    for igroup in np.arange(0,len(ghgi_mob_groups)):\n",
    "        sum_emi[iyear] += vars()[ghgi_mob_groups[igroup]][iyear]\n",
    "        \n",
    "    summary_emi = EPA_mobcomb_total.iloc[0,iyear+1]  \n",
    "    #Check 1 - make sure that the sums from all the regions equal the totals reported\n",
    "    diff1 = abs(sum_emi[iyear] - summary_emi)/((sum_emi[iyear] + summary_emi)/2)\n",
    "    if DEBUG==1:\n",
    "        print(summary_emi)\n",
    "        print(sum_emi[iyear])\n",
    "    if diff1 < 0.0001:\n",
    "        print('Year ', year_range[iyear],': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear],': FAIL (check Production & summary tabs): ', diff1,'%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Step 4. Grid Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1. Allocate emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.1.1 Assign the Appropriate Proxy Variable Names (state & grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names on the *left* need to match the 'ProxyMapping' 'State_Proxy_Group' names \n",
    "# (these are initialized in Step 2). \n",
    "# The names on the *right* are the variable names used to caluclate the proxies in this code.\n",
    "# Names on the right need to match those from the code in Step 2\n",
    "\n",
    "#state proxies are in dimensions (urban/rural x road type x state x year)\n",
    "State_Passenger = vmt_pas\n",
    "State_Light = vmt_lig\n",
    "State_Heavy = vmt_hea\n",
    "State_AllRoads = vmt_tot\n",
    "\n",
    "\n",
    "#state --> 0.01 proxies\n",
    "Map_Roads = map_roads\n",
    "Map_Waterways = map_waterways\n",
    "Map_Railroads = map_rail\n",
    "Map_Pop = pop_abs\n",
    "Map_Farm = map_crop\n",
    "Map_Mines = map_mines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.1.2. Allocate to the State level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate state-level emissions\n",
    "# Emissions in kt\n",
    "# State data = national GHGI emissions * state proxy/national total\n",
    "\n",
    "# Note that national emissions are retained for groups that do not have state proxies (identified in the mapping file)\n",
    "# and are gridded in the next step\n",
    "DEBUG = 1\n",
    "\n",
    "# Make placeholder emission arrays for each group\n",
    "# Urban/Rural flag == 2 indicates that the proxy data have both road and region type information\n",
    "# Urbal/Rural flag == 1 indicates that the proxy data has region type information\n",
    "for igroup in np.arange(0,len(proxy_mob_map)):\n",
    "    if proxy_mob_map.loc[igroup,'Urban_Rural_Flag'] ==2:\n",
    "        vars()['State_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([2,3,len(State_ANSI),num_years])\n",
    "        vars()['NonState_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([2,3,num_years])\n",
    "    elif proxy_mob_map.loc[igroup,'Urban_Rural_Flag'] ==1:\n",
    "        vars()['State_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([2,len(State_ANSI),num_years])\n",
    "        vars()['NonState_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([2,num_years])\n",
    "    else:\n",
    "        vars()['State_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(State_ANSI),num_years])\n",
    "        vars()['NonState_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])\n",
    "        \n",
    "#Loop over years\n",
    "for iyear in np.arange(num_years):\n",
    "    #Loop over states\n",
    "    for istate in np.arange(len(State_ANSI)):\n",
    "        for igroup in np.arange(0,len(proxy_mob_map)):    \n",
    "            if proxy_mob_map.loc[igroup,'State_Proxy_Group'] != '-' and proxy_mob_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "                if proxy_mob_map.loc[igroup,'Urban_Rural_Flag'] ==2:\n",
    "                    for iregion in np.arange(0,2):\n",
    "                        for iroad in np.arange(0,3):\n",
    "                            vars()['State_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][iregion,iroad,istate,iyear] = \\\n",
    "                                vars()[proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                        data_fn.safe_div(vars()[proxy_mob_map.loc[igroup,'State_Proxy_Group']][iregion,iroad,istate,iyear], \\\n",
    "                                         np.sum(vars()[proxy_mob_map.loc[igroup,'State_Proxy_Group']][:,:,:,iyear]))\n",
    "                elif proxy_mob_map.loc[igroup,'Urban_Rural_Flag'] ==1:\n",
    "                    for iregion in np.arange(0,2):\n",
    "                        vars()['State_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][iregion,istate,iyear] = \\\n",
    "                                vars()[proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                        data_fn.safe_div(vars()[proxy_mob_map.loc[igroup,'State_Proxy_Group']][iregion,istate,iyear], \\\n",
    "                                         np.sum(vars()[proxy_mob_map.loc[igroup,'State_Proxy_Group']][:,:,iyear]))\n",
    "\n",
    "            else:\n",
    "                vars()['NonState_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][iyear] = vars()[proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "                \n",
    "# Check sum of all gridded emissions + emissions not included in state allocation\n",
    "print('QA/QC #1: Check weighted emissions against GHGI')   \n",
    "for iyear in np.arange(0,num_years):\n",
    "    summary_emi = EPA_mobcomb_total.iloc[0,iyear+1] \n",
    "    calc_emi = 0\n",
    "    for igroup in np.arange(0,len(proxy_mob_map)):\n",
    "        if proxy_mob_map.loc[igroup,'Urban_Rural_Flag'] ==2:\n",
    "            calc_emi +=  np.sum(vars()['State_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][:,:,:,iyear])+\\\n",
    "                        np.sum(vars()['NonState_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])\n",
    "        elif proxy_mob_map.loc[igroup,'Urban_Rural_Flag'] ==1:\n",
    "            calc_emi +=  np.sum(vars()['State_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])+\\\n",
    "                        np.sum(vars()['NonState_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][:,iyear])\n",
    "        else:\n",
    "            calc_emi +=  np.sum(vars()['State_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][:,iyear])\n",
    "            calc_emi += vars()['NonState_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][iyear] #np.sum(Emissions[:,iyear]) + Emissions_nongrid[iyear] + Emissions_nonstate[iyear]\n",
    "    if DEBUG ==1:\n",
    "        print(summary_emi)\n",
    "        print(calc_emi)\n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0002:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.3 Allocate emissions to the CONUS region (0.1x0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Allocate State-Level emissions (kt) onto a 0.1x0.1 grid using gridcell level 'Proxy_Groups'\n",
    "\n",
    "DEBUG =1\n",
    "\n",
    "# For each year, (2a) distribute state-level emissions onto a grid using proxies defined above ....\n",
    "# To speed up the code, masks are used rather than looping individually through each lat/lon. \n",
    "# In this case, a mask of 1's is made for the grid cells that match the ANSI values for a given state\n",
    "# The masked values are set to zero, remaining values = 1. \n",
    "# AK and HI and territories are removed from the analysis at this stage. \n",
    "# The emissions allocated to each state are at 0.01x0.01 degree resolution, as required to calculate accurate 'mask'\n",
    "# arrays for each state. \n",
    "# (2b) For emission groups that were not first allocated to states, national emissions for those groups are gridded\n",
    "# based on the relevant gridded proxy arrays (0.1x0.1 resolution). These emissions are at 0.1x0.1 degrees resolution. \n",
    "# (2c) - record 'not mapped' emission groups in the 'non-grid' array\n",
    "\n",
    "print('**QA/QC Check: Sum of national gridded emissions vs. GHGI national emissions')\n",
    "running_sum = np.zeros([len(proxy_mob_map),num_years])\n",
    "\n",
    "for igroup in np.arange(4,len(proxy_mob_map)):\n",
    "    proxy_temp = vars()[proxy_mob_map.loc[igroup,'Proxy_Group']]\n",
    "    proxy_temp_nongrid = vars()[proxy_mob_map.loc[igroup,'Proxy_Group']+'_nongrid']\n",
    "    \n",
    "    \n",
    "    #2a. Step through each state (if group was previously allocated to state level)\n",
    "    if proxy_mob_map.loc[igroup,'State_Proxy_Group'] != '-' and \\\n",
    "        proxy_mob_map.loc[igroup,'State_Proxy_Group'] != 'state_not_mapped':\n",
    "        print('Group:',igroup,'of ',len(proxy_mob_map))\n",
    "        vars()['Ext_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']+'_01'] = np.zeros([len(lat001),len(lon001),num_years])\n",
    "\n",
    "        for istate in np.arange(0,len(State_ANSI)):\n",
    "            #print(igroup,istate)\n",
    "            \n",
    "            if State_ANSI['abbr'][istate] not in {'AK','HI'} and istate < 51:\n",
    "                mask_state = np.ma.ones(np.shape(state_ANSI_map))\n",
    "                mask_state = np.ma.masked_where(state_ANSI_map != State_ANSI['ansi'][istate], mask_state)\n",
    "                mask_state = np.ma.filled(mask_state,0) \n",
    "                if proxy_mob_map.loc[igroup, 'Grid_Urban_Rural_Flag'] ==2:\n",
    "                    if proxy_mob_map.loc[igroup, 'Urban_Rural_Flag'] ==2:\n",
    "                        for iyear in np.arange(0,num_years):\n",
    "                            for iregion in np.arange(0,2):\n",
    "                                for iroad in np.arange(0,3):\n",
    "                                    emi_temp = vars()['State_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][iregion,iroad,istate,iyear]\n",
    "                                    #print(emi_temp)\n",
    "                                    if np.sum(mask_state*proxy_temp[iregion,iroad,:,:,iyear]) > 0 and emi_temp > 0: \n",
    "                                    # if state is on grid and proxy for that state is non-zero\n",
    "                                        weighted_array = data_fn.safe_div(mask_state*proxy_temp[iregion,iroad,:,:,iyear], \\\n",
    "                                                                      np.sum(mask_state*proxy_temp[iregion,iroad,:,:,iyear]))\n",
    "                                        #weighted_array_01 = data_fn.regrid001_to_01(weighted_array, Lat_01, Lon_01)\n",
    "                                        #print(np.sum(weighted_array))\n",
    "                                        Emissions_array_001[:,:,iyear] += emi_temp*weighted_array#_01\n",
    "                                        vars()['Ext_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']+'_01'][:,:,iyear]+=emi_temp*weighted_array\n",
    "                                        running_sum[igroup,iyear] += np.sum(emi_temp*weighted_array)\n",
    "                                    else:\n",
    "                                    #for imonth in np.arange(0,num_months):\n",
    "                                        Emissions_nongrid[iyear] += emi_temp\n",
    "                                        running_sum[igroup,iyear] += np.sum(emi_temp)\n",
    "                        print(running_sum[igroup,iyear])\n",
    "                \n",
    "                    elif proxy_mob_map.loc[igroup, 'Urban_Rural_Flag'] ==1:\n",
    "                        for iyear in np.arange(0, num_years):\n",
    "                            for iregion in np.arange(0,2):\n",
    "                                #for iroad in np.arange(0,3):\n",
    "                                emi_temp = vars()['State_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][iregion,istate,iyear]\n",
    "                                if np.sum(mask_state*np.sum(proxy_temp[iregion,:,:,:,iyear],axis=0)) > 0 and emi_temp > 0: \n",
    "                                    # if state is on grid and proxy for that state is non-zero\n",
    "                                    weighted_array = data_fn.safe_div(mask_state*np.sum(proxy_temp[iregion,:,:,:,iyear],axis=0), \\\n",
    "                                                                  np.sum(mask_state*np.sum(proxy_temp[iregion,:,:,:,iyear],axis=0)))\n",
    "                                    #weighted_array_01 = data_fn.regrid001_to_01(weighted_array, Lat_01, Lon_01)\n",
    "                                    Emissions_array_001[:,:,iyear] += emi_temp*weighted_array#_01\n",
    "                                    vars()['Ext_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']+'_01'][:,:,iyear]+=emi_temp*weighted_array\n",
    "                                    #print(np.sum(weighted_array))\n",
    "                                    running_sum[igroup,iyear] += np.sum(emi_temp*weighted_array)\n",
    "                                    #print(emi_temp)\n",
    "                                    #print(np.sum(emi_temp*weighted_array))\n",
    "                                else:\n",
    "                                    #for imonth in np.arange(0,num_months):\n",
    "                                    Emissions_nongrid[iyear] += emi_temp\n",
    "                                    running_sum[igroup,iyear] += np.sum(emi_temp)\n",
    "\n",
    "            else:\n",
    "                if proxy_mob_map.loc[igroup, 'Urban_Rural_Flag'] ==2:\n",
    "                    for iyear in np.arange(0, num_years):\n",
    "                        Emissions_nongrid[iyear] += np.sum(vars()['State_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][:,:,istate,iyear])\n",
    "                        running_sum[igroup,iyear] += np.sum(vars()['State_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][:,:,istate,iyear])    \n",
    "\n",
    "                elif proxy_mob_map.loc[igroup, 'Urban_Rural_Flag'] ==1:\n",
    "                    for iyear in np.arange(0, num_years):\n",
    "                        Emissions_nongrid[iyear] += np.sum(vars()['State_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][:,istate,iyear])\n",
    "                        running_sum[igroup,iyear] += np.sum(vars()['State_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][:,istate,iyear])    \n",
    "         \n",
    "    #2b. if emissions were not allocated to state, allocate national total to grid here (these are in 0.1x0.1 resolution)\n",
    "    elif proxy_mob_map.loc[igroup,'State_Proxy_Group'] == '-':\n",
    "        vars()['Ext_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']+'_temp'] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            temp_sum = np.sum(vars()[proxy_mob_map.loc[igroup,'Proxy_Group']][:,:,iyear])+np.sum(vars()[proxy_mob_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear])\n",
    "            emi_temp= vars()[proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                       data_fn.safe_div(vars()[proxy_mob_map.loc[igroup,'Proxy_Group']][:,:,iyear], temp_sum)\n",
    "            Emissions_array_01_temp[:,:,iyear] += emi_temp\n",
    "            vars()['Ext_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']+'_temp'][:,:,iyear]+= emi_temp\n",
    "            #print(np.sum(vars()['Ext_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']+'_temp'][:,:,iyear]))\n",
    "            Emissions_nongrid[iyear] += vars()[proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][iyear] *\\\n",
    "                        data_fn.safe_div(vars()[proxy_mob_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear], temp_sum)\n",
    "            ##DEBUG## running_count += vars()[proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "            running_sum[igroup,iyear] += np.sum(emi_temp) + \\\n",
    "                        vars()[proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][iyear] *\\\n",
    "                        data_fn.safe_div(vars()[proxy_mob_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear], temp_sum)    \n",
    "        print(running_sum[igroup,iyear])\n",
    "    #2c. this is the case that GHGI emissions are not mapped (e.g., specified outside of CONUS in the GHGI)\n",
    "    elif proxy_mob_map.loc[igroup,'Proxy_Group'] == 'Map_not_mapped':  \n",
    "        for iyear in np.arange(0, num_years):\n",
    "            Emissions_nongrid[iyear] += vars()[proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "            running_sum[igroup,iyear] += vars()[proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][iyear] \n",
    "        print(running_sum[igroup,iyear])\n",
    "\n",
    "for igroup in np.arange(0, len(proxy_mob_map)):\n",
    "    vars()['Ext_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "\n",
    "for iyear in np.arange(0, num_years):    \n",
    "    Emissions_array_01[:,:,iyear] = data_fn.regrid001_to_01(Emissions_array_001[:,:,iyear], Lat_01, Lon_01)\n",
    "    Emissions_array_01[:,:,iyear] += Emissions_array_01_temp[:,:,iyear]\n",
    "    calc_emi = np.sum(Emissions_array_01[:,:,iyear]) + np.sum(Emissions_nongrid[iyear]) \n",
    "    calc_emi2 = 0\n",
    "    for igroup in np.arange(0, len(proxy_mob_map)):\n",
    "        if proxy_mob_map.loc[igroup,'State_Proxy_Group'] != '-' and proxy_mob_map.loc[igroup,'State_Proxy_Group'] != 'state_not_mapped':\n",
    "            vars()['Ext_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]= data_fn.regrid001_to_01(vars()['Ext_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']+'_01'][:,:,iyear], Lat_01, Lon_01)\n",
    "            calc_emi2 += np.sum(vars()['Ext_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])\n",
    "        else:\n",
    "            #print(proxy_mob_map.loc[igroup,'GHGI_Emi_Group'])\n",
    "            vars()['Ext_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += vars()['Ext_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']+'_temp'][:,:,iyear]\n",
    "            calc_emi2 += np.sum(vars()['Ext_'+proxy_mob_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])\n",
    "    calc_emi2 += np.sum(Emissions_nongrid[iyear]) \n",
    "\n",
    "    summary_emi = EPA_mobcomb_total.iloc[0,iyear+1] \n",
    "    emi_diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if DEBUG==1:\n",
    "        print(calc_emi)\n",
    "        print(calc_emi2)\n",
    "        print(summary_emi)\n",
    "    if abs(emi_diff) < 0.0001:\n",
    "        print('Year '+ year_range_str[iyear]+': Difference < 0.01%: PASS')\n",
    "    else: \n",
    "        print('Year '+ year_range_str[iyear]+': Difference > 0.01%: FAIL, diff: '+str(emi_diff))\n",
    "        \n",
    "ct = datetime.datetime.now() \n",
    "print(\"current time:\", ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1.4 Save gridded emissions (kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save gridded emissions for each gridding group - for extension\n",
    "\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(grid_emi_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "unique_groups = np.unique(proxy_mob_map['GHGI_Emi_Group'])\n",
    "unique_groups = unique_groups[unique_groups != 'Emi_not_mapped']\n",
    "\n",
    "nc_out = Dataset(grid_emi_outputfile, 'r+', format='NETCDF4')\n",
    "#nc_out.createDimension('state', len(State_ANSI))\n",
    "\n",
    "for igroup in np.arange(0,len(unique_groups)):\n",
    "    print('Ext_'+unique_groups[igroup])\n",
    "    if len(np.shape(vars()['Ext_'+unique_groups[igroup]])) ==4:\n",
    "        ghgi_temp = np.sum(vars()[unique_groups[igroup]],axis=3) #sum month data if data is monthly\n",
    "    else:\n",
    "        ghgi_temp = vars()['Ext_'+unique_groups[igroup]]\n",
    "\n",
    "    # Write data to netCDF\n",
    "    data_out = nc_out.createVariable('Ext_'+unique_groups[igroup], 'f8', ('lat', 'lon','year'), zlib=True)\n",
    "    data_out[:,:,:] = ghgi_temp[:,:,:]\n",
    "\n",
    "#save nongrid data to calculate non-grid fraction extension\n",
    "data_out = nc_out.createVariable('Emissions_nongrid', 'f8', ('year'), zlib=True)  \n",
    "data_out[:] = Emissions_nongrid[:]\n",
    "nc_out.close()\n",
    "\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded emissions (kt) written to file: {}\" .format(os.getcwd())+grid_emi_outputfile)\n",
    "print(' ')\n",
    "\n",
    "del data_out, ghgi_temp, nc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Calculate Gridded Emission Fluxes (molec./cm2/s) (0.1x0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert emissions to emission flux\n",
    "# convert kt to molec/cm2/s\n",
    "\n",
    "Flux_array_01 = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Flux_array_01_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "print('**QA/QC Check: Sum of national gridded emissions vs. GHGI national emissions')\n",
    "  \n",
    "for iyear in np.arange(0,num_years):\n",
    "    calc_emi = 0\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "        #month_days = month_day_leap\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        #month_days = month_day_nonleap\n",
    "        \n",
    "    #for imonth in np.arange(0,num_months):\n",
    "    conversion_factor_01 = 10**9 * Avogadro / float(Molarch4 *year_days * 24 * 60 *60) / area_matrix_01\n",
    "    Flux_array_01_annual[:,:,iyear] = Emissions_array_01[:,:,iyear]*conversion_factor_01\n",
    "    #convert back to mass to check\n",
    "    conversion_factor_annual = 10**9 * Avogadro / float(Molarch4 *year_days * 24 * 60 *60) / area_matrix_01\n",
    "    calc_emi = np.sum(Flux_array_01_annual[:,:,iyear]/conversion_factor_annual)+np.sum(Emissions_nongrid[iyear])\n",
    "    #calc_emi += np.sum(Emissions_nongrid[iyear,:])\n",
    "    summary_emi = EPA_mobcomb_total.iloc[0,iyear+1] \n",
    "    emi_diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if DEBUG==1:\n",
    "        print(calc_emi)\n",
    "        print(summary_emi)\n",
    "    if abs(emi_diff) < 0.0001:\n",
    "        print('Year '+ year_range_str[iyear]+': Difference < 0.01%: PASS')\n",
    "    else: \n",
    "        print('Year '+ year_range_str[iyear]+': Difference > 0.01%: FAIL, diff: '+str(emi_diff))\n",
    "        \n",
    "Flux_Emissions_Total_annual = Flux_array_01_annual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 5. Write netCDF\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearly data\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(gridded_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write data to netCDF\n",
    "nc_out = Dataset(gridded_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Total_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded stationary combustion fluxes written to file: {}\" .format(os.getcwd())+gridded_outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Step 6. Plot Gridded Data\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.1. Plot Annual Emission Fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot Annual Data\n",
    "scale_max = 10\n",
    "save_flag = 0\n",
    "save_fig = ''\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_str,scale_max,save_flag,save_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.2 Plot Difference between first and last inventory year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot difference between last and first year\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_diff_str,save_flag,save_outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = datetime.datetime.now() \n",
    "ft = ct.timestamp() \n",
    "time_elapsed = (ft-it)/(60*60)\n",
    "print('Time to run: '+str(time_elapsed)+' hours')\n",
    "print('** GEPA_1A_Combustion_Mobile: COMPLETE **')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

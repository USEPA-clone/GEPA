{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded EPA Methane Inventory\n",
    "## Category: 1B2b Natural Gas Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Authors: \n",
    "Joannes D. Maasakkers, Erin E. McDuffie\n",
    "#### Date Last Updated: \n",
    "see Step 0\n",
    "#### Notebook Purpose: \n",
    "This Notebook calculates and reports annual and monthly gridded methane emission fluxes (molec./cm2/s) from natural gas systems exploration and production segments in the CONUS region between 2012-2018. \n",
    "#### Summary & Notes:\n",
    "EPA GHGI gas exploration and production emissions are read in from the GHGI Natural Gas Systems workbook at the NEMS level, where available (national otherwise). Emissions are then distributed onto a 0.1°x0.1° grid as a function of emission group. The activity/proxy data used to spatially distribute emissions from each group include well locations and production levels from Enverus (DI and Prism), EIA state-level Lease Condensate production data, NEI 4km grid data (for the states of IL and IN), and BOEM GOADS platform emissions and location data for Federal Offshore emissions. Emissions data are calculated as a function of month, largely determined by whether a well was producing in a particular month or not (from Enverus). Some proxy data are only available with annual data and are allocated evenly across each month. Both monthly and annual emission fluxes (molec./cm2/s) are written to final netCDFs in the ‘/code/Final_Gridded_Data/’ folder.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Step 0. Set-Up Notebook Modules, Functions, and Local Parameters and Constants\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm working directory & print last update time\n",
    "import os\n",
    "import time\n",
    "modtime = os.path.getmtime('./1B2b_Natural_Gas_Systems_Production.ipynb')\n",
    "modificationTime = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(modtime))\n",
    "print(\"This file was last modified on: \", modificationTime)\n",
    "print('')\n",
    "print(\"The directory we are working in is {}\" .format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Include plots within notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import pyodbc\n",
    "import PyPDF2 as pypdf\n",
    "import tabula as tb\n",
    "import shapefile as shp\n",
    "from datetime import datetime\n",
    "from copy import copy\n",
    "import pyproj \n",
    "\n",
    "# Import additional modules\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# Load netCDF (for manipulating netCDF file types)\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Set up ticker\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#add path for the global function module (file)\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../Global_Functions/'))\n",
    "#print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Load functions\n",
    "import data_load_functions as data_load_fn\n",
    "import data_functions as data_fn\n",
    "import data_IO_functions as data_IO_fn\n",
    "import data_plot_functions as data_plot_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SPECIFY RECALS ##\n",
    "\n",
    "# Specify which sections to re-calculate or load from previously saved arrays\n",
    "# for time saving purposes\n",
    "#0 = load from saved files, 1 = re-calculate\n",
    "\n",
    "# 1) ReCalc Enverus Production Data?\n",
    "ReCalc_Enverus =1\n",
    "\n",
    "# 2) ReCalc Offshore GOADS Data\n",
    "ReCalc_GOADS = 0\n",
    "\n",
    "# 3) Re-Calc NEI Indiana and Illinois data?\n",
    "ReCalc_NEI = 0\n",
    "\n",
    "# 4) ReCalc Lease Condensate Data?\n",
    "ReCalc_Condensates = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT Files\n",
    "# Assign global file names\n",
    "global_filenames = data_load_fn.load_global_file_names()\n",
    "State_ANSI_inputfile = global_filenames[0]\n",
    "#County_ANSI_inputfile = global_filenames[1]\n",
    "#pop_map_inputfile = global_filenames[2]\n",
    "Grid_area01_inputfile = global_filenames[3]\n",
    "Grid_area001_inputfile = global_filenames[4]\n",
    "#Grid_state001_ansi_inputfile = global_filenames[5]\n",
    "#Grid_county001_ansi_inputfile = global_filenames[6]\n",
    "globalinputlocation = global_filenames[0][0:20]\n",
    "print(globalinputlocation)\n",
    "\n",
    "# EPA Inventory Data\n",
    "EPA_NG_prod_inputfile = globalinputlocation+'GHGI/Ch3_Energy/NaturalGasSystems_1990-2018_GHGI_2020-04-11.xlsx'\n",
    "\n",
    "#proxy mapping file\n",
    "NG_Mapping_inputfile = './InputData/NaturalGas_Production_ProxyMapping.xlsx'\n",
    "\n",
    "#NEI grid reference\n",
    "NEI_grid_ref_inputfile = globalinputlocation+'Gridded/NEI_Reference_Grid_LCC_to_WGS84_latlon.shp'\n",
    "\n",
    "#ERG/NEI Spatial Surrogate Data\n",
    "ERG_NEI_inputloc = globalinputlocation+'NEI/ERG_ILINData/CONUS_SA_FILES_'\n",
    "ERG_NEI_inputloc_2018 = globalinputlocation+'NEI/ERG_ILINData/IL_IN_ALLOCATED_WELL_LEVEL_DATA_2018_2019/IL_IN_WELL_LEVEL_DATA.accdb'\n",
    "\n",
    "#ERG Processed Well Count and Production Notebook\n",
    "Enverus_WellCounts_inputfile = globalinputlocation+'Enverus/Enverus DrillingInfo Processing - Well Counts_2021-03-17.xlsx'\n",
    "Enverus_WellProd_inputfile = globalinputlocation+'Enverus/Enverus DrillingInfo Processing - Well Prod_2021-03-17.xlsx'\n",
    "\n",
    "#Activity Data\n",
    "Enverus_Prism_inputdata_2019 = globalinputlocation+ 'Enverus/Production/prism_monthly_2019_110221.csv'\n",
    "Enverus_Prism_inputdata_2018 = globalinputlocation+ 'Enverus/Production/prism_monthly_2018_110221.csv'\n",
    "Enverus_Prism_inputdata_2017 = globalinputlocation+ 'Enverus/Production/prism_monthly_2017_110221.csv'\n",
    "Enverus_Prism_inputdata_2016 = globalinputlocation+ 'Enverus/Production/prism_monthly_2016_110221.csv'\n",
    "Enverus_Prism_inputdata_2015 = globalinputlocation+ 'Enverus/Production/prism_monthly_2015_110221.csv'\n",
    "Enverus_Prism_inputdata_2014 = globalinputlocation+ 'Enverus/Production/prism_monthly_2014_110221.csv'\n",
    "Enverus_Prism_inputdata_2013 = globalinputlocation+ 'Enverus/Production/prism_monthly_2013_110221.csv'\n",
    "Enverus_Prism_inputdata_2012 = globalinputlocation+ 'Enverus/Production/prism_monthly_2012_110221.csv'\n",
    "\n",
    "Enverus_DI_inputdata_2019 = globalinputlocation+ 'Enverus/Production/didsk_monthly_2019_102621.csv'\n",
    "Enverus_DI_inputdata_2018 = globalinputlocation+ 'Enverus/Production/didsk_monthly_2018_102621.csv'\n",
    "Enverus_DI_inputdata_2017 = globalinputlocation+ 'Enverus/Production/didsk_monthly_2017_102621.csv'\n",
    "Enverus_DI_inputdata_2016 = globalinputlocation+ 'Enverus/Production/didsk_monthly_2016_102621.csv'\n",
    "Enverus_DI_inputdata_2015 = globalinputlocation+ 'Enverus/Production/didsk_monthly_2015_102621.csv'\n",
    "Enverus_DI_inputdata_2014 = globalinputlocation+ 'Enverus/Production/didsk_monthly_2014_102621.csv'\n",
    "Enverus_DI_inputdata_2013 = globalinputlocation+ 'Enverus/Production/didsk_monthly_2013_102621.csv'\n",
    "Enverus_DI_inputdata_2012 = globalinputlocation+ 'Enverus/Production/didsk_monthly_2012_102621.csv'\n",
    "\n",
    "Enverus_NG_GBstations_inputfile = globalinputlocation+ 'Enverus/Midstream/Gathering_CompressorStations_CONUS_onshore_wgs84.xls'\n",
    "Enverus_NG_GBpipeline_inputfile = globalinputlocation+ 'Enverus/Midstream/Gathering_pipelines_CONUS_onshore_WGS84_01x01.xls'\n",
    "AKHI_pipelines_shp = globalinputlocation+ 'Enverus/Midstream/Gathering_pipelines_AKHI_wgs84.shp'\n",
    "CONUS_pipelines_shp = globalinputlocation+ 'Enverus/Midstream/Gathering_pipelines_CONUS_onshore_wgs84.shp'\n",
    "\n",
    "# Lease Condensate Data\n",
    "EIA_Condensate_inputfile = './InputData/NG_PROD_LC_S1_A.xls'\n",
    "\n",
    "# Offshore GOADS Data\n",
    "GOADS_11_inputfile = globalinputlocation+'BOEM/2011_Gulfwide_Platform_Inventory.accdb'\n",
    "GOADS_14_inputfile = globalinputlocation+'BOEM/2014_Gulfwide_Platform_Inventory_20161102.accdb'\n",
    "GOADS_17_inputfile = globalinputlocation+'BOEM/2017_Gulfwide_Platform_Inventory_20190705_CAP_GHG.accdb'\n",
    "ERG_GOADSEmissions_inputfile = globalinputlocation+'BOEM/BOEM GEI Emissions Data_EmissionSource_2020-03-11.xlsx'\n",
    "\n",
    "#OUTPUT FILES\n",
    "gridded_prod_outputfile = '../Final_Gridded_Data/EPA_v2_1B2b_Natural_Gas_Production.nc'\n",
    "gridded_prod_monthly_outputfile = '../Final_Gridded_Data/EPA_v2_1B2b_Natural_Gas_Production_Month.nc'\n",
    "netCDF_prod_description = 'Gridded EPA Inventory - Gas Production Emissions - IPCC Source Category 1B2b'\n",
    "title_prod_str = \"EPA methane emissions from gas production\"\n",
    "title_prod_diff_str = \"Emissions from gas production difference: 2018-2012\"\n",
    "\n",
    "gridded_expl_outputfile = '../Final_Gridded_Data/EPA_v2_1B2b_Natural_Gas_Exploration.nc'\n",
    "gridded_expl_monthly_outputfile = '../Final_Gridded_Data/EPA_v2_1B2b_Natural_Gas_Exploration_Month.nc'\n",
    "netCDF_expl_description = 'Gridded EPA Inventory - Gas Exploration Emissions - IPCC Source Category 1B2b'\n",
    "title_expl_str = \"EPA methane emissions from gas exploration\"\n",
    "title_expl_diff_str = \"Emissions from gas exploration difference: 2018-2012\"\n",
    "\n",
    "\n",
    "#output gridded proxy data\n",
    "grid_emi_outputfile = '../Final_Gridded_Data/Extension/v2_input_data/NG_Production_Grid_Emi.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local variables\n",
    "start_year = 2012  #First year in emission timeseries\n",
    "end_year = 2018    #Last year in emission timeseries\n",
    "year_range = [*range(start_year, end_year+1,1)] #List of emission years\n",
    "year_range_str=[str(i) for i in year_range]\n",
    "num_years = len(year_range)\n",
    "\n",
    "# Define constants\n",
    "Avogadro   = 6.02214129 * 10**(23)  #molecules/mol\n",
    "Molarch4   = 16.04                  #g/mol\n",
    "Res01      = 0.1                    # degrees\n",
    "\n",
    "# Continental US Lat/Lon Limits (for netCDF files)\n",
    "Lon_left = -130       #deg\n",
    "Lon_right = -60       #deg\n",
    "Lat_low  = 20         #deg\n",
    "Lat_up  = 55          #deg\n",
    "loc_dimensions = [Lat_low, Lat_up, Lon_left, Lon_right]\n",
    "\n",
    "ilat_start = int((90+Lat_low)/Res01) #1100:1450 (continental US range)\n",
    "ilat_end = int((90+Lat_up)/Res01)\n",
    "ilon_start = abs(int((-180-Lon_left)/Res01)) #500:1200 (continental US range)\n",
    "ilon_end = abs(int((-180-Lon_right)/Res01))\n",
    "\n",
    "# Number of days in each month\n",
    "month_day_leap  = [  31,  29,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_day_nonleap = [  31,  28,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_tag = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "month_dict = {'January':1, 'February':2,'March':3,'April':4,'May':5,'June':6, 'July':7,'August':8,'September':9,'October':10,\\\n",
    "             'November':11,'December':12}\n",
    "\n",
    "# Month arrays\n",
    "month_range_str = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "num_months = len(month_range_str)\n",
    "num_regions = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;\n",
    "//prevent auto-scrolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track run time\n",
    "ct = datetime.now() \n",
    "it = ct.timestamp() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Step 1. Load in State ANSI data, NEMS definitions, and Area Maps\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State-level ANSI Data\n",
    "#Read the state ANSI file array\n",
    "State_ANSI, name_dict, abbr_dict = data_load_fn.load_state_ansi(State_ANSI_inputfile)[0:3]\n",
    "#QA: number of states\n",
    "print('Read input file: '+ f\"{State_ANSI_inputfile}\")\n",
    "print('Total \"States\" found: ' + '%.0f' % len(State_ANSI))\n",
    "print(' ')\n",
    "\n",
    "# 0.01 x0.01 degree Data\n",
    "# State ANSI IDs and grid cell area (m2) maps\n",
    "#state_ANSI_map = data_load_fn.load_state_ansi_map(Grid_state001_ansi_inputfile)\n",
    "area_map, lat001, lon001 = data_load_fn.load_area_map_001(Grid_area001_inputfile)\n",
    "\n",
    "# 0.1 x0.1 degree data\n",
    "# grid cell area and state ANSI maps\n",
    "Lat01, Lon01 = data_load_fn.load_area_map_01(Grid_area01_inputfile)[1:3]\n",
    "#Select relevant Continental 0.1 x0.1 domain\n",
    "Lat_01 = Lat01[ilat_start:ilat_end]\n",
    "Lon_01 = Lon01[ilon_start:ilon_end]\n",
    "area_matrix_01 = data_fn.regrid001_to_01(area_map, Lat_01, Lon_01)\n",
    "area_matrix_01 *= 10000  #convert from m2 to cm2\n",
    "#state_ANSI_map_01 = data_fn.regrid001_to_01(state_ANSI_map, Lat_01, Lon_01)\n",
    "del area_map, lat001, lon001, global_filenames\n",
    "\n",
    "# Print time\n",
    "ct = datetime.now() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make NEMS State classifications\n",
    "# Treat NM and TX separately since these states cover multiple NEMS regions\n",
    "\n",
    "#0 = NE, 1 = MC, 2 = RM, 3 = SW, 4 = WC, 5 = GC, 6 = offshore\n",
    "NEMS_State = pd.read_excel(EPA_NG_prod_inputfile, sheet_name = \"Drivers Production\", usecols = \"B:H\", skiprows = 5, nrows = 39)\n",
    "NEMS_State = NEMS_State.fillna(0)\n",
    "NM_idx = NEMS_State.index[NEMS_State['State'].str.contains('New Mexico')].tolist()\n",
    "TX_idx = NEMS_State.index[NEMS_State['State'].str.contains('Texas')].tolist()\n",
    "idx = NM_idx+TX_idx\n",
    "NEMS_State= NEMS_State.drop(NEMS_State.index[idx])\n",
    "NEMS_State.reset_index(drop=True,inplace=True)\n",
    "#print(NEMS_State)\n",
    "\n",
    "NEMS_State['NEMS'] = 0\n",
    "NEMS_State['Ansi'] = 0\n",
    "\n",
    "for istate in np.arange(len(NEMS_State)):\n",
    "    if NEMS_State['NE'][istate] == 1:\n",
    "        NEMS_State.loc[istate,'NEMS'] = 0\n",
    "        NEMS_State.loc[istate,'NEMS_Region'] = 'North East'\n",
    "    elif NEMS_State['MC'][istate] == 1:\n",
    "        NEMS_State.loc[istate,'NEMS'] = 1\n",
    "        NEMS_State.loc[istate,'NEMS_Region'] = 'Midcontinent'\n",
    "    elif NEMS_State['RM'][istate] == 1:\n",
    "        NEMS_State.loc[istate,'NEMS'] = 2\n",
    "        NEMS_State.loc[istate,'NEMS_Region'] = 'Rocky Mountain'\n",
    "    elif NEMS_State['SW'][istate] == 1:\n",
    "        NEMS_State.loc[istate,'NEMS'] = 3\n",
    "        NEMS_State.loc[istate,'NEMS_Region'] = 'South West'\n",
    "    elif NEMS_State['WC'][istate] == 1:\n",
    "        NEMS_State.loc[istate,'NEMS'] = 4\n",
    "        NEMS_State.loc[istate,'NEMS_Region'] = 'West Coast'\n",
    "    elif NEMS_State['GC'][istate] == 1:\n",
    "        NEMS_State.loc[istate,'NEMS'] = 5\n",
    "        NEMS_State.loc[istate,'NEMS_Region'] = 'Gulf Coast'\n",
    "    else:\n",
    "        print('Error for', NEMS_State['State'][istate])\n",
    "    NEMS_State.loc[istate,'Ansi'] = list(abbr_dict.keys())[list(abbr_dict.values()).index(name_dict[NEMS_State['State'][istate]])]\n",
    "    \n",
    "print(NEMS_State)\n",
    "\n",
    "NEMS_dict = {'North East':0, 'Midcontinent':1,'Rocky Mountain':2,'South West':3,'West Coast':4,'Gulf Coast':5}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 2: Read-in and Format Proxy Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 Read In Proxy Mapping File & Make Proxy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load GHGI Mapping Groups\n",
    "names = pd.read_excel(NG_Mapping_inputfile, sheet_name = \"GHGI Map - E&P\", usecols = \"A:B\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "ghgi_prod_map = pd.read_excel(NG_Mapping_inputfile, sheet_name = \"GHGI Map - E&P\", usecols = \"A:B\", skiprows = 2, names = colnames)\n",
    "#drop rows with no data, remove the parentheses and \"\"\n",
    "ghgi_prod_map = ghgi_prod_map[ghgi_prod_map['GHGI_Emi_Group'] != 'na']\n",
    "ghgi_prod_map = ghgi_prod_map[ghgi_prod_map['GHGI_Emi_Group'].notna()]\n",
    "ghgi_prod_map['GHGI_Source']= ghgi_prod_map['GHGI_Source'].str.replace(r\"\\(\",\"\")\n",
    "ghgi_prod_map['GHGI_Source']= ghgi_prod_map['GHGI_Source'].str.replace(r\"\\)\",\"\")\n",
    "ghgi_prod_map['GHGI_Source']= ghgi_prod_map['GHGI_Source'].str.replace(r\"\\[\",\"\")\n",
    "ghgi_prod_map['GHGI_Source']= ghgi_prod_map['GHGI_Source'].str.replace(r\"\\]\",\"\")\n",
    "ghgi_prod_map['GHGI_Source']= ghgi_prod_map['GHGI_Source'].str.replace(r'\"',\"\")\n",
    "ghgi_prod_map.reset_index(inplace=True, drop=True)\n",
    "display(ghgi_prod_map)\n",
    "\n",
    "#load emission group - proxy map\n",
    "names = pd.read_excel(NG_Mapping_inputfile, sheet_name = \"Proxy Map - E&P\", usecols = \"A:D\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "proxy_prod_map = pd.read_excel(NG_Mapping_inputfile, sheet_name = \"Proxy Map - E&P\", usecols = \"A:D\", skiprows = 1, names = colnames)\n",
    "display((proxy_prod_map))\n",
    "\n",
    "#create empty proxy and emission group arrays (add months for proxy variables that have monthly data)\n",
    "for igroup in np.arange(0,len(proxy_prod_map)):\n",
    "    #print(igroup)\n",
    "    if proxy_prod_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "        if proxy_prod_map.loc[igroup,'NEMS_Data'] ==1:\n",
    "            vars()[proxy_prod_map.loc[igroup,'Proxy_Group']] = np.zeros([num_regions-1,len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "            vars()[proxy_prod_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_regions-1,num_years,num_months])\n",
    "            vars()[ghgi_prod_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_regions-1,len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "        else:\n",
    "            vars()[proxy_prod_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "            vars()[proxy_prod_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years,num_months])\n",
    "            vars()[ghgi_prod_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])#\n",
    "\n",
    "    elif proxy_prod_map.loc[igroup, 'Month_Flag'] == 0:\n",
    "        if proxy_prod_map.loc[igroup,'NEMS_Data'] ==1:\n",
    "            vars()[proxy_prod_map.loc[igroup,'Proxy_Group']] = np.zeros([num_regions-1,len(Lat_01),len(Lon_01),num_years])\n",
    "            vars()[proxy_prod_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_regions-1,num_years])\n",
    "            vars()[ghgi_prod_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_regions-1,len(Lat_01),len(Lon_01),num_years])\n",
    "        else:\n",
    "            vars()[proxy_prod_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "            vars()[proxy_prod_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "            vars()[ghgi_prod_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        \n",
    "\n",
    "emi_group_names = np.unique(ghgi_prod_map['GHGI_Emi_Group'])\n",
    "#print(emi_group_names)\n",
    "#print(np.unique(proxy_prod_map['GHGI_Emi_Group']))\n",
    "print('QA/QC: Is the number of emission groups the same for the proxy and emissions tabs?')\n",
    "if (len(emi_group_names) == len(np.unique(proxy_prod_map['GHGI_Emi_Group']))):\n",
    "    print('PASS')\n",
    "else:\n",
    "    print('FAIL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 State Condensate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read in the EIA condensate file (A1) and then format for later use\n",
    "\n",
    "## 1) Read the condensate file\n",
    "state_condensates = pd.read_excel(EIA_Condensate_inputfile, skiprows=2, sheet_name = 'Data 1')\n",
    "state_condensates['Date'] = state_condensates['Date'].astype(str)\n",
    "state_condensates['Date'] = [state_condensates['Date'][i][0:4] for i in np.arange(len(state_condensates))]   #extract the year\n",
    "state_condensates = state_condensates.T\n",
    "state_condensates.columns = state_condensates.iloc[0]\n",
    "state_condensates = state_condensates.drop(state_condensates.index[[0]])\n",
    "Names_cond = state_condensates.index.values.tolist()\n",
    "state_condensates.reset_index(drop=True,inplace=True)\n",
    "state_condensates['State'] = Names_cond\n",
    "state_condensates['NEMS'] = 0\n",
    "state_condensates = state_condensates.fillna(0)\n",
    "#print(state_condensates)\n",
    "\n",
    "# 2) drop extra regions that are not state-specific\n",
    "idx1 = state_condensates.index[state_condensates['State'].str.contains('Calif--')].tolist()\n",
    "idx2 = state_condensates.index[state_condensates['State'].str.contains('California--State')].tolist()\n",
    "idx3 = state_condensates.index[state_condensates['State'].str.contains('Louisiana--')].tolist()\n",
    "idx4 = state_condensates.index[state_condensates['State'].str.contains('New Mexico ')].tolist()\n",
    "idx5 = state_condensates.index[state_condensates['State'].str.contains('Texas ')].tolist()\n",
    "idx6 = state_condensates.index[state_condensates['State'].str.contains('Federal Offshore')].tolist()\n",
    "idx7 = state_condensates.index[state_condensates['State'].str.contains('Lower 48')].tolist()\n",
    "idx8 = state_condensates.index[state_condensates['State'].str.contains('Utah and Wyoming')].tolist()\n",
    "idx = idx1+idx2+idx3+idx4+idx5+idx6+idx7+idx8\n",
    "state_condensates= state_condensates.drop(state_condensates.index[idx])\n",
    "state_condensates.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "# 3) add State names and NEMS regions to each entry\n",
    "for istate in np.arange(0,len(State_ANSI)):          \n",
    "        if state_condensates['State'].str.contains(State_ANSI['name'][istate]).any() and \\\n",
    "            State_ANSI['name'][istate] != 'New Mexico' and State_ANSI['name'][istate] != 'Texas':\n",
    "            match_state = np.where(state_condensates['State'].str.contains(State_ANSI['name'][istate]))[0][:]\n",
    "            nems_match = np.where(NEMS_State['State']==State_ANSI['name'][istate])[0][0]\n",
    "            state_condensates.loc[match_state,'State'] = State_ANSI['name'][istate]\n",
    "            state_condensates.loc[match_state,'NEMS'] = NEMS_State['NEMS'][nems_match] \n",
    "        elif State_ANSI['name'][istate] == 'New Mexico':\n",
    "            #SouthWest\n",
    "            match_state = np.where(state_condensates['State'].str.contains(State_ANSI['name'][istate] and '--East'))[0][:]\n",
    "            state_condensates.loc[match_state,'State'] = State_ANSI['name'][istate]\n",
    "            state_condensates.loc[match_state,'NEMS'] = 3\n",
    "            #Rocky Mountain\n",
    "            match_state = np.where(state_condensates['State'].str.contains(State_ANSI['name'][istate] and '--West'))[0][:]\n",
    "            state_condensates.loc[match_state,'State'] = State_ANSI['name'][istate]\n",
    "            state_condensates.loc[match_state,'NEMS'] = 2\n",
    "        elif State_ANSI['name'][istate] == 'Texas':\n",
    "            #0 = NE, 1 = MC, 2 = RM, 3 = SW, 4 = WC, 5 = GC\n",
    "            #Gulf Coast\n",
    "            match_str = ['District 1 ','District 2','District 3','District 4','District 5','District 6']\n",
    "            pattern = '|'.join(match_str)\n",
    "            match_state = np.where(state_condensates['State'].str.contains(State_ANSI['name'][istate] and pattern))[0][:]\n",
    "            state_condensates.loc[match_state,'State'] = State_ANSI['name'][istate]\n",
    "            state_condensates.loc[match_state,'NEMS'] = 5\n",
    "            #SouthWest\n",
    "            match_str = ['District 7','District 8','District 9']\n",
    "            pattern = '|'.join(match_str)\n",
    "            match_state = np.where(state_condensates['State'].str.contains(State_ANSI['name'][istate] and pattern))[0][:]\n",
    "            state_condensates.loc[match_state,'State'] = State_ANSI['name'][istate]\n",
    "            state_condensates.loc[match_state,'NEMS'] = 3\n",
    "            #Mid-Continent\n",
    "            match_str = ['District 10']\n",
    "            pattern = '|'.join(match_str)\n",
    "            match_state = np.where(state_condensates['State'].str.contains(State_ANSI['name'][istate] and pattern))[0][:]\n",
    "            state_condensates.loc[match_state,'State'] = State_ANSI['name'][istate]\n",
    "            state_condensates.loc[match_state,'NEMS'] = 1\n",
    "            #Other Offshore - delete\n",
    "            idx = state_condensates.index[state_condensates['State'].str.contains(State_ANSI['name'][istate] and 'Offshore')].tolist()\n",
    "            state_condensates= state_condensates.drop(state_condensates.index[idx])\n",
    "            state_condensates.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "#3.5) Calculate Misc. barrels\n",
    "Sum_LeaseC = np.sum(state_condensates.iloc[1:,:-2]) #sum all years of data (not including regions)\n",
    "Misc_LeaseC = state_condensates.iloc[0,:-2] - Sum_LeaseC[:]\n",
    "#Drop extra rows\n",
    "idx1 = state_condensates.index[state_condensates['State'].str.contains('Gulf')].tolist()\n",
    "idx1 += state_condensates.index[state_condensates['State'].str.contains('U.S')].tolist()\n",
    "state_condensates= state_condensates.drop(state_condensates.index[idx1])\n",
    "\n",
    "NE_factor = 0.57\n",
    "MC_factor = 0.14\n",
    "RM_factor = 0.21\n",
    "WC_factor = 0.07\n",
    "\n",
    "state_condensates.loc[state_condensates.shape[0]+1] = Misc_LeaseC*RM_factor\n",
    "state_condensates.loc[state_condensates.shape[0],'State'] = 'Arizona'\n",
    "state_condensates.loc[state_condensates.shape[0],'NEMS'] = '2'\n",
    "state_condensates.loc[state_condensates.shape[0]+1] = Misc_LeaseC*NE_factor\n",
    "state_condensates.loc[state_condensates.shape[0],'State'] = 'Illinois'\n",
    "state_condensates.loc[state_condensates.shape[0],'NEMS'] = '0'\n",
    "state_condensates.loc[state_condensates.shape[0]+1] = Misc_LeaseC*NE_factor\n",
    "state_condensates.loc[state_condensates.shape[0],'State'] = 'Indiana'\n",
    "state_condensates.loc[state_condensates.shape[0],'NEMS'] = '0'\n",
    "state_condensates.loc[state_condensates.shape[0]+1] = Misc_LeaseC*NE_factor\n",
    "state_condensates.loc[state_condensates.shape[0],'State'] = 'Maryland'\n",
    "state_condensates.loc[state_condensates.shape[0],'NEMS'] = '0'\n",
    "state_condensates.loc[state_condensates.shape[0]+1] = Misc_LeaseC*MC_factor\n",
    "state_condensates.loc[state_condensates.shape[0],'State'] = 'Missouri'\n",
    "state_condensates.loc[state_condensates.shape[0],'NEMS'] = '1'\n",
    "state_condensates.loc[state_condensates.shape[0]+1] = Misc_LeaseC*MC_factor\n",
    "state_condensates.loc[state_condensates.shape[0],'State'] = 'Nebraska'\n",
    "state_condensates.loc[state_condensates.shape[0],'NEMS'] = '1'\n",
    "state_condensates.loc[state_condensates.shape[0]+1] = Misc_LeaseC*RM_factor\n",
    "state_condensates.loc[state_condensates.shape[0],'State'] = 'Nevada'\n",
    "state_condensates.loc[state_condensates.shape[0],'NEMS'] = '2'\n",
    "state_condensates.loc[state_condensates.shape[0]+1] = Misc_LeaseC*NE_factor\n",
    "state_condensates.loc[state_condensates.shape[0],'State'] = 'New York'\n",
    "state_condensates.loc[state_condensates.shape[0],'NEMS'] = '0'\n",
    "state_condensates.loc[state_condensates.shape[0]+1] = Misc_LeaseC*NE_factor\n",
    "state_condensates.loc[state_condensates.shape[0],'State'] = 'Ohio'\n",
    "state_condensates.loc[state_condensates.shape[0],'NEMS'] = '0'\n",
    "state_condensates.loc[state_condensates.shape[0]+1] = Misc_LeaseC*WC_factor\n",
    "state_condensates.loc[state_condensates.shape[0],'State'] = 'Oregon'\n",
    "state_condensates.loc[state_condensates.shape[0],'NEMS'] = '4'\n",
    "state_condensates.loc[state_condensates.shape[0]+1] = Misc_LeaseC*NE_factor\n",
    "state_condensates.loc[state_condensates.shape[0],'State'] = 'Pennsylvania'\n",
    "state_condensates.loc[state_condensates.shape[0],'NEMS'] = '0'\n",
    "state_condensates.loc[state_condensates.shape[0]+1] = Misc_LeaseC*RM_factor\n",
    "state_condensates.loc[state_condensates.shape[0],'State'] = 'South Dakota'\n",
    "state_condensates.loc[state_condensates.shape[0],'NEMS'] = '2'\n",
    "state_condensates.loc[state_condensates.shape[0]+1] = Misc_LeaseC*0.21\n",
    "state_condensates.loc[state_condensates.shape[0],'State'] = 'Arizona'\n",
    "state_condensates.loc[state_condensates.shape[0],'NEMS'] = '2'\n",
    "state_condensates.loc[state_condensates.shape[0]+1] = Misc_LeaseC*NE_factor\n",
    "state_condensates.loc[state_condensates.shape[0],'State'] = 'Tennessee'\n",
    "state_condensates.loc[state_condensates.shape[0],'NEMS'] = '0'\n",
    "state_condensates.loc[state_condensates.shape[0]+1] = Misc_LeaseC*NE_factor\n",
    "state_condensates.loc[state_condensates.shape[0],'State'] = 'Virginia'\n",
    "state_condensates.loc[state_condensates.shape[0],'NEMS'] = '0'\n",
    "state_condensates.reset_index(drop=True,inplace=True)\n",
    "\n",
    "#print(state_condensates)\n",
    "\n",
    "# 4) Reformat into time series of condensate values as a function of state and NEMS region \n",
    "start_year_idx = state_condensates.columns.get_loc(str(start_year))\n",
    "end_year_idx = state_condensates.columns.get_loc(str(end_year))+1\n",
    "\n",
    "#Create condensates array\n",
    "state_cond_prod = np.zeros([num_regions, len(State_ANSI), num_years])\n",
    "\n",
    "for irow in np.arange(0,len(state_condensates)):\n",
    "    istate = np.where(State_ANSI['name'] == state_condensates['State'][irow])[0][0]\n",
    "    inems = int(state_condensates['NEMS'][irow])\n",
    "\n",
    "    if state_condensates['State'][irow] == 'Texas' or state_condensates['State'][irow] =='New Mexico':\n",
    "        state_cond_prod[inems,istate,:] = state_cond_prod[inems,istate,:] + state_condensates.iloc[irow,start_year_idx:end_year_idx].values\n",
    "    else:\n",
    "        state_cond_prod[inems,istate,:] = state_cond_prod[inems,istate,:] + state_condensates.iloc[irow,start_year_idx:end_year_idx].values\n",
    "\n",
    "print('QA/QC: Check that all data is accounted for in the reformatted array')\n",
    "diff = np.sum(state_condensates.iloc[:,start_year_idx:end_year_idx].values)-np.sum(state_cond_prod)\n",
    "if diff ==0:\n",
    "    print('PASS')\n",
    "else:\n",
    "    print('FAIL: Check condensate array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Read-In GOADS Emissions & Make Map Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1.1 - Initialize arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize GOADS maps array (will be assigned to proxy map variable later)\n",
    "Map_GOADSmajor_emissions = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Map_GOADSminor_emissions = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. 2011 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read In data for 2011 (use for 2012, 2014, and 2017). Interpolate between for missing years\n",
    "# goal: populating Map_FedGOM_Offshore to allocate federal offshore GOM emissions (state GOM allocated with Enverus)\n",
    "\n",
    "#Only run if need to save new file (takes a few hours to run)\n",
    "if ReCalc_GOADS ==1:\n",
    "    ## 2011\n",
    "    # Read In and Format 2011 BEOM Data\n",
    "    driver_str = r'Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ='+GOADS_11_inputfile+';'''\n",
    "    conn = pyodbc.connect(driver_str)\n",
    "    GOADS_locations = pd.read_sql(\"SELECT * FROM tblPointER\", conn)\n",
    "    GOADS_emissions = pd.read_sql(\"SELECT * FROM tblPointEM\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    # Format Location Data\n",
    "    GOADS_locations = GOADS_locations[[\"strStateFacilityIdentifier\",\"strEmissionReleasePointID\",\"dblXCoordinate\",\"dblYCoordinate\"]]\n",
    "    #Create platform-by-platform file\n",
    "    GOADS_locations_Unique = pd.DataFrame({'strStateFacilityIdentifier':GOADS_locations['strStateFacilityIdentifier'].unique()})\n",
    "    GOADS_locations_Unique['lon'] = 0.0\n",
    "    GOADS_locations_Unique['lat'] = 0.0\n",
    "    GOADS_locations_Unique['strEmissionReleasePointID'] = ''\n",
    "\n",
    "    for iplatform in np.arange(len(GOADS_locations_Unique)):\n",
    "        match_platform = np.where(GOADS_locations['strStateFacilityIdentifier'] == GOADS_locations_Unique['strStateFacilityIdentifier'][iplatform])[0][0]\n",
    "        GOADS_locations_Unique.loc[iplatform,'lon',] = GOADS_locations['dblXCoordinate'][match_platform]\n",
    "        GOADS_locations_Unique.loc[iplatform,'lat',] = GOADS_locations['dblYCoordinate'][match_platform]\n",
    "        GOADS_locations_Unique.loc[iplatform,'strEmissionReleasePointID'] = GOADS_locations['strEmissionReleasePointID'][match_platform][:3]\n",
    "\n",
    "    GOADS_locations_Unique.reset_index(inplace=True, drop=True)\n",
    "    #display(GOADS_locations_Unique)\n",
    "\n",
    "    #print(GOADS_emissions.columns)\n",
    "    #Format Emissions Data (clean lease data string)\n",
    "    GOADS_emissions = GOADS_emissions[[\"strStateFacilityIdentifier\",\"strPollutantCode\",\"dblEmissionNumericValue\",\"BOEM-MONTH\",\n",
    "                                  \"BOEM-LEASE_NUM\",\"BOEM-COMPLEX_ID\"]]\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('OCS','')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('-','')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace(' ','')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('G1477','G01477')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('G73','00073')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('G605','00605')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('G72','00072')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('G599','00599')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('G7155','G07155')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('G2357','G02357')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('G4921','G04921')\n",
    "    GOADS_emissions['Emis_tg'] = 0.0\n",
    "    GOADS_emissions['Emis_tg'] = 9.0718474E-7 * GOADS_emissions['dblEmissionNumericValue'] #convert short tons to Tg\n",
    "    GOADS_emissions = GOADS_emissions[GOADS_emissions['strPollutantCode'] == 'CH4']\n",
    "    GOADS_emissions.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    #display(GOADS_emissions)\n",
    "\n",
    "    # Use ERG Preprocessed data to determine if major or minor and oil or gas\n",
    "    ERG_complex_crosswalk = pd.read_excel(ERG_GOADSEmissions_inputfile, sheet_name = \"Complex Emissions by Source\", usecols = \"AJ:AM\", nrows = 11143)\n",
    "\n",
    "    # add data to map array, for the closest year to 2011\n",
    "    year_diff = [abs(x - 2011) for x in year_range]\n",
    "    iyear = year_diff.index(min(year_diff))\n",
    "\n",
    "    #assign oil vs gas by lease/complex ID\n",
    "    GOADS_emissions['LEASE_TYPE'] =''\n",
    "    GOADS_emissions['MAJOR_STRUC'] =''\n",
    "    for istruc in np.arange(0,len(GOADS_emissions)):\n",
    "        imatch = np.where(np.logical_and(ERG_complex_crosswalk['BOEM COMPLEX ID.2']==int(GOADS_emissions['BOEM-COMPLEX_ID'][istruc]),\\\n",
    "                            ERG_complex_crosswalk['Year.2'] == 2011))\n",
    "        if np.size(imatch) >0:\n",
    "            imatch = imatch[0][0]\n",
    "            GOADS_emissions.loc[istruc,'LEASE_TYPE'] = ERG_complex_crosswalk['Oil Gas Defn FINAL.1'][imatch]\n",
    "            GOADS_emissions.loc[istruc,'MAJOR_STRUC'] = ERG_complex_crosswalk['Major / Minor.1'][imatch]\n",
    "        else:\n",
    "            print(istruc, GOADS_emissions['BOEM-COMPLEX_ID'][istruc])\n",
    "\n",
    "        # for all gas platforms, match the platform to the emissions\n",
    "        if GOADS_emissions['LEASE_TYPE'][istruc] =='Gas':\n",
    "            match_platform = np.where(GOADS_locations_Unique.strStateFacilityIdentifier==GOADS_emissions['strStateFacilityIdentifier'][istruc])[0][0]\n",
    "            ilat = int((GOADS_locations_Unique['lat'][match_platform] - Lat_low)/Res01)\n",
    "            ilon = int((GOADS_locations_Unique['lon'][match_platform] - Lon_left)/Res01)\n",
    "            imonth = GOADS_emissions['BOEM-MONTH'][istruc]-1 #dict is 1-12, not 0-11\n",
    "            if GOADS_emissions['MAJOR_STRUC'][istruc] =='Major':\n",
    "                Map_GOADSmajor_emissions[ilat,ilon,iyear,imonth] += GOADS_emissions['Emis_tg'][istruc]\n",
    "            else:\n",
    "                Map_GOADSminor_emissions[ilat,ilon,iyear,imonth] += GOADS_emissions['Emis_tg'][istruc]\n",
    "            \n",
    "            \n",
    "    # sum complexes and emissions for diagnostic\n",
    "    majcplx = GOADS_emissions[(GOADS_emissions['MAJOR_STRUC']=='Major')]\n",
    "    majcplx = majcplx[majcplx['LEASE_TYPE'] =='Gas']\n",
    "    num_majcplx = majcplx['BOEM-COMPLEX_ID'].unique()\n",
    "    #print(np.shape(num_majcplx))\n",
    "    mincplx = GOADS_emissions[GOADS_emissions['MAJOR_STRUC']=='Minor']\n",
    "    mincplx = mincplx[mincplx['LEASE_TYPE'] =='Gas']\n",
    "    num_mincplx = mincplx['BOEM-COMPLEX_ID'].unique()\n",
    "    #print(np.size(num_mincplx))            \n",
    "    del GOADS_emissions\n",
    "    print('Number of Major Gas Complexes: ',(np.size(num_majcplx)))\n",
    "    print('Emissions (Tg): ',np.sum(Map_GOADSmajor_emissions[:,:,iyear,:]))\n",
    "    print('Number of Minor Gas Complexes: ',(np.size(num_mincplx)))\n",
    "    print('Emissions (Tg): ',np.sum(Map_GOADSminor_emissions[:,:,iyear,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. 2014 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2014\n",
    "\n",
    "if ReCalc_GOADS ==1:\n",
    "    #Read In and Format 2014 BEOM Data\n",
    "    driver_str = r'Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ='+GOADS_14_inputfile+';'''\n",
    "    conn = pyodbc.connect(driver_str)\n",
    "    GOADS_emissions = pd.read_sql(\"SELECT * FROM 2014_Gulfwide_Platform_20161102\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    GOADS_emissions = GOADS_emissions[[\"PLATFORM_ID\",\"X_COORDINATE\",\"Y_COORDINATE\",\"POLLUTANT_CODE\",\"EMISSIONS_VALUE\",\"MONTH\",\\\n",
    "                                  \"LEASE_NUMBER\",\"COMPLEX_ID\"]]\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('OCS','')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('-','')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace(' ','')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G1477','G01477')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G73','00073')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G605','00605')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G72','00072')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G599','00599')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G7155','G07155')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G2357','G02357')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G4921','G04921')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('GO2839','G02839')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('GO5761','G05761')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('GO0026','00026')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('GO3194','G03194')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G1034','G01034')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G0456','G00456')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G0060','G00060')\n",
    "    GOADS_emissions['Emis_tg'] = 0.0\n",
    "    GOADS_emissions['Emis_tg'] = 9.0718474E-7 * GOADS_emissions['EMISSIONS_VALUE'] #convert short tons to Tg\n",
    "    GOADS_emissions = GOADS_emissions[GOADS_emissions['POLLUTANT_CODE'] == 'CH4']\n",
    "    GOADS_emissions.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    #assign oil vs gas by lease/complex ID\n",
    "    # add data to map array, for the closest year to 2014\n",
    "    year_diff = [abs(x - 2014) for x in year_range]\n",
    "    iyear = year_diff.index(min(year_diff))\n",
    "    GOADS_emissions['LEASE_TYPE'] =''\n",
    "    GOADS_emissions['MAJOR_STRUC'] =''\n",
    "    for istruc in np.arange(0,len(GOADS_emissions)):\n",
    "        imatch = np.where(np.logical_and(ERG_complex_crosswalk['BOEM COMPLEX ID.2']==int(GOADS_emissions['COMPLEX_ID'][istruc]),\\\n",
    "                            ERG_complex_crosswalk['Year.2'] == 2014))\n",
    "        if np.size(imatch) >0:\n",
    "            imatch = imatch[0][0]\n",
    "            GOADS_emissions.loc[istruc,'LEASE_TYPE'] = ERG_complex_crosswalk['Oil Gas Defn FINAL.1'][imatch]\n",
    "            GOADS_emissions.loc[istruc,'MAJOR_STRUC'] = ERG_complex_crosswalk['Major / Minor.1'][imatch]\n",
    "        else:\n",
    "            print(istruc, GOADS_emissions['COMPLEX_ID'][istruc])\n",
    "    #display(GOADS_emissions)\n",
    "\n",
    "    #for iplatform in np.arange(len(GOADS_emissions)):\n",
    "        if GOADS_emissions['LEASE_TYPE'][istruc] =='Gas':\n",
    "            #then for all oil platforms, match the platform to the emissions\n",
    "            ilat = int((GOADS_emissions['Y_COORDINATE'][istruc] - Lat_low)/Res01)\n",
    "            ilon = int((GOADS_emissions['X_COORDINATE'][istruc] - Lon_left)/Res01)\n",
    "            month_str = GOADS_emissions['MONTH'][istruc]             \n",
    "            imonth = month_dict[GOADS_emissions['MONTH'][istruc]]-1 #dict is 1-12, not 0-11\n",
    "            if GOADS_emissions['MAJOR_STRUC'][istruc] =='Major':\n",
    "                Map_GOADSmajor_emissions[ilat,ilon,iyear,imonth] += GOADS_emissions['Emis_tg'][istruc]\n",
    "            else:\n",
    "                Map_GOADSminor_emissions[ilat,ilon,iyear,imonth] += GOADS_emissions['Emis_tg'][istruc]\n",
    "\n",
    "    # sum complexes and emissions for diagnostic\n",
    "    majcplx = GOADS_emissions[(GOADS_emissions['MAJOR_STRUC']=='Major')]\n",
    "    majcplx = majcplx[majcplx['LEASE_TYPE'] =='Gas']\n",
    "    num_majcplx = majcplx['COMPLEX_ID'].unique()\n",
    "    #print(np.shape(num_majcplx))\n",
    "    mincplx = GOADS_emissions[GOADS_emissions['MAJOR_STRUC']=='Minor']\n",
    "    mincplx = mincplx[mincplx['LEASE_TYPE'] =='Gas']\n",
    "    num_mincplx = mincplx['COMPLEX_ID'].unique()\n",
    "    #print(np.size(num_mincplx))            \n",
    "    del GOADS_emissions\n",
    "    print('Number of Major Gas Complexes: ',(np.size(num_majcplx)))\n",
    "    print('Emissions (Tg): ',np.sum(Map_GOADSmajor_emissions[:,:,iyear,:]))\n",
    "    print('Number of Minor Gas Complexes: ',(np.size(num_mincplx)))\n",
    "    print('Emissions (Tg): ',np.sum(Map_GOADSminor_emissions[:,:,iyear,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. 2017 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2017\n",
    "if ReCalc_GOADS ==1:\n",
    "    #Read In and Format 2017 BEOM Data\n",
    "    driver_str = r'Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ='+GOADS_17_inputfile+';'''\n",
    "    conn = pyodbc.connect(driver_str)\n",
    "    GOADS_emissions = pd.read_sql(\"SELECT * FROM 2017_Gulfwide_Platform_20190705_CAP_GHG\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    GOADS_emissions = GOADS_emissions[[\"PLATFORM_ID\",\"X_COORDINATE\",\"Y_COORDINATE\",\"POLLUTANT_CODE\",\"EMISSIONS_VALUE\",\"Month\",\\\n",
    "                                   \"LEASE_NUMBER\",\"COMPLEX_ID\"]]\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('OCS','')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('-','')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace(' ','')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G1477','G01477')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G73','00073')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G605','00605')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G72','00072')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G599','00599')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G7155','G07155')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G2357','G02357')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G4921','G04921')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('GO2839','G02839')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('GO2893','G02893')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('GO5761','G05761')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('GO0026','00026')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('GO3194','G03194')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G1034','G01034')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G0456','G00456')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G0060','G00060')\n",
    "    GOADS_emissions['Emis_tg'] = 0.0\n",
    "    GOADS_emissions['Emis_tg'] = 9.0718474E-7 * GOADS_emissions['EMISSIONS_VALUE'] #convert short tons to Tg\n",
    "    GOADS_emissions = GOADS_emissions[GOADS_emissions['POLLUTANT_CODE'] == 'CH4']\n",
    "    GOADS_emissions.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    #assign oil vs gas by lease/complex ID\n",
    "    # add data to map array, for the closest year to 2014\n",
    "    year_diff = [abs(x - 2017) for x in year_range]\n",
    "    iyear = year_diff.index(min(year_diff))\n",
    "    GOADS_emissions['LEASE_TYPE'] =''\n",
    "    GOADS_emissions['MAJOR_STRUC'] =''\n",
    "    for istruc in np.arange(0,len(GOADS_emissions)):\n",
    "        imatch = np.where(np.logical_and(ERG_complex_crosswalk['BOEM COMPLEX ID.2']==int(GOADS_emissions['COMPLEX_ID'][istruc]),\\\n",
    "                            ERG_complex_crosswalk['Year.2'] == 2017))\n",
    "        if np.size(imatch) >0:\n",
    "            imatch = imatch[0][0]\n",
    "            GOADS_emissions.loc[istruc,'LEASE_TYPE'] = ERG_complex_crosswalk['Oil Gas Defn FINAL.1'][imatch]\n",
    "            GOADS_emissions.loc[istruc,'MAJOR_STRUC'] = ERG_complex_crosswalk['Major / Minor.1'][imatch]\n",
    "        else:\n",
    "            print(istruc, GOADS_emissions[\"COMPLEX_ID\"][istruc])\n",
    "    #display(GOADS_emissions)\n",
    "\n",
    "    #for iplatform in np.arange(len(GOADS_emissions)):\n",
    "        if GOADS_emissions['LEASE_TYPE'][istruc] =='Gas':\n",
    "            #then for all oil platforms, match the platform to the emissions\n",
    "            ilat = int((GOADS_emissions['Y_COORDINATE'][istruc] - Lat_low)/Res01)\n",
    "            ilon = int((GOADS_emissions['X_COORDINATE'][istruc] - Lon_left)/Res01)\n",
    "            imonth = month_dict[GOADS_emissions['Month'][istruc]]-1 #dict is 1-12, not 0-11\n",
    "            if GOADS_emissions['MAJOR_STRUC'][istruc] =='Major':\n",
    "                Map_GOADSmajor_emissions[ilat,ilon,iyear,imonth] += GOADS_emissions['Emis_tg'][istruc]\n",
    "            else:\n",
    "                Map_GOADSminor_emissions[ilat,ilon,iyear,imonth] += GOADS_emissions['Emis_tg'][istruc]\n",
    "\n",
    "    # sum complexes and emissions for diagnostic\n",
    "    majcplx = GOADS_emissions[(GOADS_emissions['MAJOR_STRUC']=='Major')]\n",
    "    majcplx = majcplx[majcplx['LEASE_TYPE'] =='Gas']\n",
    "    num_majcplx = majcplx[\"COMPLEX_ID\"].unique()\n",
    "    #print(np.shape(num_majcplx))\n",
    "    mincplx = GOADS_emissions[GOADS_emissions['MAJOR_STRUC']=='Minor']\n",
    "    mincplx = mincplx[mincplx['LEASE_TYPE'] =='Gas']\n",
    "    num_mincplx = mincplx[\"COMPLEX_ID\"].unique()\n",
    "    #print(np.size(num_mincplx))            \n",
    "    print('Number of Major Gas Complexes: ',(np.size(num_majcplx)))\n",
    "    print('Emissions: ',np.sum(Map_GOADSmajor_emissions[:,:,iyear,:]))\n",
    "    print('Number of Minor Gas Complexes: ',(np.size(num_mincplx)))\n",
    "    print('Emissions: ',np.sum(Map_GOADSminor_emissions[:,:,iyear,:]))\n",
    "    #clean (remove unused arrays)\n",
    "    del GOADS_emissions, majcplx, mincplx\n",
    "    del ERG_complex_crosswalk, GOADS_locations, GOADS_locations_Unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. Interpolate Data & Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpolate and save resulting proxy maps, unless loading from previous calculation\n",
    "\n",
    "if ReCalc_GOADS ==1:\n",
    "    #2011 data applied to 2012\n",
    "    # 2014 data applied to 2013-2015\n",
    "    # 2017 data applied 2016 forward\n",
    "    Map_GOADSmajor_emissions[:,:,1,:] = Map_GOADSmajor_emissions[:,:,2,:]\n",
    "    Map_GOADSmajor_emissions[:,:,2,:] = Map_GOADSmajor_emissions[:,:,2,:]\n",
    "    Map_GOADSmajor_emissions[:,:,3,:] = Map_GOADSmajor_emissions[:,:,2,:]\n",
    "    Map_GOADSmajor_emissions[:,:,4,:] = Map_GOADSmajor_emissions[:,:,5,:]\n",
    "    Map_GOADSmajor_emissions[:,:,6,:] = Map_GOADSmajor_emissions[:,:,5,:]\n",
    "    \n",
    "    Map_GOADSminor_emissions[:,:,1,:] = Map_GOADSminor_emissions[:,:,2,:]\n",
    "    Map_GOADSminor_emissions[:,:,2,:] = Map_GOADSminor_emissions[:,:,2,:]\n",
    "    Map_GOADSminor_emissions[:,:,3,:] = Map_GOADSminor_emissions[:,:,2,:]\n",
    "    Map_GOADSminor_emissions[:,:,4,:] = Map_GOADSminor_emissions[:,:,5,:]\n",
    "    Map_GOADSminor_emissions[:,:,6,:] = Map_GOADSminor_emissions[:,:,5,:]\n",
    "    \n",
    "    np.save('./IntermediateOutputs/GOADSmajor_gas_tempoutput', Map_GOADSmajor_emissions)\n",
    "    np.save('./IntermediateOutputs/GOADSminor_gas_tempoutput', Map_GOADSminor_emissions)\n",
    "else:\n",
    "    Map_GOADSmajor_emissions = np.load('./IntermediateOutputs/GOADSmajor_gas_tempoutput.npy')\n",
    "    Map_GOADSminor_emissions = np.load('./IntermediateOutputs/GOADSminor_gas_tempoutput.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Well and Production Data (from Enverus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 Read In & Combine Each Year of Prism & DI Monthly Data (from Enverus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data come from Enverus, both Drilling Info and Prism\n",
    "# The reason 2 datasets are used is because Prism does not include all states\n",
    "# So remaining states, or those with more DI coverage are taken from DI\n",
    "\n",
    "#Only re-do this section if data need to be re-calculated (ReCalc_Enverus ==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read In and Format the Prism and DI data \n",
    "# 1. Read Data\n",
    "# 2. Drop unsed columns, rename columns to match between DI and Prism\n",
    "# 3. Combine DI and Prism into one data array\n",
    "# 4. Calculate annual cummulate production totals\n",
    "# 5. Save the data as a year-specific variable\n",
    "\n",
    "#Based on ERGs logic, active wells are determined based on their production levels and not producing status\n",
    "\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    \n",
    "    #DI data\n",
    "    DI_data = pd.read_csv(vars()['Enverus_DI_inputdata_' +year_range_str[iyear]])\n",
    "    DI_data = DI_data.drop(columns=['ENTITY_ID','API_UWI','OPERATOR_COMPANY_NAME','AAPG_FULL_ERG',\\\n",
    "                           'FIELD','RESERVOIR','LAST_PROD_DATE','DRILL_TYPE','CUM_GAS','CUM_OIL','CUM_WATER'])\n",
    "    DI_data.rename({'WELL_COUNT_ID':'WELL_COUNT','DI_BASIN':'BASIN','NEMS_REGION_ERG':'NEMS_REGION',\\\n",
    "                    'SURFACE_LATITUDE_WGS84':'LATITUDE','SURFACE_LONGITUDE_WGS84':'LONGITUDE','MONTHLY_WATER_01':'WATERPROD_01',\\\n",
    "                   'MONTHLY_WATER_02':'WATERPROD_02','MONTHLY_WATER_03':'WATERPROD_03','MONTHLY_WATER_04':'WATERPROD_04',\\\n",
    "                   'MONTHLY_WATER_05':'WATERPROD_05','MONTHLY_WATER_06':'WATERPROD_06','MONTHLY_WATER_07':'WATERPROD_07',\\\n",
    "                   'MONTHLY_WATER_08':'WATERPROD_08','MONTHLY_WATER_09':'WATERPROD_09','MONTHLY_WATER_10':'WATERPROD_10',\\\n",
    "                   'MONTHLY_WATER_11':'WATERPROD_11','MONTHLY_WATER_12':'WATERPROD_12','MONTHLY_OIL_01':'OILPROD_01',\\\n",
    "                   'MONTHLY_OIL_02':'OILPROD_02','MONTHLY_OIL_03':'OILPROD_03','MONTHLY_OIL_04':'OILPROD_04',\\\n",
    "                   'MONTHLY_OIL_05':'OILPROD_05','MONTHLY_OIL_06':'OILPROD_06','MONTHLY_OIL_07':'OILPROD_07',\\\n",
    "                   'MONTHLY_OIL_08':'OILPROD_08','MONTHLY_OIL_09':'OILPROD_09','MONTHLY_OIL_10':'OILPROD_10',\\\n",
    "                   'MONTHLY_OIL_11':'OILPROD_11','MONTHLY_OIL_12':'OILPROD_12','MONTHLY_GAS_01':'GASPROD_01',\\\n",
    "                   'MONTHLY_GAS_02':'GASPROD_02','MONTHLY_GAS_03':'GASPROD_03','MONTHLY_GAS_04':'GASPROD_04',\\\n",
    "                   'MONTHLY_GAS_05':'GASPROD_05','MONTHLY_GAS_06':'GASPROD_06','MONTHLY_GAS_07':'GASPROD_07',\\\n",
    "                   'MONTHLY_GAS_08':'GASPROD_08','MONTHLY_GAS_09':'GASPROD_09','MONTHLY_GAS_10':'GASPROD_10',\\\n",
    "                   'MONTHLY_GAS_11':'GASPROD_11','MONTHLY_GAS_12':'GASPROD_12'},axis=1, inplace=True)\n",
    "    DI_data['WELL_COUNT'] = 1\n",
    "    \n",
    "    \n",
    "\n",
    "    #Prism Data\n",
    "    Prism_data = pd.read_csv(vars()['Enverus_Prism_inputdata_'+year_range_str[iyear]])\n",
    "    Prism_data = Prism_data.drop(columns=['WELLID','API_UWI','RSOPERATOR','TRAJECTORY','FIELD','RSREGION','FORMATION',\\\n",
    "                                         'TOTALFLUIDPUMPED_BBL','SPUDDATE'])\n",
    "    Prism_data.rename({'RSBASIN':'BASIN','COMPLETIONDATE':'COMPLETION_DATE','SPUDDATE':'SPUD_DATE','FIRSTPRODDATE':'FIRST_PROD_DATE',\\\n",
    "                     'OILGRAVITY_API':'OIL_GRAVITY','WATERPROD_BBL_01':'WATERPROD_01',\\\n",
    "                    'WATERPROD_BBL_02':'WATERPROD_02','WATERPROD_BBL_03':'WATERPROD_03','WATERPROD_BBL_04':'WATERPROD_04',\\\n",
    "                   'WATERPROD_BBL_05':'WATERPROD_05','WATERPROD_BBL_06':'WATERPROD_06','WATERPROD_BBL_07':'WATERPROD_07',\\\n",
    "                   'WATERPROD_BBL_08':'WATERPROD_08','WATERPROD_BBL_09':'WATERPROD_09','WATERPROD_BBL_10':'WATERPROD_10',\\\n",
    "                   'WATERPROD_BBL_11':'WATERPROD_11','WATERPROD_BBL_12':'WATERPROD_12','LIQUIDSPROD_BBL_01':'OILPROD_01',\\\n",
    "                   'LIQUIDSPROD_BBL_02':'OILPROD_02','LIQUIDSPROD_BBL_03':'OILPROD_03','LIQUIDSPROD_BBL_04':'OILPROD_04',\\\n",
    "                   'LIQUIDSPROD_BBL_05':'OILPROD_05','LIQUIDSPROD_BBL_06':'OILPROD_06','LIQUIDSPROD_BBL_07':'OILPROD_07',\\\n",
    "                   'LIQUIDSPROD_BBL_08':'OILPROD_08','LIQUIDSPROD_BBL_09':'OILPROD_09','LIQUIDSPROD_BBL_10':'OILPROD_10',\\\n",
    "                   'LIQUIDSPROD_BBL_11':'OILPROD_11','LIQUIDSPROD_BBL_12':'OILPROD_12','GASPROD_MCF_01':'GASPROD_01',\\\n",
    "                   'GASPROD_MCF_02':'GASPROD_02','GASPROD_MCF_03':'GASPROD_03','GASPROD_MCF_04':'GASPROD_04',\\\n",
    "                   'GASPROD_MCF_05':'GASPROD_05','GASPROD_MCF_06':'GASPROD_06','GASPROD_MCF_07':'GASPROD_07',\\\n",
    "                   'GASPROD_MCF_08':'GASPROD_08','GASPROD_MCF_09':'GASPROD_09','GASPROD_MCF_10':'GASPROD_10',\\\n",
    "                   'GASPROD_MCF_11':'GASPROD_11','GASPROD_MCF_12':'GASPROD_12','RSWELLSTATUS':'PRODUCING_STATUS'},axis=1,inplace=True)\n",
    "    #\n",
    "    #Prism_data = Prism_data[Prism_data['PRODUCING_STATUS'] == 'PRODUCING']\n",
    "    Prism_data['WELL_COUNT'] = 1\n",
    "    #print(Prism_data)\n",
    "    \n",
    "    #combine into one array with common column names, replace nans with zeros, and sum annual production\n",
    "    Enverus_data = pd.concat([DI_data,Prism_data], ignore_index=True)\n",
    "    Enverus_data.loc[:,Enverus_data.columns.str.contains('GASPROD_')] = Enverus_data.loc[:,Enverus_data.columns.str.contains('GASPROD_')].fillna(0)\n",
    "    Enverus_data.loc[:,Enverus_data.columns.str.contains('OILPROD_')] = Enverus_data.loc[:,Enverus_data.columns.str.contains('OILPROD_')].fillna(0)\n",
    "    Enverus_data.loc[:,Enverus_data.columns.str.contains('WATERPROD_')] = Enverus_data.loc[:,Enverus_data.columns.str.contains('WATERPROD_')].fillna(0)\n",
    "\n",
    "    #Calculate cummulative annual production totals for Gas, Oil, Water\n",
    "    Enverus_data['CUM_GAS'] = Enverus_data.loc[:,Enverus_data.columns.str.contains('GASPROD_')].sum(1)\n",
    "    Enverus_data['CUM_OIL'] = Enverus_data.loc[:,Enverus_data.columns.str.contains('OILPROD_')].sum(1)\n",
    "    Enverus_data['CUM_WATER'] = Enverus_data.loc[:,Enverus_data.columns.str.contains('WATERPROD_')].sum(1)\n",
    "    \n",
    "    Enverus_data['NEMS_CODE'] = Enverus_data['NEMS_REGION'].map(NEMS_dict)\n",
    "    \n",
    "    #save out the data for that year\n",
    "    vars()['Enverus_data_'+year_range_str[iyear]] = Enverus_data.copy()\n",
    "    print('Load Complete: Year '+year_range_str[iyear])\n",
    "    \n",
    "    del DI_data #save memory space \n",
    "    \n",
    "    #define default values for a new row in this table (to be used later during data corrections)\n",
    "    default = {'WELL_COUNT': 0, 'STATE':'','COUNTY':'','BASIN':'','AAPG_CODE_ERG':'UNK','NEMS_REGION':'UNK','NEMS_CODE':99,\\\n",
    "               'LATITUDE':0,'LONGITUDE':0,'PRODUCING_STATUS':'','RESERVOIR_TYPE':'','COMPLETION_DATE':'','SPUD_DATE':'',\\\n",
    "               'FIRST_PROD_DATE':'','HF':'', 'OFFSHORE':'','OIL_GRAVITY':'','GOR':-99,'GOR_QUAL':'','PROD_FLAG':'',\\\n",
    "               'OILPROD_01':0, 'GASPROD_01':0, 'WATERPROD_01':0,'OILPROD_02':0, 'GASPROD_02':0, 'WATERPROD_02':0,\\\n",
    "          'OILPROD_03':0, 'GASPROD_03':0, 'WATERPROD_03':0,'OILPROD_04':0, 'GASPROD_04':0, 'WATERPROD_04':0,\\\n",
    "          'OILPROD_05':0, 'GASPROD_05':0, 'WATERPROD_05':0,'OILPROD_06':0, 'GASPROD_06':0, 'WATERPROD_06':0,\\\n",
    "          'OILPROD_07':0, 'GASPROD_07':0, 'WATERPROD_07':0,'OILPROD_08':0, 'GASPROD_08':0, 'WATERPROD_08':0,\\\n",
    "          'OILPROD_09':0, 'GASPROD_09':0, 'WATERPROD_09':0,'OILPROD_10':0, 'GASPROD_10':0, 'WATERPROD_10':0,\\\n",
    "          'OILPROD_11':0, 'GASPROD_11':0, 'WATERPROD_11':0,'OILPROD_12':0, 'GASPROD_12':0, 'WATERPROD_12':0}\n",
    "    \n",
    "display(Enverus_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correct the NEMS Code for missing NEMS_REGIONS\n",
    "# Note OFFSHORE regions will have NaN as NEMS_Code\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    enverus_data_temp = vars()['Enverus_data_'+year_range_str[iyear]].copy()\n",
    "    list_well = enverus_data_temp.index[pd.isna(enverus_data_temp.loc[:,'NEMS_REGION'])].tolist()\n",
    "    if np.size(list_well) > 0:\n",
    "        for irow in list_well: \n",
    "            match_state = np.where(NEMS_State['Ansi']==enverus_data_temp['STATE'][irow])[0][0]\n",
    "            enverus_data_temp.loc[irow,'NEMS_CODE'] = NEMS_State['NEMS'][match_state].astype(int)\n",
    "    vars()['Enverus_data_'+year_range_str[iyear]] = enverus_data_temp.copy()\n",
    "#print(NEMS_State)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3.2 - Correct Enverus Data for Select States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1) Read In Coverage Table from State Well Counts File from ERG\n",
    "# (specifies the first year with bad data and which years need to be corrected; \n",
    "# all years including and after the first bad year of data need to be corrected)\n",
    "\n",
    "ERG_StateWellCounts_FirstBadDataYear = pd.read_excel(Enverus_WellCounts_inputfile, sheet_name = \"2021 - Coverage\", usecols = \"A:B\", skiprows = 2, nrows = 40)\n",
    "ERG_StateWellCounts_FirstBadDataYear['date'] = pd.to_datetime(ERG_StateWellCounts_FirstBadDataYear['Date to USE'], errors = 'coerce')\n",
    "ERG_StateWellCounts_FirstBadDataYear['year'] = pd.DatetimeIndex(ERG_StateWellCounts_FirstBadDataYear['date']).year.fillna(end_year+100).astype(int)\n",
    "\n",
    "# 2) Loops through the each state and year in Enverus to determine if the data for that particualar year needs to \n",
    "# be corrected. At the moment, the only corrections ERG makes to the data is to use the prior year of data if there\n",
    "# is no new Enverus data reportd for that state. If a particular state is not included for any years in the Enverus\n",
    "# dataset, then a row of zeros is added to the Enverus table for that year. \n",
    "\n",
    "for istate in np.arange(0,len(State_ANSI)):\n",
    "    correctdata =0\n",
    "    state_str = State_ANSI['abbr'][istate]\n",
    "    firstbadyear = ERG_StateWellCounts_FirstBadDataYear['year'][ERG_StateWellCounts_FirstBadDataYear['State'] == state_str].values\n",
    "    if firstbadyear.size  == 0:\n",
    "        firstbadyear = end_year+5 #if state isn't included in correction list, don't correct any data\n",
    "    \n",
    "    for iyear in np.arange(0,num_years):\n",
    "        enverus_data_temp= vars()['Enverus_data_'+year_range_str[iyear]].copy()\n",
    "        state_list = np.unique(enverus_data_temp['STATE'])\n",
    "        if state_str in state_list:\n",
    "            inlist =1\n",
    "        else:\n",
    "            inlist = 0\n",
    "        if inlist ==1 or correctdata==1: #if the state is included in Enverus data, or had data for at least one good year\n",
    "            #if first year, correctdata will be zero, but inlist will also be zero if no Enverus data\n",
    "            #check to see whether corrections are necessary for the given year/state\n",
    "            if year_range[iyear] == (firstbadyear-1):\n",
    "                print(state_str,year_range[iyear],'last good year')\n",
    "                # This is the last year of good data. Do not correct the data but save\n",
    "                # but so that this data can be used for all following years for that state\n",
    "                temp_data = enverus_data_temp[enverus_data_temp['STATE'] == state_str]\n",
    "                lastgoodyear = year_range_str[iyear]\n",
    "                correctdata=1\n",
    "            elif year_range[iyear] >= firstbadyear: \n",
    "                print(state_str,year_range[iyear])\n",
    "                #correct data for all years equal to and after the first bad year (remove old data first if necessary)\n",
    "                if inlist == 1:\n",
    "                    enverus_data_temp = enverus_data_temp[enverus_data_temp['STATE'] != state_str]\n",
    "                enverus_data_temp = pd.concat([enverus_data_temp,temp_data],ignore_index=True)\n",
    "                print(state_str +' data for ' +year_range_str[iyear] +' were corrected with '+lastgoodyear+' data')\n",
    "            else:\n",
    "                no_corrections =1\n",
    "                \n",
    "        if inlist==0 and correctdata==0:\n",
    "        #if there is no Enverus data for a given state, and there was no good data, add a row with default values\n",
    "            temp_row = {'STATE':state_str}\n",
    "            enverus_data_temp = enverus_data_temp.append({**default,**temp_row}, ignore_index=True)\n",
    "            print(state_str +' has no Enverus data in the year ' +year_range_str[iyear]+', default values set')\n",
    "            \n",
    "        #resave that year of Enverus data\n",
    "        enverus_data_temp.reset_index(drop=True,inplace=True)\n",
    "        vars()['Enverus_data_'+year_range_str[iyear]] = enverus_data_temp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.4. - Calculate Fractional Monthly Condensate Arrays \n",
    "(EIA condensate production (bbl) relative to producing Enverus gas wells by month in each state and region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculate the number of wells from the EIA Condensate Data Set (converted to monthly counts) relative to the number of\n",
    "# producing non-associated gas wells each month in the Enverus dataset (for each state and nems region)\n",
    "\n",
    "#1 ) Initialize arrays\n",
    "cond_env_well_count = np.zeros([len(State_ANSI),num_regions,num_years,num_months])\n",
    "cond_month_wellfrac = np.zeros([len(State_ANSI),num_regions,num_years,num_months])\n",
    "cond_weighted_well_count = np.zeros([len(State_ANSI),num_regions,num_years,num_months])\n",
    "\n",
    "if ReCalc_Condensates == 1:\n",
    "    # 2) Calculate the number of producing non-associated gas wells (onshore) each month\n",
    "    # (NA classification based on annual production, then producing well if production > 0 in the given month)\n",
    "    for iyear in np.arange(0,num_years):  \n",
    "        enverus_data_temp = vars()['Enverus_data_'+year_range_str[iyear]].copy()\n",
    "        list_well1 = enverus_data_temp.index[enverus_data_temp.loc[:,'OFFSHORE'] == 'N'].tolist()\n",
    "        list_well2 = enverus_data_temp.index[enverus_data_temp.loc[:,'CUM_GAS'] > 0 ].tolist()\n",
    "        list1_as_set = set(list_well1)\n",
    "        intersection = list1_as_set.intersection(list_well2) #find common elements between both lists\n",
    "        list_well_total = list(intersection)\n",
    "        for iwell in list_well_total:\n",
    "            if ((data_fn.safe_div(enverus_data_temp['CUM_GAS'][iwell],float(enverus_data_temp['CUM_OIL'][iwell]))) > 100 or \\\n",
    "                    enverus_data_temp['GOR_QUAL'][iwell] =='Gas only'):\n",
    "                inems = enverus_data_temp['NEMS_CODE'][iwell].astype(int)\n",
    "                #if inems < 0:\n",
    "                #    print(iwell)\n",
    "                istate = np.where(State_ANSI['abbr'] == enverus_data_temp['STATE'][iwell])[0][0]\n",
    "                for imonth in np.arange(0,num_months):\n",
    "                    prod_str = 'GASPROD_'+month_tag[imonth]  \n",
    "                    if enverus_data_temp[prod_str][iwell] >0:\n",
    "                        cond_env_well_count[istate,inems,iyear,imonth] = \\\n",
    "                                cond_env_well_count[istate,inems,iyear,imonth] + enverus_data_temp['WELL_COUNT'][iwell]\n",
    "\n",
    "    # 3) Calculate monthly weighted Enverus well counts ( = number of producing wells each month * number of days in month)\n",
    "    for iyear in np.arange(0,num_years):    \n",
    "        for imonth in np.arange(0, num_months):\n",
    "            # 4) Normalize \n",
    "            # (monthly condensate well fraction = annual state condensate well counts * number of days in month / (all well counts from Enverus * number of days in month)))\n",
    "            # Results in the the fraction of wells each month in the condensate data set relative to in the Enverus dataset\n",
    "            for istate in np.arange(0,len(State_ANSI)):\n",
    "                for inems in np.arange(0,num_regions):\n",
    "                    if state_cond_prod[inems,istate,iyear] > 0:\n",
    "                        cond_month_wellfrac[istate,inems,iyear,imonth] = \\\n",
    "                        data_fn.safe_div((state_cond_prod[inems,istate,iyear]),float(np.sum(cond_env_well_count[istate,inems,iyear,:])))\n",
    "    #this calculates the condensate production volume per well for each state, region, year, and month              \n",
    "    np.save('./IntermediateOutputs/Condensates_wellcount_tempoutput', cond_env_well_count)\n",
    "    np.save('./IntermediateOutputs/Condensates_wellfrac_tempoutput', cond_month_wellfrac)\n",
    "    np.save('./IntermediateOutputs/Condensates_stateprod_tempoutput', state_cond_prod)\n",
    "\n",
    "else:\n",
    "    cond_env_well_count = np.load('./IntermediateOutputs/Condensates_wellcount_tempoutput.npy')\n",
    "    cond_month_wellfrac = np.load('./IntermediateOutputs/Condensates_wellfrac_tempoutput.npy')\n",
    "    state_cond_prod = np.load('./IntermediateOutputs/Condensates_stateprod_tempoutput.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.5 Convert Enverus Well and Production Arrays, and Condensate Array into Gridded Location Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear variables\n",
    "del ERG_StateWellCounts_FirstBadDataYear\n",
    "del Prism_data\n",
    "del colnames\n",
    "del names\n",
    "del state_condensates\n",
    "del temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make Annual gridded arrays (maps) of well data (a well will be counted every month if there is any production that year)\n",
    "# Includes NA Gas Wells and Production onshore in the CONUS region\n",
    "# source emissions are related to the presence of a well and its production status (no emission if no production)\n",
    "# Details: ERG does not include a well in the national count if there is no (cummulative) oil or gas production from that well.\n",
    "# Wells are not considered active for a given year if there is no production data that year\n",
    "# This may cause wells that are completed but not yet producing to be dropped from the national count. \n",
    "# ERG has developed their own logic to determine if a well is an HF well or not and that result is included in the \n",
    "# HF variable in this dataset. This method does not rely on the Enverus well 'Producing Status'\n",
    "# Well Type (e.g., non-associated gas well) is determined based on annual production GOR at that well (CUM OIL/ CUM GAS), \n",
    "# but the prsence of a well will only be included in maps in months where monthly gas prod > \n",
    "\n",
    "\n",
    "#Define well location/production arrays  \n",
    "Map_EnvAllwell = np.zeros([num_regions-1,len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvNonAssocProd = np.zeros([num_regions-1,len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvBasin220 = np.zeros([len(Lat_01), len(Lon_01),num_years, num_months]) \n",
    "Map_EnvBasin395 = np.zeros([len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvBasin430 = np.zeros([len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvBasinOther = np.zeros([len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvLeaseC = np.zeros([num_regions-1,len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvNonAssoc_HF = np.zeros([num_regions-1,len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvNonAssoc_Conv = np.zeros([num_regions-1,len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvCoalBed = np.zeros([num_regions-1,len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvNonAssocExp_HF_comp = np.zeros([num_regions-1,len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvNonAssocExp_Conv_comp = np.zeros([num_regions-1,len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvGasWellExp_drilled = np.zeros([num_regions-1, len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvStateGOM_Offshore=  np.zeros([len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "#nongrid\n",
    "Map_EnvAllwell_nongrid = np.zeros([num_regions-1,num_years, num_months])\n",
    "Map_EnvNonAssocProd_nongrid = np.zeros([num_regions-1,num_years, num_months])\n",
    "Map_EnvBasin220_nongrid = np.zeros([num_years, num_months]) \n",
    "Map_EnvBasin395_nongrid = np.zeros([num_years, num_months])\n",
    "Map_EnvBasin430_nongrid = np.zeros([num_years, num_months])\n",
    "Map_EnvBasinOther_nongrid = np.zeros([num_years, num_months])\n",
    "Map_EnvLeaseC_nongrid = np.zeros([num_regions-1,num_years, num_months])\n",
    "Map_EnvNonAssoc_HF_nongrid = np.zeros([num_regions-1,num_years, num_months])\n",
    "Map_EnvNonAssoc_Conv_nongrid = np.zeros([num_regions-1,num_years, num_months])\n",
    "Map_EnvCoalBed_nongrid = np.zeros([num_regions-1,num_years, num_months])\n",
    "Map_EnvNonAssocExp_HF_comp_nongrid = np.zeros([num_regions-1,num_years, num_months])\n",
    "Map_EnvNonAssocExp_Conv_comp_nongrid = np.zeros([num_regions-1,num_years, num_months])\n",
    "Map_EnvGasWellExp_drilled_nongrid = np.zeros([num_regions-1,num_years, num_months])\n",
    "Map_EnvStateGOM_Offshore_nongrid = np.zeros([num_years, num_months])\n",
    "#ReCalc_Enverus=1 #*****\n",
    "if ReCalc_Enverus ==1:\n",
    "    \n",
    "    for iyear in np.arange(0,num_years):\n",
    "        enverus_data_temp = vars()['Enverus_data_'+year_range_str[iyear]].copy()\n",
    "        nocompdate = 0 #record the number of wells that don't have reported completion dates (but have production in that given year)\n",
    "        nodrill = 0 #record the number of wells that don't have drilling information\n",
    "        nooffshore = 0\n",
    "        \n",
    "        #loop through each row (e.g., well) in the Enverus dataset (for both onnshore and offshore gas wells wells)\n",
    "        # This will not include wells that have zero gas production in a given year, but is consistant with the GHGI approach.\n",
    "        list_onshore_wells = enverus_data_temp.index[enverus_data_temp.loc[:,'OFFSHORE'] == 'N'].tolist()\n",
    "        list_offshore_wells = enverus_data_temp.index[enverus_data_temp.loc[:,'OFFSHORE'] == 'Y'].tolist()\n",
    "        list_gas_wells = enverus_data_temp.index[enverus_data_temp.loc[:,'CUM_GAS'] > 0].tolist()\n",
    "        #find onshore gas wells based on common list elements...\n",
    "        list1_as_set = set(list_onshore_wells)\n",
    "        intersection = list1_as_set.intersection(list_gas_wells)\n",
    "        list_onshore_gas_wells = list(intersection)\n",
    "        #find offshore gas wells based on common list elements...\n",
    "        list1_as_set = set(list_offshore_wells)\n",
    "        intersection = list1_as_set.intersection(list_gas_wells)\n",
    "        list_offshore_gas_wells = list(intersection)\n",
    "    \n",
    "        # for onshore gas wells... \n",
    "        for iwell in list_onshore_gas_wells:\n",
    "            #Check if location is within CONUS\n",
    "            if enverus_data_temp['LONGITUDE'][iwell] > Lon_left and enverus_data_temp['LONGITUDE'][iwell] < Lon_right \\\n",
    "                and enverus_data_temp['LATITUDE'][iwell] > Lat_low and enverus_data_temp['LATITUDE'][iwell] < Lat_up:\n",
    "                #find index of lon and lat, and NEMS region\n",
    "                ilat = int((enverus_data_temp['LATITUDE'][iwell] - Lat_low)/Res01)\n",
    "                ilon = int((enverus_data_temp['LONGITUDE'][iwell] - Lon_left)/Res01)\n",
    "                inems = enverus_data_temp['NEMS_CODE'][iwell].astype(int)\n",
    "            \n",
    "                if ((data_fn.safe_div(enverus_data_temp['CUM_GAS'][iwell],float(enverus_data_temp['CUM_OIL'][iwell]))) > 100 or \\\n",
    "                    enverus_data_temp['GOR_QUAL'][iwell] =='Gas only'):\n",
    "                    # if non-associated gas well, \n",
    "                    for imonth in np.arange(0,num_months):\n",
    "                    #count wells in map only for months where there is gas production (emissions ~ when production is occuring)\n",
    "                        prod_str = 'GASPROD_'+month_tag[imonth]  \n",
    "                        if enverus_data_temp[prod_str][iwell] >0:\n",
    "                            Map_EnvAllwell[inems,ilat,ilon,iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell] #includes non-assoc. wells only\n",
    "                            Map_EnvNonAssocProd[inems,ilat,ilon,iyear,imonth] += enverus_data_temp[prod_str][iwell] # production from non-assoc. gas wells only\n",
    "                        \n",
    "                            #save basin-specific production levels for onshore non-associated gas wells\n",
    "                            if enverus_data_temp['AAPG_CODE_ERG'][iwell] =='220':\n",
    "                                Map_EnvBasin220[ilat,ilon,iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                            elif enverus_data_temp['AAPG_CODE_ERG'][iwell] =='395':\n",
    "                                Map_EnvBasin395[ilat,ilon,iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                            elif enverus_data_temp['AAPG_CODE_ERG'][iwell] =='430':\n",
    "                                Map_EnvBasin430[ilat,ilon,iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                            else: \n",
    "                                Map_EnvBasinOther[ilat,ilon,iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                        \n",
    "                            #Add condesate wells\n",
    "                            istate = np.where(State_ANSI['abbr'] == enverus_data_temp['STATE'][iwell])[0][0]\n",
    "                            Map_EnvLeaseC[inems,ilat,ilon,iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell]*cond_month_wellfrac[istate,inems,iyear,imonth]\n",
    "                            #print(cond_month_wellfrac[iyear,imonth,istate,inems])\n",
    "                            #[mi,abbr_dict[wells_all_prod['STATE'][i]],nemsi]\n",
    "                        \n",
    "                            if enverus_data_temp['HF'][iwell] == 'Y':\n",
    "                            #is it an HF well or not?\n",
    "                                Map_EnvNonAssoc_HF[inems,ilat,ilon,iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell] #non-assoc. HF wells\n",
    "                            else:     \n",
    "                                Map_EnvNonAssoc_Conv[inems,ilat,ilon,iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell] #non-assoc. conventional wells\n",
    "                    \n",
    "                        prod_str = 'WATERPROD_'+month_tag[imonth]  \n",
    "                        if enverus_data_temp[prod_str][iwell] >0 and \\\n",
    "                            (enverus_data_temp['BASIN'][iwell] =='POWDER RIVER' or enverus_data_temp['BASIN'][iwell] =='BLACK WARRIOR'):\n",
    "                        # save produced water volumes from non-associated gas wells in the powder river and black warrior basins\n",
    "                            Map_EnvCoalBed[inems,ilat,ilon,iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                        \n",
    "                    if isinstance(enverus_data_temp['COMPLETION_DATE'][iwell],float):\n",
    "                    #if Non-associated gas well (onshore), regardless of whether the well this month is producing, \n",
    "                    # determine whether the given well was completed this year, and if so, assign it to the correct month,\n",
    "                    # if not completed in the current year, then don't add completion year (assume it was captured already in previous year loop)\n",
    "                    # if completion date is NaN, do not record anywhere (may undercount). Will also undercount if well completed in\n",
    "                    # one year but does not start producing until the next. \n",
    "                        if np.isnan(enverus_data_temp['COMPLETION_DATE'][iwell]):\n",
    "                            nocompdate = nocompdate +1\n",
    "                    else:\n",
    "                        month = enverus_data_temp['COMPLETION_DATE'][iwell][5:7] #extract the month\n",
    "                        #print(month)\n",
    "                        year = enverus_data_temp['COMPLETION_DATE'][iwell][0:4] #extract year\n",
    "                        #print(year)\n",
    "                        if year_range_str[iyear] == year:\n",
    "                        # if completed in the current year, add to the correct month map\n",
    "                        #print('here')\n",
    "                            for imonth in np.arange(0, num_months):\n",
    "                                if month_tag[imonth] == month:\n",
    "                                    #print('here, month')\n",
    "                                    if enverus_data_temp['HF'][iwell] == 'Y':\n",
    "                                        #print('here, HF')\n",
    "                                        Map_EnvNonAssocExp_HF_comp[inems,ilat,ilon,iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell] #includes completions from non-associated HF gas wells that were producing in the same year\n",
    "                                    else:\n",
    "                                        #print('here, non-HF')\n",
    "                                        Map_EnvNonAssocExp_Conv_comp[inems,ilat,ilon,iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell] #includes completions from non-associated conventional wells that were producing in the same year\n",
    "                \n",
    "                    if isinstance(enverus_data_temp['SPUD_DATE'][iwell],float):\n",
    "                    #if Non-associated gas well (onshore), regardless of whether the well this month is producing, \n",
    "                    # determine whether the given well was drilled this year, and if so, assign it to the correct *YEAR* \n",
    "                    # assign based on SPUD Date, unless Null, then check to see if producing in the current year\n",
    "                    # NOTE: the National inventory looks for first production date in the nexy year to see if drilled this year. \n",
    "                    # This logic is too difficult to implement here, so only counted if first_prod_date is in current year\n",
    "                        if np.isnan(enverus_data_temp['SPUD_DATE'][iwell]):\n",
    "                            if isinstance(enverus_data_temp['FIRST_PROD_DATE'][iwell],float):\n",
    "                                if np.isnan(enverus_data_temp['FIRST_PROD_DATE'][iwell]):\n",
    "                                    nodrill += 1\n",
    "                            else:\n",
    "                                year = enverus_data_temp['FIRST_PROD_DATE'][iwell][0:4] #extract year\n",
    "                                if year_range_str[iyear] == year:\n",
    "                                    Map_EnvGasWellExp_drilled[inems,ilat,ilon,iyear,:] += enverus_data_temp['WELL_COUNT'][iwell]\n",
    "                    else:\n",
    "                        year = enverus_data_temp['SPUD_DATE'][iwell][0:4] #extract year\n",
    "                        #print(year)\n",
    "                        if year_range_str[iyear] == year:\n",
    "                        # if completed in the current year, add to the correct month map\n",
    "                            Map_EnvGasWellExp_drilled[inems,ilat,ilon,iyear,:] += enverus_data_temp['WELL_COUNT'][iwell]\n",
    "                \n",
    "            #if not in coninental US grid, still count those wells in non-grid arrays (does not include offshore, dealt with next)\n",
    "            # same logic sequence as above\n",
    "            else:\n",
    "                inems = enverus_data_temp['NEMS_CODE'][iwell].astype(int) \n",
    "                if ((data_fn.safe_div(enverus_data_temp['CUM_GAS'][iwell],float(enverus_data_temp['CUM_OIL'][iwell]))) > 100 or \\\n",
    "                    enverus_data_temp['GOR_QUAL'][iwell] =='Gas only'):\n",
    "                    for imonth in np.arange(0,num_months):\n",
    "                    #count wells in map only for months where there is gas production (emissions ~ when production is occuring)\n",
    "                        prod_str = 'GASPROD_'+month_tag[imonth]  \n",
    "                        if enverus_data_temp[prod_str][iwell] >0:\n",
    "                        #check if an non-assoc. gas well\n",
    "                            Map_EnvAllwell_nongrid[inems,iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell] #includes non-assoc. wells only\n",
    "                            Map_EnvNonAssocProd_nongrid[inems,iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                        \n",
    "                            #save basin-specific production levels for onshore non-associated gas wells\n",
    "                            if enverus_data_temp['AAPG_CODE_ERG'][iwell] =='220':\n",
    "                                Map_EnvBasin220_nongrid[iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                            elif enverus_data_temp['AAPG_CODE_ERG'][iwell] =='395':\n",
    "                                Map_EnvBasin395_nongrid[iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                            elif enverus_data_temp['AAPG_CODE_ERG'][iwell] =='430':\n",
    "                                Map_EnvBasin430_nongrid[iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                            else: \n",
    "                                Map_EnvBasinOther_nongrid[iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                            \n",
    "                            istate = np.where(State_ANSI['abbr'] == enverus_data_temp['STATE'][iwell])[0][0]\n",
    "                            Map_EnvLeaseC_nongrid[inems,iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell]*cond_month_wellfrac[istate,inems,iyear,imonth]\n",
    "\n",
    "                            if enverus_data_temp['HF'][iwell] == 'Y':\n",
    "                                Map_EnvNonAssoc_HF_nongrid[inems,iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell]\n",
    "                            else:     \n",
    "                                Map_EnvNonAssoc_Conv_nongrid[inems,iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell]\n",
    "                 \n",
    "                        prod_str = 'WATERPROD_'+month_tag[imonth]  \n",
    "                        if enverus_data_temp[prod_str][iwell] >0 and \\\n",
    "                            (enverus_data_temp['BASIN'][iwell] =='POWDER RIVER' or enverus_data_temp['BASIN'][iwell] =='BLACK WARRIOR'):\n",
    "                        # save produced water volumes from non-associated gas wells in the powder river and black warrior basins\n",
    "                            Map_EnvCoalBed_nongrid[inems,iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                        \n",
    "                    if isinstance(enverus_data_temp['COMPLETION_DATE'][iwell],float): \n",
    "                        if np.isnan(enverus_data_temp['COMPLETION_DATE'][iwell]):\n",
    "                            nocompdate = nocompdate +1\n",
    "                    else:\n",
    "                        month = enverus_data_temp['COMPLETION_DATE'][iwell][5:7] #extract the month\n",
    "                        year = enverus_data_temp['COMPLETION_DATE'][iwell][0:4]\n",
    "                        if year_range_str[iyear] == year:\n",
    "                            for imonth in np.arange(0, num_months):\n",
    "                                if month_tag[imonth] == month:\n",
    "                                    if enverus_data_temp['HF'][iwell] == 'Y':\n",
    "                                        Map_EnvNonAssocExp_HF_comp_nongrid[inems,iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell]\n",
    "                                    else:\n",
    "                                        Map_EnvNonAssocExp_Conv_comp_nongrid[inems,iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell]\n",
    "            \n",
    "                    if isinstance(enverus_data_temp['SPUD_DATE'][iwell],float):\n",
    "                        if np.isnan(enverus_data_temp['SPUD_DATE'][iwell]):\n",
    "                            if isinstance(enverus_data_temp['FIRST_PROD_DATE'][iwell],float):\n",
    "                                if np.isnan(enverus_data_temp['FIRST_PROD_DATE'][iwell]):\n",
    "                                    nodrill += 1\n",
    "                            else:\n",
    "                                year = enverus_data_temp['FIRST_PROD_DATE'][iwell][0:4] #extract year\n",
    "                                if year_range_str[iyear] == year:\n",
    "                                    Map_EnvGasWellExp_drilled_nongrid[inems,iyear,:] += enverus_data_temp['WELL_COUNT'][iwell]\n",
    "                    else:\n",
    "                        year = enverus_data_temp['SPUD_DATE'][iwell][0:4] #extract year\n",
    "                        #print(year)\n",
    "                        if year_range_str[iyear] == year:\n",
    "                        # if completed in the current year, add to the correct month map\n",
    "                            Map_EnvGasWellExp_drilled_nongrid[inems,iyear,:] += enverus_data_temp['WELL_COUNT'][iwell]      \n",
    "            \n",
    "                    \n",
    "        #for offshore gas well locations... \n",
    "        # EPA State GOM offshore emissions will be allocated based on Enverus production for\n",
    "        # offshore emissions in GOM states (AL, LA, TX, etc). \n",
    "        # Offshore emissions (in NGOM region) are not included in the ERG well count nor here. \n",
    "        # Federal offshore emissions are allocated later based on BOEM GOADS platform emissions\n",
    "        for iwell in list_offshore_gas_wells:\n",
    "\n",
    "            #Check if location is on grid\n",
    "            if enverus_data_temp['LONGITUDE'][iwell] > Lon_left and enverus_data_temp['LONGITUDE'][iwell] < Lon_right \\\n",
    "                and enverus_data_temp['LATITUDE'][iwell] > Lat_low and enverus_data_temp['LATITUDE'][iwell] < Lat_up:\n",
    "                #Set ilon and ilat\n",
    "                ilat = int((enverus_data_temp['LATITUDE'][iwell] - Lat_low)/Res01)\n",
    "                ilon = int((enverus_data_temp['LONGITUDE'][iwell] - Lon_left)/Res01)\n",
    "                \n",
    "                #figure out how to deal with this ....\n",
    "                # check if non-associated gas well (offshore)\n",
    "                #if enverus_data_temp['CUM_GAS'][iwell] > 0:\n",
    "                if ((data_fn.safe_div(enverus_data_temp['CUM_GAS'][iwell],float(enverus_data_temp['CUM_OIL'][iwell]))) > 100 or \\\n",
    "                    enverus_data_temp['GOR_QUAL'][iwell] =='Gas only'):\n",
    "                    if enverus_data_temp['STATE'][iwell] in {'AL','FL','LA','MS','TX'}:\n",
    "                        for imonth in np.arange(0,num_months):\n",
    "                        #count wells in map only for months where there is gas production (emissions ~ when production is occuring)\n",
    "                            prod_str = 'GASPROD_'+month_tag[imonth]  \n",
    "                            if enverus_data_temp[prod_str][iwell] >0:\n",
    "                                Map_EnvStateGOM_Offshore[ilat,ilon,iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "            else:\n",
    "                nooffshore +=1\n",
    "                #print(\"Error - No offshore outside of the domain\")#display(EPA_emi_prod_NG_CH4)           \n",
    "                \n",
    "\n",
    "        print('Enverus data not included in this analysis:')\n",
    "        print('Year: ',year_range_str[iyear])\n",
    "        print('Wells without drilling information (no Spud or Production data): ',nodrill)\n",
    "        print('Wells without completion dates: ',nocompdate)\n",
    "        print('Wells offshore and outside of grid domain: ',nooffshore)\n",
    "    \n",
    "    #save current status of datafiles\n",
    "    #Define well location/production arrays\n",
    "    np.savez('./IntermediateOutputs/Gas_EnvAllWell_tempout', x=Map_EnvAllwell, y=Map_EnvAllwell_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Gas_EnvNonAssocProd_tempout', x=Map_EnvNonAssocProd, y=Map_EnvNonAssocProd_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Gas_EnvBasin220_tempout', x=Map_EnvBasin220, y=Map_EnvBasin220_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Gas_EnvBasin395_tempout', x=Map_EnvBasin395, y=Map_EnvBasin395_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Gas_EnvBasin430_tempout', x=Map_EnvBasin430, y=Map_EnvBasin430_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Gas_EnvBasinOther_tempout', x=Map_EnvBasinOther, y=Map_EnvBasinOther_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Gas_EnvLeaseC_tempout', x=Map_EnvLeaseC, y=Map_EnvLeaseC_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Gas_EnvNonAssoc_HF_tempout', x=Map_EnvNonAssoc_HF, y=Map_EnvNonAssoc_HF_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Gas_EnvNonAssoc_Conv_tempout', x=Map_EnvNonAssoc_Conv, y=Map_EnvNonAssoc_Conv_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Gas_EnvCoalBed_tempout', x=Map_EnvCoalBed, y=Map_EnvCoalBed_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Gas_EnvNonAssocExp_HF_comp_tempout', x=Map_EnvNonAssocExp_HF_comp, y=Map_EnvNonAssocExp_HF_comp_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Gas_EnvNonAssocExp_Conv_comp_tempout', x=Map_EnvNonAssocExp_Conv_comp, y=Map_EnvNonAssocExp_Conv_comp_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Gas_EnvGasWellExp_drilled_tempout', x=Map_EnvGasWellExp_drilled, y=Map_EnvGasWellExp_drilled_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Gas_EnvStateGOM_Offshore_tempout', x=Map_EnvStateGOM_Offshore, y=Map_EnvStateGOM_Offshore_nongrid)\n",
    "else:\n",
    "    #load previously saved files\n",
    "    npzfile = np.load('./IntermediateOutputs/Gas_EnvAllWell_tempout.npz')\n",
    "    Map_EnvAllwell = npzfile['x']\n",
    "    Map_EnvAllwell_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Gas_EnvNonAssocProd_tempout.npz')\n",
    "    Map_EnvNonAssocProd = npzfile['x']\n",
    "    Map_EnvNonAssocProd_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Gas_EnvBasin220_tempout.npz')\n",
    "    Map_EnvBasin220 = npzfile['x']\n",
    "    Map_EnvBasin220_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Gas_EnvBasin395_tempout.npz')\n",
    "    Map_EnvBasin395 = npzfile['x']\n",
    "    Map_EnvBasin395_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Gas_EnvBasin430_tempout.npz')\n",
    "    Map_EnvBasin430 = npzfile['x']\n",
    "    Map_EnvBasin430_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Gas_EnvBasinOther_tempout.npz')\n",
    "    Map_EnvBasinOther = npzfile['x']\n",
    "    Map_EnvBasinOther_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Gas_EnvLeaseC_tempout.npz')\n",
    "    Map_EnvLeaseC = npzfile['x']\n",
    "    Map_EnvLeaseC_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Gas_EnvNonAssoc_HF_tempout.npz')\n",
    "    Map_EnvNonAssoc_HF = npzfile['x']\n",
    "    Map_EnvNonAssoc_HF_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Gas_EnvNonAssoc_Conv_tempout.npz')\n",
    "    Map_EnvNonAssoc_Conv = npzfile['x']\n",
    "    Map_EnvNonAssoc_Conv_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Gas_EnvCoalBed_tempout.npz')\n",
    "    Map_EnvCoalBed = npzfile['x']\n",
    "    Map_EnvCoalBed_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Gas_EnvNonAssocExp_HF_comp_tempout.npz')\n",
    "    Map_EnvNonAssocExp_HF_comp = npzfile['x']\n",
    "    Map_EnvNonAssocExp_HF_comp_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Gas_EnvNonAssocExp_Conv_comp_tempout.npz')\n",
    "    Map_EnvNonAssocExp_Conv_comp = npzfile['x']\n",
    "    Map_EnvNonAssocExp_Conv_comp_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Gas_EnvGasWellExp_drilled_tempout.npz')\n",
    "    Map_EnvGasWellExp_drilled = npzfile['x']\n",
    "    Map_EnvGasWellExp_drilled_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Gas_EnvStateGOM_Offshore_tempout.npz')\n",
    "    Map_EnvStateGOM_Offshore = npzfile['x']\n",
    "    Map_EnvStateGOM_Offshore_nongrid = npzfile['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.6 Correct Missing IL/IN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Process\n",
    "# 1. Read the GHGI well and production statistics from the GHGI (contain corrected IL and IN data)\n",
    "# 2. Read in the relevant NEI data (from both file formats) and place onto GEPA grid (including reproj of NEI data)\n",
    "# 3. Scale the NEI prxy maps to the corresponding state level values from Step 1.\n",
    "# 4. Calculate the lease condensate proxy for IL/IN using the same method as the Enverus data\n",
    "# 5. Place the scaled NEI grid data on the appropriate Enverus proxy grids. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.6.1 Read in GHGI State-Level Well Statistics for IL/IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.6.1.1 Well Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1. Read in National Well Statistics for IL/IN (from ERG Wells Processing Workbook)\n",
    "# for scaling the NEI proxies to GHGI totals so that IL and IN are correctly weighted relative to \n",
    "# the relative weights in the GHGI (e.g., consistent well counts in IL/In relative to national total)\n",
    "# There may be absolute differences in the NEI due to different data processing. \n",
    "# In otherwords, we want to take the relative spatial information from the NEI, but not the absolute values\n",
    "# Use the well count data for 2016, 2017, 2018, and 2019 - corrected by ERG \n",
    "\n",
    "Env_ILIN_wells = pd.read_excel(Enverus_WellCounts_inputfile, sheet_name = \"2020 PR - State\", skiprows = 4)\n",
    "Env_ILIN_wells = Env_ILIN_wells.drop(columns = ['Category','WELLCOUNT_16', 'WELLCOUNT_17','WELLCOUNT_18'])\n",
    "Env_ILIN_wells.rename(columns={Env_ILIN_wells.columns[Env_ILIN_wells.columns.get_loc('WELLCOUNT_16_ERG')]:'WELLCOUNT_16'}, inplace=True)\n",
    "Env_ILIN_wells.rename(columns={Env_ILIN_wells.columns[Env_ILIN_wells.columns.get_loc('WELLCOUNT_17_ERG')]:'WELLCOUNT_17'}, inplace=True)\n",
    "Env_ILIN_wells.rename(columns={Env_ILIN_wells.columns[Env_ILIN_wells.columns.get_loc('WELLCOUNT_18_ERG')]:'WELLCOUNT_18'}, inplace=True)\n",
    "#Env_ILIN_wells.rename(columns={Env_ILIN_wells.columns[Env_ILIN_wells.columns.get_loc('WELLCOUNT_19_ERG')]:'WELLCOUNT_19'}, inplace=True)\n",
    "Env_ILIN_wells = Env_ILIN_wells.fillna(0)\n",
    "Env_ILIN_wells = Env_ILIN_wells[(Env_ILIN_wells['STATE']=='IL') | (Env_ILIN_wells['STATE']=='IN')]\n",
    "Env_ILIN_wells.reset_index(inplace=True, drop=True)\n",
    "Env_ILIN_wells['NEMS'] = 0 #IN and IL are both in the north east region\n",
    "#display(Env_ILIN_wells)\n",
    "\n",
    "#2 Calculate Well Counts of Each Well Type for each NEMS region\n",
    "# ERG Query codes\n",
    "# 1 = Non-Associated Gas Wells, #2 = Oil Wells\n",
    "# 3 = Associated Gas Wells (not included in total well counts)\n",
    "# 4  = Gas Wells (non-associated) with Hydraulic Fracturing\n",
    "# 5 = Gas Well Completions with Hydraulic Fracturing\n",
    "# 6 = Oil Wells with Hydraulic Fracturing, \n",
    "# 7 = Oil well completions with hydraulic fracturing\n",
    "# 8 = All Gas Well Completions, \n",
    "# 9 All Oil well completions\n",
    "# 10a = Gas Wells Drilled, #10 b = Oil Wells Drilled\n",
    "# 10c = Dry Wells Drilled\n",
    "\n",
    "Well_NonAssoc_ILIN = np.zeros([2,num_years]) #all NA wells\n",
    "Well_NonAssoc_HF_ILIN = np.zeros([2,num_years]) #HF NA wells\n",
    "Well_NonAssoc_Conv_ILIN = np.zeros([2, num_years]) #NA conventional wells (all NA - HF NA)\n",
    "Well_NonAssoc_HF_comp_ILIN = np.zeros([2, num_years]) #HF NA well completions\n",
    "Well_NonAssoc_Conv_comp_ILIN = np.zeros([2, num_years]) #NA conventional well completions (all NA - HF NA)\n",
    "Well_Allwell_gas_comp_ILIN = np.zeros([2, num_years]) #all NA well completions\n",
    "\n",
    "Well_Gaswell_drilled_ILIN = np.zeros([2, num_years]) #will end up being corrected total gas wells drilled (inclduign fraction of dry wells)\n",
    "Well_Oilwell_drilled_ILIN = np.zeros([2, num_years]) # oil wells drilled\n",
    "Well_Drywell_drilled_ILIN = np.zeros([2, num_years]) # dry wells drilled\n",
    "\n",
    "#Well_CoalBed_ILIN = np.zeros([num_regions, len(State_ANSI), num_years]) #***NO COAL BED METHANE WELL DATA - UNCLEAR WHERE GHGI PRODUCED WATER INPUTS ARE FROM\n",
    "\n",
    "# 1) Get all well count data for non-HF wells and completions\n",
    "start_year_idx = Env_ILIN_wells.columns.get_loc('WELLCOUNT_'+str(start_year)[2:4])\n",
    "end_year_idx = Env_ILIN_wells.columns.get_loc('WELLCOUNT_'+str(end_year)[2:4])+1\n",
    "\n",
    "for idx in np.arange(0,len(Env_ILIN_wells)):\n",
    "    if Env_ILIN_wells['STATE'][idx] == 'IL':\n",
    "        istate =0\n",
    "    else:\n",
    "        istate =1\n",
    "\n",
    "    if Env_ILIN_wells['QUERY_NMBR'][idx] ==1:\n",
    "        Well_NonAssoc_ILIN[istate,] = Well_NonAssoc_ILIN[istate,]+Env_ILIN_wells.iloc[idx,start_year_idx:end_year_idx]\n",
    "    elif Env_ILIN_wells['QUERY_NMBR'][idx] ==4:\n",
    "        Well_NonAssoc_HF_ILIN[istate,] = Well_NonAssoc_HF_ILIN[istate,]+Env_ILIN_wells.iloc[idx,start_year_idx:end_year_idx]\n",
    "    elif Env_ILIN_wells['QUERY_NMBR'][idx] ==5:\n",
    "        Well_NonAssoc_HF_comp_ILIN[istate,] = Well_NonAssoc_HF_comp_ILIN[istate,]+Env_ILIN_wells.iloc[idx,start_year_idx:end_year_idx]\n",
    "    elif Env_ILIN_wells['QUERY_NMBR'][idx] ==8:   \n",
    "        Well_Allwell_gas_comp_ILIN[istate,] = Well_Allwell_gas_comp_ILIN[istate,]+Env_ILIN_wells.iloc[idx,start_year_idx:end_year_idx]\n",
    "    elif Env_ILIN_wells['QUERY_NMBR'][idx] =='10a':   \n",
    "        Well_Gaswell_drilled_ILIN[istate,] = Well_Gaswell_drilled_ILIN[istate,]+Env_ILIN_wells.iloc[idx,start_year_idx:end_year_idx]\n",
    "    elif Env_ILIN_wells['QUERY_NMBR'][idx] =='10c':\n",
    "        Well_Drywell_drilled_ILIN[istate,] = Well_Drywell_drilled_ILIN[istate,]+Env_ILIN_wells.iloc[idx,start_year_idx:end_year_idx]\n",
    "    elif Env_ILIN_wells['QUERY_NMBR'][idx] =='10b':\n",
    "        Well_Oilwell_drilled_ILIN[istate,] = Well_Oilwell_drilled_ILIN[istate,]+Env_ILIN_wells.iloc[idx,start_year_idx:end_year_idx]\n",
    "\n",
    "# Calculate Conventional well counts and completions (All gas wells - HF gas wells)\n",
    "Well_NonAssoc_Conv_ILIN = Well_NonAssoc_ILIN - Well_NonAssoc_HF_ILIN\n",
    "Well_NonAssoc_Conv_comp_ILIN = Well_Allwell_gas_comp_ILIN - Well_NonAssoc_HF_comp_ILIN\n",
    "\n",
    "# Calculate total number of wells drilled wells, accounting for fraction of dry wells (= total - corrected NG wells) \n",
    "for istate in np.arange(0,2):\n",
    "    for iyear in np.arange(0,num_years):\n",
    "        Well_Gaswell_drilled_ILIN[istate,iyear] = Well_Gaswell_drilled_ILIN[istate,iyear] + Well_Drywell_drilled_ILIN[istate,iyear] \\\n",
    "                                        * (data_fn.safe_div(Well_Gaswell_drilled_ILIN[istate,iyear],\\\n",
    "                                        (Well_Gaswell_drilled_ILIN[istate,iyear]+Well_Oilwell_drilled_ILIN[istate,iyear])))\n",
    "\n",
    "print('IL/IN GHGI total counts')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    print('Year: ', year_range_str[iyear])\n",
    "    #Print final well counts ** ADD IN QA/QC with final wells notebook later **\n",
    "    print('Non Associated:       ',np.sum(Well_NonAssoc_ILIN[:,iyear]))\n",
    "    print('Non Associated Conv:  ',np.sum(Well_NonAssoc_Conv_ILIN[:,iyear]))\n",
    "    print('Non Associated HF:    ',np.sum(Well_NonAssoc_HF_ILIN[:,iyear]))\n",
    "    print('All well gas comp:    ',np.sum(Well_Allwell_gas_comp_ILIN[:,iyear]))\n",
    "    print('Non Associated HF comp: ',np.sum(Well_NonAssoc_HF_comp_ILIN[:,iyear]))\n",
    "    print('Non Associated Conv comp: ',np.sum(Well_NonAssoc_Conv_comp_ILIN[:,iyear]))\n",
    "    print('Wells Drilled:        ',np.sum(Well_Gaswell_drilled_ILIN[:,iyear]))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.6.1.2 Well Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ERG Processed Well Production Data (from Prism/Enverus)\n",
    "# Gas produced from wells in each NEMS region, state, and Basin (units of MCF (gas))\n",
    "\n",
    "# Includes Gas production from NA gas wells (DOES NOT CURRENTLY INCLUDE GAS PRODUCTION FROM OIL WELLS)\n",
    "\n",
    "# Use the well count data for 2016, 2017, 2018, and 2019 - corrected by ERG \n",
    "Env_ILIN_wellsprod = pd.read_excel(Enverus_WellProd_inputfile, sheet_name = \"GHG_DATA_AAPG_MAR19\", skiprows = 1)\n",
    "\n",
    "#drop oil production data\n",
    "match = np.where(Env_ILIN_wellsprod.columns.str.contains('SUMOFLIQ'))[0][:]\n",
    "Env_ILIN_wellsprod = Env_ILIN_wellsprod.drop(Env_ILIN_wellsprod.columns[match], axis=1)\n",
    "\n",
    "#replace with ERG recalculations\n",
    "Env_ILIN_wellsprod = Env_ILIN_wellsprod.drop(columns = ['SUMOFGAS_16', 'SUMOFGAS_17','SUMOFGAS_18',])\n",
    "Env_ILIN_wellsprod.rename(columns={Env_ILIN_wellsprod.columns[Env_ILIN_wellsprod.columns.get_loc('SUMOFGAS_16_ERG')]:'SUMOFGAS_16'}, inplace=True)\n",
    "Env_ILIN_wellsprod.rename(columns={Env_ILIN_wellsprod.columns[Env_ILIN_wellsprod.columns.get_loc('SUMOFGAS_17_ERG')]:'SUMOFGAS_17'}, inplace=True)\n",
    "Env_ILIN_wellsprod.rename(columns={Env_ILIN_wellsprod.columns[Env_ILIN_wellsprod.columns.get_loc('SUMOFGAS_18_ERG')]:'SUMOFGAS_18'}, inplace=True)\n",
    "Env_ILIN_wellsprod = Env_ILIN_wellsprod.fillna(0)\n",
    "Env_ILIN_wellsprod = Env_ILIN_wellsprod[(Env_ILIN_wellsprod['STATE']=='IL') | (Env_ILIN_wellsprod['STATE']=='IN')]\n",
    "Env_ILIN_wellsprod.reset_index(inplace=True, drop=True)\n",
    "\n",
    "Env_ILIN_wellsprod['NEMS'] = 0 #all data are in northeast region\n",
    "\n",
    "# Extract the gas production data from non-associated gas wells (QRY = 1) and gas produced from oil wells (QRY = 2)\n",
    "# and assign to each basin-specific array based on the reported state and AAPG Code as determined by ERG in the workbook\n",
    "Wellprod_other_ILIN = np.zeros([2, num_years])\n",
    "\n",
    "start_year_idx = Env_ILIN_wellsprod.columns.get_loc('SUMOFGAS_'+str(start_year)[2:4])\n",
    "end_year_idx = Env_ILIN_wellsprod.columns.get_loc('SUMOFGAS_'+str(end_year)[2:4])+1\n",
    "\n",
    "for idx in np.arange(0,len(Env_ILIN_wellsprod)):\n",
    "    #inems = Wellprod_other_ILIN['NEMS'][idx]\n",
    "    #if inems !=6:\n",
    "    if Env_ILIN_wellsprod['STATE'][idx] == 'IL':\n",
    "        istate =0\n",
    "    else:\n",
    "        istate =1\n",
    "        if Env_ILIN_wellsprod['QUERY_NMBR'][idx] ==1: #or Env_all_wellsprod['QUERY_NMBR'][idx] ==2:\n",
    "            if Env_ILIN_wellsprod['AAPG_CODE_ERG'][idx] != 220 and Env_ILIN_wellsprod['AAPG_CODE_ERG'][idx] != 395 and Env_ILIN_wellsprod['AAPG_CODE_ERG'][idx] != 430: \n",
    "                Wellprod_other_ILIN[istate,] = Wellprod_other_ILIN[istate,] + Env_ILIN_wellsprod.iloc[idx,start_year_idx:end_year_idx]\n",
    "                \n",
    "#Print final well counts ** ADD IN QA/QC with final wells notebook later **\n",
    "print('IL/IN GHGI total production')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    print('Year: ', year_range_str[iyear])\n",
    "    print('Other Basin Production: ',np.sum(Wellprod_other_ILIN[:,iyear]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.6.2 Read In/Format NEI Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.6.2.1 Read in all data prior to 2018 (text file format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1 Read in relevant files by year (for all years before 2018 [2018 read from different file type])\n",
    "# Data are in a text file format where each row of data contains the surrogate code, FIPS code, column and row location\n",
    "# (on the NEI CONUS1 grid), and the absolute, fractional, and running sum of data (e.g., counts or production) in the\n",
    "# given FIPS region. \n",
    "# The absolute data are placed onto the GEPA grid by using an NEI reference map shapefile to map the data location\n",
    "# from the NEI CONUS grid cell indexes to the corresponding latitude and longitude values in the GEPA grid. \n",
    "### Note - the 2016 data from the NEI is on a non-standard grid where lat/lons are unknown. Can change later if needed, or\n",
    "# can interpolate between years if more accurate\n",
    "\n",
    "NEI_files = ['/USA_698_NOFILL.txt', '/USA_678_NOFILL.txt', '/USA_696_NOFILL.txt', '/USA_671_NOFILL.txt']\n",
    "data_names = ['map_NEI_gas_wells', 'map_NEI_gas_completions','map_NEI_gas_production','map_NEI_gas_drilledwells']\n",
    "\n",
    "for ivar in np.arange(0,len(data_names)):\n",
    "    vars()[data_names[ivar]] = np.zeros([2,len(Lat_01),len(Lon_01),num_years])\n",
    "\n",
    "# only recalc the data if required (set in Step 0)\n",
    "if ReCalc_NEI ==1:\n",
    "    \n",
    "    #read in the NEI grid refernece shapefile (contains the lat/lons of each NEI coordinate)\n",
    "    shape = shp.Reader(NEI_grid_ref_inputfile)\n",
    "\n",
    "    #make the map arrays of aboslute values (counts and mcf)\n",
    "    for ivar in np.arange(0,len(data_names)):\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            if year_range_str[iyear] == '2012':\n",
    "                year = '2011'\n",
    "            elif year_range_str[iyear] == '2013' or year_range_str[iyear] == '2014' or year_range_str[iyear] == '2015':\n",
    "                year = '2014'\n",
    "            elif year_range_str[iyear] == '2016' or year_range_str[iyear] == '2017':\n",
    "                year = '2017'\n",
    "            elif year_range_str[iyear] == '2018':\n",
    "                continue\n",
    "            else:\n",
    "                print('NEI DATA MISSING FOR YEAR ',year_range_str[iyear])\n",
    "            path = ERG_NEI_inputloc+year+NEI_files[ivar]\n",
    "            data_temp = pd.read_csv(path, sep='\\t', skiprows = 25)\n",
    "            data_temp = data_temp.drop([\"!\"], axis=1)\n",
    "            data_temp.columns = ['Code','FIPS','COL','ROW','Frac','Abs','FIPS_Total','FIPS_Running_Sum']\n",
    "            data_temp['Lat'] = np.zeros([len(data_temp)])\n",
    "            data_temp['Lon'] = np.zeros([len(data_temp)])\n",
    "            colmin = 1332\n",
    "            colmax=0\n",
    "            rowmin = 1548\n",
    "            rowmax=0\n",
    "            counter =0\n",
    "        \n",
    "            #Create the boundary box\n",
    "            for idx in np.arange(0,len(data_temp)):\n",
    "                if str(data_temp['FIPS'][idx]).startswith('17') or str(data_temp['FIPS'][idx]).startswith('18'):\n",
    "                    icol = data_temp['COL'][idx]\n",
    "                    irow = data_temp['ROW'][idx]\n",
    "                    if icol > colmax:\n",
    "                        colmax =icol\n",
    "                    if icol < colmin:\n",
    "                        colmin = icol\n",
    "                    if irow > rowmax:\n",
    "                        rowmax = irow\n",
    "                    if irow < rowmin:\n",
    "                        rowmin  = irow\n",
    "            \n",
    "            #Extract the relevant indicies from the NEI reference shapefile\n",
    "            array_temp = np.zeros([4,((colmax+1-colmin)*(rowmax+1-rowmin))]) #make an array to save col, row, lat, lon\n",
    "            idx=0\n",
    "            for rec in shape.iterRecords():\n",
    "                if (int(rec['cellid'][0:4]) <= colmax and int(rec['cellid'][0:4]) >= colmin) \\\n",
    "                    and (int(rec['cellid'][5:]) <= rowmax and int(rec['cellid'][5:]) >= rowmin):\n",
    "                        array_temp[0,idx] = int(rec['cellid'][0:4])   #column index\n",
    "                        array_temp[1,idx] = int(rec['cellid'][5:])    #row index\n",
    "                        array_temp[2,idx] = rec['Latitude']           #latitude\n",
    "                        array_temp[3,idx] = rec['Longitude']          #longitude\n",
    "                        idx +=1\n",
    "                        #print(idx,int(rec['cellid'][0:4]),int(rec['cellid'][5:]))\n",
    "    \n",
    "            #Use this array to locate and assign the lat lon values to the NEI datafile and then place onto grid\n",
    "            for idx in np.arange(0,len(data_temp)):\n",
    "                if str(data_temp['FIPS'][idx]).startswith('17') or str(data_temp['FIPS'][idx]).startswith('18'):\n",
    "                    icol = data_temp['COL'][idx]\n",
    "                    irow = data_temp['ROW'][idx]\n",
    "                    match = np.where((icol == array_temp[0,:]) & (irow == array_temp[1,:]))[0][0]\n",
    "                    #print(match)\n",
    "                    data_temp.loc[idx,'Lat'] = array_temp[2,match]\n",
    "                    data_temp.loc[idx,'Lon'] = array_temp[3,match]\n",
    "                    ilat = int((data_temp['Lat'][idx] - Lat_low)/Res01)\n",
    "                    ilon = int((data_temp['Lon'][idx] - Lon_left)/Res01)\n",
    "                    if str(data_temp['FIPS'][idx]).startswith('17'):\n",
    "                        vars()[data_names[ivar]][0,ilat,ilon,iyear] += data_temp.loc[idx,'Abs']\n",
    "                    else:\n",
    "                        vars()[data_names[ivar]][1,ilat,ilon,iyear] += data_temp.loc[idx,'Abs']\n",
    "\n",
    "    np.save('./IntermediateOutputs/NEI_gaswell_tempoutput', map_NEI_gas_wells)\n",
    "    np.save('./IntermediateOutputs/NEI_gascomp_tempoutput', map_NEI_gas_completions)\n",
    "    np.save('./IntermediateOutputs/NEI_gasprod_tempoutput', map_NEI_gas_production)\n",
    "    np.save('./IntermediateOutputs/NEI_gasdrill_tempoutput', map_NEI_gas_drilledwells)\n",
    "\n",
    "else:\n",
    "    map_NEI_gas_wells = np.load('./IntermediateOutputs/NEI_gaswell_tempoutput.npy')\n",
    "    map_NEI_gas_completions = np.load('./IntermediateOutputs/NEI_gascomp_tempoutput.npy')\n",
    "    map_NEI_gas_production = np.load('./IntermediateOutputs/NEI_gasprod_tempoutput.npy')\n",
    "    map_NEI_gas_drilledwells = np.load('./IntermediateOutputs/NEI_gasdrill_tempoutput.npy')\n",
    "            \n",
    "            \n",
    "print('IL/IN NEI totals')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    print('Year: ', year_range_str[iyear])\n",
    "    print('Non Associated (Conv + HF):    ',np.sum(map_NEI_gas_wells[:,:,:,iyear]))\n",
    "    print('All well gas comp (Conv + HF): ',np.sum(map_NEI_gas_completions[:,:,:,iyear]))\n",
    "    print('Gas production:                ',np.sum(map_NEI_gas_production[:,:,:,iyear]))\n",
    "    print('Wells Drilled:                 ',np.sum(map_NEI_gas_drilledwells[:,:,:,iyear]))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.6.2.2 Read in 2018 data (MS Access data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read in 2018 NEI data from different datafile format\n",
    "    \n",
    "if ReCalc_NEI ==1:    \n",
    "    #Read in the data\n",
    "    driver_str = r'Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ='+ERG_NEI_inputloc_2018+';'''\n",
    "    conn = pyodbc.connect(driver_str)\n",
    "    NEI_2018_ILIN_wells = pd.read_sql(\"SELECT * FROM 2018_IL_IN_WELLS\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    data_temp = NEI_2018_ILIN_wells[(NEI_2018_ILIN_wells['ACTIVE_WELL_FLAG'] ==1) & (NEI_2018_ILIN_wells['WELL_TYPE'] == 'GAS')]\n",
    "    data_temp.reset_index(inplace=True, drop=True)\n",
    "    data_temp.fillna(\"\",inplace=True)\n",
    "\n",
    "    #find 2018 index\n",
    "    year_diff = [abs(x - 2018) for x in year_range]\n",
    "    iyear = year_diff.index(min(year_diff))\n",
    "\n",
    "    # place data on map for each state (for active wells, production, completions, and drilled wells)\n",
    "    for iwell in np.arange(0,len(data_temp)):\n",
    "        ilat = int((data_temp['LATITUDE'][iwell] - Lat_low)/Res01)\n",
    "        ilon = int((data_temp['LONGITUDE'][iwell] - Lon_left)/Res01)\n",
    "        if str(data_temp['FIPS_CODE'][iwell]).startswith('17'):# or str(data_temp['FIPS_CODE'][iwell]).startswith('18'):\n",
    "            istate = 0\n",
    "        else:\n",
    "            istate =1\n",
    "        map_NEI_gas_wells[istate,ilat,ilon,iyear] += 1\n",
    "        map_NEI_gas_production[istate,ilat,ilon,iyear] += data_temp.loc[iwell,'SUM_GAS']\n",
    "        if '2018' in data_temp['COMPLETION_DATE'][iwell]:\n",
    "            map_NEI_gas_completions[istate,ilat,ilon,iyear] += 1\n",
    "        if '2018' in data_temp['SPUD_DATE'][iwell]:\n",
    "            map_NEI_gas_drilledwells[istate,ilat,ilon,iyear] += 1\n",
    "\n",
    "    np.save('./IntermediateOutputs/NEI_gaswell_tempoutput', map_NEI_gas_wells)\n",
    "    np.save('./IntermediateOutputs/NEI_gascomp_tempoutput', map_NEI_gas_completions)\n",
    "    np.save('./IntermediateOutputs/NEI_gasprod_tempoutput', map_NEI_gas_production)\n",
    "    np.save('./IntermediateOutputs/NEI_gasdrill_tempoutput', map_NEI_gas_drilledwells)\n",
    "else:\n",
    "    map_NEI_gas_wells = np.load('./IntermediateOutputs/NEI_gaswell_tempoutput.npy')\n",
    "    map_NEI_gas_completions = np.load('./IntermediateOutputs/NEI_gascomp_tempoutput.npy')\n",
    "    map_NEI_gas_production = np.load('./IntermediateOutputs/NEI_gasprod_tempoutput.npy')\n",
    "    map_NEI_gas_drilledwells = np.load('./IntermediateOutputs/NEI_gasdrill_tempoutput.npy')\n",
    "    \n",
    "print('IL/IN NEI totals')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    print('Year: ', year_range_str[iyear])\n",
    "    print('Non Associated (Conv + HF):    ',np.sum(map_NEI_gas_wells[:,:,:,iyear]))\n",
    "    print('All well gas comp (Conv + HF): ',np.sum(map_NEI_gas_completions[:,:,:,iyear]))\n",
    "    print('Gas production:                ',np.sum(map_NEI_gas_production[:,:,:,iyear]))\n",
    "    print('Wells Drilled:                 ',np.sum(map_NEI_gas_drilledwells[:,:,:,iyear]))\n",
    "    print(' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.6.4 Scale NEI absolute values to GHGI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale the absolute NEI data by the corresponding GHGI counts so that the IL/IN data are not over or under-weighted\n",
    "# relative to the IL/IN activity data used in the GHGI\n",
    "# without the scaling, the national emissions would likley be overallocated to these two states as the NEI well and \n",
    "# production counts are higher than those used for these states in the current GHGI\n",
    "\n",
    "#make extra required arrays (HF and Conv will have the same spatial distribution as all gas wells/completions)\n",
    "map_NEI_gas_wells_conv = map_NEI_gas_wells.copy()\n",
    "map_NEI_gas_wells_HF = map_NEI_gas_wells.copy()\n",
    "map_NEI_gas_completions_conv = map_NEI_gas_completions.copy()\n",
    "map_NEI_gas_completions_HF = map_NEI_gas_completions.copy()\n",
    "\n",
    "#if ReCalc_NEI ==1:\n",
    "\n",
    "print('QA/QC: Check that NEI data is scaled to GHGI activity data')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    # ratio = sum(GHGI)/ sum(NEI)\n",
    "    \n",
    "    #1) conventional gas wells (same spatial distribution as all NEI gas wells)\n",
    "    ratio_temp = data_fn.safe_div(np.sum(Well_NonAssoc_Conv_ILIN[:,iyear]),np.sum(map_NEI_gas_wells[:,:,:,iyear]))\n",
    "    map_NEI_gas_wells_conv[:,:,:,iyear] *= ratio_temp\n",
    "    \n",
    "    #2) HF gas wells (same spatial distribution as all NEI gas wells)\n",
    "    ratio_temp = data_fn.safe_div(np.sum(Well_NonAssoc_HF_ILIN[:,iyear]),np.sum(map_NEI_gas_wells[:,:,:,iyear]))\n",
    "    map_NEI_gas_wells_HF[:,:,:,iyear] *= ratio_temp\n",
    "    \n",
    "    # 3) all gas wells\n",
    "    ratio_temp = data_fn.safe_div(np.sum(Well_NonAssoc_ILIN[:,iyear]),np.sum(map_NEI_gas_wells[:,:,:,iyear]))\n",
    "    map_NEI_gas_wells[:,:,:,iyear] *= ratio_temp\n",
    "    \n",
    "    #4) Conv gas well completions (same spatial distribution as all gas well completions)\n",
    "    ratio_temp = data_fn.safe_div(np.sum(Well_NonAssoc_Conv_comp_ILIN[:,iyear]),np.sum(map_NEI_gas_completions[:,:,:,iyear]))\n",
    "    map_NEI_gas_completions_conv[:,:,:,iyear] *= ratio_temp\n",
    "    \n",
    "    #5) HF gas well completions (same spatial distribution as all gas well completions)\n",
    "    ratio_temp = data_fn.safe_div(np.sum(Well_NonAssoc_HF_comp_ILIN[:,iyear]),np.sum(map_NEI_gas_completions[:,:,:,iyear]))\n",
    "    map_NEI_gas_completions_HF[:,:,:,iyear] *= ratio_temp\n",
    "    \n",
    "    #6) all gas well completions\n",
    "    ratio_temp = data_fn.safe_div(np.sum(Well_Allwell_gas_comp_ILIN[:,iyear]),np.sum(map_NEI_gas_completions[:,:,:,iyear]))\n",
    "    map_NEI_gas_completions[:,:,:,iyear] *= ratio_temp\n",
    "    \n",
    "    #7) gas wells drilled\n",
    "    ratio_temp = data_fn.safe_div(np.sum(Well_Gaswell_drilled_ILIN[:,iyear]),np.sum(map_NEI_gas_drilledwells[:,:,:,iyear]))\n",
    "    if pd.isna(ratio_temp):\n",
    "        ratio_temp = 0    #if there is no GHGI data, but there is NEI data, scale to zero counts\n",
    "    map_NEI_gas_drilledwells[:,:,:,iyear] *= ratio_temp\n",
    "    #print(np.sum(map_NEI_gas_drilledwells[:,:,:,iyear]))\n",
    "    \n",
    "    #8) Gas production volumes\n",
    "    ratio_temp = data_fn.safe_div(np.sum(Wellprod_other_ILIN[:,iyear]),np.sum(map_NEI_gas_production[:,:,:,iyear]))\n",
    "    if pd.isna(ratio_temp):\n",
    "        ratio_temp = 0     #if there is no GHGI data, but there is NEI data, scale to zero counts\n",
    "    map_NEI_gas_production[:,:,:,iyear] *= ratio_temp\n",
    "    #print(ratio_temp)\n",
    "    \n",
    "    diff1 = (np.sum(Well_NonAssoc_ILIN[:,iyear]) - np.sum(map_NEI_gas_wells[:,:,:,iyear])) +\\\n",
    "            (np.sum(Well_NonAssoc_Conv_ILIN[:,iyear]) - np.sum(map_NEI_gas_wells_conv[:,:,:,iyear])) +\\\n",
    "            (np.sum(Well_NonAssoc_HF_ILIN[:,iyear]) - np.sum(map_NEI_gas_wells_HF[:,:,:,iyear])) + \\\n",
    "            (np.sum(Well_Allwell_gas_comp_ILIN[:,iyear]) - np.sum(map_NEI_gas_completions[:,:,:,iyear])) +\\\n",
    "            (np.sum(Well_NonAssoc_Conv_comp_ILIN[:,iyear]) - np.sum(map_NEI_gas_completions_conv[:,:,:,iyear])) +\\\n",
    "            (np.sum(Well_NonAssoc_HF_comp_ILIN[:,iyear]) - np.sum(map_NEI_gas_completions_HF[:,:,:,iyear])) + \\\n",
    "            (np.sum(Well_Gaswell_drilled_ILIN[:,iyear]) - np.sum(map_NEI_gas_drilledwells[:,:,:,iyear])) + \\\n",
    "            (np.sum(Wellprod_other_ILIN[:,iyear]) - np.sum(map_NEI_gas_production[:,:,:,iyear]))\n",
    "    \n",
    "    if abs(diff1) < 1e-12:\n",
    "        print('Year ', year_range_str[iyear],\":\",\"PASS\")\n",
    "    else:\n",
    "        print('Year ', year_range_str[iyear],\":\",\"CHECK\", diff1)\n",
    "    \n",
    "    print('Non Associated (Conv + HF):    ',np.sum(map_NEI_gas_wells[:,:,:,iyear]))\n",
    "    print('Non Associated (Conv):         ',np.sum(map_NEI_gas_wells_conv[:,:,:,iyear]))\n",
    "    print('Non Associated (HF):           ',np.sum(map_NEI_gas_wells_HF[:,:,:,iyear]))\n",
    "    print('All well gas comp (Conv + HF): ',np.sum(map_NEI_gas_completions[:,:,:,iyear]))\n",
    "    print('All well gas comp (Conv):      ',np.sum(map_NEI_gas_completions_conv[:,:,:,iyear]))\n",
    "    print('All well gas comp (HF):        ',np.sum(map_NEI_gas_completions_HF[:,:,:,iyear]))\n",
    "    print('Gas production:                ',np.sum(map_NEI_gas_production[:,:,:,iyear]))\n",
    "    print('Wells Drilled:                 ',np.sum(map_NEI_gas_drilledwells[:,:,:,iyear]))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.6.5 Correct Lease Condensate Data for IL/IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculate the lease condensate production in IL/IN\n",
    "# based on number of wells * condensate/well ratio. \n",
    "# Number of wells have already been scaled to GHGI totals, so no further scaling is required here\n",
    "\n",
    "map_NEI_LeaseC = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "inems = 0 #both IL/IN in northeast region\n",
    "\n",
    "print('Check condensate production levels match corrected EIA data')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    #Calc IL condensate (from condensate/well * wells)\n",
    "    istate = np.where(State_ANSI['abbr'] == 'IL')[0][0]\n",
    "    cond_per_well = data_fn.safe_div((state_cond_prod[inems,istate,iyear]),float(np.sum(map_NEI_gas_wells[0,:,:,iyear])))\n",
    "    map_NEI_LeaseC[:,:,iyear] += map_NEI_gas_wells[0,:,:,iyear]*cond_per_well\n",
    "    #Calc IN condensate (from condensate/well * wells)\n",
    "    istate = np.where(State_ANSI['abbr'] == 'IN')[0][0]\n",
    "    cond_per_well = data_fn.safe_div((state_cond_prod[inems,istate,iyear]),float(np.sum(map_NEI_gas_wells[1,:,:,iyear])))\n",
    "    map_NEI_LeaseC[:,:,iyear] += map_NEI_gas_wells[1,:,:,iyear]*cond_per_well\n",
    "\n",
    "    diff = np.sum(map_NEI_LeaseC[:,:,iyear])-np.sum(state_cond_prod[inems,13:15,iyear])\n",
    "    if diff < 1e-12:\n",
    "        print('Year', year_range_str[iyear], ': PASS')\n",
    "    else:\n",
    "        print('Year', year_range_str[iyear], ': CHECK', diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.6.6 Add the NEI data to the relevant Enverus Proxy Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#3. add maps to relevant Enverus maps\n",
    "# add absolute values to the Enverus maps above (then the weighted calculations below can remain unchanged)\n",
    "# The same values are assigned to each month (e.g., no temporal resolution is applied to IL or IN data)\n",
    "# NOTE: Proxy maps need to be reloaded if this code is run more than once\n",
    "inems =0\n",
    "for iyear in np.arange(0,num_years):\n",
    "    for imonth in np.arange(0,num_months):\n",
    "        Map_EnvAllwell[inems,:,:,iyear,imonth] += (1/12)*(map_NEI_gas_wells[0,:,:,iyear]+map_NEI_gas_wells[1,:,:,iyear])\n",
    "        Map_EnvNonAssocProd[inems,:,:,iyear,imonth] += (1/12)*(map_NEI_gas_production[0,:,:,iyear]+map_NEI_gas_production[1,:,:,iyear])\n",
    "        Map_EnvBasinOther[:,:,iyear,imonth] += (1/12)*(map_NEI_gas_production[0,:,:,iyear]+map_NEI_gas_production[1,:,:,iyear])\n",
    "        Map_EnvLeaseC[inems,:,:,iyear,imonth] += (1/12)*(map_NEI_LeaseC[:,:,iyear])\n",
    "        Map_EnvNonAssoc_HF[inems,:,:,iyear,imonth] += (1/12)*(map_NEI_gas_wells_HF[0,:,:,iyear]+map_NEI_gas_wells_HF[1,:,:,iyear])\n",
    "        Map_EnvNonAssoc_Conv[inems,:,:,iyear,imonth] += (1/12)*(map_NEI_gas_wells_conv[0,:,:,iyear]+map_NEI_gas_wells_conv[1,:,:,iyear])\n",
    "        Map_EnvNonAssocExp_HF_comp[inems,:,:,iyear,imonth] += (1/12)*(map_NEI_gas_completions_HF[0,:,:,iyear]+map_NEI_gas_completions_HF[1,:,:,iyear])\n",
    "        Map_EnvNonAssocExp_Conv_comp[inems,:,:,iyear,imonth] += (1/12)*(map_NEI_gas_completions_conv[0,:,:,iyear]+map_NEI_gas_completions_conv[1,:,:,iyear])\n",
    "        Map_EnvGasWellExp_drilled[inems,:,:,iyear,imonth] += (1/12)*(map_NEI_gas_drilledwells[0,:,:,iyear]+map_NEI_gas_drilledwells[1,:,:,iyear])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.7. Save Map_EnvAllwell for use in Transmission notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First combine all NEMS and monthly data into national annual well counts (both on and off grid)\n",
    "fileloc = '../GEPA_Gas_Transmission/InputData/Map_Enverus_NAGasWellLocations_ongrid.nc'\n",
    "Map_output = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Map_output_nongrid = np.zeros([num_years])\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    for imonth in np.arange(0,num_months):\n",
    "        for inems in np.arange(0,num_regions-1):\n",
    "            Map_output[:,:,iyear] += Map_EnvAllwell[inems,:,:,iyear,imonth]#Map_EnvAllwell[inems,ilat,ilon,iyear,imonth]   \n",
    "            Map_output_nongrid[iyear] += Map_EnvAllwell_nongrid[inems,iyear,imonth]\n",
    "\n",
    "data_IO_fn.initialize_netCDF(fileloc, 'Non-Associated Gas Well Locations', 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write the OnGrid Data to netCDF\n",
    "nc_out = Dataset(fileloc, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Map_output\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded Non-Associated Gas Wells saved to file: {}\", fileloc)#.format(os.getcwd())+fileloc)\n",
    "print('')\n",
    "\n",
    "#Write the Off-grid data to a csv\n",
    "outfile = pd.DataFrame(Map_output_nongrid)\n",
    "outfile.to_csv('../GEPA_Gas_Transmission/InputData/Map_Enverus_NAGasWellLocations_offgrid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.8. Make G&B Station Counts Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Read In Enverus Gathering Compressor Station Data (pre-processed in ArcMap)\n",
    "# 2) Make a proxy map with compressor station locations\n",
    "\n",
    "Env_GathStation_loc = pd.read_excel(Enverus_NG_GBstations_inputfile, usecols= \"AE,AH,AI,AJ\", header = 0)\n",
    "Map_EnvGB_stations = np.zeros([len(Lat_01),len(Lon_01),num_years]) #data represent a snapshot in time that is applied to entire timeseries\n",
    "Map_EnvGB_stations_nongrid = np.zeros([num_years])\n",
    "\n",
    "for istation in np.arange(0,len(Env_GathStation_loc)):\n",
    "    if Env_GathStation_loc['Longitude'][istation] > Lon_left and Env_GathStation_loc['Longitude'][istation] < Lon_right \\\n",
    "        and Env_GathStation_loc['Latitude'][istation] > Lat_low and Env_GathStation_loc['Latitude'][istation] < Lat_up:\n",
    "        ilat = int((Env_GathStation_loc['Latitude'][istation] - Lat_low)/Res01)\n",
    "        ilon = int((Env_GathStation_loc['Longitude'][istation] - Lon_left)/Res01)\n",
    "        Map_EnvGB_stations[ilat,ilon,0] += 1\n",
    "    else:\n",
    "        Map_EnvGB_stations_nongrid[0] += 1\n",
    "    #print('Year: ',year_range[iyear])    \n",
    "print('Total Gathering Compressor Stations on grid: ',np.sum(Map_EnvGB_stations[:,:,0]))\n",
    "print('Total Gathering Compressor Stations off grid: ',np.sum(Map_EnvGB_stations_nongrid))\n",
    "\n",
    "#apply the same proxy to all years\n",
    "for iyear in np.arange(1,num_years):\n",
    "    Map_EnvGB_stations[:,:,iyear] = Map_EnvGB_stations[:,:,0]\n",
    "    Map_EnvGB_stations_nongrid[iyear] = Map_EnvGB_stations_nongrid[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.9 Read In G&B Pipeline Miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1) Read In Enverus Gathering Pipeline Data (pre-processed in ArcMap)\n",
    "# 2) Make a proxy map with the length of pipeline in each grid cell\n",
    "# 3) Calculate the ratio of GB infrastructure in AK & HI compared to the national onshore total\n",
    "\n",
    "#Step 1)\n",
    "Env_GathPipelines_loc = pd.read_excel(Enverus_NG_GBpipeline_inputfile, usecols= \"C:G\", header = 0)\n",
    "Map_EnvGB_pipelines = np.zeros([len(Lat_01),len(Lon_01),num_years]) #data represent a snapshot in time that is applied to entire timeseries\n",
    "Map_EnvGB_pipelines_nongrid = np.zeros([num_years])\n",
    "\n",
    "#allocation is based on the relative pipeline length in each grid cell (pre-processed in ArcGIS)\n",
    "# Note that the sum mileage in each grid cell != original dataset mileage due to changes when data was projected\n",
    "\n",
    "# Step 2)\n",
    "for iloc in np.arange(0,len(Env_GathPipelines_loc)):\n",
    "    if Env_GathPipelines_loc['Longitude'][iloc] > Lon_left and Env_GathPipelines_loc['Longitude'][iloc] < Lon_right \\\n",
    "        and Env_GathPipelines_loc['Latitude'][iloc] > Lat_low and Env_GathPipelines_loc['Latitude'][iloc] < Lat_up:\n",
    "        ilat = int((Env_GathPipelines_loc['Latitude'][iloc] - Lat_low)/Res01)\n",
    "        ilon = int((Env_GathPipelines_loc['Longitude'][iloc] - Lon_left)/Res01)\n",
    "        Map_EnvGB_pipelines[ilat,ilon,0] += Env_GathPipelines_loc['SUM_Shape_'][iloc]\n",
    "    else:\n",
    "        Map_EnvGB_pipelines_nongrid[0] += Env_GathPipelines_loc['SUM_Shape_'][iloc]\n",
    "    #print('Year: ',year_range[iyear])    \n",
    "print('Total Gathering Pipeline length on grid: ',np.sum(Map_EnvGB_pipelines[:,:,0]))\n",
    "print('Total Gathering Pipeline length off grid: ',np.sum(Map_EnvGB_pipelines_nongrid))\n",
    "\n",
    "#apply the same proxy to all years\n",
    "for iyear in np.arange(1,num_years):\n",
    "    Map_EnvGB_pipelines[:,:,iyear] = Map_EnvGB_pipelines[:,:,0]\n",
    "    Map_EnvGB_pipelines_nongrid[iyear] = Map_EnvGB_pipelines_nongrid[0]\n",
    "    \n",
    "#Step 3)\n",
    "#1. Open Gathering_pipelines_wgs.shp\n",
    "#2. sum the miles field\n",
    "#3. Open Gathering_pipelines_AKHI_wgs84.shp\n",
    "#4. sum the miles field\n",
    "#5. Ratio the AKHI miles / (conus + AKHI miles)\n",
    "#6. Apply this ratio and subtract from the GB pipeline fields (save this fraction as 'not_mapped')\n",
    "\n",
    "#There are no Gathering Compressor Stations in AK or HI so no subtractions necessary for the remaining G&B emissions\n",
    "\n",
    "shape = shp.Reader(AKHI_pipelines_shp)\n",
    "AKHI_miles = 0\n",
    "for rec in shape.iterRecords():\n",
    "    AKHI_miles += rec['MILES']\n",
    "#print(AKHI_miles)\n",
    "\n",
    "shape = shp.Reader(CONUS_pipelines_shp)\n",
    "CONUS_miles = 0\n",
    "for rec in shape.iterRecords():\n",
    "    CONUS_miles += rec[\"MILES\"]\n",
    "#print(CONUS_miles)\n",
    "\n",
    "#apply this fraction and subtract from the GB pipeline emissions in step 4\n",
    "CONUS_ratio = AKHI_miles/(CONUS_miles+AKHI_miles)\n",
    "#print(CONUS_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Step 3. Read In EPA Data\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1 Natural Gas Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read In EPA Production/Exploration Emissions by NEMS Region\n",
    "# Note that these emissions do not include potential emissions reductions due to GasSTAR\n",
    "# Therefore, relevant sub-sector emissions in each region are scaled equally to account for the \n",
    "# national level GasSTAR reductions (the same way as done in the national GHGI)\n",
    "\n",
    "# Emissions are in units of Mg (= 1x10-6 Tg)\n",
    "\n",
    "names = pd.read_excel(EPA_NG_prod_inputfile, sheet_name = \"Production Sector _ Emissions\", usecols = \"A:AG\", skiprows = 3, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "EPA_emi_prod_NG_CH4 = pd.read_excel(EPA_NG_prod_inputfile, sheet_name = \"Production Sector _ Emissions\", usecols = \"A:AG\", skiprows = 3, names = colnames, nrows = 460)\n",
    "EPA_emi_prod_NG_CH4= EPA_emi_prod_NG_CH4.drop(columns = ['Unnamed: 0', 'Unnamed: 3'])\n",
    "EPA_emi_prod_NG_CH4.rename(columns={EPA_emi_prod_NG_CH4.columns[0]:'Region'}, inplace=True)\n",
    "EPA_emi_prod_NG_CH4.rename(columns={EPA_emi_prod_NG_CH4.columns[1]:'Source'}, inplace=True)\n",
    "EPA_emi_prod_NG_CH4 = EPA_emi_prod_NG_CH4.fillna('')\n",
    "EPA_emi_prod_NG_CH4 = EPA_emi_prod_NG_CH4.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_emi_prod_NG_CH4['Source']= EPA_emi_prod_NG_CH4['Source'].str.replace(r\"\\(\",\"\")\n",
    "EPA_emi_prod_NG_CH4['Source']= EPA_emi_prod_NG_CH4['Source'].str.replace(r\"\\)\",\"\")\n",
    "EPA_emi_prod_NG_CH4['Source']= EPA_emi_prod_NG_CH4['Source'].str.replace(r\"\\[\",\"\")\n",
    "EPA_emi_prod_NG_CH4['Source']= EPA_emi_prod_NG_CH4['Source'].str.replace(r\"\\]\",\"\")\n",
    "EPA_emi_prod_NG_CH4.reset_index(inplace=True, drop=True)\n",
    "region_idx_NG_CH4 = EPA_emi_prod_NG_CH4.index[EPA_emi_prod_NG_CH4['Region']!=''].tolist()  #find the index at the start of each region\n",
    "\n",
    "print('EPA GHGI Emissions w/out Reductions (Mg)')\n",
    "display(EPA_emi_prod_NG_CH4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.1.2. Read in Total NG Emissions (Production + Exploration) (kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in total production + exploration emissions (with methane reductions accounted for)\n",
    "# data are in kt\n",
    "\n",
    "names = pd.read_excel(EPA_NG_prod_inputfile, sheet_name = \"SUMMARY CH4\", usecols = \"A:AD\", skiprows = 10, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "EPA_emi_total_NG_CH4 = pd.read_excel(EPA_NG_prod_inputfile, sheet_name = \"SUMMARY CH4\", usecols = \"A:AD\", skiprows = 17, names = colnames, nrows = 5)\n",
    "EPA_emi_total_NG_CH4.rename(columns={EPA_emi_total_NG_CH4.columns[0]:'Source'}, inplace=True)\n",
    "EPA_emi_total_NG_CH4 = EPA_emi_total_NG_CH4.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_emi_total_NG_CH4.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\"EPA GHGI Emissions with Reductions (kt)\")\n",
    "display(EPA_emi_total_NG_CH4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.1.3 Read in and Format NG GasSTAR Reductions (kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in and format Gas STAR reductions data (units of Mg, converted here to kt)\n",
    "# For NG CH4, current reductions include those for Gas Engines, Compressor Starts, and 'Other'\n",
    "\n",
    "# get column names from top of spreadsheet\n",
    "col_range = 'A:AG'\n",
    "names = pd.read_excel(EPA_NG_prod_inputfile, sheet_name = \"Gas STAR Reductions\", usecols = col_range, skiprows = 5, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "\n",
    "# Load full Gas STAR page and save required reductions\n",
    "EPA_Gas_STAR_NG_CH4 = pd.read_excel(EPA_NG_prod_inputfile, sheet_name = \"Gas STAR Reductions\", usecols = col_range, skiprows = 6, names = colnames, nrows = 50)\n",
    "EPA_Gas_STAR_NG_CH4 = EPA_Gas_STAR_NG_CH4.fillna('')\n",
    "\n",
    "EPA_emi_red_NG_CH4 = EPA_Gas_STAR_NG_CH4[EPA_Gas_STAR_NG_CH4['Unnamed: 0'].str.contains('Gas Engines|Compressor Starts|Reduction: Other|Scaling factor')]\n",
    "EPA_emi_red_NG_CH4= EPA_emi_red_NG_CH4.drop(columns = ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 3'])\n",
    "EPA_emi_red_NG_CH4 = EPA_emi_red_NG_CH4.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_emi_red_NG_CH4.reset_index(inplace=True, drop = True)\n",
    "print('EPA GHGI Gas STAR Reductions (row 0-2 in Mg, row 3 in %):')\n",
    "display(EPA_emi_red_NG_CH4)\n",
    "\n",
    "#Calculate Reductions for Non-Associated Gas production sources\n",
    "Emi_red_NonAssoc_NG_CH4 = EPA_emi_red_NG_CH4[EPA_emi_red_NG_CH4['Source'].str.contains('Gas Engines|Compressor Starts')]\n",
    "start_year_idx = Emi_red_NonAssoc_NG_CH4.columns.get_loc(start_year)\n",
    "Emi_red_NonAssoc_NG_CH4 = Emi_red_NonAssoc_NG_CH4.iloc[:,start_year_idx:].sum(axis=0)/float(1000) #convert to kt\n",
    "Emi_red_NonAssoc_total_NG_CH4 = Emi_red_NonAssoc_NG_CH4\n",
    "print('EPA GHGI Non-Assoc. Reductions (kt):')\n",
    "display(Emi_red_NonAssoc_total_NG_CH4)\n",
    "\n",
    "#Calculate reduction for 'Other' production sources\n",
    "Emi_red_Other_NG_CH4 = EPA_emi_red_NG_CH4.iloc[2,start_year_idx:]/float(1000) \n",
    "Emi_red_scale_NG_CH4 = EPA_emi_red_NG_CH4.iloc[3,start_year_idx:] #read in scaling factor for 'Other' reductions\n",
    "Emi_red_Other_total_NG_CH4 = Emi_red_Other_NG_CH4*Emi_red_scale_NG_CH4   #scale 'Other' reductions\n",
    "print('EPA GHGI Other Reductions (kt):')\n",
    "display(Emi_red_Other_total_NG_CH4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.1.4. Read In and Format NG Regulation Reductions (kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in and format Regulation reductions data (units of Mg, converted here to kt)\n",
    "# Then scale EPA national emissions (for all NEMS regions) by the appropriate reductions\n",
    "# For NG CH4, current reductions include those for 'Deydrator Vents'\n",
    "\n",
    "# get column names from top of spreadsheet\n",
    "col_range = 'A:AG'\n",
    "names = pd.read_excel(EPA_NG_prod_inputfile, sheet_name = \"Regulations Reductions\", usecols = col_range, skiprows = 5, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "\n",
    "# Load full Reduction Regulations page and save required reductions\n",
    "EPA_RegRed_NG_CH4 = pd.read_excel(EPA_NG_prod_inputfile, sheet_name = \"Regulations Reductions\", usecols = col_range, skiprows = 7, names = colnames, nrows = 50)\n",
    "EPA_RegRed_NG_CH4 = EPA_RegRed_NG_CH4.fillna('')\n",
    "\n",
    "EPA_emi_regred_NG_CH4 = EPA_RegRed_NG_CH4[EPA_RegRed_NG_CH4['Source'].str.contains('Dehydrator Vents')]\n",
    "EPA_emi_regred_NG_CH4= EPA_emi_regred_NG_CH4.drop(columns = ['Unnamed: 0', 'Unnamed: 1', 'Unnamed: 3'])\n",
    "EPA_emi_regred_NG_CH4 = EPA_emi_regred_NG_CH4.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_emi_regred_NG_CH4.reset_index(inplace=True, drop = True)\n",
    "print('EPA GHGI Gas Regulation Reductions (Mg):')\n",
    "display(EPA_emi_regred_NG_CH4)\n",
    "\n",
    "# FOR THE 2021 INVENTORY ONLY - Remove 2019 Reg Reductions due to small error in Inventory Workbook (delete this line in future iterations)\n",
    "#EPA_emi_regred_NG_CH4.loc[0,2019] = 0\n",
    "##\n",
    "\n",
    "# Calculate Reductions for Non-Associated Gas production sources\n",
    "# Add reduction regulations to the Gas STAR reductions\n",
    "start_year_idx = EPA_emi_regred_NG_CH4.columns.get_loc(start_year)\n",
    "Emi_red_NonAssoc_total_NG_CH4 += (EPA_emi_regred_NG_CH4.iloc[:,start_year_idx:].sum(axis=0)/float(1000)) #convert Mg to kt\n",
    "print('EPA GHGI TOTAL Non-Assoc. Reductions (kt):')\n",
    "print(Emi_red_NonAssoc_total_NG_CH4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply GasSTAR and Regulation reductions to emissions from \n",
    "# Gas Engines, Compressor Starts, and Dehydrator Vents. \n",
    "# The the fraction of total remaining national emissions (after reductions) is \n",
    "# calculated and applied to each NEMS regions (so that each region has the same fractional reduction)\n",
    "#NOTE: Other production sector emission reductions will be applied to emissions at a later step. \n",
    "#units in Mg\n",
    "\n",
    "print('Corrected Regional and Total Emissions (Mg)')\n",
    "#correct gas engine emissions (find the fractional reduction of the national total \n",
    "# and apply that reduction fraction to each NEMS region)\n",
    "emi_temp = EPA_emi_prod_NG_CH4[EPA_emi_prod_NG_CH4['Source'] == 'Gas Engines']\n",
    "red_temp = EPA_emi_red_NG_CH4[EPA_emi_red_NG_CH4['Source'].str.contains('Gas Engines')]\n",
    "red_frac = (emi_temp.iloc[-1,2:] - red_temp.iloc[0,1:])/emi_temp.iloc[-1,2:]\n",
    "emi_update = emi_temp.iloc[:,2:] * red_frac\n",
    "for iyear in np.arange(0,num_years):\n",
    "    EPA_emi_prod_NG_CH4.loc[EPA_emi_prod_NG_CH4['Source']=='Gas Engines',year_range[iyear]] = emi_update.loc[:,year_range[iyear]]\n",
    "display(EPA_emi_prod_NG_CH4[EPA_emi_prod_NG_CH4['Source'] == 'Gas Engines'])\n",
    "\n",
    "#correct compressor start emissions\n",
    "emi_temp = EPA_emi_prod_NG_CH4[EPA_emi_prod_NG_CH4['Source'] == 'Compressor Starts']\n",
    "red_temp = EPA_emi_red_NG_CH4[EPA_emi_red_NG_CH4['Source'].str.contains('Compressor Starts')]\n",
    "red_frac = (emi_temp.iloc[-1,2:] - red_temp.iloc[0,1:])/emi_temp.iloc[-1,2:]\n",
    "emi_update = emi_temp.iloc[:,2:] * red_frac\n",
    "for iyear in np.arange(0,num_years):\n",
    "    EPA_emi_prod_NG_CH4.loc[EPA_emi_prod_NG_CH4['Source']=='Compressor Starts',year_range[iyear]] = emi_update.loc[:,year_range[iyear]]\n",
    "display(EPA_emi_prod_NG_CH4[EPA_emi_prod_NG_CH4['Source'] == 'Compressor Starts'])\n",
    "\n",
    "#Subtract Dehydrator vent regulation reductions\n",
    "emi_temp = EPA_emi_prod_NG_CH4[EPA_emi_prod_NG_CH4['Source'] == 'Dehydrator Vents']\n",
    "red_temp = EPA_emi_regred_NG_CH4[EPA_emi_regred_NG_CH4['Source'].str.contains('Dehydrator Vents')]\n",
    "red_frac = (emi_temp.iloc[-1,2:] - red_temp.iloc[0,1:])/emi_temp.iloc[-1,2:]\n",
    "emi_update = emi_temp.iloc[:,2:] * red_frac\n",
    "for iyear in np.arange(0,num_years):\n",
    "    EPA_emi_prod_NG_CH4.loc[EPA_emi_prod_NG_CH4['Source']=='Dehydrator Vents',year_range[iyear]] = emi_update.loc[:,year_range[iyear]]\n",
    "EPA_emi_prod_NG_CH4[EPA_emi_prod_NG_CH4['Source'] == 'Dehydrator Vents']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.5. Split Emissions into Gridding Groups (each Group will have the same proxy applied during the gridding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Final Emissions in Units of kt\n",
    "# Use mapping proxy and source files to split the NEMS-specific GHGI emissions (note some regions don't have NEMS emissions)\n",
    "region_idx = region_idx_NG_CH4\n",
    "EPA_emi_prod = EPA_emi_prod_NG_CH4\n",
    "start_year_idx = EPA_emi_prod.columns.get_loc(start_year)\n",
    "end_year_idx = EPA_emi_prod.columns.get_loc(end_year)+1\n",
    "red_frac = np.zeros(num_years)\n",
    "sum_emi_prod = np.zeros(num_years)\n",
    "sum_emi_expl = np.zeros(num_years)\n",
    "sum_emi2 = np.zeros(num_years)\n",
    "\n",
    "ghgi_prod_groups = ghgi_prod_map['GHGI_Emi_Group'].unique()\n",
    "\n",
    "DEBUG = 1\n",
    "\n",
    "for igroup in np.arange(0,len(ghgi_prod_groups)): #loop through all groups, finding the GHGI sources in that group and summing emissions for that region, year\n",
    "    if proxy_prod_map.loc[proxy_prod_map['GHGI_Emi_Group'] == ghgi_prod_groups[igroup], 'NEMS_Data'].values ==1: \n",
    "        vars()[ghgi_prod_groups[igroup]] = np.zeros([num_regions-1,num_years])\n",
    "        source_temp = ghgi_prod_map.loc[ghgi_prod_map['GHGI_Emi_Group'] == ghgi_prod_groups[igroup], 'GHGI_Source']\n",
    "        pattern_temp  = '|'.join(source_temp) \n",
    "        for iregion in np.arange(0,len(region_idx)-1):\n",
    "            EPA_emi_prod_region = EPA_emi_prod_NG_CH4.loc[region_idx[iregion]:region_idx[iregion+1],] \n",
    "            if iregion < num_regions-1:\n",
    "                emi_temp = EPA_emi_prod_region[EPA_emi_prod_region['Source'].str.contains(pattern_temp)]\n",
    "                vars()[ghgi_prod_groups[igroup]][iregion,:] = np.where(emi_temp.iloc[:,start_year_idx:] =='',[0],emi_temp.iloc[:,start_year_idx:]).sum(axis=0)/float(1000) #convert Mg to kt\n",
    "    elif proxy_prod_map.loc[proxy_prod_map['GHGI_Emi_Group'] == ghgi_prod_groups[igroup], 'NEMS_Data'].values ==2: #indicates offshore region\n",
    "        vars()[ghgi_prod_groups[igroup]] = np.zeros([num_years])\n",
    "        source_temp = ghgi_prod_map.loc[ghgi_prod_map['GHGI_Emi_Group'] == ghgi_prod_groups[igroup], 'GHGI_Source']\n",
    "        pattern_temp  = '|'.join(source_temp) \n",
    "        EPA_emi_prod_region = EPA_emi_prod_NG_CH4.loc[region_idx[num_regions-1]:region_idx[num_regions],] #offshore region\n",
    "        EPA_emi_prod_region.reset_index(inplace=True, drop=True)\n",
    "        mjr_idx = EPA_emi_prod_region.index[EPA_emi_prod_region['Source']=='Major Complexes'].values[0]\n",
    "        min_idx = EPA_emi_prod_region.index[EPA_emi_prod_region['Source']=='Minor Complexes'].values[0]\n",
    "        flr_idx = EPA_emi_prod_region.index[EPA_emi_prod_region['Source']=='GOM Federal Waters Flaring'].values[0]\n",
    "        sta_idx = EPA_emi_prod_region.index[EPA_emi_prod_region['Source']=='Offshore GOM State Waters'].values[0]\n",
    "        ak_idx = EPA_emi_prod_region.index[EPA_emi_prod_region['Source']=='Offshore Alaska State Waters'].values[0]\n",
    "        if 'Major' in ghgi_prod_groups[igroup]:\n",
    "            EPA_emi_prod_region = EPA_emi_prod_region.iloc[mjr_idx+1:min_idx,:] #skip the first row with zero data in this region\n",
    "        elif 'Minor' in ghgi_prod_groups[igroup]:\n",
    "            EPA_emi_prod_region = EPA_emi_prod_region.iloc[min_idx+1:flr_idx,:] #skip the first row with zero data in this region\n",
    "        elif 'Offshore_Both' in ghgi_prod_groups[igroup]:\n",
    "            EPA_emi_prod_region = EPA_emi_prod_region.iloc[flr_idx:flr_idx+1,:]\n",
    "        elif 'StateGOM' in ghgi_prod_groups[igroup]:\n",
    "            EPA_emi_prod_region = EPA_emi_prod_region.iloc[sta_idx+1:ak_idx,:]\n",
    "        else:\n",
    "            EPA_emi_prod_region = EPA_emi_prod_region.iloc[ak_idx:-1,:]\n",
    "        EPA_emi_prod_region.reset_index(inplace=True, drop=True)\n",
    "        emi_temp = EPA_emi_prod_region[EPA_emi_prod_region['Source'].str.contains(pattern_temp)]\n",
    "        vars()[ghgi_prod_groups[igroup]][:] = np.where(emi_temp.iloc[:,start_year_idx:] =='',[0],emi_temp.iloc[:,start_year_idx:]).sum(axis=0)/float(1000) #convert Mg to kt\n",
    "        \n",
    "    else: #for non-offshore groups that don't have NEMS data...\n",
    "        vars()[ghgi_prod_groups[igroup]] = np.zeros([num_years])\n",
    "        source_temp = ghgi_prod_map.loc[ghgi_prod_map['GHGI_Emi_Group'] == ghgi_prod_groups[igroup], 'GHGI_Source']\n",
    "        pattern_temp  = '|'.join(source_temp)\n",
    "        EPA_emi_prod_region = EPA_emi_prod.loc[region_idx[num_regions]+1:,] \n",
    "        EPA_emi_prod_region.reset_index(inplace=True, drop=True)\n",
    "        if 'Emi_Gath' in ghgi_prod_groups[igroup]:\n",
    "            gb_idx = EPA_emi_prod_region.index[EPA_emi_prod_region['Source']=='Gathering and Boosting'].values[0]\n",
    "            EPA_emi_prod_region = EPA_emi_prod_region.iloc[gb_idx+2:,:] #skip the first two rows with zero data in this region\n",
    "            EPA_emi_prod_region.reset_index(inplace=True, drop=True) \n",
    "        emi_temp = EPA_emi_prod_region[EPA_emi_prod_region['Source'].str.contains(pattern_temp)]\n",
    "        vars()[ghgi_prod_groups[igroup]][:] = np.where(emi_temp.iloc[:,start_year_idx:] =='',[0],emi_temp.iloc[:,start_year_idx:]).sum(axis=0)/float(1000) #convert Mg to kt\n",
    "\n",
    "\n",
    "#at this point, only sepcific reductions have been applied, not the general 'other' category (so need to add back into total) \n",
    "print('QA/QC #1: Check Exploration and Production Emission Sum against GHGI Summary Emissions (pre-reductions)')\n",
    "for iyear in np.arange(0,num_years): \n",
    "    for igroup in np.arange(0,len(ghgi_prod_groups)):\n",
    "        if proxy_prod_map.loc[proxy_prod_map['GHGI_Emi_Group'] == ghgi_prod_groups[igroup], 'NEMS_Data'].values ==1:\n",
    "            if proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_NonAssocExpWell' or \\\n",
    "                            proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_NonAssocExpHFComp' or \\\n",
    "                            proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_NonAssocExpConvComp' or \\\n",
    "                            proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_GasWellExpDrilled': \n",
    "                sum_emi_expl[iyear] += np.sum(vars()[ghgi_prod_groups[igroup]][:,iyear])\n",
    "            else:\n",
    "                sum_emi_prod[iyear] += np.sum(vars()[ghgi_prod_groups[igroup]][:,iyear])\n",
    "        else:\n",
    "            if proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_NonAssocExpWell' or \\\n",
    "                            proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_NonAssocExpHFComp' or \\\n",
    "                            proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_NonAssocExpConvComp' or \\\n",
    "                            proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_GasWellExpDrilled':\n",
    "                sum_emi_expl[iyear] += vars()[ghgi_prod_groups[igroup]][iyear]\n",
    "            else:\n",
    "                sum_emi_prod[iyear] += vars()[ghgi_prod_groups[igroup]][iyear]\n",
    "        \n",
    "    summary_emi = EPA_emi_total_NG_CH4.iloc[0,iyear+1]+EPA_emi_total_NG_CH4.iloc[1,iyear+1] \\\n",
    "                + Emi_red_Other_total_NG_CH4.iloc[iyear] #+Emi_red_NonAssoc_total_NG_CH4.iloc[iyear]\n",
    "    #Check 1 - make sure that the sums from all the regions equal the totals reported\n",
    "    diff1 = abs((sum_emi_prod[iyear]+sum_emi_expl[iyear]) - summary_emi)/(((sum_emi_expl[iyear]+sum_emi_prod[iyear]) + summary_emi)/2)\n",
    "    if DEBUG==1:\n",
    "        print(summary_emi)\n",
    "        print(sum_emi_prod[iyear]+sum_emi_expl[iyear])\n",
    "    if diff1 < 0.0001:\n",
    "        print('Year ', year_range[iyear],': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear],': FAIL (check Production & summary tabs): ', diff1,'%') \n",
    "        \n",
    "        \n",
    "#Apply 'Other' reductions and re-check against final national inventory values\n",
    "#reductions should only be applied to Production segment NOT Exploration\n",
    "print('QA/QC #2: Check Exploration and Production Emission Sum against GHGI Summary Emissions (post-reductions)')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    red_frac[iyear] = data_fn.safe_div((sum_emi_prod[iyear] - Emi_red_Other_total_NG_CH4.iloc[iyear]), sum_emi_prod[iyear])\n",
    "    for igroup in np.arange(0,len(ghgi_prod_groups)):\n",
    "        if proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_NonAssocExpWell' or \\\n",
    "                            proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_NonAssocExpHFComp' or \\\n",
    "                            proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_NonAssocExpConvComp' or \\\n",
    "                            proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_GasWellExpDrilled':\n",
    "            continue\n",
    "        else:\n",
    "            if proxy_prod_map.loc[proxy_prod_map['GHGI_Emi_Group'] == ghgi_prod_groups[igroup], 'NEMS_Data'].values ==1:\n",
    "                vars()[ghgi_prod_groups[igroup]][:,iyear] *= red_frac[iyear]\n",
    "                sum_emi2[iyear] += np.sum(vars()[ghgi_prod_groups[igroup]][:,iyear])\n",
    "            else:\n",
    "                vars()[ghgi_prod_groups[igroup]][iyear] *= red_frac[iyear]\n",
    "                sum_emi2[iyear] += vars()[ghgi_prod_groups[igroup]][iyear]\n",
    "    summary_emi = EPA_emi_total_NG_CH4.iloc[0,iyear+1]+EPA_emi_total_NG_CH4.iloc[1,iyear+1]\n",
    "    diff1 = abs((sum_emi2[iyear]+sum_emi_expl[iyear]) - summary_emi)/(((sum_emi2[iyear]+sum_emi_expl[iyear]) + summary_emi)/2)\n",
    "    if DEBUG==1:\n",
    "        print(summary_emi)\n",
    "        print(sum_emi2[iyear]+sum_emi_expl[iyear])\n",
    "    if diff1 < 0.0001:\n",
    "        print('Year ', year_range[iyear],': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear],': FAIL (check Production & summary tabs): ', diff1,'%') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Step 4. Grid Data (using spatial proxies)\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step. 4.1. Calculate the monthly and regional weighted arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1.1 Assign the Appropriate Proxy Variable Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names on the *left* need to match the 'NaturalGas_Production_ProxyMapping' 'Proxy_Group' names \n",
    "# (these are initialized in Step 2). \n",
    "# The names on the *right* are the variable names used to caluclate the proxies in this code.\n",
    "# Names on the right need to match those from the code in Step 2.5\n",
    "\n",
    "#Production segment\n",
    "Map_Allwell = Map_EnvAllwell\n",
    "Map_NonAssocProd = Map_EnvNonAssocProd\n",
    "Map_Basin220 = Map_EnvBasin220\n",
    "Map_Basin395 = Map_EnvBasin395\n",
    "Map_Basin430 = Map_EnvBasin430\n",
    "Map_BasinOther = Map_EnvBasinOther\n",
    "Map_LeaseC = Map_EnvLeaseC\n",
    "Map_NonAssocHF = Map_EnvNonAssoc_HF\n",
    "Map_NonAssocConv = Map_EnvNonAssoc_Conv\n",
    "Map_CoalBed = Map_EnvCoalBed\n",
    "Map_NonAssocExpHFComp = Map_EnvNonAssocExp_HF_comp\n",
    "Map_NonAssocExpConvComp = Map_EnvNonAssocExp_Conv_comp\n",
    "Map_GasWellExpDrilled = Map_EnvGasWellExp_drilled\n",
    "Map_StateGOMoffshore = Map_EnvStateGOM_Offshore\n",
    "#nongrid\n",
    "Map_Allwell_nongrid = Map_EnvAllwell_nongrid\n",
    "Map_NonAssocProd_nongrid = Map_EnvNonAssocProd_nongrid\n",
    "Map_Basin220_nongrid = Map_EnvBasin220_nongrid\n",
    "Map_Basin395_nongrid = Map_EnvBasin395_nongrid\n",
    "Map_Basin430_nongrid = Map_EnvBasin430_nongrid\n",
    "Map_BasinOther_nongrid = Map_EnvBasinOther_nongrid\n",
    "Map_LeaseC_nongrid = Map_EnvLeaseC_nongrid\n",
    "Map_NonAssocHF_nongrid = Map_EnvNonAssoc_HF_nongrid\n",
    "Map_NonAssocConv_nongrid = Map_EnvNonAssoc_Conv_nongrid\n",
    "Map_CoalBed_nongrid = Map_EnvCoalBed_nongrid\n",
    "Map_NonAssocExpHFComp_nongrid = Map_EnvNonAssocExp_HF_comp_nongrid\n",
    "Map_NonAssocExpConvComp_nongrid = Map_EnvNonAssocExp_Conv_comp_nongrid\n",
    "Map_GasWellExpDrilled_nongrid = Map_EnvGasWellExp_drilled_nongrid\n",
    "Map_StateGOMoffshore_nongrid = Map_EnvStateGOM_Offshore_nongrid\n",
    "#Offshore\n",
    "Map_FedGOMOffshoreMajor = Map_GOADSmajor_emissions\n",
    "Map_FedGOMOffshoreMinor = Map_GOADSminor_emissions\n",
    "Map_FedGOMOffshore_Both = Map_FedGOMOffshoreMajor + Map_FedGOMOffshoreMinor\n",
    "Map_FedGOMOffshore_Both_nongrid = Map_FedGOMOffshoreMajor_nongrid + Map_FedGOMOffshoreMinor_nongrid\n",
    "\n",
    "#Gathering and Boosting\n",
    "Map_GatheringStations          = Map_EnvGB_stations\n",
    "Map_GatheringStations_nongrid  = Map_EnvGB_stations_nongrid\n",
    "Map_GatheringPipelines         = Map_EnvGB_pipelines\n",
    "Map_GatheringPipelines_nongrid = Map_EnvGB_pipelines_nongrid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate weighting arrays\n",
    "# Find the fraction of wells (or gas production) in each grid cell, relative to the total well counts (or gas prod) (on and off grid)\n",
    "# also weight by the number of days in each month\n",
    "# Note condensate array is alrady calculated previously in Step 2.4\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "        month_days = month_day_leap\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        month_days = month_day_nonleap  \n",
    "    \n",
    "    #For each Proxy in List:\n",
    "    #Step 1a: weighted proxy ongrid = ongrid proxy * days each month\n",
    "    #Step 1b: weighted proxy offgrid = offgrid proxy * days each month\n",
    "    #Step 2a: noramlized weighted proxy ongrid = weighted proxy in each grid cell / (sum weighted proxy ongrid + weighted proxy offgrid)\n",
    "    #Step 2b: noramlized weighted proxy offgrid = weighted proxy offgrid / (sum weighted proxy ongrid + weighted proxy offgrid)\n",
    "    print('Prod. Proxy Arrays: ', year_range[iyear])\n",
    "    for isource in np.arange(0,len(proxy_prod_map)): \n",
    "        #print(proxy_prod_map.loc[isource, 'Proxy_Group'])\n",
    "        if proxy_prod_map.loc[isource, 'Month_Flag'] == 1:            \n",
    "            #first weight by the number of days in each month\n",
    "            #then normalize within each NEMS region (or country wide)\n",
    "            if proxy_prod_map.loc[isource, 'NEMS_Data'] == 1:\n",
    "                for imonth in np.arange(0, num_months):\n",
    "                    #first weight by the number of days in each month (weighted map for month = month map * number of days in each month)\n",
    "                    vars()[proxy_prod_map.loc[isource,'Proxy_Group']][:,:,:,iyear,imonth] *= month_days[imonth]\n",
    "                    vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][inems,iyear,imonth] *= month_days[imonth]\n",
    "                for inems in np.arange(0,6): #**** inems??? ****\n",
    "                    temp_sum = float(np.sum(vars()[proxy_prod_map.loc[isource,'Proxy_Group']][inems,:,:,iyear,:]) + \\\n",
    "                             np.sum(vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][inems,iyear,:]))\n",
    "                    vars()[proxy_prod_map.loc[isource,'Proxy_Group']][inems,:,:,iyear,:] = \\\n",
    "                            data_fn.safe_div(vars()[proxy_prod_map.loc[isource,'Proxy_Group']][inems,:,:,iyear,:], temp_sum)\n",
    "                    vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][inems,iyear,:] = \\\n",
    "                            data_fn.safe_div(vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][inems,iyear,:], temp_sum)\n",
    "                    proxy_sum = np.sum(vars()[proxy_prod_map.loc[isource,'Proxy_Group']][inems,:,:,iyear,:])+\\\n",
    "                        np.sum(vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][inems,iyear,:])\n",
    "                    #DEBUG# print(proxy_sum)\n",
    "                    if (proxy_sum >1.0001 or proxy_sum <0.9999) and proxy_prod_map.loc[isource,'Proxy_Group'] != 'Map_CoalBed':\n",
    "                        print('Check ', proxy_prod_map.loc[isource,'Proxy_Group'], 'NEMS ', inems, ': ', proxy_sum)\n",
    "                    #DEBUG# else:\n",
    "                    #DEBUG#    print('Pass: ', proxy_prod_map.loc[isource,'Proxy_Group'], ' ', proxy_sum)\n",
    "                        \n",
    "            elif proxy_prod_map.loc[isource, 'NEMS_Data'] == 0 or proxy_prod_map.loc[isource, 'NEMS_Data'] == 2:\n",
    "                for imonth in np.arange(0, num_months):\n",
    "                    vars()[proxy_prod_map.loc[isource,'Proxy_Group']][:,:,iyear,imonth] *= month_days[imonth]\n",
    "                    vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,imonth] *= month_days[imonth]\n",
    "                temp_sum = float(np.sum(vars()[proxy_prod_map.loc[isource,'Proxy_Group']][:,:,iyear,:]) + \\\n",
    "                        np.sum(vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,:])) \n",
    "                vars()[proxy_prod_map.loc[isource,'Proxy_Group']][:,:,iyear,:] = \\\n",
    "                        data_fn.safe_div(vars()[proxy_prod_map.loc[isource,'Proxy_Group']][:,:,iyear,:], temp_sum)\n",
    "                vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,:] = \\\n",
    "                        data_fn.safe_div(vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,:], temp_sum)\n",
    "                #DEBUG# proxy_sum = np.sum(vars()[proxy_prod_map.loc[isource,'Proxy_Group']][:,:,iyear,:])+np.sum(vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,:])\n",
    "                if proxy_sum >1.0001 or proxy_sum <0.9999:\n",
    "                    print('Check ', proxy_prod_map.loc[isource,'Proxy_Group'],': ', proxy_sum)   \n",
    "                #DEBUG# else:\n",
    "                #DEBUG#     print('Pass: ', proxy_prod_map.loc[isource,'Proxy_Group'], ' ', proxy_sum)\n",
    "\n",
    "    \n",
    "        else: #annual proxies\n",
    "            if proxy_prod_map.loc[isource, 'NEMS_Data'] == 0:\n",
    "                vars()[proxy_prod_map.loc[isource,'Proxy_Group']][:,:,iyear] *= np.sum(month_days)\n",
    "                #DEBUG# print(np.sum(vars()[proxy_prod_map.loc[iproxy,'Proxy_Group']][:,:,iyear]))\n",
    "                vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear] *= np.sum(month_days)\n",
    "                temp_sum = float(np.sum(vars()[proxy_prod_map.loc[isource,'Proxy_Group']][:,:,iyear]) + \\\n",
    "                    np.sum(vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear]))\n",
    "                vars()[proxy_prod_map.loc[isource,'Proxy_Group']][:,:,iyear] = \\\n",
    "                        data_fn.safe_div(vars()[proxy_prod_map.loc[isource,'Proxy_Group']][:,:,iyear], temp_sum)\n",
    "                vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear] = \\\n",
    "                        data_fn.safe_div(vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear], temp_sum)\n",
    "                proxy_sum = np.sum(vars()[proxy_prod_map.loc[isource,'Proxy_Group']][:,:,iyear])+np.sum(vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear])\n",
    "                if proxy_sum >1.0001 or proxy_sum <0.9999:\n",
    "                    print('Check ', proxy_prod_map.loc[isource,'Proxy_Group'],': ', proxy_sum)   \n",
    "                #DEBUG# else:\n",
    "                #DEBUG#     print('Pass: ', proxy_prod_map.loc[isource,'Proxy_Group'], ' ', proxy_sum)\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct and necessary Proxy Arrays (those that equal 0 and emissions > 0)\n",
    "# As of current GEPA version, Map_NonAssocExpHFComp is the only array that needs fixing (group 4 only)\n",
    "# These necessary corrections can be informed by comparing whether there are GHGI emission an emissions group\n",
    "# in a given NEMS regions, for a given year and whether the corresponding Map_Proxy = 0 for that same region/time period\n",
    "\n",
    "# example test (uncomment the following lines): \n",
    "#inems = 4\n",
    "#iyear = 5\n",
    "#print(np.sum(Emi_NonAssocExpHFComp[inems,iyear]))\n",
    "#print(np.sum(Map_NonAssocExpHFComp[inems,:,:,iyear,:]))\n",
    "# if Emi > 0 and Map = 0, need to correct the Map array with data from a different year\n",
    "\n",
    "Map_NonAssocExpHFComp[4,:,:,3,:] = Map_NonAssocExpHFComp[4,:,:,2,:]\n",
    "Map_NonAssocExpHFComp[4,:,:,4,:] = Map_NonAssocExpHFComp[4,:,:,2,:]\n",
    "Map_NonAssocExpHFComp[4,:,:,5,:] = Map_NonAssocExpHFComp[4,:,:,2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step. 4.2. Weight the National Data to Grid, then Calculate 0.1x0.1 degree flux maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For production segment...\n",
    "# 1) make flux array with correct dimensions\n",
    "# 2) weight monthly data by days in month (or year)\n",
    "# 3) caluclate flux as Flux = GHGI emissions * Proxy Map\n",
    "\n",
    "DEBUG=1\n",
    "\n",
    "Emissions_prod = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Emissions_prod_nongrid = np.zeros([num_years,num_months])\n",
    "Emissions_expl = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Emissions_expl_nongrid = np.zeros([num_years,num_months])\n",
    "Emi_not_mapped_sum = np.zeros(num_years)\n",
    "CONUS_red= np.zeros(num_years)\n",
    "\n",
    "if DEBUG==1:\n",
    "    total_sum = np.zeros(num_years)\n",
    "    proxy_val= np.zeros(num_years)\n",
    "    ghgi_val= np.zeros(num_years)\n",
    "\n",
    "# loop through each emissions group, gridded emissions = national emissions * proxy\n",
    "for igroup in np.arange(0,len(proxy_prod_map)):\n",
    "    #first check whether the Emi group was created (i.e., that the given proxy is used)\n",
    "    # if the given proxy is listed in the excel mapping file, but is not actually used to grid, skip and move to next proxy \n",
    "    if str(proxy_prod_map.loc[igroup,'GHGI_Emi_Group']) not in locals():\n",
    "        continue \n",
    "    print(proxy_prod_map.loc[igroup, 'Proxy_Group'])\n",
    "    #deal with groups where proxies have monthly data\n",
    "    if proxy_prod_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "        vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "        vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'] = np.zeros([num_years,num_months])\n",
    "        #print(np.shape())\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            #deal with groups where proxies have month data and national emissions are by NEMS group\n",
    "            if proxy_prod_map.loc[igroup, 'NEMS_Data'] == 1:\n",
    "                for inems in np.arange(0,6):\n",
    "                    vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,:] += \\\n",
    "                        vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][inems,iyear] * \\\n",
    "                        vars()[proxy_prod_map.loc[igroup,'Proxy_Group']][inems,:,:,iyear,:]\n",
    "                    vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear,:] += \\\n",
    "                        vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][inems,iyear] * \\\n",
    "                        vars()[proxy_prod_map.loc[igroup,'Proxy_Group']+'_nongrid'][inems,iyear,:]\n",
    "                    #print(np.sum(vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][inems,iyear]))                 \n",
    "                for imonth in np.arange(0,num_months):\n",
    "                    if proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "                        if proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_NonAssocExpWell' or \\\n",
    "                            proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_NonAssocExpHFComp' or \\\n",
    "                            proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_NonAssocExpConvComp' or \\\n",
    "                            proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_GasWellExpDrilled': \n",
    "                            #print('group:',proxy_prod_map.loc[igroup,'GHGI_Emi_Group'])\n",
    "                            Emissions_expl[:,:,iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth]\n",
    "                            Emissions_expl_nongrid[iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear,imonth]\n",
    "                        else:\n",
    "                            Emissions_prod[:,:,iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth]\n",
    "                            Emissions_prod_nongrid[iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear,imonth]\n",
    "\n",
    "                if DEBUG==1:\n",
    "                    proxy_val[iyear] = np.sum(vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,:])+\\\n",
    "                                 np.sum(vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear,:])\n",
    "                    ghgi_val[iyear] = np.sum(vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,iyear])\n",
    "                    total_sum[iyear] += proxy_val[iyear]\n",
    "            \n",
    "            #deal with groups where proxies have month data but emissions are national totals (e.g., NEMS_Data = 0 or 2)\n",
    "            elif proxy_prod_map.loc[igroup, 'NEMS_Data'] == 0 or proxy_prod_map.loc[igroup, 'NEMS_Data'] == 2:\n",
    "                vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,:] += \\\n",
    "                    vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                    vars()[proxy_prod_map.loc[igroup,'Proxy_Group']][:,:,iyear,:]\n",
    "                vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear,:] += \\\n",
    "                      vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                      vars()[proxy_prod_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear,:]\n",
    "                                \n",
    "                for imonth in np.arange(0,num_months):\n",
    "                    if proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "                        if proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_NonAssocExpWell' or \\\n",
    "                            proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_NonAssocExpHFComp' or \\\n",
    "                            proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_NonAssocExpConvComp' or \\\n",
    "                            proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_GasWellExpDrilled': \n",
    "                            #print('group:',proxy_prod_map.loc[igroup,'GHGI_Emi_Group'])\n",
    "                            Emissions_expl[:,:,iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth]\n",
    "                            Emissions_expl_nongrid[iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear,imonth]\n",
    "                        else:\n",
    "                            Emissions_prod[:,:,iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth]\n",
    "                            Emissions_prod_nongrid[iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear,imonth]\n",
    "\n",
    "                if DEBUG==1:\n",
    "                    proxy_val[iyear] = np.sum(vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,:])+\\\n",
    "                             np.sum(vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear,:])\n",
    "                    ghgi_val[iyear] = np.sum(vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][iyear])\n",
    "                    total_sum[iyear] += proxy_val[iyear]\n",
    "                                   \n",
    "    #deal with groups where proxies don't have month data\n",
    "    else:\n",
    "        vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            if proxy_prod_map.loc[igroup,'Proxy_Group'] == 'Map_GatheringPipelines':\n",
    "                 #If gathpipelines, remove AK/HI fraction of emissions, and save other fraction as not_mapped emissions\n",
    "                vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += \\\n",
    "                     (vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][iyear] -\\\n",
    "                     (vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][iyear]* CONUS_ratio)) * \\\n",
    "                     vars()[proxy_prod_map.loc[igroup,'Proxy_Group']][:,:,iyear]\n",
    "                CONUS_red[iyear] += vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][iyear]* CONUS_ratio\n",
    "            else:\n",
    "                vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += \\\n",
    "                    vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                    vars()[proxy_prod_map.loc[igroup,'Proxy_Group']][:,:,iyear]\n",
    "            vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear] += \\\n",
    "                    vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                    vars()[proxy_prod_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear]\n",
    "            if DEBUG==1:\n",
    "                proxy_val[iyear] = np.sum(vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])+\\\n",
    "                         np.sum(vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear])\n",
    "                ghgi_val[iyear] = np.sum(vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][iyear])\n",
    "                total_sum[iyear] += proxy_val[iyear]\n",
    "                        \n",
    "            for imonth in np.arange(0,num_months):\n",
    "                if proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "                    if proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_NonAssocExpWell' or \\\n",
    "                            proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_NonAssocExpHFComp' or \\\n",
    "                            proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_NonAssocExpConvComp' or \\\n",
    "                            proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_GasWellExpDrilled': \n",
    "                        #print('group:',proxy_prod_map.loc[igroup,'GHGI_Emi_Group'])\n",
    "                        Emissions_expl[:,:,iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]/num_months\n",
    "                        Emissions_expl_nongrid[iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear]/num_months\n",
    "                    else:\n",
    "                        Emissions_prod[:,:,iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]/num_months\n",
    "                        Emissions_prod_nongrid[iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear]/num_months\n",
    "\n",
    "    if DEBUG==1:\n",
    "        print(igroup, proxy_val[:])\n",
    "        print(igroup, ghgi_val[:])\n",
    "    \n",
    "#sum all the not_mapped emissions with the added AK/HI contributions (this is prod only)\n",
    "for iyear in np.arange(0,num_years): \n",
    "    Emi_not_mapped_sum[iyear] = Emi_not_mapped[iyear] + CONUS_red[iyear]\n",
    "    Emissions_prod_nongrid[iyear,:] += (1/12)*Emi_not_mapped_sum[iyear]\n",
    "    \n",
    "if DEBUG==1:\n",
    "    print(CONUS_red[iyear], Emi_not_mapped[iyear])\n",
    "\n",
    "# QA/QC gridded emissions\n",
    "# Check sum of all gridded emissions + emissions not included in gridding (e.g., AK), and other non-gridded areas\n",
    "print('QA/QC #1: Check weighted emissions against GHGI')   \n",
    "for iyear in np.arange(0,num_years):\n",
    "    calc_emi = 0\n",
    "    summary_emi = EPA_emi_total_NG_CH4.iloc[0,iyear+1]+EPA_emi_total_NG_CH4.iloc[1,iyear+1]\n",
    "    for igroup in np.arange(0,len(proxy_prod_map)):\n",
    "        if proxy_prod_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "            calc_emi += np.sum(vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,:])\n",
    "        else:\n",
    "            calc_emi += np.sum(vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])\n",
    "                \n",
    "    calc_emi += np.sum(Emissions_expl_nongrid[iyear,:]) +np.sum(Emissions_prod_nongrid[iyear,:])\n",
    "    if DEBUG==1:\n",
    "        print(summary_emi)\n",
    "        print(calc_emi)\n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0001:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2.2 Save gridded emissions (kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save gridded emissions for each gridding group - for extension\n",
    "\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(grid_emi_outputfile, netCDF_prod_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "unique_groups = np.unique(proxy_prod_map['GHGI_Emi_Group'])\n",
    "unique_groups = unique_groups[unique_groups != 'Emi_not_mapped']\n",
    "\n",
    "nc_out = Dataset(grid_emi_outputfile, 'r+', format='NETCDF4')\n",
    "\n",
    "for igroup in np.arange(0,len(unique_groups)):\n",
    "    print('Ext_'+unique_groups[igroup])\n",
    "    if len(np.shape(vars()['Ext_'+unique_groups[igroup]])) ==4:\n",
    "        ghgi_temp = np.sum(vars()['Ext_'+unique_groups[igroup]],axis=3) #sum month data if data is monthly\n",
    "    else:\n",
    "        ghgi_temp = vars()['Ext_'+unique_groups[igroup]]\n",
    "\n",
    "    # Write data to netCDF\n",
    "    data_out = nc_out.createVariable('Ext_'+unique_groups[igroup], 'f8', ('lat', 'lon','year'), zlib=True)\n",
    "    data_out[:,:,:] = ghgi_temp[:,:,:]\n",
    "\n",
    "#save nongrid data to calculate non-grid fraction extension\n",
    "data_out = nc_out.createVariable('Emissions_nongrid', 'f8', ('year'), zlib=True)  \n",
    "data_out[:] = np.sum(Emissions_prod_nongrid[:,:],axis=1)+np.sum(Emissions_expl_nongrid[:,:],axis=1)\n",
    "nc_out.close()\n",
    "\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded emissions (kt) written to file: {}\" .format(os.getcwd())+grid_emi_outputfile)\n",
    "print(' ')\n",
    "\n",
    "del data_out, ghgi_temp, nc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3 Calculate Gridded Fluxes (molec/s/cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 2 -- Calculate fluxes (molec./s/cm2)\n",
    "\n",
    "DEBUG =1\n",
    "\n",
    "#Initialize arrays\n",
    "check_sum = np.zeros([num_years])\n",
    "check_sum_annual = np.zeros([num_years])\n",
    "Flux_Emissions_Total_prod = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Flux_Emissions_Total_prod_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Flux_Emissions_Total_expl = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Flux_Emissions_Total_expl_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "for igroup in np.arange(0,len(proxy_prod_map)):\n",
    "    vars()['Flux_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_annual'] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "\n",
    "\n",
    "#Calculate fluxes\n",
    "for iyear in np.arange(0,num_years):\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "        month_days = month_day_leap\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        month_days = month_day_nonleap \n",
    "    \n",
    "    # calculate fluxes for each emissions group and national sum  (=kt * grams/kt *molec/mol *mol/g *s^-1 * cm^-2)\n",
    "    conversion_factor_annual = 10**9 * Avogadro / float(Molarch4 * np.sum(month_days) * 24 * 60 *60) / area_matrix_01\n",
    "    for igroup in np.arange(0,len(proxy_prod_map)):\n",
    "        #first check whether the Flux group was created (i.e., that the given proxy is used)\n",
    "        # if the given proxy is listed in the excel mapping file, but is not actually used to grid, skip and move to next proxy \n",
    "        if str('Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']) not in locals():\n",
    "            continue\n",
    "        \n",
    "        if proxy_prod_map.loc[igroup, 'Month_Flag'] == 0:\n",
    "            vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] *= conversion_factor_annual\n",
    "            vars()['Flux_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_annual'][:,:,iyear] = vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]\n",
    "        if proxy_prod_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "            for imonth in np.arange(0,num_months):\n",
    "                conversion_factor_month = 10**9 * Avogadro / float(Molarch4 * month_days[imonth] * 24 * 60 *60) / area_matrix_01\n",
    "                conv_factor2 = month_days[imonth]/year_days\n",
    "                vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth] *= conversion_factor_month\n",
    "                vars()['Flux_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_annual'][:,:,iyear] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth]*conv_factor2        \n",
    "    \n",
    "    #calculate national total flux    \n",
    "    for imonth in np.arange(0, num_months):\n",
    "        conversion_factor_month = 10**9 * Avogadro / float(Molarch4 * month_days[imonth] * 24 * 60 *60) / area_matrix_01\n",
    "        conv_factor2 = month_days[imonth]/year_days\n",
    "        Flux_Emissions_Total_prod[:,:,iyear,imonth] = Emissions_prod[:,:,iyear,imonth]*conversion_factor_month\n",
    "        Flux_Emissions_Total_prod_annual[:,:,iyear] += Flux_Emissions_Total_prod[:,:,iyear,imonth]*conv_factor2\n",
    "        Flux_Emissions_Total_expl[:,:,iyear,imonth] = Emissions_expl[:,:,iyear,imonth]*conversion_factor_month\n",
    "        Flux_Emissions_Total_expl_annual[:,:,iyear] += Flux_Emissions_Total_expl[:,:,iyear,imonth]*conv_factor2\n",
    "\n",
    "         \n",
    "        #calculate the monthly running flux totals and convert from flux back to mass (also calc annual sum)    \n",
    "        check_sum[iyear] += np.sum(Flux_Emissions_Total_prod[:,:,iyear,imonth]/conversion_factor_month) +\\\n",
    "                            np.sum(Flux_Emissions_Total_expl[:,:,iyear,imonth]/conversion_factor_month)\n",
    "    check_sum_annual[iyear] += np.sum(Flux_Emissions_Total_prod_annual[:,:,iyear]/conversion_factor_annual) +\\\n",
    "                                np.sum(Flux_Emissions_Total_expl_annual[:,:,iyear]/conversion_factor_annual)\n",
    "\n",
    "print(' ')\n",
    "print('QA/QC #2: Check final gridded fluxes against GHGI')  \n",
    "# for the sum, check the converted annual emissions (convert back from flux) plus all the non-gridded emissions\n",
    "for iyear in np.arange(0,num_years):\n",
    "    calc_emi = check_sum_annual[iyear] + np.sum(Emissions_prod_nongrid[iyear,:])+np.sum(Emissions_expl_nongrid[iyear,:]) #Emi_not_mapped_sum[iyear] +\n",
    "    summary_emi = EPA_emi_total_NG_CH4.iloc[0,iyear+1]+EPA_emi_total_NG_CH4.iloc[1,iyear+1]\n",
    "    if DEBUG==1:\n",
    "        print(calc_emi)\n",
    "        print(summary_emi)\n",
    "    \n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0001:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 5. Write gridded (0.1⁰x0.1⁰) data to netCDF files.\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize netCDF files\n",
    "#exploration\n",
    "data_IO_fn.initialize_netCDF(gridded_expl_outputfile, netCDF_expl_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "data_IO_fn.initialize_netCDF(gridded_expl_monthly_outputfile, netCDF_expl_description, 1, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write the Data to netCDF\n",
    "nc_out = Dataset(gridded_expl_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Total_expl_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded annual exploration fluxes written to file: {}\" .format(os.getcwd())+gridded_expl_outputfile)\n",
    "print('')\n",
    "\n",
    "nc_out = Dataset(gridded_expl_monthly_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:,:] = Flux_Emissions_Total_expl\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded monthly exploration fluxes written to file: {}\" .format(os.getcwd())+gridded_expl_monthly_outputfile)\n",
    "print('')\n",
    "\n",
    "#production\n",
    "data_IO_fn.initialize_netCDF(gridded_prod_outputfile, netCDF_prod_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "data_IO_fn.initialize_netCDF(gridded_prod_monthly_outputfile, netCDF_prod_description, 1, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write the Data to netCDF\n",
    "nc_out = Dataset(gridded_prod_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Total_prod_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded annual production fluxes written to file: {}\" .format(os.getcwd())+gridded_prod_outputfile)\n",
    "print('')\n",
    "\n",
    "nc_out = Dataset(gridded_prod_monthly_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:,:] = Flux_Emissions_Total_prod\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded monthly production fluxes written to file: {}\" .format(os.getcwd())+gridded_prod_monthly_outputfile)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 6. Plot Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Plot Annual Emission Fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot annual emissions for each year\n",
    "#xploration\n",
    "scale_max = 10\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Total_expl_annual, Lat_01, Lon_01, year_range, title_expl_str, scale_max,save_flag,save_outfile)\n",
    "\n",
    "#production\n",
    "scale_max = 10\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Total_prod_annual, Lat_01, Lon_01, year_range, title_prod_str, scale_max,save_flag,save_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Plot Difference Between First and Last Inventory Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot difference between last and first year\n",
    "#exploration\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Total_expl_annual, Lat_01, Lon_01, year_range, title_expl_diff_str,save_flag,save_outfile)\n",
    "\n",
    "#production\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Total_prod_annual, Lat_01, Lon_01, year_range, title_prod_diff_str,save_flag,save_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Plot Key Proxy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Map (well location) heatmap\n",
    "\n",
    "# Activity_Map = 0.1x0.1 map of activity data (counts or absolute units)\n",
    "# Plot_Frac    = 0 or 1 (0= plot activity data in absolute counts, 1= plot fractional activity data)\n",
    "# Lat          = 0.1 degree Lat values (select range)\n",
    "# Lon          = 0.1 degree Lon values (select range)\n",
    "# year_range   = array of inventory years\n",
    "# title_str    = title of map\n",
    "# legend_str   = title of legend\n",
    "# scale_max    = maximum of color scale\n",
    "\n",
    "map_output = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "for iyear in np.arange(0,num_years):\n",
    "    for imonth in np.arange(0,num_months):\n",
    "        for inems in np.arange(0,num_regions-1):\n",
    "            map_output[:,:,iyear] += Map_EnvLeaseC[inems,:,:,iyear,imonth]  \n",
    "\n",
    "            \n",
    "Activity_Map = map_output\n",
    "Plot_Frac = 1\n",
    "Lat = Lat_01\n",
    "Lon = Lon_01\n",
    "year_range = year_range\n",
    "title_str2 = \"Proxy - Non-Associated Gas Well Locations\"\n",
    "legend_str = \"Annual Fraction of National Well Population\"\n",
    "scale_max = 0.001\n",
    "\n",
    "for iyear in np.arange(0,1):#len(year_range)): \n",
    "    my_cmap = copy(plt.cm.get_cmap('rainbow',lut=3000))\n",
    "    my_cmap._init()\n",
    "    slopen = 200\n",
    "    alphas_slope = np.abs(np.linspace(0, 1.0, slopen))\n",
    "    alphas_stable = np.ones(3003-slopen)\n",
    "    alphas = np.concatenate((alphas_slope, alphas_stable))\n",
    "    my_cmap._lut[:,-1] = alphas\n",
    "    my_cmap.set_under('gray', alpha=0)\n",
    "    \n",
    "    Lon_cor = Lon[50:632]-0.05\n",
    "    Lat_cor = Lat[43:300]-0.05\n",
    "    \n",
    "    xpoints = Lon_cor\n",
    "    ypoints = Lat_cor\n",
    "    yp,xp = np.meshgrid(ypoints,xpoints)\n",
    "    \n",
    "    if np.shape(Activity_Map)[0] == len(year_range):\n",
    "        if Plot_Frac ==1:\n",
    "            zp = Activity_Map[iyear,43:300,50:632]/np.sum(Activity_Map[iyear,:,:])\n",
    "        else:\n",
    "            zp = Activity_Map[iyear,43:300,50:632]\n",
    "    elif np.shape(Activity_Map)[2] == len(year_range):\n",
    "        if Plot_Frac ==1:\n",
    "            zp = Activity_Map[43:300,50:632,iyear]/np.sum(Activity_Map[:,:,iyear])\n",
    "        else: \n",
    "            zp = Activity_Map[43:300,50:632,iyear]\n",
    "    #zp = zp/float(10**6 * Avogadro) * (year_days * 24 * 60 * 60) * Molarch4 * float(1e10)\n",
    "    \n",
    "    fig, ax = plt.subplots(dpi=300)\n",
    "    m = Basemap(llcrnrlon=xp.min(), llcrnrlat=yp.min(), urcrnrlon=xp.max(),\n",
    "                urcrnrlat=yp.max(), projection='merc', resolution='h', area_thresh=5000)\n",
    "    m.drawmapboundary(fill_color='Azure')\n",
    "    m.fillcontinents(color='FloralWhite', lake_color='Azure',zorder=1)\n",
    "    m.drawcoastlines(linewidth=0.5,zorder=3)\n",
    "    m.drawstates(linewidth=0.25,zorder=3)\n",
    "    m.drawcountries(linewidth=0.5,zorder=3)\n",
    "        \n",
    "        #if Plot_Frac == 1:\n",
    "        #    scale_max \n",
    "    \n",
    "    xpi,ypi = m(xp,yp)\n",
    "    plot = m.pcolor(xpi,ypi,zp.transpose(), cmap=my_cmap, vmin=10**-15, vmax=scale_max, snap=True,zorder=2)\n",
    "    #plot = m.scatter(xpi,ypi,s=20,c=zp.transpose(),cmap=my_cmap,zorder=2,vmin = 10**-15,snap = True,vmax = scale_max)\n",
    "    cb = m.colorbar(plot, location = \"bottom\", pad = \"1%\")        \n",
    "    tick_locator = ticker.MaxNLocator(nbins=5)\n",
    "    cb.locator = tick_locator\n",
    "    cb.update_ticks()\n",
    "    \n",
    "    cb.ax.set_xlabel(legend_str,fontsize=10)\n",
    "    cb.ax.tick_params(labelsize=10)\n",
    "    Titlestring = str(year_range[iyear])+' '+title_str2\n",
    "    plt.title(Titlestring, fontsize=14);\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = datetime.now() \n",
    "ft = ct.timestamp() \n",
    "time_elapsed = (ft-it)/(60*60)\n",
    "print('Time to run: '+str(time_elapsed)+' hours')\n",
    "print('** 1B2b_Natural_Gas_Systems_ProductionExploration: COMPLETE **')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

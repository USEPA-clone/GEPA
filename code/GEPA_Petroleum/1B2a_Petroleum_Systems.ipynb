{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded EPA Methane Inventory\n",
    "## Category: 1B2a Petroleum Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Authors: \n",
    "Joannes D. Maasakkers, Erin E. McDuffie\n",
    "#### Date Last Updated: \n",
    "See Step 0\n",
    "#### Notebook Purpose: \n",
    "This Notebook calculates and reports annual and monthly gridded methane emission fluxes (molec./cm2/s) from Petroleum Systems (production, transport, and refining segments) in the CONUS region between 2012-2018.\n",
    "#### Summary & Notes:\n",
    "EPA GHGI Petroleum system emissions are read in from the GHGI Petroleum Systems workbook at the national level. Emissions are then distributed onto a 0.1x0.1 degree grid as a function of emission group. The activity/proxy data used to spatially distribute emissions from each group include well locations and production levels from Enverus (DI and Prism), Greenhouse Gas Reporting Program (GHGRP) refinery emissions, and BOEM GOADS and BSEE platform emissions and location data for Federal Offshore emissions. Emissions data are calculated as a function of month, largely determined by whether a well was producing in a particular month or not (e.g., from Enverus/BOEM). Some proxy data are only available with annual data and are allocated evenly across each month. Both monthly and annual emission fluxes (molec./cm2/s) are written to final netCDFs in the ‘/code/Final_Gridded_Data/’ folder. Individual data files are written for total petroleum systems, as well as exploration&production, oil transport, and refining segments. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Step 0. Set-Up Notebook Modules, Functions, and Local Parameters and Constants\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm working directory\n",
    "import os\n",
    "import time\n",
    "modtime = os.path.getmtime('./1B2a_Petroleum_Systems.ipynb')\n",
    "modificationTime = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(modtime))\n",
    "print(\"This file was last modified on: \", modificationTime)\n",
    "print('')\n",
    "print(\"The directory we are working in is {}\" .format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Include plots within notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import pyodbc\n",
    "import PyPDF2 as pypdf\n",
    "import tabula as tb\n",
    "from datetime import datetime\n",
    "from copy import copy\n",
    "import shapefile as shp\n",
    "\n",
    "# Import additional modules\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# Load netCDF (for manipulating netCDF file types)\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Set up ticker\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#add path for the global function module (file)\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../Global_Functions/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Load functions\n",
    "import data_load_functions as data_load_fn\n",
    "import data_functions as data_fn\n",
    "import data_IO_functions as data_IO_fn\n",
    "import data_plot_functions as data_plot_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SPECIFY RECALS ##\n",
    "\n",
    "# Specify which sections to re-calculate or load from previously saved arrays\n",
    "# for time saving purposes\n",
    "#0 = load from saved files, 1 = re-calculate\n",
    "\n",
    "# 1) ReCalc Enverus Production Data?\n",
    "ReCalc_Enverus =1\n",
    "\n",
    "# 2) ReCalc Offshore GOADS Data\n",
    "ReCalc_GOADS = 0\n",
    "\n",
    "# 3) Re-Calc NEI Indiana and Illinois data?\n",
    "ReCalc_NEI = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT Files\n",
    "# Assign global file names\n",
    "global_filenames = data_load_fn.load_global_file_names()\n",
    "State_ANSI_inputfile = global_filenames[0]\n",
    "#County_ANSI_inputfile = global_filenames[1]\n",
    "#pop_map_inputfile = global_filenames[2]\n",
    "Grid_area01_inputfile = global_filenames[3]\n",
    "Grid_area001_inputfile = global_filenames[4]\n",
    "Grid_state001_ansi_inputfile = global_filenames[5]\n",
    "#Grid_county001_ansi_inputfile = global_filenames[6]\n",
    "globalinputlocation = global_filenames[0][0:20]\n",
    "print(globalinputlocation)\n",
    "\n",
    "# EPA Inventory Data\n",
    "EPA_Petr_inputfile = globalinputlocation+'GHGI/Ch3_Energy/PetroleumSystems_1990-2018_2020-04-11.xlsx'\n",
    "\n",
    "#proxy mapping file\n",
    "Petr_Mapping_inputfile = './InputData/Petroleum_ProxyMapping.xlsx'\n",
    "\n",
    "#NEI grid reference\n",
    "NEI_grid_ref_inputfile = globalinputlocation+'Gridded/NEI_Reference_Grid_LCC_to_WGS84_latlon.shp'\n",
    "\n",
    "#ERG/NEI Spatial Surrogate Data\n",
    "ERG_NEI_inputloc = globalinputlocation+'NEI/ERG_ILINData/CONUS_SA_FILES_'\n",
    "ERG_NEI_inputloc_2018 = globalinputlocation+'NEI/ERG_ILINData/IL_IN_ALLOCATED_WELL_LEVEL_DATA_2018_2019/IL_IN_WELL_LEVEL_DATA.accdb'\n",
    "\n",
    "#ERG Processed Well Count and Production Notebook\n",
    "Enverus_WellCounts_inputfile = globalinputlocation+'Enverus/Enverus DrillingInfo Processing - Well Counts_2021-03-17.xlsx'\n",
    "Enverus_WellProd_inputfile = globalinputlocation+'Enverus/Enverus DrillingInfo Processing - Well Prod_2021-03-17.xlsx'\n",
    "\n",
    "#Activity Data\n",
    "Enverus_Prism_inputdata_2019 = globalinputlocation+ 'Enverus/Production/prism_monthly_2019_110221.csv'\n",
    "Enverus_Prism_inputdata_2018 = globalinputlocation+ 'Enverus/Production/prism_monthly_2018_110221.csv'\n",
    "Enverus_Prism_inputdata_2017 = globalinputlocation+ 'Enverus/Production/prism_monthly_2017_110221.csv'\n",
    "Enverus_Prism_inputdata_2016 = globalinputlocation+ 'Enverus/Production/prism_monthly_2016_110221.csv'\n",
    "Enverus_Prism_inputdata_2015 = globalinputlocation+ 'Enverus/Production/prism_monthly_2015_110221.csv'\n",
    "Enverus_Prism_inputdata_2014 = globalinputlocation+ 'Enverus/Production/prism_monthly_2014_110221.csv'\n",
    "Enverus_Prism_inputdata_2013 = globalinputlocation+ 'Enverus/Production/prism_monthly_2013_110221.csv'\n",
    "Enverus_Prism_inputdata_2012 = globalinputlocation+ 'Enverus/Production/prism_monthly_2012_110221.csv'\n",
    "\n",
    "Enverus_DI_inputdata_2019 = globalinputlocation+ 'Enverus/Production/didsk_monthly_2019_102621.csv'\n",
    "Enverus_DI_inputdata_2018 = globalinputlocation+ 'Enverus/Production/didsk_monthly_2018_102621.csv'\n",
    "Enverus_DI_inputdata_2017 = globalinputlocation+ 'Enverus/Production/didsk_monthly_2017_102621.csv'\n",
    "Enverus_DI_inputdata_2016 = globalinputlocation+ 'Enverus/Production/didsk_monthly_2016_102621.csv'\n",
    "Enverus_DI_inputdata_2015 = globalinputlocation+ 'Enverus/Production/didsk_monthly_2015_102621.csv'\n",
    "Enverus_DI_inputdata_2014 = globalinputlocation+ 'Enverus/Production/didsk_monthly_2014_102621.csv'\n",
    "Enverus_DI_inputdata_2013 = globalinputlocation+ 'Enverus/Production/didsk_monthly_2013_102621.csv'\n",
    "Enverus_DI_inputdata_2012 = globalinputlocation+ 'Enverus/Production/didsk_monthly_2012_102621.csv'\n",
    "\n",
    "# Offshore GOADS Data\n",
    "GOADS_11_inputfile = globalinputlocation+'BOEM/2011_Gulfwide_Platform_Inventory.accdb'\n",
    "GOADS_14_inputfile = globalinputlocation+'BOEM/2014_Gulfwide_Platform_Inventory_20161102.accdb'\n",
    "GOADS_17_inputfile = globalinputlocation+'BOEM/2017_Gulfwide_Platform_Inventory_20190705_CAP_GHG.accdb'\n",
    "ERG_GOADSEmissions_inputfile = globalinputlocation+'BOEM/BOEM GEI Emissions Data_EmissionSource_2020-03-11.xlsx'\n",
    "#BSEE Pacific Data\n",
    "BSEE_platformloc_inputdata = \"./InputData/platlocpacdelimit.txt\"\n",
    "BSEE_platformmaster_inputdata = \"./InputData/platmastpacdelimit.txt\"\n",
    "BSEE_prod_2012_inputdata = \"./InputData/ogor2012pacdelimit.txt\"\n",
    "BSEE_prod_2013_inputdata = \"./InputData/ogor2013pacdelimit.txt\"\n",
    "BSEE_prod_2014_inputdata = \"./InputData/ogor2014pacdelimit.txt\"\n",
    "BSEE_prod_2015_inputdata = \"./InputData/ogor2015pacdelimit.txt\"\n",
    "BSEE_prod_2016_inputdata = \"./InputData/ogor2016pacdelimit.txt\"\n",
    "BSEE_prod_2017_inputdata = \"./InputData/ogor2017pacdelimit.txt\"\n",
    "BSEE_prod_2018_inputdata = \"./InputData/ogor2018pacdelimit.txt\"\n",
    "\n",
    "#GHGRP data\n",
    "GHGRP_facility_inputfile = './InputData/ghgrp_facility_info.CSV'\n",
    "ghgrp_refinery_inputfile = './InputData/SubpartY_PetrRefinery_Emissions.CSV' \n",
    "\n",
    "\n",
    "#OUTPUT FILES\n",
    "#Total Petroleum Systems\n",
    "gridded_outputfile = '../Final_Gridded_Data/EPA_v2_1B2a_Petroleum_Systems.nc'\n",
    "gridded_monthly_outputfile = '../Final_Gridded_Data/EPA_v2_1B2a_Petroleum_Systems_Monthly.nc'\n",
    "netCDF_description = 'Gridded EPA Inventory - Total Petroleum Systems Emissions - IPCC Source Category 1B2b'\n",
    "netCDF_description_m = 'Gridded EPA Inventory - Total Monthly Petroleum Systems Emissions - IPCC Source Category 1B2b'\n",
    "title_str = \"EPA methane emissions from petroleum systems\"\n",
    "title_diff_str = \"Emissions from petroleum systems difference: 2018-2012\"\n",
    "\n",
    "#Exploration\n",
    "gridded_expl_outputfile = '../Final_Gridded_Data/EPA_v2_1B2a_Petroleum_Systems_Exploration.nc'\n",
    "gridded_monthly_expl_outputfile = '../Final_Gridded_Data/EPA_v2_1B2a_Petroleum_Systems_Exploration_Monthly.nc'\n",
    "netCDF_description_expl = 'Gridded EPA Inventory - Petroleum Systems Emissions - IPCC Source Category 1B2a - Exploration'\n",
    "netCDF_description_expl_m = 'Gridded EPA Inventory - Monthly Petroleum Systems Emissions - IPCC Source Category 1B2a - Exploration'\n",
    "title_expl_str = \"EPA methane emissions from exploration\"\n",
    "title_diff_expl_str = \"Emissions from exploration difference: 2018-2012\"\n",
    "\n",
    "#Production\n",
    "gridded_prod_outputfile = '../Final_Gridded_Data/EPA_v2_1B2a_Petroleum_Systems_Production.nc'\n",
    "gridded_monthly_prod_outputfile = '../Final_Gridded_Data/EPA_v2_1B2a_Petroleum_Systems_Production_Monthly.nc'\n",
    "netCDF_description_prod = 'Gridded EPA Inventory - Petroleum Systems Emissions - IPCC Source Category 1B2a - Production'\n",
    "netCDF_description_prod_m = 'Gridded EPA Inventory - Monthly Petroleum Systems Emissions - IPCC Source Category 1B2a - Production'\n",
    "title_prod_str = \"EPA methane emissions from production\"\n",
    "title_diff_prod_str = \"Emissions from production difference: 2018-2012\"\n",
    "\n",
    "#Oil Transport\n",
    "gridded_trans_outputfile = '../Final_Gridded_Data/EPA_v2_1B2a_Petroleum_Systems_Transport.nc'\n",
    "gridded_monthly_trans_outputfile = '../Final_Gridded_Data/EPA_v2_1B2a_Petroleum_Systems_Transport_Monthly.nc'\n",
    "netCDF_description_trans = 'Gridded EPA Inventory - Petroleum Systems Emissions - IPCC Source Category 1B2a - Oil Transport'\n",
    "netCDF_description_trans_m = 'Gridded EPA Inventory - Monthly Petroleum Systems Emissions - IPCC Source Category 1B2a - Oil Transport'\n",
    "title_trans_str = \"EPA methane emissions from oil transport\"\n",
    "title_diff_trans_str = \"Emissions from oil transport difference: 2018-2012\"\n",
    "\n",
    "#Oil Refining\n",
    "gridded_ref_outputfile = '../Final_Gridded_Data/EPA_v2_1B2a_Petroleum_Systems_Refining.nc'\n",
    "gridded_monthly_ref_outputfile = '../Final_Gridded_Data/EPA_v2_1B2a_Petroleum_Systems_Refining_Monthly.nc'\n",
    "netCDF_description_ref = 'Gridded EPA Inventory - Petroleum Systems Emissions - IPCC Source Category 1B2a - Refining'\n",
    "netCDF_description_ref_m = 'Gridded EPA Inventory - Monthly Petroleum Systems Emissions - IPCC Source Category 1B2a - Refining'\n",
    "title_ref_str = \"EPA methane emissions from refining\"\n",
    "title_diff_ref_str = \"Emissions from refining difference: 2018-2012\"\n",
    "\n",
    "#output gridded proxy data\n",
    "grid_emi_outputfile = '../Final_Gridded_Data/Extension/v2_input_data/Petroleum_Grid_Emi.nc'\n",
    "grid_emi_prod_outputfile = '../Final_Gridded_Data/Extension/v2_input_data/Petr_Production_Grid_Emi.nc'\n",
    "grid_emi_trans_outputfile = '../Final_Gridded_Data/Extension/v2_input_data/Petr_Transport_Grid_Emi.nc'\n",
    "grid_emi_ref_outputfile = '../Final_Gridded_Data/Extension/v2_input_data/Petr_Refining_Grid_Emi.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local variables\n",
    "start_year = 2012  #First year in emission timeseries\n",
    "end_year = 2018    #Last year in emission timeseries\n",
    "year_range = [*range(start_year, end_year+1,1)] #List of emission years\n",
    "year_range_str=[str(i) for i in year_range]\n",
    "num_years = len(year_range)\n",
    "\n",
    "# Define constants\n",
    "Avogadro   = 6.02214129 * 10**(23)  #molecules/mol\n",
    "Molarch4   = 16.04                  #g/mol\n",
    "Res01      = 0.1                    # degrees\n",
    "\n",
    "# Continental US Lat/Lon Limits (for netCDF files)\n",
    "Lon_left = -130       #deg\n",
    "Lon_right = -60       #deg\n",
    "Lat_low  = 20         #deg\n",
    "Lat_up  = 55          #deg\n",
    "loc_dimensions = [Lat_low, Lat_up, Lon_left, Lon_right]\n",
    "\n",
    "ilat_start = int((90+Lat_low)/Res01) #1100:1450 (continental US range)\n",
    "ilat_end = int((90+Lat_up)/Res01)\n",
    "ilon_start = abs(int((-180-Lon_left)/Res01)) #500:1200 (continental US range)\n",
    "ilon_end = abs(int((-180-Lon_right)/Res01))\n",
    "\n",
    "# Number of days in each month\n",
    "month_day_leap  = [  31,  29,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_day_nonleap = [  31,  28,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_tag = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "month_dict = {'January':1, 'February':2,'March':3,'April':4,'May':5,'June':6, 'July':7,'August':8,'September':9,'October':10,\\\n",
    "             'November':11,'December':12}\n",
    "\n",
    "# Month arrays\n",
    "month_range_str = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "num_months = len(month_range_str)\n",
    "num_regions = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;\n",
    "//prevent auto-scrolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track run time\n",
    "ct = datetime.now() \n",
    "it = ct.timestamp() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Step 1. Load in State ANSI data, NEMS definitions, and Area Maps\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State-level ANSI Data\n",
    "#Read the state ANSI file array\n",
    "State_ANSI, name_dict, abbr_dict = data_load_fn.load_state_ansi(State_ANSI_inputfile)[0:3]\n",
    "#QA: number of states\n",
    "print('Read input file: '+ f\"{State_ANSI_inputfile}\")\n",
    "print('Total \"States\" found: ' + '%.0f' % len(State_ANSI))\n",
    "print(' ')\n",
    "\n",
    "#County ANSI Data\n",
    "#Includes State ANSI number, county ANSI number, county name, and country area (square miles)\n",
    "#pd_counties = pd.read_csv(County_ANSI_inputfile,encoding='latin-1')\n",
    "\n",
    "#QA: number of counties\n",
    "#print ('Read input file: ' + f\"{County_ANSI_inputfile}\")\n",
    "#print('Total \"Counties\" found (include PR): ' + '%.0f' % len(pd_counties))\n",
    "#print(' ')\n",
    "\n",
    "#Create a placeholder array for county data\n",
    "#county_array = np.zeros([len(pd_counties),3])\n",
    "\n",
    "#Populate array with State ANSI number (0), county ANSI number (1), and county area (2)\n",
    "#for icounty in np.arange(0,len(pd_counties)):\n",
    "#    county_array[icounty,0] = int(pd_counties.values[icounty,0])\n",
    "#    county_array[icounty,1] = int(pd_counties.values[icounty,1])\n",
    "#    county_array[icounty,2] = pd_counties.values[icounty,3]\n",
    "    \n",
    "# 0.01 x0.01 degree Data\n",
    "# State ANSI IDs and grid cell area (m2) maps\n",
    "state_ANSI_map = data_load_fn.load_state_ansi_map(Grid_state001_ansi_inputfile)\n",
    "#county_ANSI_map = data_load_fn.load_county_ansi_map(Grid_county001_ansi_inputfile)\n",
    "#county_ANSI_map = county_ANSI_map.astype('int32')\n",
    "area_map, lat001, lon001 = data_load_fn.load_area_map_001(Grid_area001_inputfile)\n",
    "\n",
    "# 0.1 x0.1 degree data\n",
    "# grid cell area and state ANSI maps\n",
    "Lat01, Lon01 = data_load_fn.load_area_map_01(Grid_area01_inputfile)[1:3]\n",
    "#Select relevant Continental 0.1 x0.1 domain\n",
    "Lat_01 = Lat01[ilat_start:ilat_end]\n",
    "Lon_01 = Lon01[ilon_start:ilon_end]\n",
    "area_matrix_01 = data_fn.regrid001_to_01(area_map, Lat_01, Lon_01)\n",
    "area_matrix_01 *= 10000  #convert from m2 to cm2\n",
    "#state_ANSI_map_01 = data_fn.regrid001_to_01(state_ANSI_map, Lat_01, Lon_01)\n",
    "\n",
    "# Print time\n",
    "ct = datetime.now() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 2: Read-in and Format Proxy Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 Read In Proxy Mapping File & Make Proxy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load GHGI Mapping Groups\n",
    "names = pd.read_excel(Petr_Mapping_inputfile, sheet_name = \"GHGI Map - E&P\", usecols = \"A:B\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "ghgi_prod_map = pd.read_excel(Petr_Mapping_inputfile, sheet_name = \"GHGI Map - E&P\", usecols = \"A:B\", skiprows = 2, names = colnames)\n",
    "#drop rows with no data, remove the parentheses and \"\"\n",
    "ghgi_prod_map = ghgi_prod_map[ghgi_prod_map['GHGI_Emi_Group'] != 'na']\n",
    "ghgi_prod_map = ghgi_prod_map[ghgi_prod_map['GHGI_Emi_Group'].notna()]\n",
    "ghgi_prod_map['GHGI_Source']= ghgi_prod_map['GHGI_Source'].str.replace(r\"\\(\",\"- \")\n",
    "ghgi_prod_map['GHGI_Source']= ghgi_prod_map['GHGI_Source'].str.replace(r\"\\)\",\"\")\n",
    "ghgi_prod_map['GHGI_Source']= ghgi_prod_map['GHGI_Source'].str.replace(r'\"',\"\")\n",
    "ghgi_prod_map.reset_index(inplace=True, drop=True)\n",
    "display(ghgi_prod_map)\n",
    "\n",
    "#load emission group - proxy map\n",
    "names = pd.read_excel(Petr_Mapping_inputfile, sheet_name = \"Proxy Map - E&P\", usecols = \"A:C\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "proxy_prod_map = pd.read_excel(Petr_Mapping_inputfile, sheet_name = \"Proxy Map - E&P\", usecols = \"A:C\", skiprows = 1, names = colnames)\n",
    "display((proxy_prod_map))\n",
    "\n",
    "#create empty proxy and emission group arrays (add months for proxy variables that have monthly data)\n",
    "for igroup in np.arange(0,len(proxy_prod_map)):\n",
    "    if proxy_prod_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "        vars()[proxy_prod_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "        vars()[proxy_prod_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years,num_months])\n",
    "        vars()[ghgi_prod_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "    else:\n",
    "        vars()[proxy_prod_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        vars()[proxy_prod_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "        vars()[ghgi_prod_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        \n",
    "#Transport\n",
    "#load GHGI Mapping Groups\n",
    "names = pd.read_excel(Petr_Mapping_inputfile, sheet_name = \"GHGI Map - Trans\", usecols = \"A:B\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "ghgi_trans_map = pd.read_excel(Petr_Mapping_inputfile, sheet_name = \"GHGI Map - Trans\", usecols = \"A:B\", skiprows = 2, names = colnames)\n",
    "#drop rows with no data, remove the parentheses and \"\"\n",
    "ghgi_trans_map = ghgi_trans_map[ghgi_trans_map['GHGI_Emi_Group'] != 'na']\n",
    "ghgi_trans_map = ghgi_trans_map[ghgi_trans_map['GHGI_Emi_Group'].notna()]\n",
    "ghgi_trans_map['GHGI_Source']= ghgi_trans_map['GHGI_Source'].str.replace(r\"\\(\",\"- \")\n",
    "ghgi_trans_map['GHGI_Source']= ghgi_trans_map['GHGI_Source'].str.replace(r\"\\)\",\"\")\n",
    "ghgi_trans_map['GHGI_Source']= ghgi_trans_map['GHGI_Source'].str.replace(r'\"',\"\")\n",
    "ghgi_trans_map.reset_index(inplace=True, drop=True)\n",
    "display(ghgi_prod_map)\n",
    "\n",
    "#load emission group - proxy map\n",
    "names = pd.read_excel(Petr_Mapping_inputfile, sheet_name = \"Proxy Map - Trans\", usecols = \"A:C\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "proxy_trans_map = pd.read_excel(Petr_Mapping_inputfile, sheet_name = \"Proxy Map - Trans\", usecols = \"A:C\", skiprows = 1, names = colnames)\n",
    "display((proxy_prod_map))\n",
    "\n",
    "#create empty proxy and emission group arrays (add months for proxy variables that have monthly data)\n",
    "for igroup in np.arange(0,len(proxy_trans_map)):\n",
    "    if proxy_trans_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "        vars()[proxy_trans_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "        vars()[proxy_trans_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years,num_months])\n",
    "        vars()[ghgi_trans_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "    else:\n",
    "        vars()[proxy_trans_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        vars()[proxy_trans_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "        vars()[ghgi_trans_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        \n",
    "#Refining\n",
    "#load GHGI Mapping Groups\n",
    "names = pd.read_excel(Petr_Mapping_inputfile, sheet_name = \"GHGI Map - Ref\", usecols = \"A:B\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "ghgi_ref_map = pd.read_excel(Petr_Mapping_inputfile, sheet_name = \"GHGI Map - Ref\", usecols = \"A:B\", skiprows = 2, names = colnames)\n",
    "#drop rows with no data, remove the parentheses and \"\"\n",
    "ghgi_ref_map = ghgi_ref_map[ghgi_ref_map['GHGI_Emi_Group'] != 'na']\n",
    "ghgi_ref_map = ghgi_ref_map[ghgi_ref_map['GHGI_Emi_Group'].notna()]\n",
    "ghgi_ref_map['GHGI_Source']= ghgi_ref_map['GHGI_Source'].str.replace(r\"\\(\",\"- \")\n",
    "ghgi_ref_map['GHGI_Source']= ghgi_ref_map['GHGI_Source'].str.replace(r\"\\)\",\"\")\n",
    "ghgi_ref_map['GHGI_Source']= ghgi_ref_map['GHGI_Source'].str.replace(r'\"',\"\")\n",
    "ghgi_ref_map.reset_index(inplace=True, drop=True)\n",
    "display(ghgi_prod_map)\n",
    "\n",
    "#load emission group - proxy map\n",
    "names = pd.read_excel(Petr_Mapping_inputfile, sheet_name = \"Proxy Map - Ref\", usecols = \"A:C\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "proxy_ref_map = pd.read_excel(Petr_Mapping_inputfile, sheet_name = \"Proxy Map - Ref\", usecols = \"A:C\", skiprows = 1, names = colnames)\n",
    "display((proxy_prod_map))\n",
    "\n",
    "#create empty proxy and emission group arrays (add months for proxy variables that have monthly data)\n",
    "for igroup in np.arange(0,len(proxy_ref_map)):\n",
    "    if proxy_ref_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "        vars()[proxy_ref_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "        vars()[proxy_ref_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years,num_months])\n",
    "        vars()[ghgi_ref_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "    else:\n",
    "        vars()[proxy_ref_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        vars()[proxy_ref_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "        vars()[ghgi_ref_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        \n",
    "#create empty arrays that will be used as part of calculations, but will be renamed or combined before the final mapping\n",
    "#Enverus state pacific\n",
    "Map_StatePacOffshore =  np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2 Read In GHGRP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GHGRP Emissions from Refineries (subpart Y, units: metric tonnes, converted to kt)\n",
    "# Also read in GHGRP facility location information and match facilities in the emissions dataset\n",
    "# with facility location information in the facilities dataset, based on matching facility IDs.\n",
    "\n",
    "# Make Map_Refineries (will be assigned to proxy map variable later)\n",
    "Map_GHGRPRefineries = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Map_GHGRPRefineries_nongrid = np.zeros([num_years])\n",
    "\n",
    "facility_info = pd.read_csv(GHGRP_facility_inputfile)\n",
    "ghgrp_facility_emissions = pd.read_csv(ghgrp_refinery_inputfile)\n",
    "#filter for methane only\n",
    "ghgrp_facility_emissions = ghgrp_facility_emissions[ghgrp_facility_emissions['Y_SUBPART_LEVEL_INFORMATION.GHG_NAME']=='Methane']\n",
    "ghgrp_facility_emissions.reset_index(drop=True,inplace=True)\n",
    "\n",
    "ghgrp_facility_emissions['Lat'] = 0\n",
    "ghgrp_facility_emissions['Lon'] = 0\n",
    "\n",
    "#find facility lat and lon based on matching facility ID\n",
    "for ifacility in np.arange(0,len(ghgrp_facility_emissions)):\n",
    "    ilocation = np.where(facility_info['V_GHG_EMITTER_FACILITIES.FACILITY_ID'] == ghgrp_facility_emissions['Y_SUBPART_LEVEL_INFORMATION.FACILITY_ID'][ifacility])[0][0]\n",
    "    ghgrp_facility_emissions.loc[ifacility,'Lat'] = facility_info['V_GHG_EMITTER_FACILITIES.LATITUDE'][ilocation]\n",
    "    ghgrp_facility_emissions.loc[ifacility,'Lon'] = facility_info['V_GHG_EMITTER_FACILITIES.LONGITUDE'][ilocation]\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    temp_data = ghgrp_facility_emissions[ghgrp_facility_emissions['Y_SUBPART_LEVEL_INFORMATION.REPORTING_YEAR']==year_range[iyear]]\n",
    "    temp_data.reset_index(drop=True,inplace=True)\n",
    "    for ifacility in np.arange(0,len(temp_data)):\n",
    "        if temp_data['Lon'][ifacility] > Lon_left and temp_data['Lon'][ifacility] < Lon_right \\\n",
    "            and temp_data['Lat'][ifacility] > Lat_low and temp_data['Lat'][ifacility] < Lat_up:\n",
    "            ilat = int((temp_data['Lat'][ifacility] - Lat_low)/Res01)\n",
    "            ilon = int((temp_data['Lon'][ifacility] - Lon_left)/Res01)\n",
    "            Map_GHGRPRefineries[ilat,ilon,iyear] += temp_data['Y_SUBPART_LEVEL_INFORMATION.GHG_QUANTITY'][ifacility]/1000\n",
    "        else:\n",
    "            Map_GHGRPRefineries_nongrid[iyear] += temp_data['Y_SUBPART_LEVEL_INFORMATION.GHG_QUANTITY'][ifacility]/1000\n",
    "    print('Year: ',year_range[iyear])    \n",
    "    print('Total refinery emissions on grid (kt): ',np.sum(Map_GHGRPRefineries[:,:,iyear]))\n",
    "    print('Total refinery emissions offgrid (kt): ',np.sum(Map_GHGRPRefineries_nongrid[iyear]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3. Read In GOADS Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1.1 - Initialize arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize GOADS maps array (will be assigned to proxy map variable later)\n",
    "Map_GOADSmajor_emissions = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Map_GOADSminor_emissions = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2.2. 2011 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read In data for 2011 (use for 2012, 2014, and 2017). Interpolate between for missing years\n",
    "# goal: populating Map_FedGOM_Offshore to allocate federal offshore GOM emissions (state GOM allocated with Enverus)\n",
    "\n",
    "#Only run if need to save new file (takes a few hours to run)\n",
    "if ReCalc_GOADS ==1:\n",
    "    ## 2011\n",
    "    # Read In and Format 2011 BEOM Data\n",
    "    driver_str = r'Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ='+GOADS_11_inputfile+';'''\n",
    "    conn = pyodbc.connect(driver_str)\n",
    "    GOADS_locations = pd.read_sql(\"SELECT * FROM tblPointER\", conn)\n",
    "    GOADS_emissions = pd.read_sql(\"SELECT * FROM tblPointEM\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    # Format Location Data\n",
    "    GOADS_locations = GOADS_locations[[\"strStateFacilityIdentifier\",\"strEmissionReleasePointID\",\"dblXCoordinate\",\\\n",
    "                                   \"dblYCoordinate\"]]\n",
    "    #Create platform-by-platform file\n",
    "    GOADS_locations_Unique = pd.DataFrame({'strStateFacilityIdentifier':GOADS_locations['strStateFacilityIdentifier'].unique()})\n",
    "    GOADS_locations_Unique['lon'] = 0.0\n",
    "    GOADS_locations_Unique['lat'] = 0.0\n",
    "    GOADS_locations_Unique['strEmissionReleasePointID'] = ''\n",
    "\n",
    "    for iplatform in np.arange(len(GOADS_locations_Unique)):\n",
    "        match_platform = np.where(GOADS_locations['strStateFacilityIdentifier'] == GOADS_locations_Unique['strStateFacilityIdentifier'][iplatform])[0][0]\n",
    "        GOADS_locations_Unique.loc[iplatform,'lon',] = GOADS_locations['dblXCoordinate'][match_platform]\n",
    "        GOADS_locations_Unique.loc[iplatform,'lat',] = GOADS_locations['dblYCoordinate'][match_platform]\n",
    "        GOADS_locations_Unique.loc[iplatform,'strEmissionReleasePointID'] = GOADS_locations['strEmissionReleasePointID'][match_platform][:3]\n",
    "\n",
    "    GOADS_locations_Unique.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    #Format Emissions Data (clean lease data string)\n",
    "    GOADS_emissions = GOADS_emissions[[\"strStateFacilityIdentifier\",\"strPollutantCode\",\"dblEmissionNumericValue\",\"BOEM-MONTH\",\n",
    "                                  \"BOEM-LEASE_NUM\",\"BOEM-COMPLEX_ID\"]]\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('OCS','')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('-','')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace(' ','')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('G1477','G01477')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('G73','00073')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('G605','00605')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('G72','00072')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('G599','00599')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('G7155','G07155')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('G2357','G02357')\n",
    "    GOADS_emissions['BOEM-LEASE_NUM'] = GOADS_emissions['BOEM-LEASE_NUM'].str.replace('G4921','G04921')\n",
    "    GOADS_emissions['Emis_tg'] = 0.0\n",
    "    GOADS_emissions['Emis_tg'] = 9.0718474E-7 * GOADS_emissions['dblEmissionNumericValue'] #convert short tons to Tg\n",
    "    GOADS_emissions = GOADS_emissions[GOADS_emissions['strPollutantCode'] == 'CH4']\n",
    "    GOADS_emissions.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    #display(GOADS_emissions)\n",
    "\n",
    "    # Use ERG Preprocessed data to determine if major or minor and oil or gas\n",
    "    ERG_complex_crosswalk = pd.read_excel(ERG_GOADSEmissions_inputfile, sheet_name = \"Complex Emissions by Source\", usecols = \"AJ:AM\", nrows = 11143)\n",
    "    #display(ERG_complex_crosswalk)\n",
    "\n",
    "    # add data to map array, for the closest year to 2011\n",
    "    year_diff = [abs(x - 2011) for x in year_range]\n",
    "    iyear = year_diff.index(min(year_diff))\n",
    "\n",
    "    #assign oil vs gas by lease/complex ID\n",
    "    GOADS_emissions['LEASE_TYPE'] =''\n",
    "    GOADS_emissions['MAJOR_STRUC'] =''\n",
    "    for istruc in np.arange(0,len(GOADS_emissions)):\n",
    "        imatch = np.where(np.logical_and(ERG_complex_crosswalk['BOEM COMPLEX ID.2']==int(GOADS_emissions['BOEM-COMPLEX_ID'][istruc]),\\\n",
    "                            ERG_complex_crosswalk['Year.2'] == 2011))\n",
    "        if np.size(imatch) >0:\n",
    "            imatch = imatch[0][0]\n",
    "            GOADS_emissions.loc[istruc,'LEASE_TYPE'] = ERG_complex_crosswalk['Oil Gas Defn FINAL.1'][imatch]\n",
    "            GOADS_emissions.loc[istruc,'MAJOR_STRUC'] = ERG_complex_crosswalk['Major / Minor.1'][imatch]\n",
    "        else:\n",
    "            print(istruc, GOADS_emissions['BOEM-COMPLEX_ID'][istruc])\n",
    "\n",
    "        # for all oil platforms, match the platform to the emissions\n",
    "        if GOADS_emissions['LEASE_TYPE'][istruc] =='Oil':\n",
    "            match_platform = np.where(GOADS_locations_Unique.strStateFacilityIdentifier==GOADS_emissions['strStateFacilityIdentifier'][istruc])[0][0]\n",
    "            ilat = int((GOADS_locations_Unique['lat'][match_platform] - Lat_low)/Res01)\n",
    "            ilon = int((GOADS_locations_Unique['lon'][match_platform] - Lon_left)/Res01)\n",
    "            imonth = GOADS_emissions['BOEM-MONTH'][istruc]-1 #dict is 1-12, not 0-11\n",
    "            if GOADS_emissions['MAJOR_STRUC'][istruc] =='Major':\n",
    "                Map_GOADSmajor_emissions[ilat,ilon,iyear,imonth] += GOADS_emissions['Emis_tg'][istruc]\n",
    "            else:\n",
    "                Map_GOADSminor_emissions[ilat,ilon,iyear,imonth] += GOADS_emissions['Emis_tg'][istruc]\n",
    "            \n",
    "            \n",
    "    # sum complexes and emissions for diagnostic\n",
    "    majcplx = GOADS_emissions[(GOADS_emissions['MAJOR_STRUC']=='Major')]\n",
    "    majcplx = majcplx[majcplx['LEASE_TYPE'] =='Oil']\n",
    "    num_majcplx = majcplx['BOEM-COMPLEX_ID'].unique()\n",
    "    mincplx = GOADS_emissions[GOADS_emissions['MAJOR_STRUC']=='Minor']\n",
    "    mincplx = mincplx[mincplx['LEASE_TYPE'] =='Oil']\n",
    "    num_mincplx = mincplx['BOEM-COMPLEX_ID'].unique()           \n",
    "    del GOADS_emissions\n",
    "    print('Number of Major Oil Complexes: ',(np.size(num_majcplx)))\n",
    "    print('Emissions (Tg): ',np.sum(Map_GOADSmajor_emissions[:,:,iyear,:]))\n",
    "    print('Number of Minor Oil Complexes: ',(np.size(num_mincplx)))\n",
    "    print('Emissions (Tg): ',np.sum(Map_GOADSminor_emissions[:,:,iyear,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. 2014 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 2014\n",
    "\n",
    "#Only run if need to save new file (takes a few hours to run)\n",
    "if ReCalc_GOADS ==1:\n",
    "    #Read In and Format 2014 BEOM Data\n",
    "    driver_str = r'Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ='+GOADS_14_inputfile+';'''\n",
    "    conn = pyodbc.connect(driver_str)\n",
    "    GOADS_emissions = pd.read_sql(\"SELECT * FROM 2014_Gulfwide_Platform_20161102\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    GOADS_emissions = GOADS_emissions[[\"PLATFORM_ID\",\"X_COORDINATE\",\"Y_COORDINATE\",\"POLLUTANT_CODE\",\"EMISSIONS_VALUE\",\"MONTH\",\\\n",
    "                                  \"LEASE_NUMBER\",\"COMPLEX_ID\"]]\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('OCS','')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('-','')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace(' ','')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G1477','G01477')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G73','00073')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G605','00605')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G72','00072')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G599','00599')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G7155','G07155')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G2357','G02357')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G4921','G04921')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('GO2839','G02839')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('GO5761','G05761')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('GO0026','00026')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('GO3194','G03194')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G1034','G01034')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G0456','G00456')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G0060','G00060')\n",
    "    GOADS_emissions['Emis_tg'] = 0.0\n",
    "    GOADS_emissions['Emis_tg'] = 9.0718474E-7 * GOADS_emissions['EMISSIONS_VALUE'] #convert short tons to Tg\n",
    "    GOADS_emissions = GOADS_emissions[GOADS_emissions['POLLUTANT_CODE'] == 'CH4']\n",
    "    GOADS_emissions.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    #assign oil vs gas by lease/complex ID\n",
    "    # add data to map array, for the closest year to 2014\n",
    "    year_diff = [abs(x - 2014) for x in year_range]\n",
    "    iyear = year_diff.index(min(year_diff))\n",
    "    GOADS_emissions['LEASE_TYPE'] =''\n",
    "    GOADS_emissions['MAJOR_STRUC'] =''\n",
    "    for istruc in np.arange(0,len(GOADS_emissions)):\n",
    "        imatch = np.where(np.logical_and(ERG_complex_crosswalk['BOEM COMPLEX ID.2']==int(GOADS_emissions['COMPLEX_ID'][istruc]),\\\n",
    "                            ERG_complex_crosswalk['Year.2'] == 2014))\n",
    "        if np.size(imatch) >0:\n",
    "            imatch = imatch[0][0]\n",
    "            GOADS_emissions.loc[istruc,'LEASE_TYPE'] = ERG_complex_crosswalk['Oil Gas Defn FINAL.1'][imatch]\n",
    "            GOADS_emissions.loc[istruc,'MAJOR_STRUC'] = ERG_complex_crosswalk['Major / Minor.1'][imatch]\n",
    "        else:\n",
    "            print(istruc, GOADS_emissions['COMPLEX_ID'][istruc])\n",
    "\n",
    "        if GOADS_emissions['LEASE_TYPE'][istruc] =='Oil':\n",
    "            #then for all oil platforms, match the platform to the emissions\n",
    "            ilat = int((GOADS_emissions['Y_COORDINATE'][istruc] - Lat_low)/Res01)\n",
    "            ilon = int((GOADS_emissions['X_COORDINATE'][istruc] - Lon_left)/Res01)\n",
    "            month_str = GOADS_emissions['MONTH'][istruc]             \n",
    "            imonth = month_dict[GOADS_emissions['MONTH'][istruc]]-1 #dict is 1-12, not 0-11\n",
    "            if GOADS_emissions['MAJOR_STRUC'][istruc] =='Major':\n",
    "                Map_GOADSmajor_emissions[ilat,ilon,iyear,imonth] += GOADS_emissions['Emis_tg'][istruc]\n",
    "            else:\n",
    "                Map_GOADSminor_emissions[ilat,ilon,iyear,imonth] += GOADS_emissions['Emis_tg'][istruc]\n",
    "\n",
    "    # sum complexes and emissions for diagnostic\n",
    "    majcplx = GOADS_emissions[(GOADS_emissions['MAJOR_STRUC']=='Major')]\n",
    "    majcplx = majcplx[majcplx['LEASE_TYPE'] =='Oil']\n",
    "    num_majcplx = majcplx['COMPLEX_ID'].unique()\n",
    "    mincplx = GOADS_emissions[GOADS_emissions['MAJOR_STRUC']=='Minor']\n",
    "    mincplx = mincplx[mincplx['LEASE_TYPE'] =='Oil']\n",
    "    num_mincplx = mincplx['COMPLEX_ID'].unique()         \n",
    "    del GOADS_emissions\n",
    "    print('Number of Major Oil Complexes: ',(np.size(num_majcplx)))\n",
    "    print('Emissions (Tg): ',np.sum(Map_GOADSmajor_emissions[:,:,iyear,:]))\n",
    "    print('Number of Minor Oil Complexes: ',(np.size(num_mincplx)))\n",
    "    print('Emissions (Tg): ',np.sum(Map_GOADSminor_emissions[:,:,iyear,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. 2017 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2017\n",
    "#Only run if need to save new file (takes a few hours to run)\n",
    "if ReCalc_GOADS ==1:\n",
    "#Read In and Format 2017 BEOM Data\n",
    "    driver_str = r'Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ='+GOADS_17_inputfile+';'''\n",
    "    conn = pyodbc.connect(driver_str)\n",
    "    GOADS_emissions = pd.read_sql(\"SELECT * FROM 2017_Gulfwide_Platform_20190705_CAP_GHG\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    GOADS_emissions = GOADS_emissions[[\"PLATFORM_ID\",\"X_COORDINATE\",\"Y_COORDINATE\",\"POLLUTANT_CODE\",\"EMISSIONS_VALUE\",\"Month\",\\\n",
    "                                   \"LEASE_NUMBER\",\"COMPLEX_ID\"]]\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('OCS','')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('-','')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace(' ','')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G1477','G01477')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G73','00073')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G605','00605')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G72','00072')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G599','00599')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G7155','G07155')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G2357','G02357')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G4921','G04921')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('GO2839','G02839')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('GO2893','G02893')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('GO5761','G05761')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('GO0026','00026')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('GO3194','G03194')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G1034','G01034')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G0456','G00456')\n",
    "    GOADS_emissions['LEASE_NUMBER'] = GOADS_emissions['LEASE_NUMBER'].str.replace('G0060','G00060')\n",
    "    GOADS_emissions['Emis_tg'] = 0.0\n",
    "    GOADS_emissions['Emis_tg'] = 9.0718474E-7 * GOADS_emissions['EMISSIONS_VALUE'] #convert short tons to Tg\n",
    "    GOADS_emissions = GOADS_emissions[GOADS_emissions['POLLUTANT_CODE'] == 'CH4']\n",
    "    GOADS_emissions.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    #assign oil vs gas by lease/complex ID\n",
    "    # add data to map array, for the closest year to 2014\n",
    "    year_diff = [abs(x - 2017) for x in year_range]\n",
    "    iyear = year_diff.index(min(year_diff))\n",
    "    GOADS_emissions['LEASE_TYPE'] =''\n",
    "    GOADS_emissions['MAJOR_STRUC'] =''\n",
    "    for istruc in np.arange(0,len(GOADS_emissions)):\n",
    "        imatch = np.where(np.logical_and(ERG_complex_crosswalk['BOEM COMPLEX ID.2']==int(GOADS_emissions['COMPLEX_ID'][istruc]),\\\n",
    "                            ERG_complex_crosswalk['Year.2'] == 2017))\n",
    "        if np.size(imatch) >0:\n",
    "            imatch = imatch[0][0]\n",
    "            GOADS_emissions.loc[istruc,'LEASE_TYPE'] = ERG_complex_crosswalk['Oil Gas Defn FINAL.1'][imatch]\n",
    "            GOADS_emissions.loc[istruc,'MAJOR_STRUC'] = ERG_complex_crosswalk['Major / Minor.1'][imatch]\n",
    "        else:\n",
    "            print(istruc, GOADS_emissions[\"COMPLEX_ID\"][istruc])\n",
    "\n",
    "        if GOADS_emissions['LEASE_TYPE'][istruc] =='Oil':\n",
    "            #then for all oil platforms, match the platform to the emissions\n",
    "            ilat = int((GOADS_emissions['Y_COORDINATE'][istruc] - Lat_low)/Res01)\n",
    "            ilon = int((GOADS_emissions['X_COORDINATE'][istruc] - Lon_left)/Res01)\n",
    "            imonth = month_dict[GOADS_emissions['Month'][istruc]]-1 #dict is 1-12, not 0-11\n",
    "            if GOADS_emissions['MAJOR_STRUC'][istruc] =='Major':\n",
    "                Map_GOADSmajor_emissions[ilat,ilon,iyear,imonth] += GOADS_emissions['Emis_tg'][istruc]\n",
    "            else:\n",
    "                Map_GOADSminor_emissions[ilat,ilon,iyear,imonth] += GOADS_emissions['Emis_tg'][istruc]\n",
    "\n",
    "    # sum complexes and emissions for diagnostic\n",
    "    majcplx = GOADS_emissions[(GOADS_emissions['MAJOR_STRUC']=='Major')]\n",
    "    majcplx = majcplx[majcplx['LEASE_TYPE'] =='Oil']\n",
    "    num_majcplx = majcplx[\"COMPLEX_ID\"].unique()\n",
    "    mincplx = GOADS_emissions[GOADS_emissions['MAJOR_STRUC']=='Minor']\n",
    "    mincplx = mincplx[mincplx['LEASE_TYPE'] =='Oil']\n",
    "    num_mincplx = mincplx[\"COMPLEX_ID\"].unique()          \n",
    "    del GOADS_emissions\n",
    "    print('Number of Major Oil Complexes: ',(np.size(num_majcplx)))\n",
    "    print('Emissions: ',np.sum(Map_GOADSmajor_emissions[:,:,iyear,:]))\n",
    "    print('Number of Minor Oil Complexes: ',(np.size(num_mincplx)))\n",
    "    print('Emissions: ',np.sum(Map_GOADSminor_emissions[:,:,iyear,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5. Interpolate GOADS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2011 data applied to 2012\n",
    "# 2014 data applied to 2013-2015\n",
    "# 2017 data applied 2016 forward\n",
    "\n",
    "if ReCalc_GOADS ==1:\n",
    "    Map_GOADSmajor_emissions[:,:,1,:] = Map_GOADSmajor_emissions[:,:,2,:]\n",
    "    Map_GOADSmajor_emissions[:,:,2,:] = Map_GOADSmajor_emissions[:,:,2,:]\n",
    "    Map_GOADSmajor_emissions[:,:,3,:] = Map_GOADSmajor_emissions[:,:,2,:]\n",
    "    Map_GOADSmajor_emissions[:,:,4,:] = Map_GOADSmajor_emissions[:,:,5,:]\n",
    "    Map_GOADSmajor_emissions[:,:,6,:] = Map_GOADSmajor_emissions[:,:,5,:]\n",
    "    \n",
    "    Map_GOADSminor_emissions[:,:,1,:] = Map_GOADSminor_emissions[:,:,2,:]\n",
    "    Map_GOADSminor_emissions[:,:,2,:] = Map_GOADSminor_emissions[:,:,2,:]\n",
    "    Map_GOADSminor_emissions[:,:,3,:] = Map_GOADSminor_emissions[:,:,2,:]\n",
    "    Map_GOADSminor_emissions[:,:,4,:] = Map_GOADSminor_emissions[:,:,5,:]\n",
    "    Map_GOADSminor_emissions[:,:,6,:] = Map_GOADSminor_emissions[:,:,5,:]\n",
    "    \n",
    "    np.save('./IntermediateOutputs/GOADSmajor_oil_tempoutput', Map_GOADSmajor_emissions)\n",
    "    np.save('./IntermediateOutputs/GOADSminor_oil_tempoutput', Map_GOADSminor_emissions)\n",
    "else:\n",
    "    Map_GOADSmajor_emissions = np.load('./IntermediateOutputs/GOADSmajor_oil_tempoutput.npy')\n",
    "    Map_GOADSminor_emissions = np.load('./IntermediateOutputs/GOADSminor_oil_tempoutput.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3 Read In BSEE Data (for Federal Offshore Pacfic platforms)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Bureau of Saftey and Environmental Enforcement has Pacific Platform Data\n",
    "#https://www.data.bsee.gov/Main/PacificPlatform.aspx\n",
    "#https://www.data.bsee.gov/Main/PacificProduction.aspx\n",
    "\n",
    "Use Pacific Platform Information --> Platform Locations to get lat, lon, and complex ID\n",
    "Use Pacific Platform Information --> Platform Masters to get complex ID and lease ID\n",
    "Use Pacific Production (by year) to get lease ID and monthly oil and gas production volumes\n",
    "\n",
    "1. Map/Format to get the monthly oil and gas production for each platform and their locations\n",
    "2. Sum monthly production for each platform to get annual oil and gas production and calculate GOR for each\n",
    "3. Use GOR to determine oil vs gas platform\n",
    "4. Then map relevant EPA Federal offshore emissions to the monthly oil or gas production volumes in each lat/lon cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#0) Intialize empty array\n",
    "Map_BSEEOffshore = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "\n",
    "\n",
    "#1) Read in platform location information\n",
    "#header information from: https://www.data.bsee.gov/Main/HtmlPage.aspx?page=pacplatformLocations\n",
    "plat_loc = pd.read_csv(BSEE_platformloc_inputdata, sep=\",\", header=None)\n",
    "plat_loc.columns = [\"DISTRICT\", \"COMPLEX_ID\", \"STRUCT_NUMBER\", \"AREA_CODE\",\"BLOCK\",\"STRUCT_NAME\",\"NS_DIST\",\"NS_CODE\",\\\n",
    "\"EW_DIST\",\"EW_CODE\",\"X\",\"Y\",\"LON\",\"LAT\"]\n",
    "#display(plat_loc)\n",
    "\n",
    "#2) Read in masterplatform information\n",
    "#header information from: https://www.data.bsee.gov/Main/HtmlPage.aspx?page=pacplatformMasters\n",
    "plat_master = pd.read_csv(BSEE_platformmaster_inputdata, sep=\",\", header=None)\n",
    "plat_master.columns = [\"COMPLEX_ID\", \"ABANDON_FLAG\", \"ALOC_FLAG\", \"ATTEND_FLAG\",\"COND_PROD_FLAG\",\"SHORE_DIST\",\\\n",
    "                       \"DRILL_FLAG\",\"FIRED_VESSEL_FLAG\",\"GAS_PROD_FLAG\",\"GAS_FLARE_FLAG\",\"MMS_NUM\",\"MANNED_FLAG\",\\\n",
    "                       \"MAJOR_COMPLEX_FLAG\",\"LEASE_NUMBER\",\"LAST_REV_DATE\",\"LAST_MTR_FLAG\",\"INJEC_CODE\",\"HELIPORT_FLAG\",\\\n",
    "                      \"WORKOVER_FLAG\",\"WATER_PROD_FLAG\",\"DEPTH\",\"TANK_GUAGE_FLAG\",\"SUL_PROD_FLAG\",\"SUBDIST_CODE\",\"STORE_TANK_FLAG\",\\\n",
    "                      \"RIG_COUNT\",\"QTR_CODE\",\"PROD_EQMT_FLAG\",\"PROD_FLAG\",\"POWER_SOURCE\",\"POWER_GEN_FLAG\",\"OIL_PROD_FLAG\",\"GAS_SALE\",\\\n",
    "                      \"FIELD_NAME\",\"DISTRICT\",\"CRANE_CODE\",\"COMP_FLAG\",\"COMGL_PROD_FLAG\",\"BED_COUNT\",\"AREA_CODE\",\"BLOCK\",\"METER_PROVER\"]\n",
    "\n",
    "#3) Add lease number to location array\n",
    "plat_loc['LEASE'] = ''\n",
    "for iplatform in np.arange(0,len(plat_loc)):\n",
    "    imatch = np.where(plat_master['COMPLEX_ID'] == plat_loc['COMPLEX_ID'][iplatform])[0][0]\n",
    "    plat_loc.loc[iplatform,'LEASE'] = plat_master['LEASE_NUMBER'][imatch]\n",
    "\n",
    "#4) Find the production data for each platform\n",
    "#header information from: \n",
    "for iyear in np.arange(0,num_years):\n",
    "    bsee_temp = plat_loc.copy()\n",
    "    for imonth in np.arange(0,num_months):\n",
    "        bsee_temp['OIL'+month_tag[imonth]] = 0\n",
    "        bsee_temp['GAS'+month_tag[imonth]] = 0\n",
    "    \n",
    "    OGOR_PAC = pd.read_csv(vars()['BSEE_prod_'+year_range_str[iyear]+'_inputdata'], sep=\",\", header=None)\n",
    "    OGOR_PAC.columns = [\"LEASE\", \"COMP_NAME\", \"PROD_DATE\", \"PROD_DAYS\",\"PROD_CODE\",\"MONTH_OILVOL\",\\\n",
    "                       \"MONTH_GASVOL\",\"MONTH_WATERVOL\",\"WELL_API\",\"WELL_STATUS\",\"AREA_BLOCK\",\"OPER_NUM\",\\\n",
    "                       \"OPER_NAME\",\"FIELD\",\"INJ_VOL\",\"PROD_INTV\",\"FIRST_PROD_DATE\",\"UNIT_NUM\"]\n",
    "    for iplatform in np.arange(0,len(plat_loc)):\n",
    "        imatch = np.where(OGOR_PAC['LEASE'] == plat_loc['LEASE'][iplatform])[0]\n",
    "        temp = OGOR_PAC.iloc[imatch,:]\n",
    "        temp.reset_index(drop=True,inplace=True)\n",
    "        for idx in np.arange(0,len(temp)):\n",
    "            if temp.loc[idx,'PROD_DAYS'] >0 :\n",
    "                month_str = str(temp.loc[idx,'PROD_DATE'])[4:6]\n",
    "                month_loc = 'OIL'+month_str\n",
    "                bsee_temp.loc[iplatform,month_loc] += temp.loc[idx,'MONTH_OILVOL']\n",
    "                month_loc = 'GAS'+month_str\n",
    "                bsee_temp.loc[iplatform,month_loc] += temp.loc[idx,'MONTH_GASVOL']\n",
    "    bsee_temp['CUM_GAS'] = bsee_temp.loc[:,bsee_temp.columns.str.contains('GAS')].sum(1)\n",
    "    bsee_temp['CUM_OIL'] = bsee_temp.loc[:,bsee_temp.columns.str.contains('OIL')].sum(1)     \n",
    "    vars()['BSEE_platform_prod_'+year_range_str[iyear]] = bsee_temp.copy()\n",
    "\n",
    "\n",
    "#4) Correct the production data for platforms with the same lease number \n",
    "# (evenly divide production between all platforms on a given lease)\n",
    "for iyear in np.arange(0,num_years):\n",
    "    bsee_temp = vars()['BSEE_platform_prod_'+year_range_str[iyear]].copy()\n",
    "    unique_lease = np.unique(bsee_temp['LEASE'])\n",
    "    for ilease in np.arange(0,len(unique_lease)):\n",
    "        imatch = np.where(bsee_temp['LEASE']== unique_lease[ilease])[0]\n",
    "        temp = bsee_temp[bsee_temp['LEASE']== unique_lease[ilease]]\n",
    "        num_lease = np.shape(temp)[0]\n",
    "        bsee_temp.loc[imatch,bsee_temp.columns.str.contains('OIL')] /= num_lease\n",
    "        bsee_temp.loc[imatch,bsee_temp.columns.str.contains('GAS')] /= num_lease\n",
    "    vars()['BSEE_platform_prod_'+year_range_str[iyear]] = bsee_temp.copy()    \n",
    "\n",
    "print('Annual Total Pacific Offshore Oil Prod:')\n",
    "#5) Make Map of Offshore Pacific Oil production for oil platforms (Annual GOR < 100)\n",
    "for iyear in np.arange(0,num_years):\n",
    "    bsee_temp = vars()['BSEE_platform_prod_'+year_range_str[iyear]].copy()\n",
    "    for iplatform in np.arange(0,len(bsee_temp)):\n",
    "        if bsee_temp['LON'][iplatform] > Lon_left and bsee_temp['LON'][iplatform] < Lon_right \\\n",
    "            and bsee_temp['LAT'][iplatform] > Lat_low and bsee_temp['LAT'][iplatform] < Lat_up:\n",
    "            #find index of lon and lat\n",
    "            ilat = int((bsee_temp['LAT'][iplatform] - Lat_low)/Res01)\n",
    "            ilon = int((bsee_temp['LON'][iplatform] - Lon_left)/Res01)\n",
    "            if ((data_fn.safe_div(bsee_temp['CUM_GAS'][iplatform],float(bsee_temp['CUM_OIL'][iplatform]))) <= 100):\n",
    "                # if oil well, \n",
    "                for imonth in np.arange(0,num_months):\n",
    "                #count production in map only for months where there is oil production (emissions ~ when production is occuring)\n",
    "                    prod_str = 'OIL'+month_tag[imonth]  \n",
    "                    Map_BSEEOffshore[ilat,ilon,iyear,imonth] += bsee_temp[prod_str][iplatform] # production from oil complexes only\n",
    "\n",
    "    print('Year ' +year_range_str[iyear]+': ', np.sum(Map_BSEEOffshore[:,:,iyear,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.4 Well and Production Data (from Enverus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.4.1 Read In Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read In and Format the Prism and DI data \n",
    "# 1. Read Data\n",
    "# 2. Drop unsed columns, rename columns to match between DI and Prism\n",
    "# 3. Combine DI and Prism into one data array\n",
    "# 4. Calculate annual cummulate production totals\n",
    "# 5. Save the data as a year-specific variable\n",
    "\n",
    "#Based on ERGs logic, active wells are determined based on their production levels and not producing status\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    \n",
    "    #DI data\n",
    "    DI_data = pd.read_csv(vars()['Enverus_DI_inputdata_' +year_range_str[iyear]])\n",
    "    DI_data = DI_data.drop(columns=['ENTITY_ID','API_UWI','OPERATOR_COMPANY_NAME','AAPG_FULL_ERG',\\\n",
    "                           'FIELD','RESERVOIR','LAST_PROD_DATE','DRILL_TYPE','CUM_GAS','CUM_OIL','CUM_WATER'])\n",
    "    DI_data.rename({'WELL_COUNT_ID':'WELL_COUNT','DI_BASIN':'BASIN','NEMS_REGION_ERG':'NEMS_REGION',\\\n",
    "                    'SURFACE_LATITUDE_WGS84':'LATITUDE','SURFACE_LONGITUDE_WGS84':'LONGITUDE','MONTHLY_WATER_01':'WATERPROD_01',\\\n",
    "                   'MONTHLY_WATER_02':'WATERPROD_02','MONTHLY_WATER_03':'WATERPROD_03','MONTHLY_WATER_04':'WATERPROD_04',\\\n",
    "                   'MONTHLY_WATER_05':'WATERPROD_05','MONTHLY_WATER_06':'WATERPROD_06','MONTHLY_WATER_07':'WATERPROD_07',\\\n",
    "                   'MONTHLY_WATER_08':'WATERPROD_08','MONTHLY_WATER_09':'WATERPROD_09','MONTHLY_WATER_10':'WATERPROD_10',\\\n",
    "                   'MONTHLY_WATER_11':'WATERPROD_11','MONTHLY_WATER_12':'WATERPROD_12','MONTHLY_OIL_01':'OILPROD_01',\\\n",
    "                   'MONTHLY_OIL_02':'OILPROD_02','MONTHLY_OIL_03':'OILPROD_03','MONTHLY_OIL_04':'OILPROD_04',\\\n",
    "                   'MONTHLY_OIL_05':'OILPROD_05','MONTHLY_OIL_06':'OILPROD_06','MONTHLY_OIL_07':'OILPROD_07',\\\n",
    "                   'MONTHLY_OIL_08':'OILPROD_08','MONTHLY_OIL_09':'OILPROD_09','MONTHLY_OIL_10':'OILPROD_10',\\\n",
    "                   'MONTHLY_OIL_11':'OILPROD_11','MONTHLY_OIL_12':'OILPROD_12','MONTHLY_GAS_01':'GASPROD_01',\\\n",
    "                   'MONTHLY_GAS_02':'GASPROD_02','MONTHLY_GAS_03':'GASPROD_03','MONTHLY_GAS_04':'GASPROD_04',\\\n",
    "                   'MONTHLY_GAS_05':'GASPROD_05','MONTHLY_GAS_06':'GASPROD_06','MONTHLY_GAS_07':'GASPROD_07',\\\n",
    "                   'MONTHLY_GAS_08':'GASPROD_08','MONTHLY_GAS_09':'GASPROD_09','MONTHLY_GAS_10':'GASPROD_10',\\\n",
    "                   'MONTHLY_GAS_11':'GASPROD_11','MONTHLY_GAS_12':'GASPROD_12'},axis=1, inplace=True)\n",
    "    \n",
    "    DI_data['WELL_COUNT'] = 1\n",
    "\n",
    "    #Prism Data\n",
    "    Prism_data = pd.read_csv(vars()['Enverus_Prism_inputdata_'+year_range_str[iyear]])\n",
    "    Prism_data = Prism_data.drop(columns=['WELLID','API_UWI','RSOPERATOR','TRAJECTORY','FIELD','RSREGION','FORMATION',\\\n",
    "                                         'TOTALFLUIDPUMPED_BBL','SPUDDATE'])\n",
    "    Prism_data.rename({'RSBASIN':'BASIN','COMPLETIONDATE':'COMPLETION_DATE','SPUDDATE':'SPUD_DATE','FIRSTPRODDATE':'FIRST_PROD_DATE',\\\n",
    "                     'OILGRAVITY_API':'OIL_GRAVITY','WATERPROD_BBL_01':'WATERPROD_01',\\\n",
    "                    'WATERPROD_BBL_02':'WATERPROD_02','WATERPROD_BBL_03':'WATERPROD_03','WATERPROD_BBL_04':'WATERPROD_04',\\\n",
    "                   'WATERPROD_BBL_05':'WATERPROD_05','WATERPROD_BBL_06':'WATERPROD_06','WATERPROD_BBL_07':'WATERPROD_07',\\\n",
    "                   'WATERPROD_BBL_08':'WATERPROD_08','WATERPROD_BBL_09':'WATERPROD_09','WATERPROD_BBL_10':'WATERPROD_10',\\\n",
    "                   'WATERPROD_BBL_11':'WATERPROD_11','WATERPROD_BBL_12':'WATERPROD_12','LIQUIDSPROD_BBL_01':'OILPROD_01',\\\n",
    "                   'LIQUIDSPROD_BBL_02':'OILPROD_02','LIQUIDSPROD_BBL_03':'OILPROD_03','LIQUIDSPROD_BBL_04':'OILPROD_04',\\\n",
    "                   'LIQUIDSPROD_BBL_05':'OILPROD_05','LIQUIDSPROD_BBL_06':'OILPROD_06','LIQUIDSPROD_BBL_07':'OILPROD_07',\\\n",
    "                   'LIQUIDSPROD_BBL_08':'OILPROD_08','LIQUIDSPROD_BBL_09':'OILPROD_09','LIQUIDSPROD_BBL_10':'OILPROD_10',\\\n",
    "                   'LIQUIDSPROD_BBL_11':'OILPROD_11','LIQUIDSPROD_BBL_12':'OILPROD_12','GASPROD_MCF_01':'GASPROD_01',\\\n",
    "                   'GASPROD_MCF_02':'GASPROD_02','GASPROD_MCF_03':'GASPROD_03','GASPROD_MCF_04':'GASPROD_04',\\\n",
    "                   'GASPROD_MCF_05':'GASPROD_05','GASPROD_MCF_06':'GASPROD_06','GASPROD_MCF_07':'GASPROD_07',\\\n",
    "                   'GASPROD_MCF_08':'GASPROD_08','GASPROD_MCF_09':'GASPROD_09','GASPROD_MCF_10':'GASPROD_10',\\\n",
    "                   'GASPROD_MCF_11':'GASPROD_11','GASPROD_MCF_12':'GASPROD_12','RSWELLSTATUS':'PRODUCING_STATUS'},axis=1,inplace=True)\n",
    "    #\n",
    "    Prism_data['WELL_COUNT'] = 1\n",
    "    \n",
    "    #combine into one array with common column names, replace nans with zeros, and sum annual production\n",
    "    Enverus_data = pd.concat([DI_data,Prism_data], ignore_index=True)\n",
    "    Enverus_data.loc[:,Enverus_data.columns.str.contains('GASPROD_')] = Enverus_data.loc[:,Enverus_data.columns.str.contains('GASPROD_')].fillna(0)\n",
    "    Enverus_data.loc[:,Enverus_data.columns.str.contains('OILPROD_')] = Enverus_data.loc[:,Enverus_data.columns.str.contains('OILPROD_')].fillna(0)\n",
    "    Enverus_data.loc[:,Enverus_data.columns.str.contains('WATERPROD_')] = Enverus_data.loc[:,Enverus_data.columns.str.contains('WATERPROD_')].fillna(0)\n",
    "\n",
    "    #Calculate cummulative annual production totals for Gas, Oil, Water\n",
    "    Enverus_data['CUM_GAS'] = Enverus_data.loc[:,Enverus_data.columns.str.contains('GASPROD_')].sum(1)\n",
    "    Enverus_data['CUM_OIL'] = Enverus_data.loc[:,Enverus_data.columns.str.contains('OILPROD_')].sum(1)\n",
    "    Enverus_data['CUM_WATER'] = Enverus_data.loc[:,Enverus_data.columns.str.contains('WATERPROD_')].sum(1)\n",
    "    \n",
    "    Enverus_data['NEMS_CODE'] = 0;#Enverus_data['NEMS_REGION'].map(NEMS_dict) Don't need to correct, not used in Petr analysis\n",
    "    \n",
    "    #save out the data for that year\n",
    "    vars()['Enverus_data_'+year_range_str[iyear]] = Enverus_data.copy()\n",
    "    print('Load Complete: Year '+year_range_str[iyear])\n",
    "    \n",
    "    del DI_data #save memory space \n",
    "    \n",
    "    #define default values for a new row in this table (to be used later during data corrections)\n",
    "    default = {'WELL_COUNT': 0, 'STATE':'','COUNTY':'','BASIN':'','AAPG_CODE_ERG':'UNK','NEMS_REGION':'UNK','NEMS_CODE':99,\\\n",
    "               'LATITUDE':0,'LONGITUDE':0,'PRODUCING_STATUS':'','RESERVOIR_TYPE':'','COMPLETION_DATE':'','SPUD_DATE':'',\\\n",
    "               'FIRST_PROD_DATE':'','HF':'', 'OFFSHORE':'','OIL_GRAVITY':'','GOR':-99,'GOR_QUAL':'','PROD_FLAG':'',\\\n",
    "               'OILPROD_01':0, 'GASPROD_01':0, 'WATERPROD_01':0,'OILPROD_02':0, 'GASPROD_02':0, 'WATERPROD_02':0,\\\n",
    "          'OILPROD_03':0, 'GASPROD_03':0, 'WATERPROD_03':0,'OILPROD_04':0, 'GASPROD_04':0, 'WATERPROD_04':0,\\\n",
    "          'OILPROD_05':0, 'GASPROD_05':0, 'WATERPROD_05':0,'OILPROD_06':0, 'GASPROD_06':0, 'WATERPROD_06':0,\\\n",
    "          'OILPROD_07':0, 'GASPROD_07':0, 'WATERPROD_07':0,'OILPROD_08':0, 'GASPROD_08':0, 'WATERPROD_08':0,\\\n",
    "          'OILPROD_09':0, 'GASPROD_09':0, 'WATERPROD_09':0,'OILPROD_10':0, 'GASPROD_10':0, 'WATERPROD_10':0,\\\n",
    "          'OILPROD_11':0, 'GASPROD_11':0, 'WATERPROD_11':0,'OILPROD_12':0, 'GASPROD_12':0, 'WATERPROD_12':0}\n",
    "    \n",
    "display(Enverus_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.4.2 Correct Enverus Data for Select States (following ERG procedure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1) Read In Coverage Table from State Well Counts File from ERG\n",
    "# (specifies the first year with bad data and which years need to be corrected; \n",
    "# all years including and after the first bad year of data need to be corrected)\n",
    "\n",
    "ERG_StateWellCounts_FirstBadDataYear = pd.read_excel(Enverus_WellCounts_inputfile, sheet_name = \"2021 - Coverage\", usecols = \"A:B\", skiprows = 2, nrows = 40)\n",
    "ERG_StateWellCounts_FirstBadDataYear['date'] = pd.to_datetime(ERG_StateWellCounts_FirstBadDataYear['Date to USE'], errors = 'coerce')\n",
    "ERG_StateWellCounts_FirstBadDataYear['year'] = pd.DatetimeIndex(ERG_StateWellCounts_FirstBadDataYear['date']).year.fillna(end_year+100).astype(int)\n",
    "\n",
    "# 2) Loops through the each state and year in Enverus to determine if the data for that particualar year needs to \n",
    "# be corrected. At the moment, the only corrections ERG makes to the data is to use the prior year of data if there\n",
    "# is no new Enverus data reportd for that state. If a particular state is not included for any years in the Enverus\n",
    "# dataset, then a row of zeros is added to the Enverus table for that year. \n",
    "\n",
    "for istate in np.arange(0,len(State_ANSI)):\n",
    "    correctdata =0\n",
    "    state_str = State_ANSI['abbr'][istate]\n",
    "    firstbadyear = ERG_StateWellCounts_FirstBadDataYear['year'][ERG_StateWellCounts_FirstBadDataYear['State'] == state_str].values\n",
    "    if firstbadyear.size  == 0:\n",
    "        firstbadyear = end_year+5 #if state isn't included in correction list, don't correct any data\n",
    "    \n",
    "    for iyear in np.arange(0,num_years):\n",
    "        enverus_data_temp= vars()['Enverus_data_'+year_range_str[iyear]].copy()\n",
    "        state_list = np.unique(enverus_data_temp['STATE'])\n",
    "        if state_str in state_list:\n",
    "            inlist =1\n",
    "        else:\n",
    "            inlist = 0\n",
    "        if inlist ==1 or correctdata==1: #if the state is included in Enverus data, or had data for at least one good year\n",
    "            #if first year, correctdata will be zero, but inlist will also be zero if no Enverus data\n",
    "            #check to see whether corrections are necessary for the given year/state\n",
    "            if year_range[iyear] == (firstbadyear-1):\n",
    "                print(state_str,year_range[iyear],'last good year')\n",
    "                # This is the last year of good data. Do not correct the data but save\n",
    "                # but so that this data can be used for all following years for that state\n",
    "                temp_data = enverus_data_temp[enverus_data_temp['STATE'] == state_str]\n",
    "                lastgoodyear = year_range_str[iyear]\n",
    "                correctdata=1\n",
    "            elif year_range[iyear] >= firstbadyear: \n",
    "                #correct data for all years equal to and after the first bad year (remove old data first if necessary)\n",
    "                if inlist == 1:\n",
    "                    enverus_data_temp = enverus_data_temp[enverus_data_temp['STATE'] != state_str]\n",
    "                enverus_data_temp = pd.concat([enverus_data_temp,temp_data],ignore_index=True)\n",
    "                print(state_str +' data for ' +year_range_str[iyear] +' were corrected with '+lastgoodyear+' data')\n",
    "            else:\n",
    "                no_corrections =1\n",
    "                \n",
    "        if inlist==0 and correctdata==0:\n",
    "        #if there is no Enverus data for a given state, and there was no good data, add a row with default values\n",
    "            temp_row = {'STATE':state_str}\n",
    "            enverus_data_temp = enverus_data_temp.append({**default,**temp_row}, ignore_index=True)\n",
    "            print(state_str +' has no Enverus data in the year ' +year_range_str[iyear]+', default values set')\n",
    "            \n",
    "        #resave that year of Enverus data\n",
    "        enverus_data_temp.reset_index(drop=True,inplace=True)\n",
    "        vars()['Enverus_data_'+year_range_str[iyear]] = enverus_data_temp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.5 Convert Enverus Well and Production Arrays into Gridded Location Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear variables\n",
    "del ERG_StateWellCounts_FirstBadDataYear\n",
    "del Prism_data\n",
    "del colnames\n",
    "del names\n",
    "del temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Annual gridded arrays (maps) of well data (a well will be counted every month if there is any production that year)\n",
    "# Includes Oil Wells and Production onshore in the CONUS region\n",
    "# source emissions are related to the presence of a well and its production status (no emission if no production)\n",
    "\n",
    "#Define well location/production arrays \n",
    "Map_EnvAllwell = np.zeros([len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvOilProd = np.zeros([len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvBasin220 = np.zeros([len(Lat_01), len(Lon_01),num_years, num_months]) \n",
    "Map_EnvBasin360 = np.zeros([len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvBasin395 = np.zeros([len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvBasin430 = np.zeros([len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvBasinOther = np.zeros([len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvHFOilWell = np.zeros([len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvConvOilWell = np.zeros([len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvHFComp = np.zeros([len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvConvComp = np.zeros([len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvOilWellDrilled = np.zeros([len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvStateGOMOffshore = np.zeros([len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "Map_EnvStatePacOffshore = np.zeros([len(Lat_01), len(Lon_01),num_years, num_months])\n",
    "#nongrid\n",
    "Map_EnvAllwell_nongrid = np.zeros([num_years, num_months])\n",
    "Map_EnvOilProd_nongrid = np.zeros([num_years, num_months])\n",
    "Map_EnvBasin220_nongrid = np.zeros([num_years, num_months]) \n",
    "Map_EnvBasin360_nongrid = np.zeros([num_years, num_months])\n",
    "Map_EnvBasin395_nongrid = np.zeros([num_years, num_months])\n",
    "Map_EnvBasin430_nongrid = np.zeros([num_years, num_months])\n",
    "Map_EnvBasinOther_nongrid = np.zeros([num_years, num_months])\n",
    "Map_EnvHFOilWell_nongrid = np.zeros([num_years, num_months])\n",
    "Map_EnvConvOilWell_nongrid = np.zeros([num_years, num_months])\n",
    "Map_EnvHFComp_nongrid = np.zeros([num_years, num_months])\n",
    "Map_EnvConvComp_nongrid = np.zeros([num_years, num_months])\n",
    "Map_EnvOilWellDrilled_nongrid = np.zeros([num_years, num_months])\n",
    "Map_EnvStateGOMOffshore_nongrid = np.zeros([num_years, num_months])\n",
    "Map_EnvStatePacOffshore_nongrid = np.zeros([num_years, num_months])\n",
    "\n",
    "if ReCalc_Enverus ==1:\n",
    "    for iyear in np.arange(0,num_years):\n",
    "        enverus_data_temp = vars()['Enverus_data_'+year_range_str[iyear]].copy()\n",
    "        nocompdate = 0 #record the number of wells that don't have reported completion dates (but have production in that given year)\n",
    "        nodrill = 0 #record the number of wells that don't have drilling information\n",
    "        nooffshore = 0\n",
    "        \n",
    "        #loop through each row (e.g., well) in the Enverus dataset (for both onnshore and offshore gas wells wells)\n",
    "        # This will not include wells that have zero gas production in a given year, but is consistant with the GHGI approach.\n",
    "        list_onshore_wells = enverus_data_temp.index[enverus_data_temp.loc[:,'OFFSHORE'] == 'N'].tolist()\n",
    "        list_offshore_wells = enverus_data_temp.index[enverus_data_temp.loc[:,'OFFSHORE'] == 'Y'].tolist()\n",
    "        list_oil_wells = enverus_data_temp.index[enverus_data_temp.loc[:,'CUM_OIL'] > 0].tolist()\n",
    "        #find onshore oil wells based on common list elements...\n",
    "        list1_as_set = set(list_onshore_wells)\n",
    "        intersection = list1_as_set.intersection(list_oil_wells)\n",
    "        list_onshore_oil_wells = list(intersection)\n",
    "        #find offshore oil wells based on common list elements...\n",
    "        list1_as_set = set(list_offshore_wells)\n",
    "        intersection = list1_as_set.intersection(list_oil_wells)\n",
    "        list_offshore_oil_wells = list(intersection)\n",
    "    \n",
    "        # for onshore oil wells... \n",
    "        for iwell in list_onshore_oil_wells:\n",
    "            #Check if location is within CONUS\n",
    "            if enverus_data_temp['LONGITUDE'][iwell] > Lon_left and enverus_data_temp['LONGITUDE'][iwell] < Lon_right \\\n",
    "                and enverus_data_temp['LATITUDE'][iwell] > Lat_low and enverus_data_temp['LATITUDE'][iwell] < Lat_up:\n",
    "                #find index of lon and lat, and NEMS region\n",
    "                ilat = int((enverus_data_temp['LATITUDE'][iwell] - Lat_low)/Res01)\n",
    "                ilon = int((enverus_data_temp['LONGITUDE'][iwell] - Lon_left)/Res01)\n",
    "            \n",
    "                if ((data_fn.safe_div(enverus_data_temp['CUM_GAS'][iwell],float(enverus_data_temp['CUM_OIL'][iwell]))) <= 100 and \\\n",
    "                    ((enverus_data_temp['GOR_QUAL'][iwell] =='Liq only') or (enverus_data_temp['GOR_QUAL'][iwell] =='Liq+Gas'))):\n",
    "                    # if oil well, \n",
    "                    for imonth in np.arange(0,num_months):\n",
    "                    #count wells in map only for months where there is oil production (emissions ~ when production is occuring)\n",
    "                        prod_str = 'OILPROD_'+month_tag[imonth]  \n",
    "                        if enverus_data_temp[prod_str][iwell] >0:\n",
    "                            Map_EnvAllwell[ilat,ilon,iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell] #includes oil wells only\n",
    "                            Map_EnvOilProd[ilat,ilon,iyear,imonth] += enverus_data_temp[prod_str][iwell] # production from non-assoc. gas wells only\n",
    "                        \n",
    "                            #save basin-specific production levels for onshore non-associated gas wells\n",
    "                            if enverus_data_temp['AAPG_CODE_ERG'][iwell] =='220':\n",
    "                                Map_EnvBasin220[ilat,ilon,iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                            elif enverus_data_temp['AAPG_CODE_ERG'][iwell] =='360':\n",
    "                                Map_EnvBasin360[ilat,ilon,iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                            elif enverus_data_temp['AAPG_CODE_ERG'][iwell] =='395':\n",
    "                                Map_EnvBasin395[ilat,ilon,iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                            elif enverus_data_temp['AAPG_CODE_ERG'][iwell] =='430':\n",
    "                                Map_EnvBasin430[ilat,ilon,iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                            else: \n",
    "                                Map_EnvBasinOther[ilat,ilon,iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                        \n",
    "                            if enverus_data_temp['HF'][iwell] == 'Y':\n",
    "                            #is it an HF well or not?\n",
    "                                Map_EnvHFOilWell[ilat,ilon,iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell] #oil HF wells\n",
    "                            else:     \n",
    "                                Map_EnvConvOilWell[ilat,ilon,iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell] #oil conventional wells\n",
    "                                            \n",
    "                    if isinstance(enverus_data_temp['COMPLETION_DATE'][iwell],float):\n",
    "                    #if oil well (onshore), regardless of whether the well this month is producing, \n",
    "                    # determine whether the given well was completed this year, and if so, assign it to the correct month,\n",
    "                    # if not completed in the current year, then don't add completion year (assume it was captured already in previous year loop)\n",
    "                    # if completion date is NaN, do not record anywhere (may undercount). Will also undercount if well completed in\n",
    "                    # one year but does not start producing until the next. \n",
    "                        if np.isnan(enverus_data_temp['COMPLETION_DATE'][iwell]):\n",
    "                            nocompdate = nocompdate +1\n",
    "                    else:\n",
    "                        month = enverus_data_temp['COMPLETION_DATE'][iwell][5:7] #extract the month\n",
    "                        year = enverus_data_temp['COMPLETION_DATE'][iwell][0:4] #extract year\n",
    "                        if year_range_str[iyear] == year:\n",
    "                        # if completed in the current year, add to the correct month map\n",
    "                            for imonth in np.arange(0, num_months):\n",
    "                                if month_tag[imonth] == month:\n",
    "                                    if enverus_data_temp['HF'][iwell] == 'Y':\n",
    "                                        Map_EnvHFComp[ilat,ilon,iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell] #includes completions from non-associated HF gas wells that were producing in the same year\n",
    "                                    else:\n",
    "                                        #print('here, non-HF')\n",
    "                                        Map_EnvConvComp[ilat,ilon,iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell] #includes completions from non-associated conventional wells that were producing in the same year\n",
    "                \n",
    "                    if isinstance(enverus_data_temp['SPUD_DATE'][iwell],float):\n",
    "                    #if oil well (onshore), regardless of whether the well this month is producing, \n",
    "                    # determine whether the given well was drilled this year, and if so, assign it to the correct *YEAR* \n",
    "                    # assign based on SPUD Date, unless Null, then check to see if producing in the current year\n",
    "                    # NOTE: the National inventory looks for first production date in the nexy year to see if drilled this year. \n",
    "                    # This logic is too difficult to implement here, so only counted if first_prod_date is in current year\n",
    "                        if np.isnan(enverus_data_temp['SPUD_DATE'][iwell]):\n",
    "                            if isinstance(enverus_data_temp['FIRST_PROD_DATE'][iwell],float):\n",
    "                                if np.isnan(enverus_data_temp['FIRST_PROD_DATE'][iwell]):\n",
    "                                    nodrill += 1\n",
    "                            else:\n",
    "                                year = enverus_data_temp['FIRST_PROD_DATE'][iwell][0:4] #extract year\n",
    "                                if year_range_str[iyear] == year:\n",
    "                                    Map_EnvOilWellDrilled[ilat,ilon,iyear,:] += enverus_data_temp['WELL_COUNT'][iwell]\n",
    "                    else:\n",
    "                        year = enverus_data_temp['SPUD_DATE'][iwell][0:4] #extract year\n",
    "                        if year_range_str[iyear] == year:\n",
    "                        # if completed in the current year, add to the correct month map\n",
    "                            Map_EnvOilWellDrilled[ilat,ilon,iyear,:] += enverus_data_temp['WELL_COUNT'][iwell]\n",
    "                \n",
    "            #if not in CONUS grid, still count those wells in non-grid arrays (does not include offshore, dealt with next)\n",
    "            # same logic sequence as above\n",
    "            else:\n",
    "                #inems = enverus_data_temp['NEMS_CODE'][iwell].astype(int) \n",
    "                if ((data_fn.safe_div(enverus_data_temp['CUM_GAS'][iwell],float(enverus_data_temp['CUM_OIL'][iwell]))) <= 100 and \\\n",
    "                    ((enverus_data_temp['GOR_QUAL'][iwell] =='Liq only') or (enverus_data_temp['GOR_QUAL'][iwell] =='Liq+Gas'))):\n",
    "                    for imonth in np.arange(0,num_months):\n",
    "                    #count wells in map only for months where there is gas production (emissions ~ when production is occuring)\n",
    "                        prod_str = 'OILPROD_'+month_tag[imonth]  \n",
    "                        if enverus_data_temp[prod_str][iwell] >0:\n",
    "                        #check if an oil well\n",
    "                            Map_EnvAllwell_nongrid[iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell] #includes oil wells only\n",
    "                            Map_EnvOilProd_nongrid[iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                        \n",
    "                            #save basin-specific production levels for onshore non-associated gas wells\n",
    "                            if enverus_data_temp['AAPG_CODE_ERG'][iwell] =='220':\n",
    "                                Map_EnvBasin220_nongrid[iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                            elif enverus_data_temp['AAPG_CODE_ERG'][iwell] =='360':\n",
    "                                Map_EnvBasin360_nongrid[iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                            elif enverus_data_temp['AAPG_CODE_ERG'][iwell] =='395':\n",
    "                                Map_EnvBasin395_nongrid[iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                            elif enverus_data_temp['AAPG_CODE_ERG'][iwell] =='430':\n",
    "                                Map_EnvBasin430_nongrid[iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                            else: \n",
    "                                Map_EnvBasinOther_nongrid[iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                                \n",
    "                            if enverus_data_temp['HF'][iwell] == 'Y':\n",
    "                                Map_EnvHFOilWell_nongrid[iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell]\n",
    "                            else:     \n",
    "                                Map_EnvConvOilWell_nongrid[iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell]\n",
    "                                         \n",
    "                    if isinstance(enverus_data_temp['COMPLETION_DATE'][iwell],float): \n",
    "                        if np.isnan(enverus_data_temp['COMPLETION_DATE'][iwell]):\n",
    "                            nocompdate = nocompdate +1\n",
    "                    else:\n",
    "                        month = enverus_data_temp['COMPLETION_DATE'][iwell][5:7] #extract the month\n",
    "                        year = enverus_data_temp['COMPLETION_DATE'][iwell][0:4]\n",
    "                        if year_range_str[iyear] == year:\n",
    "                            for imonth in np.arange(0, num_months):\n",
    "                                if month_tag[imonth] == month:\n",
    "                                    if enverus_data_temp['HF'][iwell] == 'Y':\n",
    "                                        Map_EnvHFComp_nongrid[iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell]\n",
    "                                    else:\n",
    "                                        Map_EnvConvComp_nongrid[iyear,imonth] += enverus_data_temp['WELL_COUNT'][iwell]\n",
    "            \n",
    "                    if isinstance(enverus_data_temp['SPUD_DATE'][iwell],float):\n",
    "                        if np.isnan(enverus_data_temp['SPUD_DATE'][iwell]):\n",
    "                            if isinstance(enverus_data_temp['FIRST_PROD_DATE'][iwell],float):\n",
    "                                if np.isnan(enverus_data_temp['FIRST_PROD_DATE'][iwell]):\n",
    "                                    nodrill += 1\n",
    "                            else:\n",
    "                                year = enverus_data_temp['FIRST_PROD_DATE'][iwell][0:4] #extract year\n",
    "                                if year_range_str[iyear] == year:\n",
    "                                    Map_EnvOilWellDrilled_nongrid[iyear,:] += enverus_data_temp['WELL_COUNT'][iwell]\n",
    "                    else:\n",
    "                        year = enverus_data_temp['SPUD_DATE'][iwell][0:4] #extract year\n",
    "                        if year_range_str[iyear] == year:\n",
    "                        # if completed in the current year, add to the correct month map\n",
    "                            Map_EnvOilWellDrilled_nongrid[iyear,:] += enverus_data_temp['WELL_COUNT'][iwell]      \n",
    "            \n",
    "                    \n",
    "        #for offshore gas well locations... \n",
    "        # EPA State GOM offshore emissions will be allocated based on Enverus production for\n",
    "        # offshore emissions in GOM states (AL, LA, TX, etc). \n",
    "        # Offshore emissions (in NGOM region) are not included in the ERG well count nor here. \n",
    "        # Federal offshore emissions are allocated later based on BOEM GOADS platform emissions\n",
    "        for iwell in list_offshore_oil_wells:\n",
    "\n",
    "            #Check if location is on grid\n",
    "            if enverus_data_temp['LONGITUDE'][iwell] > Lon_left and enverus_data_temp['LONGITUDE'][iwell] < Lon_right \\\n",
    "                and enverus_data_temp['LATITUDE'][iwell] > Lat_low and enverus_data_temp['LATITUDE'][iwell] < Lat_up:\n",
    "                #Set ilon and ilat\n",
    "                ilat = int((enverus_data_temp['LATITUDE'][iwell] - Lat_low)/Res01)\n",
    "                ilon = int((enverus_data_temp['LONGITUDE'][iwell] - Lon_left)/Res01)\n",
    "                \n",
    "                #figure out how to deal with this ....\n",
    "                # check if non-associated gas well (offshore)\n",
    "                if ((data_fn.safe_div(enverus_data_temp['CUM_GAS'][iwell],float(enverus_data_temp['CUM_OIL'][iwell]))) <= 100 and \\\n",
    "                    ((enverus_data_temp['GOR_QUAL'][iwell] =='Liq only') or (enverus_data_temp['GOR_QUAL'][iwell] =='Liq+Gas'))):\n",
    "                    if enverus_data_temp['STATE'][iwell] in {'AL','FL','LA','MS','TX'}:\n",
    "                        for imonth in np.arange(0,num_months):\n",
    "                        #count wells in map only for months where there is gas production (emissions ~ when production is occuring)\n",
    "                            prod_str = 'OILPROD_'+month_tag[imonth]  \n",
    "                            if enverus_data_temp[prod_str][iwell] >0:\n",
    "                                Map_EnvStateGOMOffshore[ilat,ilon,iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "                    elif enverus_data_temp['STATE'][iwell] in {'CA'}:\n",
    "                        for imonth in np.arange(0,num_months):\n",
    "                        #count wells in map only for months where there is gas production (emissions ~ when production is occuring)\n",
    "                            prod_str = 'OILPROD_'+month_tag[imonth]  \n",
    "                            if enverus_data_temp[prod_str][iwell] >0:\n",
    "                                Map_EnvStatePacOffshore[ilat,ilon,iyear,imonth] += enverus_data_temp[prod_str][iwell]\n",
    "            else:\n",
    "                nooffshore +=1\n",
    "                #print(\"Error - No offshore outside of the domain\")#display(EPA_emi_prod_NG_CH4)           \n",
    "                \n",
    "\n",
    "        print('Enverus data not included in this analysis:')\n",
    "        print('Year: '+year_range_str[iyear])\n",
    "        print('Wells without drilling information (no Spud or Production data): ',nodrill)\n",
    "        print('Wells without completion dates: ',nocompdate)\n",
    "        print('Wells offshore and outside of grid domain: ',nooffshore)\n",
    "\n",
    "    #save current status of datafiles\n",
    "    np.savez('./IntermediateOutputs/Oil_EnvAllWell_tempout', x=Map_EnvAllwell, y=Map_EnvAllwell_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Oil_EnvOilProd_tempout', x=Map_EnvOilProd, y=Map_EnvOilProd_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Oil_EnvBasin220_tempout', x=Map_EnvBasin220, y=Map_EnvBasin220_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Oil_EnvBasin360_tempout', x=Map_EnvBasin360, y=Map_EnvBasin360_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Oil_EnvBasin395_tempout', x=Map_EnvBasin395, y=Map_EnvBasin395_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Oil_EnvBasin430_tempout', x=Map_EnvBasin430, y=Map_EnvBasin430_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Oil_EnvBasinOther_tempout', x=Map_EnvBasinOther, y=Map_EnvBasinOther_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Oil_EnvHFOilWell_tempout', x=Map_EnvHFOilWell, y=Map_EnvHFOilWell_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Oil_EnvConvOilWell_tempout', x=Map_EnvConvOilWell, y=Map_EnvConvOilWell_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Oil_EnvHFComp_tempout', x=Map_EnvHFComp, y=Map_EnvHFComp_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Oil_EnvConvComp_tempout', x=Map_EnvConvComp, y=Map_EnvConvComp_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Oil_EnvOilWellDrilled_tempout', x=Map_EnvOilWellDrilled, y=Map_EnvOilWellDrilled_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Oil_EnvStateGOMOffshore_tempout', x=Map_EnvStateGOMOffshore, y=Map_EnvStateGOMOffshore_nongrid)\n",
    "    np.savez('./IntermediateOutputs/Oil_EnvStatePacOffshore_tempout', x=Map_EnvStatePacOffshore, y=Map_EnvStatePacOffshore_nongrid)\n",
    "\n",
    "else:\n",
    "    #load previously saved files\n",
    "    npzfile = np.load('./IntermediateOutputs/Oil_EnvAllWell_tempout.npz')\n",
    "    Map_EnvAllwell = npzfile['x']\n",
    "    Map_EnvAllwell_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Oil_EnvOilProd_tempout.npz')\n",
    "    Map_EnvOilProd = npzfile['x']\n",
    "    Map_EnvOilProd_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Oil_EnvBasin220_tempout.npz')\n",
    "    Map_EnvBasin220 = npzfile['x']\n",
    "    Map_EnvBasin220_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Oil_EnvBasin360_tempout.npz')\n",
    "    Map_EnvBasin360 = npzfile['x']\n",
    "    Map_EnvBasin360_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Oil_EnvBasin430_tempout.npz')\n",
    "    Map_EnvBasin430 = npzfile['x']\n",
    "    Map_EnvBasin430_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Oil_EnvBasinOther_tempout.npz')\n",
    "    Map_EnvBasinOther = npzfile['x']\n",
    "    Map_EnvBasinOther_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Oil_EnvBasin395_tempout.npz')\n",
    "    Map_EnvBasin395 = npzfile['x']\n",
    "    Map_EnvBasin395_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Oil_EnvHFOilWell_tempout.npz')\n",
    "    Map_EnvHFOilWell = npzfile['x']\n",
    "    Map_EnvHFOilWell_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Oil_EnvConvOilWell_tempout.npz')\n",
    "    Map_EnvConvOilWell = npzfile['x']\n",
    "    Map_EnvConvOilWell_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Oil_EnvHFComp_tempout.npz')\n",
    "    Map_EnvHFComp = npzfile['x']\n",
    "    Map_EnvHFComp_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Oil_EnvConvComp_tempout.npz')\n",
    "    Map_EnvConvComp = npzfile['x']\n",
    "    Map_EnvConvComp_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Oil_EnvOilWellDrilled_tempout.npz')\n",
    "    Map_EnvOilWellDrilled = npzfile['x']\n",
    "    Map_EnvOilWellDrilled_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Oil_EnvStateGOMOffshore_tempout.npz')\n",
    "    Map_EnvStateGOMOffshore = npzfile['x']\n",
    "    Map_EnvStateGOMOffshore_nongrid = npzfile['y']\n",
    "    npzfile = np.load('./IntermediateOutputs/Oil_EnvStatePacOffshore_tempout.npz')\n",
    "    Map_EnvStatePacOffshore = npzfile['x']\n",
    "    Map_EnvStatePacOffshore_nongrid = npzfile['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.6 Oil Gravities [not included]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.7 Correct Missing IL/IN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Process\n",
    "# 1. Read the GHGI well and production statistics from the GHGI (contain corrected IL and IN data)\n",
    "# 2. Read in the relevant NEI data (from both file formats) and place onto GEPA grid (including reproj of NEI data)\n",
    "# 3. Scale the NEI proxy maps to the corresponding state level values from Step 1.\n",
    "# 4. Calculate the lease condensate proxy for IL/IN using the same method as the Enverus data\n",
    "# 5. Place the scaled NEI grid data on the appropriate Enverus proxy grids. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.7.1 Read in GHGI State-Level Well Statistics for IL/IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.7.1.1 Well Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1. Read in National Well Statistics for IL/IN (from ERG Wells Processing Workbook)\n",
    "# for scaling the NEI proxies to GHGI totals so that IL and IN are correctly weighted relative to \n",
    "# the relative weights in the GHGI (e.g., consistent well counts in IL/In relative to national total)\n",
    "# There may be absolute differences in the NEI due to different data processing. \n",
    "# In otherwords, we want to take the relative spatial information from the NEI, but not the absolute values\n",
    "# Use the well count data for 2016, 2017, 2018, and 2019 - corrected by ERG \n",
    "\n",
    "Env_ILIN_wells = pd.read_excel(Enverus_WellCounts_inputfile, sheet_name = \"2020 PR - State\", skiprows = 4)\n",
    "Env_ILIN_wells = Env_ILIN_wells.drop(columns = ['Category','WELLCOUNT_16', 'WELLCOUNT_17','WELLCOUNT_18'])\n",
    "Env_ILIN_wells.rename(columns={Env_ILIN_wells.columns[Env_ILIN_wells.columns.get_loc('WELLCOUNT_16_ERG')]:'WELLCOUNT_16'}, inplace=True)\n",
    "Env_ILIN_wells.rename(columns={Env_ILIN_wells.columns[Env_ILIN_wells.columns.get_loc('WELLCOUNT_17_ERG')]:'WELLCOUNT_17'}, inplace=True)\n",
    "Env_ILIN_wells.rename(columns={Env_ILIN_wells.columns[Env_ILIN_wells.columns.get_loc('WELLCOUNT_18_ERG')]:'WELLCOUNT_18'}, inplace=True)\n",
    "Env_ILIN_wells = Env_ILIN_wells.fillna(0)\n",
    "Env_ILIN_wells = Env_ILIN_wells[(Env_ILIN_wells['STATE']=='IL') | (Env_ILIN_wells['STATE']=='IN')]\n",
    "Env_ILIN_wells.reset_index(inplace=True, drop=True)\n",
    "Env_ILIN_wells['NEMS'] = 0 #IN and IL are both in the north east region\n",
    "\n",
    "#2 Calculate Well Counts of Each Well Type for each NEMS region\n",
    "# ERG Query codes\n",
    "# 1 = Non-Associated Gas Wells, #2 = Oil Wells\n",
    "# 3 = Associated Gas Wells (not included in total well counts)\n",
    "# 4  = Gas Wells (non-associated) with Hydraulic Fracturing\n",
    "# 5 = Gas Well Completions with Hydraulic Fracturing\n",
    "# 6 = Oil Wells with Hydraulic Fracturing, \n",
    "# 7 = Oil well completions with hydraulic fracturing\n",
    "# 8 = All Gas Well Completions, \n",
    "# 9 All Oil well completions\n",
    "# 10a = Gas Wells Drilled, #10 b = Oil Wells Drilled\n",
    "# 10c = Dry Wells Drilled\n",
    "\n",
    "Well_Allwell_ILIN = np.zeros([2,num_years]) #all oil wells\n",
    "Well_HFOilWell_ILIN = np.zeros([2,num_years]) #HF oil wells\n",
    "Well_ConvOilWell_ILIN = np.zeros([2, num_years]) # conventional oil wells (all - HF)\n",
    "Well_HFComp_ILIN = np.zeros([2, num_years]) #HF oil well completions\n",
    "Well_ConvComp_ILIN = np.zeros([2, num_years]) # oil conventional well completions (all - HF)\n",
    "Well_AllComp_ILIN = np.zeros([2, num_years]) #all oil well completions\n",
    "\n",
    "Well_Gaswell_drilled_ILIN = np.zeros([2, num_years]) # gas wells drilled\n",
    "Well_Oilwell_drilled_ILIN = np.zeros([2, num_years]) # will end up being corrected total oil wells drilled (inclduign fraction of dry wells)\n",
    "Well_Drywell_drilled_ILIN = np.zeros([2, num_years]) # dry wells drilled\n",
    "\n",
    "# 1) Get all well count data for non-HF wells and completions\n",
    "start_year_idx = Env_ILIN_wells.columns.get_loc('WELLCOUNT_'+str(start_year)[2:4])\n",
    "end_year_idx = Env_ILIN_wells.columns.get_loc('WELLCOUNT_'+str(end_year)[2:4])+1\n",
    "\n",
    "for idx in np.arange(0,len(Env_ILIN_wells)):\n",
    "    if Env_ILIN_wells['STATE'][idx] == 'IL':\n",
    "        istate =0\n",
    "    else:\n",
    "        istate =1\n",
    "\n",
    "    if Env_ILIN_wells['QUERY_NMBR'][idx] ==2:\n",
    "        Well_Allwell_ILIN[istate,] = Well_Allwell_ILIN[istate,]+Env_ILIN_wells.iloc[idx,start_year_idx:end_year_idx]\n",
    "    elif Env_ILIN_wells['QUERY_NMBR'][idx] ==6:\n",
    "        Well_HFOilWell_ILIN[istate,] = Well_HFOilWell_ILIN[istate,]+Env_ILIN_wells.iloc[idx,start_year_idx:end_year_idx]\n",
    "    elif Env_ILIN_wells['QUERY_NMBR'][idx] ==7:\n",
    "        Well_HFComp_ILIN[istate,] = Well_HFComp_ILIN[istate,]+Env_ILIN_wells.iloc[idx,start_year_idx:end_year_idx]\n",
    "    elif Env_ILIN_wells['QUERY_NMBR'][idx] ==9:   \n",
    "        Well_AllComp_ILIN[istate,] = Well_AllComp_ILIN[istate,]+Env_ILIN_wells.iloc[idx,start_year_idx:end_year_idx]\n",
    "    elif Env_ILIN_wells['QUERY_NMBR'][idx] =='10a':   \n",
    "        Well_Gaswell_drilled_ILIN[istate,] = Well_Gaswell_drilled_ILIN[istate,]+Env_ILIN_wells.iloc[idx,start_year_idx:end_year_idx]\n",
    "    elif Env_ILIN_wells['QUERY_NMBR'][idx] =='10c':\n",
    "        Well_Drywell_drilled_ILIN[istate,] = Well_Drywell_drilled_ILIN[istate,]+Env_ILIN_wells.iloc[idx,start_year_idx:end_year_idx]\n",
    "    elif Env_ILIN_wells['QUERY_NMBR'][idx] =='10b':\n",
    "        Well_Oilwell_drilled_ILIN[istate,] = Well_Oilwell_drilled_ILIN[istate,]+Env_ILIN_wells.iloc[idx,start_year_idx:end_year_idx]\n",
    "\n",
    "# Calculate Conventional well counts and completions (All gas wells - HF gas wells)\n",
    "Well_ConvOilWell_ILIN = Well_Allwell_ILIN - Well_HFOilWell_ILIN\n",
    "Well_ConvComp_ILIN = Well_AllComp_ILIN - Well_HFComp_ILIN\n",
    "\n",
    "# Calculate total number of wells drilled wells, accounting for fraction of dry wells (= total - corrected NG wells) \n",
    "for istate in np.arange(0,2):\n",
    "    for iyear in np.arange(0,num_years):\n",
    "        Well_Oilwell_drilled_ILIN[istate,iyear] = Well_Oilwell_drilled_ILIN[istate,iyear] + Well_Drywell_drilled_ILIN[istate,iyear] \\\n",
    "                                        * (data_fn.safe_div(Well_Oilwell_drilled_ILIN[istate,iyear],\\\n",
    "                                        (Well_Gaswell_drilled_ILIN[istate,iyear]+Well_Oilwell_drilled_ILIN[istate,iyear])))\n",
    "\n",
    "print('IL/IN GHGI total counts')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    print('Year: ', year_range_str[iyear])\n",
    "    #Print final well counts ** ADD IN QA/QC with final wells notebook later **\n",
    "    print('All oil wells:     ',np.sum(Well_Allwell_ILIN[:,iyear]))\n",
    "    print('Oil wells Conv:    ',np.sum(Well_ConvOilWell_ILIN[:,iyear]))\n",
    "    print('Oil wells HF:      ',np.sum(Well_HFOilWell_ILIN[:,iyear]))\n",
    "    print('All oil well comp: ',np.sum(Well_AllComp_ILIN[:,iyear]))\n",
    "    print('Oil HF comp:       ',np.sum(Well_HFComp_ILIN[:,iyear]))\n",
    "    print('Oil Conv comp:     ',np.sum(Well_ConvComp_ILIN[:,iyear]))\n",
    "    print('Wells Drilled:     ',np.sum(Well_Oilwell_drilled_ILIN[:,iyear]))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.7.1.2 Well Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERG Processed Well Production Data (from Prism/Enverus)\n",
    "# Gas produced from wells in each NEMS region, state, and Basin (units of MCF (gas))\n",
    "\n",
    "# Includes Gas production from NA gas wells (DOES NOT CURRENTLY INCLUDE GAS PRODUCTION FROM OIL WELLS)\n",
    "\n",
    "# Use the well count data for 2016, 2017, 2018, and 2019 - corrected by ERG \n",
    "Env_ILIN_wellsprod = pd.read_excel(Enverus_WellProd_inputfile, sheet_name = \"GHG_DATA_AAPG_MAR19\", skiprows = 1)\n",
    "\n",
    "#drop oil production data\n",
    "match = np.where(Env_ILIN_wellsprod.columns.str.contains('SUMOFGAS'))[0][:]\n",
    "Env_ILIN_wellsprod = Env_ILIN_wellsprod.drop(Env_ILIN_wellsprod.columns[match], axis=1)\n",
    "\n",
    "#replace with ERG recalculations\n",
    "Env_ILIN_wellsprod = Env_ILIN_wellsprod.drop(columns = ['SUMOFLIQ_16', 'SUMOFLIQ_17','SUMOFLIQ_18',])\n",
    "Env_ILIN_wellsprod.rename(columns={Env_ILIN_wellsprod.columns[Env_ILIN_wellsprod.columns.get_loc('SUMOFLIQ_16_ERG')]:'SUMOFLIQ_16'}, inplace=True)\n",
    "Env_ILIN_wellsprod.rename(columns={Env_ILIN_wellsprod.columns[Env_ILIN_wellsprod.columns.get_loc('SUMOFLIQ_17_ERG')]:'SUMOFLIQ_17'}, inplace=True)\n",
    "Env_ILIN_wellsprod.rename(columns={Env_ILIN_wellsprod.columns[Env_ILIN_wellsprod.columns.get_loc('SUMOFLIQ_18_ERG')]:'SUMOFLIQ_18'}, inplace=True)\n",
    "Env_ILIN_wellsprod = Env_ILIN_wellsprod.fillna(0)\n",
    "Env_ILIN_wellsprod = Env_ILIN_wellsprod[(Env_ILIN_wellsprod['STATE']=='IL') | (Env_ILIN_wellsprod['STATE']=='IN')]\n",
    "Env_ILIN_wellsprod.reset_index(inplace=True, drop=True)\n",
    "#display(Env_ILIN_wellsprod)\n",
    "\n",
    "#Env_ILIN_wellsprod['NEMS'] = 0 #all data are in northeast region\n",
    "\n",
    "# Extract the gas production data from non-associated gas wells (QRY = 1) and gas produced from oil wells (QRY = 2)\n",
    "# and assign to each basin-specific array based on the reported state and AAPG Code as determined by ERG in the workbook\n",
    "Wellprod_other_ILIN = np.zeros([2, num_years])\n",
    "\n",
    "start_year_idx = Env_ILIN_wellsprod.columns.get_loc('SUMOFLIQ_'+str(start_year)[2:4])\n",
    "end_year_idx = Env_ILIN_wellsprod.columns.get_loc('SUMOFLIQ_'+str(end_year)[2:4])+1\n",
    "\n",
    "for idx in np.arange(0,len(Env_ILIN_wellsprod)):\n",
    "    if Env_ILIN_wellsprod['STATE'][idx] == 'IL':\n",
    "        istate =0\n",
    "    else:\n",
    "        istate =1\n",
    "        if Env_ILIN_wellsprod['QUERY_NMBR'][idx] ==2: # production from oil wells\n",
    "            if Env_ILIN_wellsprod['AAPG_CODE_ERG'][idx] != 220 and Env_ILIN_wellsprod['AAPG_CODE_ERG'][idx] != 395 and Env_ILIN_wellsprod['AAPG_CODE_ERG'][idx] != 430: \n",
    "                Wellprod_other_ILIN[istate,] = Wellprod_other_ILIN[istate,] + Env_ILIN_wellsprod.iloc[idx,start_year_idx:end_year_idx]\n",
    "                \n",
    "#Print final well counts ** ADD IN QA/QC with final wells notebook later **\n",
    "print('IL/IN GHGI total production')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    print('Year: ', year_range_str[iyear])\n",
    "    print('Other Basin Production: ',np.sum(Wellprod_other_ILIN[:,iyear]))\n",
    "    #print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.7.2 Read In/Format NEI Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.6.2.1 Read in all data prior to 2018 (text file format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1 Read in relevant files by year (for all years before 2018 [2018 read from different file type])\n",
    "# Data are in a text file format where each row of data contains the surrogate code, FIPS code, column and row location\n",
    "# (on the NEI CONUS1 grid), and the absolute, fractional, and running sum of data (e.g., counts or production) in the\n",
    "# given FIPS region. \n",
    "# The absolute data are placed onto the GEPA grid by using an NEI reference map shapefile to map the data location\n",
    "# from the NEI CONUS grid cell indexes to the corresponding latitude and longitude values in the GEPA grid. \n",
    "### Note - the 2016 data from the NEI is on a non-standard grid where lat/lons are unknown. Can change later if needed, or\n",
    "# can interpolat ebetween years if more accurate\n",
    "\n",
    "NEI_files = ['/USA_695_NOFILL.txt', '/USA_685_NOFILL.txt', '/USA_694_NOFILL.txt', '/USA_681_NOFILL.txt']\n",
    "data_names = ['map_NEI_oil_wells', 'map_NEI_oil_completions','map_NEI_oil_production','map_NEI_oil_drilledwells']\n",
    "\n",
    "for ivar in np.arange(0,len(data_names)):\n",
    "    vars()[data_names[ivar]] = np.zeros([2,len(Lat_01),len(Lon_01),num_years])\n",
    "\n",
    "# only recalc the data if required (set in Step 0)\n",
    "if ReCalc_NEI ==1:\n",
    "    \n",
    "    #read in the NEI grid refernece shapefile (contains the lat/lons of each NEI coordinate)\n",
    "    shape = shp.Reader(NEI_grid_ref_inputfile)\n",
    "\n",
    "    #make the map arrays of aboslute values (counts and mcf)\n",
    "    for ivar in np.arange(0,len(data_names)):\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            if year_range_str[iyear] == '2012':\n",
    "                year = '2011'\n",
    "            elif year_range_str[iyear] == '2013' or year_range_str[iyear] == '2014' or year_range_str[iyear] == '2015':\n",
    "                year = '2014'\n",
    "            elif year_range_str[iyear] == '2016' or year_range_str[iyear] == '2017':\n",
    "                year = '2017'\n",
    "            elif year_range_str[iyear] == '2018':\n",
    "                continue\n",
    "            else:\n",
    "                print('NEI DATA MISSING FOR YEAR ',year_range_str[iyear])\n",
    "            path = ERG_NEI_inputloc+year+NEI_files[ivar]\n",
    "            data_temp = pd.read_csv(path, sep='\\t', skiprows = 25)\n",
    "            data_temp = data_temp.drop([\"!\"], axis=1)\n",
    "            data_temp.columns = ['Code','FIPS','COL','ROW','Frac','Abs','FIPS_Total','FIPS_Running_Sum']\n",
    "            data_temp['Lat'] = np.zeros([len(data_temp)])\n",
    "            data_temp['Lon'] = np.zeros([len(data_temp)])\n",
    "            colmin = 1332\n",
    "            colmax=0\n",
    "            rowmin = 1548\n",
    "            rowmax=0\n",
    "            counter =0\n",
    "        \n",
    "            #Create the boundary box\n",
    "            for idx in np.arange(0,len(data_temp)):\n",
    "                if str(data_temp['FIPS'][idx]).startswith('17') or str(data_temp['FIPS'][idx]).startswith('18'):\n",
    "                    icol = data_temp['COL'][idx]\n",
    "                    irow = data_temp['ROW'][idx]\n",
    "                    if icol > colmax:\n",
    "                        colmax =icol\n",
    "                    if icol < colmin:\n",
    "                        colmin = icol\n",
    "                    if irow > rowmax:\n",
    "                        rowmax = irow\n",
    "                    if irow < rowmin:\n",
    "                        rowmin  = irow\n",
    "            \n",
    "            #Extract the relevant indicies from the NEI reference shapefile\n",
    "            array_temp = np.zeros([4,((colmax+1-colmin)*(rowmax+1-rowmin))]) #make an array to save col, row, lat, lon\n",
    "            idx=0\n",
    "            for rec in shape.iterRecords():\n",
    "                if (int(rec['cellid'][0:4]) <= colmax and int(rec['cellid'][0:4]) >= colmin) \\\n",
    "                    and (int(rec['cellid'][5:]) <= rowmax and int(rec['cellid'][5:]) >= rowmin):\n",
    "                        array_temp[0,idx] = int(rec['cellid'][0:4])   #column index\n",
    "                        array_temp[1,idx] = int(rec['cellid'][5:])    #row index\n",
    "                        array_temp[2,idx] = rec['Latitude']           #latitude\n",
    "                        array_temp[3,idx] = rec['Longitude']          #longitude\n",
    "                        idx +=1\n",
    "    \n",
    "            #Use this array to locate and assign the lat lon values to the NEI datafile and then place onto grid\n",
    "            for idx in np.arange(0,len(data_temp)):\n",
    "                if str(data_temp['FIPS'][idx]).startswith('17') or str(data_temp['FIPS'][idx]).startswith('18'):\n",
    "                    icol = data_temp['COL'][idx]\n",
    "                    irow = data_temp['ROW'][idx]\n",
    "                    match = np.where((icol == array_temp[0,:]) & (irow == array_temp[1,:]))[0][0]\n",
    "                    data_temp.loc[idx,'Lat'] = array_temp[2,match]\n",
    "                    data_temp.loc[idx,'Lon'] = array_temp[3,match]\n",
    "                    ilat = int((data_temp['Lat'][idx] - Lat_low)/Res01)\n",
    "                    ilon = int((data_temp['Lon'][idx] - Lon_left)/Res01)\n",
    "                    if str(data_temp['FIPS'][idx]).startswith('17'):\n",
    "                        vars()[data_names[ivar]][0,ilat,ilon,iyear] += data_temp.loc[idx,'Abs']\n",
    "                    else:\n",
    "                        vars()[data_names[ivar]][1,ilat,ilon,iyear] += data_temp.loc[idx,'Abs']\n",
    "\n",
    "    np.save('./IntermediateOutputs/NEI_oilwell_tempoutput', map_NEI_oil_wells)\n",
    "    np.save('./IntermediateOutputs/NEI_oilcomp_tempoutput', map_NEI_oil_completions)\n",
    "    np.save('./IntermediateOutputs/NEI_oilprod_tempoutput', map_NEI_oil_production)\n",
    "    np.save('./IntermediateOutputs/NEI_oildrill_tempoutput', map_NEI_oil_drilledwells)\n",
    "\n",
    "else:\n",
    "    map_NEI_oil_wells = np.load('./IntermediateOutputs/NEI_oilwell_tempoutput.npy')\n",
    "    map_NEI_oil_completions = np.load('./IntermediateOutputs/NEI_oilcomp_tempoutput.npy')\n",
    "    map_NEI_oil_production = np.load('./IntermediateOutputs/NEI_oilprod_tempoutput.npy')\n",
    "    map_NEI_oil_drilledwells = np.load('./IntermediateOutputs/NEI_oildrill_tempoutput.npy')\n",
    "            \n",
    "            \n",
    "print('IL/IN NEI totals')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    print('Year: ', year_range_str[iyear])\n",
    "    print('Oil wells (Conv + HF):         ',np.sum(map_NEI_oil_wells[:,:,:,iyear]))\n",
    "    print('All oil well comp (Conv + HF): ',np.sum(map_NEI_oil_completions[:,:,:,iyear]))\n",
    "    print('Oil production:                ',np.sum(map_NEI_oil_production[:,:,:,iyear]))\n",
    "    print('Wells Drilled:                 ',np.sum(map_NEI_oil_drilledwells[:,:,:,iyear]))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.7.2.2 Read in 2018 data (MS Access data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read in 2018 NEI data from different datafile format\n",
    "    \n",
    "if ReCalc_NEI ==1:    \n",
    "    #Read in the data\n",
    "    driver_str = r'Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ='+ERG_NEI_inputloc_2018+';'''\n",
    "    conn = pyodbc.connect(driver_str)\n",
    "    NEI_2018_ILIN_wells = pd.read_sql(\"SELECT * FROM 2018_IL_IN_WELLS\", conn)\n",
    "    conn.close()\n",
    "\n",
    "    data_temp = NEI_2018_ILIN_wells[(NEI_2018_ILIN_wells['ACTIVE_WELL_FLAG'] ==1) & \\\n",
    "                                    (NEI_2018_ILIN_wells['WELL_TYPE'] == 'OIL')]\n",
    "    data_temp.reset_index(inplace=True, drop=True)\n",
    "    data_temp.fillna(\"\",inplace=True)\n",
    "\n",
    "    #find 2018 index\n",
    "    year_diff = [abs(x - 2018) for x in year_range]\n",
    "    iyear = year_diff.index(min(year_diff))\n",
    "\n",
    "    # place data on map for each state (for active wells, production, completions, and drilled wells)\n",
    "    for iwell in np.arange(0,len(data_temp)):\n",
    "        ilat = int((data_temp['LATITUDE'][iwell] - Lat_low)/Res01)\n",
    "        ilon = int((data_temp['LONGITUDE'][iwell] - Lon_left)/Res01)\n",
    "        if str(data_temp['FIPS_CODE'][iwell]).startswith('17'):# or str(data_temp['FIPS_CODE'][iwell]).startswith('18'):\n",
    "            istate = 0\n",
    "        else:\n",
    "            istate =1\n",
    "        map_NEI_oil_wells[istate,ilat,ilon,iyear] += 1\n",
    "        map_NEI_oil_production[istate,ilat,ilon,iyear] += data_temp.loc[iwell,'SUM_LIQ']\n",
    "        if '2018' in data_temp['COMPLETION_DATE'][iwell]:\n",
    "            map_NEI_oil_completions[istate,ilat,ilon,iyear] += 1\n",
    "        if '2018' in data_temp['SPUD_DATE'][iwell]:\n",
    "            map_NEI_oil_drilledwells[istate,ilat,ilon,iyear] += 1\n",
    "\n",
    "    np.save('./IntermediateOutputs/NEI_oilwell_tempoutput', map_NEI_oil_wells)\n",
    "    np.save('./IntermediateOutputs/NEI_oilcomp_tempoutput', map_NEI_oil_completions)\n",
    "    np.save('./IntermediateOutputs/NEI_oilprod_tempoutput', map_NEI_oil_production)\n",
    "    np.save('./IntermediateOutputs/NEI_oildrill_tempoutput', map_NEI_oil_drilledwells)\n",
    "else:\n",
    "    map_NEI_oil_wells = np.load('./IntermediateOutputs/NEI_oilwell_tempoutput.npy')\n",
    "    map_NEI_oil_completions = np.load('./IntermediateOutputs/NEI_oilcomp_tempoutput.npy')\n",
    "    map_NEI_oil_production = np.load('./IntermediateOutputs/NEI_oilprod_tempoutput.npy')\n",
    "    map_NEI_oil_drilledwells = np.load('./IntermediateOutputs/NEI_oildrill_tempoutput.npy')\n",
    "    \n",
    "print('IL/IN NEI totals')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    print('Year: ', year_range_str[iyear])\n",
    "    print('Oil wells (Conv + HF):         ',np.sum(map_NEI_oil_wells[:,:,:,iyear]))\n",
    "    print('All oil well comp (Conv + HF): ',np.sum(map_NEI_oil_completions[:,:,:,iyear]))\n",
    "    print('Oil production:                ',np.sum(map_NEI_oil_production[:,:,:,iyear]))\n",
    "    print('Wells Drilled:                 ',np.sum(map_NEI_oil_drilledwells[:,:,:,iyear]))\n",
    "    print(' ')\n",
    "\n",
    "#display(data_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.7.4 Scale NEI absolute values to GHGI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale the absolute NEI data by the corresponding GHGI counts so that the IL/IN data are not over or under-weighted\n",
    "# relative to the IL/IN activity data used in the GHGI\n",
    "# without the scaling, the national emissions would likley be overallocated to these two states as the NEI well and \n",
    "# production counts are higher than those used for these states in the current GHGI\n",
    "\n",
    "#make extra required arrays (HF and Conv will have the same spatial distribution as all gas wells/completions)\n",
    "map_NEI_oil_wells_conv = map_NEI_oil_wells.copy()\n",
    "map_NEI_oil_wells_HF = map_NEI_oil_wells.copy()\n",
    "map_NEI_oil_completions_conv = map_NEI_oil_completions.copy()\n",
    "map_NEI_oil_completions_HF = map_NEI_oil_completions.copy()\n",
    "\n",
    "#if ReCalc_NEI ==1:\n",
    "\n",
    "print('QA/QC: Check that NEI data is scaled to GHGI activity data')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    # ratio = sum(GHGI)/ sum(NEI)\n",
    "    \n",
    "    #1) conventional oil wells (same spatial distribution as all NEI oil wells)\n",
    "    ratio_temp = data_fn.safe_div(np.sum(Well_ConvOilWell_ILIN[:,iyear]),np.sum(map_NEI_oil_wells[:,:,:,iyear]))\n",
    "    map_NEI_oil_wells_conv[:,:,:,iyear] *= ratio_temp\n",
    "    \n",
    "    #2) HF oil wells (same spatial distribution as all NEI oil wells)\n",
    "    ratio_temp = data_fn.safe_div(np.sum(Well_HFOilWell_ILIN[:,iyear]),np.sum(map_NEI_oil_wells[:,:,:,iyear]))\n",
    "    map_NEI_oil_wells_HF[:,:,:,iyear] *= ratio_temp\n",
    "    \n",
    "    # 3) all oil wells\n",
    "    ratio_temp = data_fn.safe_div(np.sum(Well_Allwell_ILIN[:,iyear]),np.sum(map_NEI_oil_wells[:,:,:,iyear]))\n",
    "    map_NEI_oil_wells[:,:,:,iyear] *= ratio_temp\n",
    "    \n",
    "    #4) Conv oil well completions (same spatial distribution as all oil well completions)\n",
    "    ratio_temp = data_fn.safe_div(np.sum(Well_ConvOilWell_ILIN[:,iyear]),np.sum(map_NEI_oil_completions[:,:,:,iyear]))\n",
    "    map_NEI_oil_completions_conv[:,:,:,iyear] *= ratio_temp\n",
    "    \n",
    "    #5) HF oil well completions (same spatial distribution as all oil well completions)\n",
    "    ratio_temp = data_fn.safe_div(np.sum(Well_HFComp_ILIN[:,iyear]),np.sum(map_NEI_oil_completions[:,:,:,iyear]))\n",
    "    map_NEI_oil_completions_HF[:,:,:,iyear] *= ratio_temp\n",
    "    \n",
    "    #6) all oil well completions\n",
    "    ratio_temp = data_fn.safe_div(np.sum(Well_AllComp_ILIN[:,iyear]),np.sum(map_NEI_oil_completions[:,:,:,iyear]))\n",
    "    map_NEI_oil_completions[:,:,:,iyear] *= ratio_temp\n",
    "    \n",
    "    #7) oil wells drilled\n",
    "    ratio_temp = data_fn.safe_div(np.sum(Well_Oilwell_drilled_ILIN[:,iyear]),np.sum(map_NEI_oil_drilledwells[:,:,:,iyear]))\n",
    "    if pd.isna(ratio_temp):\n",
    "        ratio_temp = 0    #if there is no GHGI data, but there is NEI data, scale to zero counts\n",
    "    map_NEI_oil_drilledwells[:,:,:,iyear] *= ratio_temp\n",
    "    \n",
    "    #8) oil production volumes\n",
    "    ratio_temp = data_fn.safe_div(np.sum(Wellprod_other_ILIN[:,iyear]),np.sum(map_NEI_oil_production[:,:,:,iyear]))\n",
    "    if pd.isna(ratio_temp):\n",
    "        ratio_temp = 0     #if there is no GHGI data, but there is NEI data, scale to zero counts\n",
    "    map_NEI_oil_production[:,:,:,iyear] *= ratio_temp\n",
    "    \n",
    "    diff1 = (np.sum(Well_Allwell_ILIN[:,iyear]) - np.sum(map_NEI_oil_wells[:,:,:,iyear])) +\\\n",
    "            (np.sum(Well_ConvOilWell_ILIN[:,iyear]) - np.sum(map_NEI_oil_wells_conv[:,:,:,iyear])) +\\\n",
    "            (np.sum(Well_HFOilWell_ILIN[:,iyear]) - np.sum(map_NEI_oil_wells_HF[:,:,:,iyear])) + \\\n",
    "            (np.sum(Well_AllComp_ILIN[:,iyear]) - np.sum(map_NEI_oil_completions[:,:,:,iyear])) +\\\n",
    "            (np.sum(Well_ConvComp_ILIN[:,iyear]) - np.sum(map_NEI_oil_completions_conv[:,:,:,iyear])) +\\\n",
    "            (np.sum(Well_HFComp_ILIN[:,iyear]) - np.sum(map_NEI_oil_completions_HF[:,:,:,iyear])) + \\\n",
    "            (np.sum(Well_Oilwell_drilled_ILIN[:,iyear]) - np.sum(map_NEI_oil_drilledwells[:,:,:,iyear])) + \\\n",
    "            (np.sum(Wellprod_other_ILIN[:,iyear]) - np.sum(map_NEI_oil_production[:,:,:,iyear]))\n",
    "    \n",
    "    if abs(diff1) < 1e-12:\n",
    "        print('Year ', year_range_str[iyear],\":\",\"PASS\")\n",
    "    else:\n",
    "        print('Year ', year_range_str[iyear],\":\",\"CHECK\", diff1)\n",
    "    \n",
    "    print('Oil wells (Conv + HF):         ',np.sum(map_NEI_oil_wells[:,:,:,iyear]))\n",
    "    print('Oil wells (Conv):              ',np.sum(map_NEI_oil_wells_conv[:,:,:,iyear]))\n",
    "    print('Oil wells (HF):                ',np.sum(map_NEI_oil_wells_HF[:,:,:,iyear]))\n",
    "    print('All oil well comp (Conv + HF): ',np.sum(map_NEI_oil_completions[:,:,:,iyear]))\n",
    "    print('All oil well comp (Conv):      ',np.sum(map_NEI_oil_completions_conv[:,:,:,iyear]))\n",
    "    print('All oil well comp (HF):        ',np.sum(map_NEI_oil_completions_HF[:,:,:,iyear]))\n",
    "    print('Oil production:                ',np.sum(map_NEI_oil_production[:,:,:,iyear]))\n",
    "    print('Wells Drilled:                 ',np.sum(map_NEI_oil_drilledwells[:,:,:,iyear]))\n",
    "    print(' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.7.5 Add the NEI data to the relevant Enverus Proxy Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add maps to relevant Enverus maps\n",
    "# add absolute values to the Enverus maps above (then the weighted calculations below can remain unchanged)\n",
    "# The same values are assigned to each month (e.g., no temporal resolution is applied to IL or IN data)\n",
    "# NOTE: Proxy maps need to be reloaded if this code is run more than once\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    for imonth in np.arange(0,num_months):\n",
    "        Map_EnvAllwell[:,:,iyear,imonth] += (1/12)*(map_NEI_oil_wells[0,:,:,iyear]+map_NEI_oil_wells[1,:,:,iyear])\n",
    "        Map_EnvOilProd[:,:,iyear,imonth] += (1/12)*(map_NEI_oil_production[0,:,:,iyear]+map_NEI_oil_production[1,:,:,iyear])\n",
    "        Map_EnvBasinOther[:,:,iyear,imonth] += (1/12)*(map_NEI_oil_production[0,:,:,iyear]+map_NEI_oil_production[1,:,:,iyear])\n",
    "        Map_EnvHFOilWell[:,:,iyear,imonth] += (1/12)*(map_NEI_oil_wells_HF[0,:,:,iyear]+map_NEI_oil_wells_HF[1,:,:,iyear])\n",
    "        Map_EnvConvOilWell[:,:,iyear,imonth] += (1/12)*(map_NEI_oil_wells_conv[0,:,:,iyear]+map_NEI_oil_wells_conv[1,:,:,iyear])\n",
    "        Map_EnvHFComp[:,:,iyear,imonth] += (1/12)*(map_NEI_oil_completions_HF[0,:,:,iyear]+map_NEI_oil_completions_HF[1,:,:,iyear])\n",
    "        Map_EnvConvComp[:,:,iyear,imonth] += (1/12)*(map_NEI_oil_completions_conv[0,:,:,iyear]+map_NEI_oil_completions_conv[1,:,:,iyear])\n",
    "        Map_EnvOilWellDrilled[:,:,iyear,imonth] += (1/12)*(map_NEI_oil_drilledwells[0,:,:,iyear]+map_NEI_oil_drilledwells[1,:,:,iyear])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Step 3. Read In EPA GHGI Data\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1. Prodcution and Exploration Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Emissions are in units of MT (= 1x10-6 Tg)\n",
    "\n",
    "names = pd.read_excel(EPA_Petr_inputfile, sheet_name = \"Production_CH4 (MT)\", usecols = \"A:AE\", skiprows = 3, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "EPA_emi_prod_Petr = pd.read_excel(EPA_Petr_inputfile, sheet_name = \"Production_CH4 (MT)\", usecols = \"A:AE\", skiprows = 5, names = colnames, nrows = 126)\n",
    "EPA_emi_prod_Petr= EPA_emi_prod_Petr.drop(columns = ['Emission\\nSource No.'])\n",
    "EPA_emi_prod_Petr.rename(columns={EPA_emi_prod_Petr.columns[0]:'Source'}, inplace=True)\n",
    "EPA_emi_prod_Petr['Source']= EPA_emi_prod_Petr['Source'].str.replace(r\"\\(\",\"- \")\n",
    "EPA_emi_prod_Petr['Source']= EPA_emi_prod_Petr['Source'].str.replace(r\"\\)\",\"\")\n",
    "EPA_emi_prod_Petr['Source']= EPA_emi_prod_Petr['Source'].str.replace(r'\"',\"\")\n",
    "EPA_emi_prod_Petr = EPA_emi_prod_Petr.fillna('')\n",
    "EPA_emi_prod_Petr = EPA_emi_prod_Petr.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_emi_prod_Petr.reset_index(inplace=True, drop=True)\n",
    "display(EPA_emi_prod_Petr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2. Read in Petroleum Transport "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Emissions are in units of MT (= 1x10-6 Tg)\n",
    "\n",
    "names = pd.read_excel(EPA_Petr_inputfile, sheet_name = \"Transportation Emissions\", usecols = \"A:AG\", skiprows = 32, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "EPA_emi_trans_Petr = pd.read_excel(EPA_Petr_inputfile, sheet_name = \"Transportation Emissions\", usecols = \"A:AG\", skiprows = 34, names = colnames, nrows = 20)\n",
    "EPA_emi_trans_Petr= EPA_emi_trans_Petr.drop(columns = ['Emission\\nSource No.', 'Unnamed: 2', 'Emission Units'])\n",
    "EPA_emi_trans_Petr.rename(columns={EPA_emi_trans_Petr.columns[0]:'Source'}, inplace=True)\n",
    "EPA_emi_trans_Petr = EPA_emi_trans_Petr.fillna('')\n",
    "EPA_emi_trans_Petr = EPA_emi_trans_Petr.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_emi_trans_Petr.reset_index(inplace=True, drop=True)\n",
    "display(EPA_emi_trans_Petr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3. Read in Petroleum Refining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Emissions are in units of MT (= 1x10-6 Tg)\n",
    "\n",
    "names = pd.read_excel(EPA_Petr_inputfile, sheet_name = \"Refinery Emissions\", usecols = \"A:AG\", skiprows = 7, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "EPA_emi_ref_Petr = pd.read_excel(EPA_Petr_inputfile, sheet_name = \"Refinery Emissions\", usecols = \"A:AG\", skiprows = 8, names = colnames, nrows = 29)\n",
    "EPA_emi_ref_Petr= EPA_emi_ref_Petr.drop(columns = ['Emission\\nSource No.', 'Scaling Factor for 1990-2009 ','Units'])\n",
    "EPA_emi_ref_Petr.rename(columns={EPA_emi_ref_Petr.columns[0]:'Source'}, inplace=True)\n",
    "EPA_emi_ref_Petr = EPA_emi_ref_Petr.fillna('')\n",
    "EPA_emi_ref_Petr = EPA_emi_ref_Petr.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_emi_ref_Petr.reset_index(inplace=True, drop=True)\n",
    "display(EPA_emi_ref_Petr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.4. Read in Total Petroleum Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in total production + exploration emissions (with methane reductions accounted for)\n",
    "# data are in kt\n",
    "\n",
    "names = pd.read_excel(EPA_Petr_inputfile, sheet_name = \"CH4 Summary\", usecols = \"A:AD\", skiprows = 4, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "EPA_emi_total_Petr = pd.read_excel(EPA_Petr_inputfile, sheet_name = \"CH4 Summary\", usecols = \"A:AD\", skiprows = 19, names = colnames, nrows = 5)\n",
    "EPA_emi_total_Petr.rename(columns={EPA_emi_total_Petr.columns[0]:'Source'}, inplace=True)\n",
    "EPA_emi_total_Petr = EPA_emi_total_Petr.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_emi_total_Petr.reset_index(inplace=True, drop=True)\n",
    "display(EPA_emi_total_Petr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.5. Split Emissions into Scaling Groups (from Petroleum_ProxyMapping.xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_year_idx = EPA_emi_prod_Petr.columns.get_loc(start_year)\n",
    "end_year_idx = EPA_emi_prod_Petr.columns.get_loc(end_year)+1\n",
    "ghgi_prod_groups = ghgi_prod_map['GHGI_Emi_Group'].unique()\n",
    "ghgi_trans_groups = ghgi_trans_map['GHGI_Emi_Group'].unique()\n",
    "ghgi_ref_groups = ghgi_ref_map['GHGI_Emi_Group'].unique()\n",
    "\n",
    "for igroup in np.arange(0,len(ghgi_prod_groups)):\n",
    "    vars()[ghgi_prod_groups[igroup]] = np.zeros(num_years)\n",
    "    source_temp = ghgi_prod_map.loc[ghgi_prod_map['GHGI_Emi_Group'] == ghgi_prod_groups[igroup], 'GHGI_Source']\n",
    "    pattern_temp  = '|'.join(source_temp) \n",
    "    emi_temp = EPA_emi_prod_Petr[EPA_emi_prod_Petr['Source'].str.contains(pattern_temp)]\n",
    "    vars()[ghgi_prod_groups[igroup]][:] = np.where(emi_temp.iloc[:,start_year_idx:] =='',[0],emi_temp.iloc[:,start_year_idx:]).sum(axis=0)/float(1000)\n",
    "    \n",
    "for igroup in np.arange(0,len(ghgi_trans_groups)):\n",
    "    vars()[ghgi_trans_groups[igroup]] = np.zeros(num_years)\n",
    "    source_temp = ghgi_trans_map.loc[ghgi_trans_map['GHGI_Emi_Group'] == ghgi_trans_groups[igroup], 'GHGI_Source']\n",
    "    pattern_temp  = '|'.join(source_temp) \n",
    "    emi_temp = EPA_emi_trans_Petr[EPA_emi_trans_Petr['Source'].str.contains(pattern_temp)]\n",
    "    vars()[ghgi_trans_groups[igroup]][:] = np.where(emi_temp.iloc[:,start_year_idx:] =='',[0],emi_temp.iloc[:,start_year_idx:]).sum(axis=0)/float(1000)\n",
    "    \n",
    "for igroup in np.arange(0,len(ghgi_ref_groups)):\n",
    "    vars()[ghgi_ref_groups[igroup]] = np.zeros(num_years)\n",
    "    source_temp = ghgi_ref_map.loc[ghgi_ref_map['GHGI_Emi_Group'] == ghgi_ref_groups[igroup], 'GHGI_Source']\n",
    "    pattern_temp  = '|'.join(source_temp) \n",
    "    emi_temp = EPA_emi_ref_Petr[EPA_emi_ref_Petr['Source'].str.contains(pattern_temp)]\n",
    "    vars()[ghgi_ref_groups[igroup]][:] = np.where(emi_temp.iloc[:,start_year_idx:] =='',[0],emi_temp.iloc[:,start_year_idx:]).sum(axis=0)/float(1000)\n",
    "\n",
    "    \n",
    "print('QA/QC: Check Production, Transport, Refining Emission Sum against GHGI Summary Emissions')\n",
    "for iyear in np.arange(0,num_years): \n",
    "    sum_emi = 0\n",
    "    for igroup in np.arange(0,len(ghgi_prod_groups)):\n",
    "        sum_emi += vars()[ghgi_prod_groups[igroup]][iyear]\n",
    "    for igroup in np.arange(0,len(ghgi_trans_groups)):\n",
    "        sum_emi += vars()[ghgi_trans_groups[igroup]][iyear]\n",
    "    for igroup in np.arange(0,len(ghgi_ref_groups)):\n",
    "        sum_emi += vars()[ghgi_ref_groups[igroup]][iyear]\n",
    "        \n",
    "    summary_emi = EPA_emi_total_Petr.iloc[0,iyear+1]+EPA_emi_total_Petr.iloc[1,iyear+1] +EPA_emi_total_Petr.iloc[2,iyear+1]+\\\n",
    "                    EPA_emi_total_Petr.iloc[3,iyear+1]\n",
    "    #Check 1 - make sure that the sums from all the regions equal the totals reported\n",
    "    diff1 = abs(sum_emi - summary_emi)/((sum_emi + summary_emi)/2)\n",
    "    print(summary_emi)\n",
    "    print(sum_emi)\n",
    "    if diff1 < 0.0001:\n",
    "        print('Year ', year_range[iyear],': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear],': FAIL (check Production & summary tabs): ', diff1,'%') \n",
    "        \n",
    "## Note: The numbers will not be exactly the same do to conversions and rounding in the Transport sector (between the \n",
    "## Transportation Emissions tab and the CH4 summary tab). This is not an error, just a difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "## Step 4. Grid Data (using spatial proxies)\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step. 4.1. Calculate the monthly weighted proxy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1.1 Assign the Appropriate Proxy Variable Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names on the left need to match the 'Petroleum_ProxyMapping' 'Proxy_Group' names (these are initialized in Step 2). \n",
    "# The names on the right are the variable names used to caluclate the proxies in this code.\n",
    "\n",
    "#Production segment\n",
    "Map_Allwell = Map_EnvAllwell\n",
    "Map_OilProd = Map_EnvOilProd\n",
    "Map_Basin220 = Map_Basin220\n",
    "Map_Allwell = Map_EnvAllwell\n",
    "Map_Basin220 = Map_EnvBasin220\n",
    "Map_Basin360 = Map_EnvBasin360\n",
    "Map_Basin395 = Map_EnvBasin395\n",
    "Map_Basin430 = Map_EnvBasin430\n",
    "Map_BasinOther = Map_EnvBasinOther\n",
    "Map_HFOilWell = Map_EnvHFOilWell\n",
    "Map_ConvOilWell = Map_EnvConvOilWell\n",
    "Map_HFComp = Map_EnvHFComp\n",
    "Map_ConvComp = Map_EnvConvComp\n",
    "Map_OilWellDrilled = Map_EnvOilWellDrilled\n",
    "Map_StateGOMOffshore = Map_EnvStateGOMOffshore\n",
    "Map_StatePacOffshore = Map_EnvStatePacOffshore\n",
    "#nongrid\n",
    "Map_Allwell_nongrid = Map_EnvAllwell_nongrid\n",
    "Map_OilProd_nongrid = Map_EnvOilProd_nongrid\n",
    "Map_Basin220_nongrid = Map_EnvBasin220_nongrid\n",
    "Map_Basin360_nongrid = Map_EnvBasin360_nongrid\n",
    "Map_Basin395_nongrid = Map_EnvBasin395_nongrid\n",
    "Map_Basin430_nongrid = Map_EnvBasin430_nongrid\n",
    "Map_BasinOther_nongrid = Map_EnvBasinOther_nongrid\n",
    "Map_HFOilWell_nongrid = Map_EnvHFOilWell_nongrid\n",
    "Map_ConvOilWell_nongrid = Map_EnvConvOilWell_nongrid\n",
    "Map_HFComp_nongrid = Map_EnvHFComp_nongrid\n",
    "Map_ConvComp_nongrid = Map_EnvConvComp_nongrid\n",
    "Map_OilWellDrilled_nongrid = Map_EnvOilWellDrilled_nongrid\n",
    "Map_StateGOMOffshore_nongrid = Map_EnvStateGOMOffshore_nongrid\n",
    "Map_StatePacOffshore_nongrid = Map_EnvStatePacOffshore_nongrid\n",
    "#Offshore\n",
    "Map_FedGOMOffshoreMajor = Map_GOADSmajor_emissions\n",
    "Map_FedGOMOffshoreMinor = Map_GOADSminor_emissions\n",
    "Map_FedGOMOffshore_Both = Map_FedGOMOffshoreMajor + Map_FedGOMOffshoreMinor\n",
    "Map_FedGOMOffshore_Both_nongrid = Map_FedGOMOffshoreMajor_nongrid + Map_FedGOMOffshoreMinor_nongrid\n",
    "Map_FedPacOffshore = Map_BSEEOffshore\n",
    "Map_PacOffshore = Map_FedPacOffshore+ Map_EnvStatePacOffshore #Pacfic map includes state and federal production\n",
    "\n",
    "#Transpot\n",
    "Map_TransRefining = Map_GHGRPRefineries\n",
    "Map_TransRefining_nongrid = Map_GHGRPRefineries_nongrid\n",
    "Map_TransOnshore = Map_OilProd \n",
    "Map_TransOnshore_nongrid = Map_OilProd_nongrid \n",
    "Map_TransOffshore = Map_StateGOMOffshore #need to switch this later on\n",
    "Map_TransOffshore_nongrid = Map_StateGOMOffshore_nongrid #need to switch this later on\n",
    "\n",
    "#Refining\n",
    "Map_Refineries = Map_GHGRPRefineries\n",
    "Map_Refineries_nongrid = Map_GHGRPRefineries_nongrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1.2 Calculate weighted arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate weighting arrays\n",
    "# Find the fraction of wells (or gas production) in each grid cell, relative to the total well counts (or gas prod) (on and off grid)\n",
    "# also weight by the number of days in each month\n",
    "\n",
    "\n",
    "for iyear in np.arange(0,num_years):\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "        month_days = month_day_leap\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        month_days = month_day_nonleap      \n",
    "    \n",
    "    #Production\n",
    "    print('Prod. Proxy Arrays: ', year_range[iyear])\n",
    "    for isource in np.arange(0,len(proxy_prod_map)): \n",
    "        if proxy_prod_map.loc[isource, 'Month_Flag'] == 1:\n",
    "            for imonth in np.arange(0, num_months):\n",
    "                #first weight by the number of days in each month (weighted map for month = month map * number of days in each month)\n",
    "                vars()[proxy_prod_map.loc[isource,'Proxy_Group']][:,:,iyear,imonth] *= month_days[imonth]\n",
    "                vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,imonth] *= month_days[imonth]\n",
    "            #then normalize\n",
    "            temp_sum = float(np.sum(vars()[proxy_prod_map.loc[isource,'Proxy_Group']][:,:,iyear,:]) + \\\n",
    "                             np.sum(vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,:]))\n",
    "            vars()[proxy_prod_map.loc[isource,'Proxy_Group']][:,:,iyear,:] = \\\n",
    "                data_fn.safe_div(vars()[proxy_prod_map.loc[isource,'Proxy_Group']][:,:,iyear,:], temp_sum)\n",
    "            vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,:] = \\\n",
    "                data_fn.safe_div(vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,:], temp_sum)\n",
    "            proxy_sum = np.sum(vars()[proxy_prod_map.loc[isource,'Proxy_Group']][:,:,iyear,:])+np.sum(vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,:])\n",
    "            if proxy_sum >1.0001 or proxy_sum <0.9999:\n",
    "                print('Check ', proxy_prod_map.loc[isource,'Proxy_Group'], ': ', proxy_sum)\n",
    "            else:\n",
    "                print('PASS')\n",
    "        else:\n",
    "            vars()[proxy_prod_groups[isource]][:,:,iyear] *= np.sum(month_days)\n",
    "            vars()[proxy_prod_groups[isource]+'_nongrid'][iyear] *= np.sum(month_days)  \n",
    "            temp_sum = float(np.sum(vars()[proxy_prod_groups[isource]][:,:,iyear]) + np.sum(vars()[proxy_prod_groups[isource]+'_nongrid'][iyear]))\n",
    "            vars()[proxy_prod_groups[isource]][:,:,iyear] = data_fn.safe_div(vars()[proxy_prod_groups[isource]][:,:,iyear], temp_sum)\n",
    "            vars()[proxy_prod_groups[isource]+'_nongrid'][iyear] = data_fn.safe_div(vars()[proxy_prod_groups[isource]+'_nongrid'][iyear], temp_sum)\n",
    "            proxy_sum = np.sum(vars()[proxy_prod_map.loc[isource,'Proxy_Group']][:,:,iyear])+np.sum(vars()[proxy_prod_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear])\n",
    "            if proxy_sum >1.0001 or proxy_sum <0.9999:\n",
    "                print('Check', proxy_prod_map.loc[isource,'Proxy_Group'], ': ', proxy_sum)\n",
    "            else:\n",
    "                print('PASS')\n",
    "    \n",
    "    #Transport    \n",
    "    print('Transport Proxy Arrays: ', year_range[iyear])\n",
    "    for isource in np.arange(0,len(proxy_trans_map)): \n",
    "        if proxy_trans_map.loc[isource, 'Month_Flag'] == 1:\n",
    "            for imonth in np.arange(0, num_months):\n",
    "                #first weight by the number of days in each month (weighted map for month = month map * number of days in each month)\n",
    "                vars()[proxy_trans_map.loc[isource,'Proxy_Group']][:,:,iyear,imonth] *= month_days[imonth]\n",
    "                vars()[proxy_trans_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,imonth] *= month_days[imonth]\n",
    "            #then normalize\n",
    "            temp_sum = float(np.sum(vars()[proxy_trans_map.loc[isource,'Proxy_Group']][:,:,iyear,:]) + \\\n",
    "                    np.sum(vars()[proxy_trans_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,:]))\n",
    "            vars()[proxy_trans_map.loc[isource,'Proxy_Group']][:,:,iyear,:] = \\\n",
    "                data_fn.safe_div(vars()[proxy_trans_map.loc[isource,'Proxy_Group']][:,:,iyear,:], temp_sum)\n",
    "            vars()[proxy_trans_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,:] = \\\n",
    "                data_fn.safe_div(vars()[proxy_trans_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,:], temp_sum)\n",
    "            proxy_sum = np.sum(vars()[proxy_trans_map.loc[isource,'Proxy_Group']][:,:,iyear,:])+np.sum(vars()[proxy_trans_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,:])\n",
    "            if proxy_sum >1.0001 or proxy_sum <0.9999:\n",
    "                print('Check ', proxy_trans_map.loc[isource,'Proxy_Group'], ': ', proxy_sum)\n",
    "            else:\n",
    "                print('PASS')\n",
    "        else:\n",
    "            vars()[proxy_trans_map.loc[isource,'Proxy_Group']][:,:,iyear] *= np.sum(month_days)\n",
    "            vars()[proxy_trans_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear] *= np.sum(month_days)  \n",
    "            temp_sum = float(np.sum(vars()[proxy_trans_map.loc[isource,'Proxy_Group']][:,:,iyear]) + np.sum(vars()[proxy_trans_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear]))\n",
    "            vars()[proxy_trans_map.loc[isource,'Proxy_Group']][:,:,iyear] = data_fn.safe_div(vars()[proxy_trans_map.loc[isource,'Proxy_Group']][:,:,iyear], temp_sum)\n",
    "            vars()[proxy_trans_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear] = data_fn.safe_div(vars()[proxy_trans_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear], temp_sum)\n",
    "            proxy_sum = np.sum(vars()[proxy_trans_map.loc[isource,'Proxy_Group']][:,:,iyear])+np.sum(vars()[proxy_trans_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear])\n",
    "            if proxy_sum >1.0001 or proxy_sum <0.9999:\n",
    "                print('Check', proxy_trans_map.loc[isource,'Proxy_Group'], ': ', proxy_sum)\n",
    "            else:\n",
    "                print('PASS')\n",
    "                \n",
    "    #refining\n",
    "    print('Refining Proxy Arrays: ', year_range[iyear])\n",
    "    for isource in np.arange(0,len(proxy_ref_map)): \n",
    "        if proxy_ref_map.loc[isource, 'Month_Flag'] == 1:\n",
    "            for imonth in np.arange(0, num_months):\n",
    "                #first weight by the number of days in each month (weighted map for month = month map * number of days in each month)\n",
    "                vars()[proxy_ref_map.loc[isource,'Proxy_Group']][:,:,iyear,imonth] *= month_days[imonth]\n",
    "                vars()[proxy_ref_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,imonth] *= month_days[imonth]\n",
    "            #then normalize\n",
    "            temp_sum = float(np.sum(vars()[proxy_ref_map.loc[isource,'Proxy_Group']][:,:,iyear,:]) + \\\n",
    "                         np.sum(vars()[proxy_ref_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,:]))\n",
    "            vars()[proxy_ref_map.loc[isource,'Proxy_Group']][:,:,iyear,:] = \\\n",
    "                data_fn.safe_div(vars()[proxy_ref_map.loc[isource,'Proxy_Group']][:,:,iyear,:], temp_sum)\n",
    "            vars()[proxy_ref_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,:] = \\\n",
    "                data_fn.safe_div(vars()[proxy_ref_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,:], temp_sum)\n",
    "            proxy_sum = np.sum(vars()[proxy_ref_map.loc[isource,'Proxy_Group']][:,:,iyear,:])+np.sum(vars()[proxy_ref_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear,:])\n",
    "            if proxy_sum >1.0001 or proxy_sum <0.9999:\n",
    "                print('Check ', proxy_ref_map.loc[isource,'Proxy_Group'], ': ', proxy_sum)\n",
    "            else:\n",
    "                print('PASS')\n",
    "\n",
    "        else:\n",
    "            vars()[proxy_ref_map.loc[isource,'Proxy_Group']][:,:,iyear] *= np.sum(month_days)\n",
    "            vars()[proxy_ref_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear] *= np.sum(month_days)  \n",
    "            temp_sum = float(np.sum(vars()[proxy_ref_map.loc[isource,'Proxy_Group']][:,:,iyear]) + np.sum(vars()[proxy_ref_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear]))\n",
    "            vars()[proxy_ref_map.loc[isource,'Proxy_Group']][:,:,iyear] = data_fn.safe_div(vars()[proxy_ref_map.loc[isource,'Proxy_Group']][:,:,iyear], temp_sum)\n",
    "            vars()[proxy_ref_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear] = data_fn.safe_div(vars()[proxy_ref_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear], temp_sum)\n",
    "            proxy_sum = np.sum(vars()[proxy_ref_map.loc[isource,'Proxy_Group']][:,:,iyear])+np.sum(vars()[proxy_ref_map.loc[isource,'Proxy_Group']+'_nongrid'][iyear])\n",
    "            if proxy_sum >1.0001 or proxy_sum <0.9999:\n",
    "                print('Check', proxy_ref_map.loc[isource,'Proxy_Group'], ': ', proxy_sum)\n",
    "            else:\n",
    "                print('PASS')\n",
    "\n",
    "    \n",
    "    #calculate average map weighted by well counts and production\n",
    "    Map_Both[:,:,iyear,:] = 0.5 * (Map_Allwell[:,:,iyear,:]+Map_OilProd[:,:,iyear,:])  \n",
    "    Map_Both_nongrid[iyear,:] = 0.5 * (Map_Allwell_nongrid[iyear,:]+Map_OilProd_nongrid[iyear,:])  \n",
    "    #set 'not-mapped' array to 1 so that the emissions will be included in the calculated total\n",
    "    Map_not_mapped[:,:,iyear,:] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step. 4.2. Allocate to CONUS 0.1x0.1 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each of the production, transport, and refining segments...\n",
    "# 1) make flux array with correct dimensions\n",
    "# 2) weight monthly data by days in month (or year)\n",
    "# 3) caluclate flux as Flux = GHGI emissions * Map\n",
    "\n",
    "Emissions = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Emissions_expl = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Emissions_prod = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Emissions_trans = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Emissions_ref = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Emissions_expl_nongrid = np.zeros([num_years,num_months])\n",
    "Emissions_prod_nongrid = np.zeros([num_years,num_months])\n",
    "Emissions_trans_nongrid = np.zeros([num_years,num_months])\n",
    "Emissions_ref_nongrid = np.zeros([num_years,num_months])\n",
    "Emi_not_mapped_sum = np.zeros(num_years)\n",
    "DEBUG=1\n",
    "if DEBUG==1:\n",
    "    total_sum = np.zeros(num_years)\n",
    "    proxy_val= np.zeros(num_years)\n",
    "    ghgi_val= np.zeros(num_years)\n",
    "    \n",
    "#Production\n",
    "for igroup in np.arange(0,len(proxy_prod_map)):\n",
    "    if proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "        if proxy_prod_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "            vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "            vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'] = np.zeros([num_years,num_months])\n",
    "            for iyear in np.arange(0,num_years):\n",
    "                vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,:] += \\\n",
    "                vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][iyear] * vars()[proxy_prod_map.loc[igroup,'Proxy_Group']][:,:,iyear,:]\n",
    "                vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear,:] += vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][iyear] * vars()[proxy_prod_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear,:]\n",
    "            \n",
    "                for imonth in np.arange(0,num_months):\n",
    "                    if proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_OilWellExp' or \\\n",
    "                        proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_ConvCompExp' or \\\n",
    "                        proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_HFCompExp' or \\\n",
    "                        proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_OilWellDrilledExp':\n",
    "                        Emissions[:,:,iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth]\n",
    "                        Emissions_expl_nongrid[iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear,imonth]\n",
    "                        Emissions_expl[:,:,iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth]\n",
    "                    else:\n",
    "                        Emissions[:,:,iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth]\n",
    "                        Emissions_prod_nongrid[iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear,imonth]\n",
    "                        Emissions_prod[:,:,iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth]\n",
    "\n",
    "                if DEBUG==1:\n",
    "                    proxy_val[iyear] = np.sum(vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,:])+\\\n",
    "                                     np.sum(vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear,:])\n",
    "                    ghgi_val[iyear] = np.sum(vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][iyear])\n",
    "                    total_sum[iyear] += proxy_val[iyear]\n",
    "\n",
    "        else:\n",
    "            vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "            vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "            for iyear in np.arange(0,num_years):\n",
    "                vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][iyear] * vars()[proxy_prod_map.loc[igroup,'Proxy_Group']][:,:,iyear]\n",
    "                vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear] += vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][iyear] * vars()[proxy_prod_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear]\n",
    "                if proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_OilWellExp' or \\\n",
    "                    proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_ConvCompExp' or \\\n",
    "                    proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_HFCompExp' or \\\n",
    "                    proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] == 'Emi_OilWellDrilledExp':\n",
    "                    Emissions[:,:,iyear,:] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]/num_months\n",
    "                    Emissions_expl[:,:,iyear,:] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]/num_months\n",
    "                    Emissions_expl_nongrid[iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear]/num_months\n",
    "                else:\n",
    "                    Emissions[:,:,iyear,:] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]/num_months\n",
    "                    Emissions_prod[:,:,iyear,:] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]/num_months\n",
    "                    Emissions_prod_nongrid[iyear,imonth] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear]/num_months\n",
    " \n",
    "                if DEBUG==1:\n",
    "                    proxy_val[iyear] = np.sum(vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])+\\\n",
    "                             np.sum(vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear])\n",
    "                    ghgi_val[iyear] = np.sum(vars()[proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][iyear])\n",
    "                    total_sum[iyear] += proxy_val[iyear]\n",
    "\n",
    "        if DEBUG==1:\n",
    "            #these two variables should be the same if code is working properly\n",
    "            print(igroup, proxy_val[:])\n",
    "            print(igroup, ghgi_val[:])\n",
    "\n",
    "    else:\n",
    "        ###NOTE: currently all non-mapped emissions are in the production segment\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            Emissions_prod_nongrid[iyear,:] += (1/12)*Emi_not_mapped[iyear]\n",
    "\n",
    "# Transport\n",
    "for igroup in np.arange(0,len(proxy_trans_map)):\n",
    "    if proxy_trans_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "        vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "        vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'] = np.zeros([num_years,num_months])\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,:] += vars()[proxy_trans_map.loc[igroup,'GHGI_Emi_Group']][iyear] * vars()[proxy_trans_map.loc[igroup,'Proxy_Group']][:,:,iyear,:]\n",
    "            vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear,:] += vars()[proxy_trans_map.loc[igroup,'GHGI_Emi_Group']][iyear] * vars()[proxy_trans_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear,:]\n",
    "            for imonth in np.arange(0,num_months):\n",
    "                if proxy_trans_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "                    Emissions[:,:,iyear,imonth] += vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth]\n",
    "                    Emissions_trans_nongrid[iyear,imonth] += vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear,imonth]\n",
    "                    Emissions_trans[:,:,iyear,imonth] += vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth]\n",
    "\n",
    "    else:\n",
    "        vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += vars()[proxy_trans_map.loc[igroup,'GHGI_Emi_Group']][iyear] * vars()[proxy_trans_map.loc[igroup,'Proxy_Group']][:,:,iyear]\n",
    "            vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear] += vars()[proxy_trans_map.loc[igroup,'GHGI_Emi_Group']][iyear] * vars()[proxy_trans_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear]\n",
    "            for imonth in np.arange(0,num_months):\n",
    "                if proxy_trans_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "                    Emissions[:,:,iyear,imonth] += (vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]/num_months) #distribute emissions evenly over each month\n",
    "                    Emissions_trans_nongrid[iyear,imonth] += vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear]/num_months\n",
    "                    Emissions_trans[:,:,iyear,imonth] += (vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]/num_months) #distribute emissions evenly over each month\n",
    "\n",
    "# Refining\n",
    "for igroup in np.arange(0,len(proxy_ref_map)):\n",
    "    if proxy_ref_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "        vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "        vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'] = np.zeros([num_years,num_months])\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,:] += vars()[proxy_ref_map.loc[igroup,'GHGI_Emi_Group']][iyear] * vars()[proxy_ref_map.loc[igroup,'Proxy_Group']][:,:,iyear,:]\n",
    "            vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear,:] += vars()[proxy_ref_map.loc[igroup,'GHGI_Emi_Group']][iyear] * vars()[proxy_ref_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear,:]\n",
    "            for imonth in np.arange(0,num_months):\n",
    "                if proxy_ref_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "                    Emissions[:,:,iyear,imonth] += vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth]\n",
    "                    Emissions_ref_nongrid[iyear,imonth] += vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear, imonth]\n",
    "                    Emissions_ref[:,:,iyear,imonth] += vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth]\n",
    "\n",
    "    else:\n",
    "        vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "        vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += vars()[proxy_ref_map.loc[igroup,'GHGI_Emi_Group']][iyear] * vars()[proxy_ref_map.loc[igroup,'Proxy_Group']][:,:,iyear]\n",
    "            vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear] += vars()[proxy_ref_map.loc[igroup,'GHGI_Emi_Group']][iyear] * vars()[proxy_ref_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear]\n",
    "            for imonth in np.arange(0,num_months):\n",
    "                if proxy_ref_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "                    Emissions[:,:,iyear,imonth] += (vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]/num_months)\n",
    "                    Emissions_ref_nongrid[iyear,imonth] += vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']+'_nongrid'][iyear]/num_months\n",
    "                    Emissions_ref[:,:,iyear,imonth] += (vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]/num_months)\n",
    "\n",
    "\n",
    "# QA/QC gridded emissions\n",
    "# Check sum of all gridded emissions + emissions not included in gridding (e.g., AK), and other non-gridded areas\n",
    "print('QA/QC #1: Check weighted emissions against GHGI')   \n",
    "for iyear in np.arange(0,num_years):\n",
    "    calc_emi = 0\n",
    "    summary_emi = EPA_emi_total_Petr.iloc[0,iyear+1]+EPA_emi_total_Petr.iloc[1,iyear+1] +EPA_emi_total_Petr.iloc[2,iyear+1]+\\\n",
    "    EPA_emi_total_Petr.iloc[3,iyear+1]\n",
    "    \n",
    "    for igroup in np.arange(0,len(proxy_prod_map)):\n",
    "        if proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "            if proxy_prod_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "                calc_emi += np.sum(vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,:])\n",
    "            else:\n",
    "                calc_emi += np.sum(vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])\n",
    "    for igroup in np.arange(0,len(proxy_trans_map)):\n",
    "        if proxy_trans_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "            calc_emi += np.sum(vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,:])\n",
    "        else:\n",
    "            calc_emi += np.sum(vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])\n",
    "    for igroup in np.arange(0,len(proxy_ref_map)):\n",
    "        if proxy_ref_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "            calc_emi += np.sum(vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,:])\n",
    "        else:\n",
    "            calc_emi += np.sum(vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])           \n",
    "    \n",
    "    calc_emi += np.sum(Emissions_expl_nongrid[iyear,:]) +np.sum(Emissions_prod_nongrid[iyear,:])+\\\n",
    "                np.sum(Emissions_trans_nongrid[iyear,:])+np.sum(Emissions_ref_nongrid[iyear,:])\n",
    "    \n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    #check two\n",
    "    calc_emi2 =  np.sum(Emissions_prod[:,:,iyear,:]) + np.sum(Emissions_trans[:,:,iyear,:]) +\\\n",
    "                 np.sum(Emissions_ref[:,:,iyear,:])+np.sum(Emissions_expl[:,:,iyear,:])+\\\n",
    "                 np.sum(Emissions_expl_nongrid[iyear,:])+np.sum(Emissions_prod_nongrid[iyear,:])+\\\n",
    "                 np.sum(Emissions_trans_nongrid[iyear,:])+np.sum(Emissions_ref_nongrid[iyear,:])\n",
    "    if DEBUG==1:\n",
    "        print(calc_emi)\n",
    "        print(calc_emi2)\n",
    "        print(summary_emi)\n",
    "    if diff < 0.0001:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2.2 Save gridded emissions (kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save gridded emissions for each gridding group - for extension\n",
    "\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(grid_emi_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "unique_groups2 = (np.unique(proxy_prod_map['GHGI_Emi_Group']))\n",
    "unique_groups2 = list(unique_groups2[unique_groups2 != 'Emi_not_mapped'])\n",
    "unique_groups3 = list(np.unique(proxy_trans_map['GHGI_Emi_Group']))\n",
    "unique_groups4 = list(np.unique(proxy_ref_map['GHGI_Emi_Group']))\n",
    "unique_groups = unique_groups2+unique_groups3+unique_groups4\n",
    "print(unique_groups2)\n",
    "\n",
    "nc_out = Dataset(grid_emi_outputfile, 'r+', format='NETCDF4')\n",
    "\n",
    "for igroup in np.arange(0,len(unique_groups)):\n",
    "    print('Ext_'+unique_groups[igroup])\n",
    "    if len(np.shape(vars()['Ext_'+unique_groups[igroup]])) ==4:\n",
    "        ghgi_temp = np.sum(vars()['Ext_'+unique_groups[igroup]],axis=3) #sum month data if data is monthly\n",
    "    else:\n",
    "        ghgi_temp = vars()['Ext_'+unique_groups[igroup]]\n",
    "\n",
    "    # Write data to netCDF\n",
    "    data_out = nc_out.createVariable('Ext_'+unique_groups[igroup], 'f8', ('lat', 'lon','year'), zlib=True)\n",
    "    data_out[:,:,:] = ghgi_temp[:,:,:]\n",
    "\n",
    "#save nongrid data to calculate non-grid fraction extension\n",
    "data_out = nc_out.createVariable('Emissions_expl_nongrid', 'f8', ('year'), zlib=True)  \n",
    "data_out[:] = np.sum(Emissions_expl_nongrid[:,:],axis=1)\n",
    "\n",
    "#save nongrid data to calculate non-grid fraction extension\n",
    "data_out = nc_out.createVariable('Emissions_prod_nongrid', 'f8', ('year'), zlib=True)  \n",
    "data_out[:] = np.sum(Emissions_prod_nongrid[:,:],axis=1)\n",
    "\n",
    "#save nongrid data to calculate non-grid fraction extension\n",
    "data_out = nc_out.createVariable('Emissions_trans_nongrid', 'f8', ('year'), zlib=True)  \n",
    "data_out[:] = np.sum(Emissions_trans_nongrid[:,:],axis=1)\n",
    "\n",
    "#save nongrid data to calculate non-grid fraction extension\n",
    "data_out = nc_out.createVariable('Emissions_ref_nongrid', 'f8', ('year'), zlib=True)  \n",
    "data_out[:] = np.sum(Emissions_ref_nongrid[:,:],axis=1)\n",
    "\n",
    "nc_out.close()\n",
    "\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded emissions (kt) written to file: {}\" .format(os.getcwd())+grid_emi_outputfile)\n",
    "print(' ')\n",
    "\n",
    "del data_out, ghgi_temp, nc_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3 Calculate Gridded Fluxes (molec/s/cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 2 -- Calculate fluxes (molec./s/cm2)\n",
    "#Convert emissions to emission flux\n",
    "# conversion: kt emissions to molec/cm2/s flux\n",
    "DEBUG = 1\n",
    "\n",
    "### NOTE: Individual Flux arrays are not summing correctly - but are not reported anywhere\n",
    "\n",
    "#Initialize arrays\n",
    "check_sum = np.zeros([num_years])\n",
    "check_sum_annual = np.zeros([num_years])\n",
    "check_sum_annual2 = np.zeros([num_years])\n",
    "check_sum_annual3= np.zeros([num_years])\n",
    "check_sum_annual4 = np.zeros([num_years])\n",
    "Flux_Emissions_Total = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Flux_Emissions_Total_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Flux_Emissions_Expl = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Flux_Emissions_Expl_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Flux_Emissions_Prod = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Flux_Emissions_Prod_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Flux_Emissions_Trans = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Flux_Emissions_Trans_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Flux_Emissions_Ref = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "Flux_Emissions_Ref_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "\n",
    "for igroup in np.arange(0,len(proxy_prod_map)):\n",
    "    vars()['Flux_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_annual'] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "for igroup in np.arange(0,len(proxy_trans_map)):\n",
    "    vars()['Flux_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']+'_annual'] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "for igroup in np.arange(0,len(proxy_ref_map)):\n",
    "    vars()['Flux_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']+'_annual'] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "\n",
    "\n",
    "#Calculate fluxes\n",
    "for iyear in np.arange(0,num_years):\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "        month_days = month_day_leap\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        month_days = month_day_nonleap \n",
    "    \n",
    "    # calculate fluxes for annual data  (=kt * grams/kt *molec/mol *mol/g *s^-1 * cm^-2)\n",
    "    conversion_factor_annual = 10**9 * Avogadro / float(Molarch4 * np.sum(month_days) * 24 * 60 *60) / area_matrix_01\n",
    "    for igroup in np.arange(0,len(proxy_prod_map)):\n",
    "        if proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "            if proxy_prod_map.loc[igroup, 'Month_Flag'] == 0:\n",
    "                vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] *= conversion_factor_annual\n",
    "                vars()['Flux_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_annual'][:,:,iyear] = vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]\n",
    "            \n",
    "    for igroup in np.arange(0,len(proxy_trans_map)):\n",
    "        if proxy_trans_map.loc[igroup, 'Month_Flag'] == 0:\n",
    "            vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] *= conversion_factor_annual\n",
    "            vars()['Flux_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']+'_annual'][:,:,iyear] = vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]\n",
    "    for igroup in np.arange(0,len(proxy_ref_map)):\n",
    "        if proxy_ref_map.loc[igroup, 'Month_Flag'] == 0:\n",
    "            vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] *= conversion_factor_annual\n",
    "            vars()['Flux_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']+'_annual'][:,:,iyear] = vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear]\n",
    "            \n",
    "    for imonth in np.arange(0, num_months):\n",
    "        conversion_factor_month = 10**9 * Avogadro / float(Molarch4 * month_days[imonth] * 24 * 60 *60) / area_matrix_01\n",
    "        conv_factor2 = month_days[imonth]/year_days\n",
    "        Flux_Emissions_Total[:,:,iyear,imonth] = Emissions[:,:,iyear,imonth]*conversion_factor_month\n",
    "        Flux_Emissions_Total_annual[:,:,iyear] += Flux_Emissions_Total[:,:,iyear,imonth]*conv_factor2\n",
    "        Flux_Emissions_Prod[:,:,iyear,imonth] = Emissions_prod[:,:,iyear,imonth]*conversion_factor_month\n",
    "        Flux_Emissions_Prod_annual[:,:,iyear] += Flux_Emissions_Prod[:,:,iyear,imonth]*conv_factor2\n",
    "        Flux_Emissions_Expl[:,:,iyear,imonth] = Emissions_expl[:,:,iyear,imonth]*conversion_factor_month\n",
    "        Flux_Emissions_Expl_annual[:,:,iyear] += Flux_Emissions_Expl[:,:,iyear,imonth]*conv_factor2\n",
    "        Flux_Emissions_Trans[:,:,iyear,imonth] = Emissions_trans[:,:,iyear,imonth]*conversion_factor_month\n",
    "        Flux_Emissions_Trans_annual[:,:,iyear] += Flux_Emissions_Trans[:,:,iyear,imonth]*conv_factor2\n",
    "        Flux_Emissions_Ref[:,:,iyear,imonth] = Emissions_ref[:,:,iyear,imonth]*conversion_factor_month\n",
    "        Flux_Emissions_Ref_annual[:,:,iyear] += Flux_Emissions_Ref[:,:,iyear,imonth]*conv_factor2\n",
    "        for igroup in np.arange(0,len(proxy_prod_map)):\n",
    "            if proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "                if proxy_prod_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "                    vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth] *= conversion_factor_month\n",
    "                    vars()['Flux_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_annual'][:,:,iyear] += vars()['Ext_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth]*conv_factor2\n",
    "        for igroup in np.arange(0,len(proxy_trans_map)):\n",
    "            if proxy_trans_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "                vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth] *= conversion_factor_month\n",
    "                vars()['Flux_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']+'_annual'][:,:,iyear] += vars()['Ext_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth]*conv_factor2\n",
    "        for igroup in np.arange(0,len(proxy_ref_map)):\n",
    "            if proxy_ref_map.loc[igroup, 'Month_Flag'] == 1:\n",
    "                vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth] *= conversion_factor_month\n",
    "                vars()['Flux_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']+'_annual'][:,:,iyear] += vars()['Ext_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear,imonth]*conv_factor2\n",
    "        \n",
    "    \n",
    "        check_sum[iyear] += np.sum(Flux_Emissions_Total[:,:,iyear,imonth]/conversion_factor_month)\n",
    "    check_sum_annual[iyear] += np.sum(Flux_Emissions_Total_annual[:,:,iyear]/conversion_factor_annual)\n",
    "    check_sum_annual2[iyear] += np.sum(Flux_Emissions_Expl_annual[:,:,iyear]/conversion_factor_annual)\n",
    "    check_sum_annual2[iyear] += np.sum(Flux_Emissions_Prod_annual[:,:,iyear]/conversion_factor_annual)\n",
    "    check_sum_annual3[iyear] += np.sum(Flux_Emissions_Trans_annual[:,:,iyear]/conversion_factor_annual)\n",
    "    check_sum_annual4[iyear] += np.sum(Flux_Emissions_Ref_annual[:,:,iyear]/conversion_factor_annual)\n",
    "\n",
    "print(' ')\n",
    "print('QA/QC #2: Check final gridded fluxes against GHGI')  \n",
    "# for the sum, check the converted annual emissions (convert back from flux) plus all the non-gridded emissions\n",
    "for iyear in np.arange(0,num_years):\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "        month_days = month_day_leap\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        month_days = month_day_nonleap \n",
    "\n",
    "    conversion_factor_annual = 10**9 * Avogadro / float(Molarch4 * np.sum(month_days) * 24 * 60 *60) / area_matrix_01\n",
    "    \n",
    "    calc_emi = check_sum_annual[iyear] + np.sum(Emissions_expl_nongrid[iyear,:]) +\\\n",
    "                np.sum(Emissions_prod_nongrid[iyear,:]) +np.sum(Emissions_trans_nongrid[iyear,:]) +np.sum(Emissions_ref_nongrid[iyear,:]) \n",
    "    calc_emi2 = check_sum_annual2[iyear] + check_sum_annual3[iyear] +check_sum_annual4[iyear] +\\\n",
    "                 np.sum(Emissions_expl_nongrid[iyear,:]) +\\\n",
    "                np.sum(Emissions_prod_nongrid[iyear,:]) +np.sum(Emissions_trans_nongrid[iyear,:]) +np.sum(Emissions_ref_nongrid[iyear,:]) \n",
    "    calc_emi3 = 0\n",
    "    for igroup in np.arange(0,len(proxy_prod_map)):\n",
    "        if proxy_prod_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "            calc_emi3 += np.sum(vars()['Flux_'+proxy_prod_map.loc[igroup,'GHGI_Emi_Group']+'_annual'][:,:,iyear]/conversion_factor_annual)\n",
    "    for igroup in np.arange(0,len(proxy_trans_map)):\n",
    "        calc_emi3 += np.sum(vars()['Flux_'+proxy_trans_map.loc[igroup,'GHGI_Emi_Group']+'_annual'][:,:,iyear]/conversion_factor_annual)\n",
    "    for igroup in np.arange(0,len(proxy_ref_map)):\n",
    "        calc_emi3 += np.sum(vars()['Flux_'+proxy_ref_map.loc[igroup,'GHGI_Emi_Group']+'_annual'][:,:,iyear]/conversion_factor_annual)          \n",
    "    calc_emi3+=np.sum(Emissions_expl_nongrid[iyear,:]) +\\\n",
    "                np.sum(Emissions_prod_nongrid[iyear,:]) +np.sum(Emissions_trans_nongrid[iyear,:]) +np.sum(Emissions_ref_nongrid[iyear,:]) \n",
    "\n",
    "    \n",
    "    summary_emi = EPA_emi_total_Petr.iloc[0,iyear+1]+EPA_emi_total_Petr.iloc[1,iyear+1]+EPA_emi_total_Petr.iloc[2,iyear+1]+\\\n",
    "                                      EPA_emi_total_Petr.iloc[3,iyear+1]\n",
    "    if DEBUG ==1:\n",
    "        print(calc_emi)\n",
    "        print(calc_emi2)\n",
    "        print(calc_emi3)\n",
    "        print(summary_emi)\n",
    "    \n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0001:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 5. Write gridded (0.1⁰x0.1⁰) data to netCDF files.\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and write netCDF files (flux units in molec/s/cm2)\n",
    "\n",
    "data_IO_fn.initialize_netCDF(gridded_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "data_IO_fn.initialize_netCDF(gridded_monthly_outputfile, netCDF_description_m, 1, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write the Data to netCDF\n",
    "nc_out = Dataset(gridded_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Total_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded annual petroleum system fluxes written to file: {}\" .format(os.getcwd())+gridded_outputfile)\n",
    "print('')\n",
    "\n",
    "nc_out = Dataset(gridded_monthly_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:,:] = Flux_Emissions_Total\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded monthly petroleum system fluxes written to file: {}\" .format(os.getcwd())+gridded_monthly_outputfile)\n",
    "print('')\n",
    "\n",
    "\n",
    "#Write Exploration Data\n",
    "data_IO_fn.initialize_netCDF(gridded_expl_outputfile, netCDF_description_expl, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "data_IO_fn.initialize_netCDF(gridded_monthly_expl_outputfile, netCDF_description_expl_m, 1, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write the Data to netCDF\n",
    "nc_out = Dataset(gridded_expl_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Expl_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded annual exploration fluxes written to file: {}\" .format(os.getcwd())+gridded_expl_outputfile)\n",
    "print('')\n",
    "\n",
    "nc_out = Dataset(gridded_monthly_expl_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:,:] = Flux_Emissions_Expl\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded monthly exploration fluxes written to file: {}\" .format(os.getcwd())+gridded_monthly_expl_outputfile)\n",
    "print('')\n",
    "\n",
    "\n",
    "#Write Production Data\n",
    "data_IO_fn.initialize_netCDF(gridded_prod_outputfile, netCDF_description_prod, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "data_IO_fn.initialize_netCDF(gridded_monthly_prod_outputfile, netCDF_description_prod_m, 1, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write the Data to netCDF\n",
    "nc_out = Dataset(gridded_prod_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Prod_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded annual production fluxes written to file: {}\" .format(os.getcwd())+gridded_prod_outputfile)\n",
    "print('')\n",
    "\n",
    "nc_out = Dataset(gridded_monthly_prod_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:,:] = Flux_Emissions_Prod\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded monthly production fluxes written to file: {}\" .format(os.getcwd())+gridded_monthly_prod_outputfile)\n",
    "print('')\n",
    "\n",
    "\n",
    "\n",
    "#Write Transport Data\n",
    "data_IO_fn.initialize_netCDF(gridded_trans_outputfile, netCDF_description_trans, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "data_IO_fn.initialize_netCDF(gridded_monthly_trans_outputfile, netCDF_description_trans_m, 1, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write the Data to netCDF\n",
    "nc_out = Dataset(gridded_trans_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Trans_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded annual transport fluxes written to file: {}\" .format(os.getcwd())+gridded_trans_outputfile)\n",
    "print('')\n",
    "\n",
    "nc_out = Dataset(gridded_monthly_trans_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:,:] = Flux_Emissions_Trans\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded monthly transport fluxes written to file: {}\" .format(os.getcwd())+gridded_monthly_trans_outputfile)\n",
    "print('')\n",
    "\n",
    "\n",
    "#Write Refining Data\n",
    "data_IO_fn.initialize_netCDF(gridded_ref_outputfile, netCDF_description_ref, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "data_IO_fn.initialize_netCDF(gridded_monthly_ref_outputfile, netCDF_description_ref_m, 1, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "# Write the Data to netCDF\n",
    "nc_out = Dataset(gridded_ref_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Ref_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded annual refining fluxes written to file: {}\" .format(os.getcwd())+gridded_ref_outputfile)\n",
    "print('')\n",
    "\n",
    "nc_out = Dataset(gridded_monthly_ref_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:,:] = Flux_Emissions_Ref\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded monthly refining fluxes written to file: {}\" .format(os.getcwd())+gridded_monthly_ref_outputfile)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 6. Plot Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Plot Annual Emission Fluxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.1 Total Petroleum System Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot annual emissions for each year (function converts from molec/cm2/s to Mg/year/km2)\n",
    "scale_max = 10\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_str, scale_max, save_flag, save_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.2 Exploration/Production Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot annual emissions for each year (function converts from molec/cm2/s to Mg/year/km2)\n",
    "scale_max = 10\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Expl_annual, Lat_01, Lon_01, year_range, title_expl_str, scale_max,save_flag,save_outfile)\n",
    "\n",
    "scale_max = 10\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Prod_annual, Lat_01, Lon_01, year_range, title_prod_str, scale_max,save_flag,save_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.3 Transport Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot annual emissions for each year (function converts from molec/cm2/s to Mg/year/km2)\n",
    "scale_max = 10\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Trans_annual, Lat_01, Lon_01, year_range, title_trans_str, scale_max,save_flag, save_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.4 Refining Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot annual emissions for each year (function converts from molec/cm2/s to Mg/year/km2)\n",
    "scale_max = 10\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Ref_annual, Lat_01, Lon_01, year_range, title_ref_str, scale_max,save_flag, save_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Plot Difference Between First and Last Inventory Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.1 Total Petroleum System Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot difference between last and first year\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_diff_str,save_flag, save_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.2 Exploration/Production Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot difference between last and first year\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Expl_annual, Lat_01, Lon_01, year_range, title_diff_expl_str,save_flag, save_outfile)\n",
    "\n",
    "# Plot difference between last and first year\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Prod_annual, Lat_01, Lon_01, year_range, title_diff_prod_str,save_flag, save_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.2 Transport Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot difference between last and first year\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Trans_annual, Lat_01, Lon_01, year_range, title_diff_trans_str,save_flag, save_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.2 Refining Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot difference between last and first year\n",
    "save_flag = 0\n",
    "save_outfile = ''\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Ref_annual, Lat_01, Lon_01, year_range, title_diff_ref_str,save_flag, save_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Plot Key Proxy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Map (well location) heatmap\n",
    "\n",
    "# Activity_Map = 0.1x0.1 map of activity data (counts or absolute units)\n",
    "# Plot_Frac    = 0 or 1 (0= plot activity data in absolute counts, 1= plot fractional activity data)\n",
    "# Lat          = 0.1 degree Lat values (select range)\n",
    "# Lon          = 0.1 degree Lon values (select range)\n",
    "# year_range   = array of inventory years\n",
    "# title_str    = title of map\n",
    "# legend_str   = title of legend\n",
    "# scale_max    = maximum of color scale\n",
    "\n",
    "Map_output = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "for iyear in np.arange(0,num_years):\n",
    "    for imonth in np.arange(0,num_months):    \n",
    "        Map_output[:,:,iyear] += Map_Allwell[:,:,iyear,imonth]  \n",
    "\n",
    "            \n",
    "Activity_Map = Map_output\n",
    "Plot_Frac = 1\n",
    "Lat = Lat_01\n",
    "Lon = Lon_01\n",
    "year_range = year_range\n",
    "title_str2 = \"Proxy - Oil Well Locations\"\n",
    "legend_str = \"Annual Fraction of National Well Population\"\n",
    "scale_max = 0.001\n",
    "\n",
    "for iyear in np.arange(6,7):#len(year_range)): \n",
    "    my_cmap = copy(plt.cm.get_cmap('rainbow',lut=3000))\n",
    "    my_cmap._init()\n",
    "    slopen = 200\n",
    "    alphas_slope = np.abs(np.linspace(0, 1.0, slopen))\n",
    "    alphas_stable = np.ones(3003-slopen)\n",
    "    alphas = np.concatenate((alphas_slope, alphas_stable))\n",
    "    my_cmap._lut[:,-1] = alphas\n",
    "    my_cmap.set_under('gray', alpha=0)\n",
    "    \n",
    "    Lon_cor = Lon[50:632]-0.05\n",
    "    Lat_cor = Lat[43:300]-0.05\n",
    "    \n",
    "    xpoints = Lon_cor\n",
    "    ypoints = Lat_cor\n",
    "    yp,xp = np.meshgrid(ypoints,xpoints)\n",
    "    \n",
    "    if np.shape(Activity_Map)[0] == len(year_range):\n",
    "        if Plot_Frac ==1:\n",
    "            zp = Activity_Map[iyear,43:300,50:632]/np.sum(Activity_Map[iyear,:,:])\n",
    "        else:\n",
    "            zp = Activity_Map[iyear,43:300,50:632]\n",
    "    elif np.shape(Activity_Map)[2] == len(year_range):\n",
    "        if Plot_Frac ==1:\n",
    "            zp = Activity_Map[43:300,50:632,iyear]/np.sum(Activity_Map[:,:,iyear])\n",
    "        else: \n",
    "            zp = Activity_Map[43:300,50:632,iyear]\n",
    "    #zp = zp/float(10**6 * Avogadro) * (year_days * 24 * 60 * 60) * Molarch4 * float(1e10)\n",
    "    \n",
    "    fig, ax = plt.subplots(dpi=300)\n",
    "    m = Basemap(llcrnrlon=xp.min(), llcrnrlat=yp.min(), urcrnrlon=xp.max(),\n",
    "                urcrnrlat=yp.max(), projection='merc', resolution='h', area_thresh=5000)\n",
    "    m.drawmapboundary(fill_color='Azure')\n",
    "    m.fillcontinents(color='FloralWhite', lake_color='Azure',zorder=1)\n",
    "    m.drawcoastlines(linewidth=0.5,zorder=3)\n",
    "    m.drawstates(linewidth=0.25,zorder=3)\n",
    "    m.drawcountries(linewidth=0.5,zorder=3)\n",
    "\n",
    "    xpi,ypi = m(xp,yp)\n",
    "    plot = m.pcolor(xpi,ypi,zp.transpose(), cmap=my_cmap, vmin=10**-15, vmax=scale_max, snap=True,zorder=2)\n",
    "    #plot = m.scatter(xpi,ypi,s=20,c=zp.transpose(),cmap=my_cmap,zorder=2,vmin = 10**-15,snap = True,vmax = scale_max)\n",
    "    cb = m.colorbar(plot, location = \"bottom\", pad = \"1%\")        \n",
    "    tick_locator = ticker.MaxNLocator(nbins=5)\n",
    "    cb.locator = tick_locator\n",
    "    cb.update_ticks()\n",
    "    \n",
    "    cb.ax.set_xlabel(legend_str,fontsize=10)\n",
    "    cb.ax.tick_params(labelsize=10)\n",
    "    Titlestring = str(year_range[iyear])+' '+title_str2\n",
    "    plt.title(Titlestring, fontsize=14);\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Refinery Locations\n",
    "\n",
    "# Activity_Map = 0.1x0.1 map of activity data (counts or absolute units)\n",
    "# Plot_Frac    = 0 or 1 (0= plot activity data in absolute counts, 1= plot fractional activity data)\n",
    "# Lat          = 0.1 degree Lat values (select range)\n",
    "# Lon          = 0.1 degree Lon values (select range)\n",
    "# year_range   = array of inventory years\n",
    "# title_str    = title of map\n",
    "# legend_str   = title of legend\n",
    "# scale_max    = maximum of color scale\n",
    "\n",
    "Activity_Map = Map_Refineries\n",
    "Plot_Frac = 1\n",
    "Lat = Lat_01\n",
    "Lon = Lon_01\n",
    "year_range = year_range\n",
    "title_str2 = \"Proxy - Petroleum Refinery Emissions\"\n",
    "legend_str = \"Annual Fraction of National Refinery Emissions\"\n",
    "scale_max = 0.05\n",
    "\n",
    "for iyear in np.arange(0,len(year_range)): \n",
    "    my_cmap = copy(plt.cm.get_cmap('rainbow',lut=3000))\n",
    "    my_cmap._init()\n",
    "    slopen = 200\n",
    "    alphas_slope = np.abs(np.linspace(0, 1.0, slopen))\n",
    "    alphas_stable = np.ones(3003-slopen)\n",
    "    alphas = np.concatenate((alphas_slope, alphas_stable))\n",
    "    my_cmap._lut[:,-1] = alphas\n",
    "    my_cmap.set_under('gray', alpha=0)\n",
    "    \n",
    "    Lon_cor = Lon[50:632]-0.05\n",
    "    Lat_cor = Lat[43:300]-0.05\n",
    "    \n",
    "    xpoints = Lon_cor\n",
    "    ypoints = Lat_cor\n",
    "    yp,xp = np.meshgrid(ypoints,xpoints)\n",
    "    \n",
    "    if np.shape(Activity_Map)[0] == len(year_range):\n",
    "        if Plot_Frac ==1:\n",
    "            zp = Activity_Map[iyear,43:300,50:632]/np.sum(Activity_Map[iyear,:,:])\n",
    "        else:\n",
    "            zp = Activity_Map[iyear,43:300,50:632]\n",
    "    elif np.shape(Activity_Map)[2] == len(year_range):\n",
    "        if Plot_Frac ==1:\n",
    "            zp = Activity_Map[43:300,50:632,iyear]/np.sum(Activity_Map[:,:,iyear])\n",
    "        else: \n",
    "            zp = Activity_Map[43:300,50:632,iyear]\n",
    "    \n",
    "    fig, ax = plt.subplots(dpi=300)\n",
    "    m = Basemap(llcrnrlon=xp.min(), llcrnrlat=yp.min(), urcrnrlon=xp.max(),\n",
    "                urcrnrlat=yp.max(), projection='merc', resolution='h', area_thresh=5000)\n",
    "    m.drawmapboundary(fill_color='Azure')\n",
    "    m.fillcontinents(color='FloralWhite', lake_color='Azure',zorder=1)\n",
    "    m.drawcoastlines(linewidth=0.5,zorder=3)\n",
    "    m.drawstates(linewidth=0.25,zorder=3)\n",
    "    m.drawcountries(linewidth=0.5,zorder=3)\n",
    "        \n",
    "    \n",
    "    xpi,ypi = m(xp,yp)\n",
    "    plot = m.scatter(xpi,ypi,s=20,c=zp.transpose(),cmap=my_cmap,zorder=2,vmin = 10**-15,snap = True,vmax = scale_max)\n",
    "    cb = m.colorbar(plot, location = \"bottom\", pad = \"1%\")        \n",
    "    tick_locator = ticker.MaxNLocator(nbins=5)\n",
    "    cb.locator = tick_locator\n",
    "    cb.update_ticks()\n",
    "    \n",
    "    cb.ax.set_xlabel(legend_str,fontsize=10)\n",
    "    cb.ax.tick_params(labelsize=10)\n",
    "    Titlestring = str(year_range[iyear])+' '+title_str2\n",
    "    plt.title(Titlestring, fontsize=14);\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = datetime.now() \n",
    "ft = ct.timestamp() \n",
    "time_elapsed = (ft-it)/(60*60)\n",
    "print('Time to run: '+str(time_elapsed)+' hours')\n",
    "print('** GEPA_1B2a_Petroleum_Systems_Production: COMPLETE **')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

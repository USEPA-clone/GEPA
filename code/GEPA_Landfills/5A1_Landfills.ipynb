{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded EPA Methane Inventory\n",
    "## Category: 5A1 Landfills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Authors: \n",
    "Erin E. McDuffie, Bram Maasakkers, Candice Chen\n",
    "#### Date Last Updated: \n",
    "see Step 0\n",
    "#### Notebook Purpose: \n",
    "This Notebook calculates and reports annual gridded (0.1°x0.1°) methane emission fluxes (molec./cm2/s) from Landfills (total, industrial, and municipal solid waste) in the CONUS region between 2012-2018. \n",
    "#### Summary & Notes:\n",
    "The national EPA GHGI emissions from MSW and industrial landfills are read in from the publicly available EPA GHG Inventory Waste annex files. Emissions are available as national totals (for entire time series). State-level allocations are also available from the 2021 State GHG Inventory for two industrial sectors (pulp and paper manufacturing and food and beverage manufacturing) within the industrial waste category. National industrial waste emissions are allocated to the state level (for each subgroup) using these relative state-level emissions data. State-level emissions for each subgroup are then allocated to the 0.01⁰x0.01⁰ CONUS grid using gridded data of facility-level emissions for each subgroup. Data are then re-gridded to 0.1⁰x0.1⁰. National MSW landfill emissions are allocated directly to the 0.1⁰x0.1⁰ CONUS grid using relative facility-level emissions for MSW landfills. All data are then converted to fluxes (molecules CH4/cm2/s). Annual emission fluxes (molecules CH4/cm2/s) for total landfills, MSW landfills, and industrial landfills are written to final netCDFs in the ‘/code/Final_Gridded_Data/’ folder. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Step 0. Set-Up Notebook Modules, Functions, and Local Parameters and Constants\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm working directory & print last update time\n",
    "import os\n",
    "import time\n",
    "modtime = os.path.getmtime('./5A1_Landfills.ipynb')\n",
    "modificationTime = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(modtime))\n",
    "print(\"This file was last modified on: \", modificationTime)\n",
    "print('')\n",
    "print(\"The directory we are working in is {}\" .format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Include plots within notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import pyodbc\n",
    "import PyPDF2 as pypdf\n",
    "import tabula as tb\n",
    "import shapefile as shp\n",
    "from datetime import datetime\n",
    "from copy import copy\n",
    "from scipy.interpolate import interp1d\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Import additional modules\n",
    "# Load plotting package Basemap \n",
    "# Must also specify project library path [unique to each user])\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# Load netCDF (for manipulating netCDF file types)\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Set up ticker\n",
    "#import matplotlib.ticker as ticker\n",
    "\n",
    "#add path for the global function module (file)\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../Global_Functions/'))\n",
    "#print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Load functions\n",
    "import data_load_functions as data_load_fn\n",
    "import data_functions as data_fn\n",
    "import data_IO_functions as data_IO_fn\n",
    "import data_plot_functions as data_plot_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT Files\n",
    "# Assign global file names\n",
    "global_filenames = data_load_fn.load_global_file_names()\n",
    "State_ANSI_inputfile = global_filenames[0]\n",
    "#County_ANSI_inputfile = global_filenames[1]\n",
    "#pop_map_inputfile = global_filenames[2]\n",
    "Grid_area01_inputfile = global_filenames[3]\n",
    "Grid_area001_inputfile = global_filenames[4]\n",
    "Grid_state001_ansi_inputfile = global_filenames[5]\n",
    "#Grid_county001_ansi_inputfile = global_filenames[6]\n",
    "globalinputlocation = global_filenames[0][0:20]\n",
    "print(globalinputlocation)\n",
    "\n",
    "# EPA Inventory Data\n",
    "EPA_landfill_inputfile = globalinputlocation+'GHGI/Ch7_Waste/Table 7-4.csv'\n",
    "\n",
    "#proxy mapping file\n",
    "Landfills_Mapping_inputfile = './InputData/Landfills_ProxyMapping.xlsx'\n",
    "\n",
    "# GHGRP Data\n",
    "ghgrp_emi_hh_inputfile = './InputData/ghgrp_subpart_hh.csv'\n",
    "ghgrp_facility_hh_inputfile = './InputData/SubpartHH_MSWlandfills_Facilities.csv'\n",
    "ghgrp_emi_tt_inputfile = './InputData/ghgrp_subpart_tt.csv'\n",
    "ghgrp_facility_tt_inputfile = './InputData/SubpartTT_INDlandfills_Facilities.csv'\n",
    "\n",
    "EPA_nonreporting_msw_inputfile = './InputData/Non-Reporting_LF_DB_2020_1.12.2021.xlsx'\n",
    "\n",
    "FRS_inputfile = globalinputlocation+'FRS/national_single/NATIONAL_SINGLE.csv'\n",
    "\n",
    "EPA_IndState_inputfile = './InputData/IND_LF_State_inv_04.20.2021.xlsx'\n",
    "FoodBeverage_inputdata = './InputData/ExcessFoodPublic_USTer_2020_R9/ExcelTables/Food Manufacturers and Processors.xlsx'\n",
    "Mills_OnLine_inputdata = './InputData/Mills_OnLine.xlsx'\n",
    "\n",
    "#OUTPUT FILES\n",
    "gridded_outputfile = '../Final_Gridded_Data/EPA_v2_6A1_Landfills.nc'\n",
    "gridded_msw_outputfile = '../Final_Gridded_Data/EPA_v2_5A1_Landfills_MSW.nc'\n",
    "gridded_ind_outputfile = '../Final_Gridded_Data/EPA_v2_5A1_Landfills_Industrial.nc'\n",
    "\n",
    "netCDF_description = 'Gridded EPA Inventory - Landfill Emissions - IPCC Source Category 6A1'\n",
    "netCDF_msw_description = 'Gridded EPA Inventory - Landfill Emissions - IPCC Source Category 5A1 - Municipal Solid Waste (MSW)'\n",
    "netCDF_ind_description = 'Gridded EPA Inventory - Landfill Emissions - IPCC Source Category 5A1 - Industrial'\n",
    "\n",
    "title_str = \"EPA methane emissions from landfills\"\n",
    "title_str_msw = \"EPA methane emissions from MSW landfills\"\n",
    "title_str_ind = \"EPA methane emissions from Industrial landfills\"\n",
    "title_diff_str = \"Emissions from landfills difference: 2018-2012\"\n",
    "title_diff_str_msw = \"Emissions from MSW landfills difference: 2018-2012\"\n",
    "title_diff_str_ind = \"Emissions from industrial landfills difference: 2018-2012\"\n",
    "\n",
    "#output gridded proxy data\n",
    "grid_emi_outputfile = '../Final_Gridded_Data/Extension/v2_input_data/Landfills_Grid_Emi.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local variables\n",
    "start_year = 2012  #First year in emission timeseries\n",
    "end_year = 2018    #Last year in emission timeseries\n",
    "year_range = [*range(start_year, end_year+1,1)] #List of emission years\n",
    "year_range_str=[str(i) for i in year_range]\n",
    "num_years = len(year_range)\n",
    "num_inv_years = len([*range(1990, end_year+1,1)]) #List of inventory years\n",
    "\n",
    "# Define constants\n",
    "Avogadro   = 6.02214129 * 10**(23)  #molecules/mol\n",
    "Molarch4   = 16.04                  #g/mol\n",
    "Res01      = 0.1                    # degrees\n",
    "Res_01     = 0.01                   # degrees\n",
    "hrs_to_yrs = 8760                   #number of hours in a year\n",
    "g_to_mt    = 1*10**(-6)             # grams to metric ton\n",
    "\n",
    "# Continental US Lat/Lon Limits (for netCDF files)\n",
    "Lon_left = -130       #deg\n",
    "Lon_right = -60       #deg\n",
    "Lat_low  = 20         #deg\n",
    "Lat_up  = 55          #deg\n",
    "loc_dimensions = [Lat_low, Lat_up, Lon_left, Lon_right]\n",
    "\n",
    "ilat_start = int((90+Lat_low)/Res01) #1100:1450 (continental US range)\n",
    "ilat_end = int((90+Lat_up)/Res01)\n",
    "ilon_start = abs(int((-180-Lon_left)/Res01)) #500:1200 (continental US range)\n",
    "ilon_end = abs(int((-180-Lon_right)/Res01))\n",
    "\n",
    "# Number of days in each month\n",
    "month_day_leap  = [  31,  29,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_day_nonleap = [  31,  28,  31,  30,  31,  30,  31,  31,  30,  31,  30,  31]\n",
    "month_tag = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "month_dict = {'January':1, 'February':2,'March':3,'April':4,'May':5,'June':6, 'July':7,'August':8,'September':9,'October':10,\\\n",
    "             'November':11,'December':12}\n",
    "\n",
    "# Month arrays\n",
    "month_range_str = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "num_months = len(month_range_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;\n",
    "//prevent auto-scrolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track run time\n",
    "ct = datetime.now() \n",
    "it = ct.timestamp() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Step 1. Load in State ANSI data, and Area Maps\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State-level ANSI Data\n",
    "#Read the state ANSI file array\n",
    "State_ANSI, name_dict, abbr_dict = data_load_fn.load_state_ansi(State_ANSI_inputfile)[0:3]\n",
    "#QA: number of states\n",
    "print('Read input file: '+ f\"{State_ANSI_inputfile}\")\n",
    "print('Total \"States\" found: ' + '%.0f' % len(State_ANSI))\n",
    "print(' ')\n",
    "\n",
    "# 0.01 x0.01 degree Data\n",
    "# State ANSI IDs and grid cell area (m2) maps\n",
    "state_ANSI_map = data_load_fn.load_state_ansi_map(Grid_state001_ansi_inputfile)\n",
    "area_map, lat001, lon001 = data_load_fn.load_area_map_001(Grid_area001_inputfile)\n",
    "\n",
    "# 0.1 x0.1 degree data\n",
    "# grid cell area and state ANSI maps\n",
    "Lat01, Lon01 = data_load_fn.load_area_map_01(Grid_area01_inputfile)[1:3]\n",
    "#Select relevant Continental 0.1 x0.1 domain\n",
    "Lat_01 = Lat01[ilat_start:ilat_end]\n",
    "Lon_01 = Lon01[ilon_start:ilon_end]\n",
    "area_matrix_01 = data_fn.regrid001_to_01(area_map, Lat_01, Lon_01)\n",
    "area_matrix_01 *= 10000  #convert from m2 to cm2\n",
    "#state_ANSI_map_01 = data_fn.regrid001_to_01(state_ANSI_map, Lat_01, Lon_01)\n",
    "del area_map#, lat001, lon001, global_filenames\n",
    "\n",
    "# Print time\n",
    "ct = datetime.now() \n",
    "print(\"current time:\", ct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 2: Read-in and Format Proxy Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 Read In Proxy Mapping File & Make Proxy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load GHGI Mapping Groups\n",
    "names = pd.read_excel(Landfills_Mapping_inputfile, sheet_name = \"GHGI Map - Landfills\", usecols = \"A:B\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "ghgi_landfill_map = pd.read_excel(Landfills_Mapping_inputfile, sheet_name = \"GHGI Map - Landfills\", usecols = \"A:B\", skiprows = 2, names = colnames)\n",
    "#drop rows with no data, remove the parentheses and \"\"\n",
    "ghgi_landfill_map = ghgi_landfill_map[ghgi_landfill_map['GHGI_Emi_Group'] != 'na']\n",
    "ghgi_landfill_map = ghgi_landfill_map[ghgi_landfill_map['GHGI_Emi_Group'].notna()]\n",
    "ghgi_landfill_map = ghgi_landfill_map[ghgi_landfill_map['GHGI_Emi_Group'] != '-']\n",
    "ghgi_landfill_map['GHGI_Source']= ghgi_landfill_map['GHGI_Source'].str.replace(r\"\\(\",\"\")\n",
    "ghgi_landfill_map['GHGI_Source']= ghgi_landfill_map['GHGI_Source'].str.replace(r\"\\)\",\"\")\n",
    "ghgi_landfill_map['GHGI_Source']= ghgi_landfill_map['GHGI_Source'].str.replace(r\"+\",\"\")\n",
    "ghgi_landfill_map.reset_index(inplace=True, drop=True)\n",
    "display(ghgi_landfill_map)\n",
    "\n",
    "#load emission group - proxy map\n",
    "names = pd.read_excel(Landfills_Mapping_inputfile, sheet_name = \"Proxy Map - Landfills\", usecols = \"A:F\",skiprows = 1, header = 0)\n",
    "colnames = names.columns.values\n",
    "proxy_landfill_map = pd.read_excel(Landfills_Mapping_inputfile, sheet_name = \"Proxy Map - Landfills\", usecols = \"A:F\", skiprows = 1, names = colnames)\n",
    "display((proxy_landfill_map))\n",
    "\n",
    "#create empty proxy and emission group arrays (for state and months, where needed)\n",
    "for igroup in np.arange(0,len(proxy_landfill_map)):\n",
    "    if proxy_landfill_map.loc[igroup, 'Grid_Month_Flag'] ==0:\n",
    "        if proxy_landfill_map.loc[igroup, 'Grid_SubGroup_Flag'] >= 1:\n",
    "            vars()[proxy_landfill_map.loc[igroup,'Proxy_Group']] = np.zeros([2,len(Lat_01),len(Lon_01),num_years])\n",
    "            vars()[proxy_landfill_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "        else:\n",
    "            vars()[proxy_landfill_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "            vars()[proxy_landfill_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years])\n",
    "    else:\n",
    "        vars()[proxy_landfill_map.loc[igroup,'Proxy_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years,num_months])\n",
    "        vars()[proxy_landfill_map.loc[igroup,'Proxy_Group']+'_nongrid'] = np.zeros([num_years,num_months])\n",
    "        \n",
    "    vars()[proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])\n",
    "    \n",
    "    if proxy_landfill_map.loc[igroup,'State_Proxy_Group'] != '-':\n",
    "        if proxy_landfill_map.loc[igroup, 'State_SubGroup_Flag'] >= 1:\n",
    "            vars()[proxy_landfill_map.loc[igroup,'State_Proxy_Group']] = np.zeros([2,len(State_ANSI),num_years])\n",
    "        else:\n",
    "            vars()[proxy_landfill_map.loc[igroup,'State_Proxy_Group']] = np.zeros([len(State_ANSI),num_years])\n",
    "    else:\n",
    "        continue # do not make state proxy variable if no variable assigned in mapping file    \n",
    "        \n",
    "emi_group_names = np.unique(ghgi_landfill_map['GHGI_Emi_Group'])\n",
    "\n",
    "print('QA/QC: Is the number of emission groups the same for the proxy and emissions tabs?')\n",
    "if (len(emi_group_names) == len(np.unique(proxy_landfill_map['GHGI_Emi_Group']))):\n",
    "    print('PASS')\n",
    "else:\n",
    "    print('FAIL')\n",
    "    print(emi_group_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2 Read In MSW Landfill Proxy Emissions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in GHGRP HH emissions for each reporting year, get lat/lons by matching to facilities\n",
    "# Place HH emissions onto a map\n",
    "## Read in Non-Reporting spreadsheet\n",
    "# Calculate the emissions for each non-reporting landfill (= WIP ind/(WIP total) * GHGRP emissions for that year)\n",
    "# remove any of these landfills that in GHGRP (not yet offramped) in the given year\n",
    "# add emissions to map\n",
    "\n",
    "#this method applies a 11% factor to all years (rather than the 9% factor for 2012-2016 emissions as done in the national \n",
    "# GHGI. However, we remove landfills that are still in the GHGRP in the earlier years, so this should help balance out the \n",
    "# overestimation of emissions from non-reproting landfills in earlier years. ) [assuming that the increase from \n",
    "# 9% to 11% is largely due to the offramping of GHGRP reporting facilitites. ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2.1 Read In GHGRP Subpart HH Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read in GHGRP Subpart HH Emissions and place onto CONUS grid\n",
    "\n",
    "#a) Read in the GHGRP facility data\n",
    "facility_info = pd.read_csv(ghgrp_facility_hh_inputfile)\n",
    "facility_emis = pd.read_csv(ghgrp_emi_hh_inputfile)\n",
    "\n",
    "#filter emissions data for methane only (in metric tonnes CH4) and for years of interest\n",
    "facility_emis = facility_emis[facility_emis['HH_SUBPART_LEVEL_INFORMATION.GHG_NAME'] == 'METHANE']\n",
    "facility_emis = facility_emis[facility_emis['HH_SUBPART_LEVEL_INFORMATION.REPORTING_YEAR'].isin(year_range)]\n",
    "facility_info = facility_info[facility_info['V_GHG_EMITTER_FACILITIES.YEAR'].isin(year_range)]\n",
    "facility_info.reset_index(inplace=True, drop=True)\n",
    "facility_emis.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#rename common columns and merge into one dataframe\n",
    "facility_info.rename(columns={'V_GHG_EMITTER_FACILITIES.YEAR':'Year', \\\n",
    "                             'V_GHG_EMITTER_FACILITIES.FACILITY_ID':'Facility_ID', \\\n",
    "                             'V_GHG_EMITTER_FACILITIES.LONGITUDE':'LONGITUDE',\n",
    "                             'V_GHG_EMITTER_FACILITIES.LATITUDE':'LATITUDE'},inplace=True)\n",
    "facility_emis.rename(columns={'HH_SUBPART_LEVEL_INFORMATION.REPORTING_YEAR':'Year', \\\n",
    "                              'HH_SUBPART_LEVEL_INFORMATION.FACILITY_ID':'Facility_ID'},inplace=True)\n",
    "ghgrp_msw = pd.merge(facility_info, facility_emis)\n",
    "ghgrp_msw['emis_kt_tot'] = ghgrp_msw['HH_SUBPART_LEVEL_INFORMATION.GHG_QUANTITY']/1e3 #convert metric tonnes to kt\n",
    "\n",
    "\n",
    "# place ghgrp emissions onto 0.1x0.1 grid and calculate national totals and non-reporting totals\n",
    "# According to the GHGI national methodology, emissions from non-reporting MSw facilities\n",
    "# contribute an additional 9% of emissions in 2012-2016 and 11% after 2016. These factors\n",
    "# were derived based on an analysis comparing waste in place (WIP) totals from GHGRP and\n",
    "# non-emissions reporting landfills.\n",
    "\n",
    "#initialize array\n",
    "map_msw_emis = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "map_msw_emis_nongrid = np.zeros([num_years])\n",
    "ghgrp_total_emis = np.zeros(num_years)\n",
    "nonrepoting_total_emis = np.zeros(num_years)\n",
    "\n",
    "for iyear in np.arange(0, num_years):\n",
    "    ghgrp_temp = ghgrp_msw[ghgrp_msw['Year'] == year_range[iyear]]\n",
    "    ghgrp_temp.reset_index(inplace=True, drop=True)\n",
    "    #display(ghgrp_temp)\n",
    "    for ifacility in np.arange(0, len(ghgrp_temp)):\n",
    "        if ghgrp_temp['LONGITUDE'][ifacility] > Lon_left and \\\n",
    "            ghgrp_temp['LONGITUDE'][ifacility] < Lon_right and \\\n",
    "            ghgrp_temp['LATITUDE'][ifacility] > Lat_low and \\\n",
    "            ghgrp_temp['LATITUDE'][ifacility] < Lat_up:\n",
    "\n",
    "            ilat = int((ghgrp_temp['LATITUDE'][ifacility] - Lat_low)/Res01)\n",
    "            ilon = int((ghgrp_temp['LONGITUDE'][ifacility] - Lon_left)/Res01)\n",
    "            map_msw_emis[ilat,ilon,iyear] += ghgrp_temp['emis_kt_tot'][ifacility]\n",
    "        else:\n",
    "            map_msw_emis_nongrid[iyear] += ghgrp_temp['emis_kt_tot'][ifacility]\n",
    "\n",
    "for iyear in np.arange(0, num_years):\n",
    "    if year_range[iyear] <= 2016:\n",
    "        factor = 0.09\n",
    "    else:\n",
    "        factor = 0.11\n",
    "    ghgrp_total_emis[iyear] = np.sum(map_msw_emis[:,:,iyear])+map_msw_emis_nongrid[iyear]\n",
    "    nonrepoting_total_emis[iyear] = ghgrp_total_emis[iyear]*factor\n",
    "    print('Year:',year_range[iyear])\n",
    "    print('GHGRP Emissions (kt):',ghgrp_total_emis[iyear])\n",
    "    print('Non-Reporting Emissions (kt):',nonrepoting_total_emis[iyear])\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2.2 Read In EPA Non-Reporting Landfills Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in Non-Reporting MSW Landfill Information and Estimate Emissions (based on waste in place)\n",
    "\n",
    "EPA_nr_msw = pd.read_excel(EPA_nonreporting_msw_inputfile, sheet_name = \"LandfillComp\", usecols = \"A:B,D,F,AP,BL:BM,BR:BS\", skiprows = 5)#, nrows = 140)#,names = colnames)\n",
    "EPA_nr_msw.rename(columns={'Landfill Name (as listed in Original Instance source)':'Name'},inplace=True)\n",
    "EPA_nr_msw.rename(columns={'HH Off-Ramped, Last Year Reported':'Last GHGRP Year'},inplace=True)\n",
    "EPA_nr_msw.rename(columns={'Avg. Est. Total WIP (MT)':'WIP_MT'},inplace=True)\n",
    "EPA_nr_msw.rename(columns={'LMOP Lat':'LAT','LMOP Long':'LON'},inplace=True)\n",
    "\n",
    "EPA_nr_msw['WBJ City'] = EPA_nr_msw['WBJ City'].astype('string')\n",
    "EPA_nr_msw['WBJ Location'] = EPA_nr_msw['WBJ Location'].astype(str)\n",
    "EPA_nr_msw['Full_Address'] = EPA_nr_msw[\"WBJ Location\"]+' '+EPA_nr_msw[\"WBJ City\"]+' '+EPA_nr_msw[\"State\"]\n",
    "EPA_nr_msw['Partial_Address'] = EPA_nr_msw[\"WBJ City\"]+' '+EPA_nr_msw[\"State\"]\n",
    "EPA_nr_msw.fillna('NaN', inplace=True)\n",
    "EPA_nr_msw = EPA_nr_msw[EPA_nr_msw['WIP_MT']>0]\n",
    "EPA_nr_msw.reset_index(inplace=True, drop=True)\n",
    "#display(EPA_nr_msw)\n",
    "print('Total Non-Reporting Landfills:',len(EPA_nr_msw))\n",
    "\n",
    "#Separate Landfills with and without location information\n",
    "#These are the landfills from the waste business jounral (WBJ) that have limited location information\n",
    "EPA_nr_msw_noloc = EPA_nr_msw[(EPA_nr_msw['LAT'] ==0) & (EPA_nr_msw['LON'] ==0)]\n",
    "EPA_nr_msw_noloc.reset_index(inplace=True, drop=True)\n",
    "\n",
    "EPA_nr_msw_loc = EPA_nr_msw[(EPA_nr_msw['LAT'] !=0) & (EPA_nr_msw['LON'] !=0)]\n",
    "EPA_nr_msw_loc.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2.3 Read in FRS Landfills dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read in FRS data to get location information for landfills with missing location information \n",
    "##This comes from the Facility Registration system, the original file (NATIONAL_SINGLE.csv is > 1.5 Gb)\n",
    "   \n",
    "FRS_facility_locs = pd.read_csv(FRS_inputfile, usecols = [2,3,5,7,8,10,17,20,21,26,27,28,31,32,34,35,36],low_memory=False)\n",
    "FRS_facility_locs.fillna(0, inplace = True)\n",
    "FRS_facility_locs = FRS_facility_locs[FRS_facility_locs['LATITUDE83'] > 0]\n",
    "FRS_facility_locs = FRS_facility_locs[FRS_facility_locs['NAICS_CODES'] != 0]\n",
    "FRS_facility_locs.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "FRS_facility_locs = FRS_facility_locs[(FRS_facility_locs['NAICS_CODES']=='562212')]\n",
    "print('Total landfills: ',len(FRS_facility_locs))\n",
    "FRS_facility_locs.reset_index(inplace=True, drop=True)\n",
    "\n",
    "FRS_facility_locs['CITY_NAME'] = FRS_facility_locs['CITY_NAME'].replace(0,'NaN')\n",
    "FRS_facility_locs.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print('Landfills selected: ', len(FRS_facility_locs))\n",
    "FRS_facility_locs.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2.4 Find Locations by matching EPA Non-Reporting Landfills (without locations) to FRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through the Non-Reporting MSW Landfill records that don't have locations, to \n",
    "# try to find matches in the FRS dataset\n",
    "# Note that there are more landfills in the FRS dataset, then then GHGRP+Non-reporting dataset\n",
    "# In the previous GEPA, all FRS landfills were used. In the GEPA v2, we only use those\n",
    "# landfills identified and used to estimate national emissions in the GHGI (i.e., \n",
    "# GHGRP + Non-Reporting landfills)\n",
    "\n",
    "EPA_nr_msw_noloc.loc[:,'found'] = 0\n",
    "\n",
    "for ifacility in np.arange(0,len(EPA_nr_msw_noloc)):\n",
    "    #first try matching by state and exact name of landfill\n",
    "    state_temp = EPA_nr_msw_noloc.loc[ifacility,'State']\n",
    "    imatch = np.where((FRS_facility_locs['PRIMARY_NAME'].str.contains(EPA_nr_msw_noloc.loc[ifacility,'Name'].upper()))\\\n",
    "                      & (FRS_facility_locs['STATE_CODE']==state_temp))[0]\n",
    "    if len(imatch) == 1:\n",
    "        EPA_nr_msw_noloc.loc[ifacility,'LAT'] = FRS_facility_locs.loc[imatch[0],'LATITUDE83']\n",
    "        EPA_nr_msw_noloc.loc[ifacility,'LON'] = FRS_facility_locs.loc[imatch[0],'LONGITUDE83']\n",
    "        EPA_nr_msw_noloc.loc[ifacility,'found'] = 1\n",
    "    elif len(imatch) > 1:\n",
    "        #if name and state match more than one entry, use the one with the higher accuracy\n",
    "        # or the first entry if the accuracy values are the same\n",
    "        FRS_temp = FRS_facility_locs.loc[imatch,:].copy()\n",
    "        new_match = np.where(np.max(FRS_temp['ACCURACY_VALUE']))[0]\n",
    "        if len(new_match) ==1:\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'LAT'] = FRS_facility_locs.loc[imatch[new_match[0]],'LATITUDE83']\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'LON'] = FRS_facility_locs.loc[imatch[new_match[0]],'LONGITUDE83']\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'found'] = 1\n",
    "        elif len(new_match)<1:\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'LAT'] = FRS_facility_locs.loc[imatch[0],'LATITUDE83']\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'LON'] = FRS_facility_locs.loc[imatch[0],'LONGITUDE83']\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'found'] = 1\n",
    "\n",
    "    #next try matching based on any of the words in name and state and city\n",
    "    if EPA_nr_msw_noloc.loc[ifacility,'found'] ==0:\n",
    "        string_temp = [x for x in EPA_nr_msw_noloc.loc[ifacility,'Name'].upper().split() \\\n",
    "                       if x not in {'LANDFILL', 'SANITARY','CITY','TOWN','OF'}]\n",
    "        #string_temp = EPA_nr_msw_noloc.loc[ifacility,'Name'].upper().split()[0:2]\n",
    "        string_temp = '|'.join(string_temp)\n",
    "        string_temp = string_temp.replace(\"(\",\"\")\n",
    "        string_temp = string_temp.replace(\")\",\"\")\n",
    "        string_temp = string_temp.replace(\"&\",\"\")\n",
    "        string_temp = string_temp.replace(\"/\",\"\")\n",
    "        #print(string_temp)\n",
    "        city_temp = EPA_nr_msw_noloc.loc[ifacility,'WBJ City'].upper()\n",
    "        imatch = np.where((FRS_facility_locs['PRIMARY_NAME'].str.contains(string_temp))\\\n",
    "                      & (FRS_facility_locs['STATE_CODE']==state_temp) & (FRS_facility_locs['CITY_NAME']==city_temp))[0]\n",
    "        #print(imatch)\n",
    "        if len(imatch) == 1:\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'LAT'] = FRS_facility_locs.loc[imatch[0],'LATITUDE83']\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'LON'] = FRS_facility_locs.loc[imatch[0],'LONGITUDE83']\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'found'] = 1\n",
    "        elif len(imatch) >1:\n",
    "            #if name and state match more than one entry, use the one with the higher accuracy\n",
    "            # or the first entry if the accuracy values are the same\n",
    "            FRS_temp = FRS_facility_locs.loc[imatch,:].copy()\n",
    "            new_match = np.where(np.max(FRS_temp['ACCURACY_VALUE']))[0]\n",
    "            if len(new_match) ==1:\n",
    "                EPA_nr_msw_noloc.loc[ifacility,'LAT'] = FRS_facility_locs.loc[imatch[new_match[0]],'LATITUDE83']\n",
    "                EPA_nr_msw_noloc.loc[ifacility,'LON'] = FRS_facility_locs.loc[imatch[new_match[0]],'LONGITUDE83']\n",
    "                EPA_nr_msw_noloc.loc[ifacility,'found'] = 1\n",
    "            elif len(new_match)<1:\n",
    "                EPA_nr_msw_noloc.loc[ifacility,'LAT'] = FRS_facility_locs.loc[imatch[0],'LATITUDE83']\n",
    "                EPA_nr_msw_noloc.loc[ifacility,'LON'] = FRS_facility_locs.loc[imatch[0],'LONGITUDE83']\n",
    "                EPA_nr_msw_noloc.loc[ifacility,'found'] = 1\n",
    "    \n",
    "    #next try matching based on state and city\n",
    "    if EPA_nr_msw_noloc.loc[ifacility,'found'] ==0:\n",
    "        string_temp = [x for x in EPA_nr_msw_noloc.loc[ifacility,'WBJ Location'].upper().split() \\\n",
    "                       if x not in {'ROAD', 'RD','HWY','HIGHWAY'}]\n",
    "        #string_temp = EPA_nr_msw_noloc.loc[ifacility,'WBJ Location'].upper().split()\n",
    "        string_temp = '|'.join(string_temp)\n",
    "        string_temp = string_temp.replace(\"(\",\"\")\n",
    "        string_temp = string_temp.replace(\")\",\"\")\n",
    "        city_temp = EPA_nr_msw_noloc.loc[ifacility,'WBJ City'].upper()\n",
    "        imatch = np.where((FRS_facility_locs['STATE_CODE']==state_temp) & (FRS_facility_locs['CITY_NAME']==city_temp))[0]\n",
    "        if len(imatch) == 1:\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'LAT'] = FRS_facility_locs.loc[imatch[0],'LATITUDE83']\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'LON'] = FRS_facility_locs.loc[imatch[0],'LONGITUDE83']\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'found'] = 1\n",
    "        elif len(imatch) >1:\n",
    "            #if city and state match more than one entry, use the one that has some matching address\n",
    "            FRS_temp = FRS_facility_locs.loc[imatch,:].copy()\n",
    "            new_match = np.where(FRS_temp['LOCATION_ADDRESS'].str.contains(string_temp))[0]\n",
    "            if len(new_match) >= 1:\n",
    "                EPA_nr_msw_noloc.loc[ifacility,'LAT'] = FRS_facility_locs.loc[imatch[new_match[0]],'LATITUDE83']\n",
    "                EPA_nr_msw_noloc.loc[ifacility,'LON'] = FRS_facility_locs.loc[imatch[new_match[0]],'LONGITUDE83']\n",
    "                EPA_nr_msw_noloc.loc[ifacility,'found'] = 1\n",
    "            \n",
    "    if EPA_nr_msw_noloc.loc[ifacility,'found'] ==0:\n",
    "        #check if state matches and city and name have any matches\n",
    "        city_temp = EPA_nr_msw_noloc.loc[ifacility,'WBJ City'].upper()\n",
    "        imatch = np.where((FRS_facility_locs['PRIMARY_NAME'].str.contains(city_temp))\\\n",
    "                      & (FRS_facility_locs['STATE_CODE']==state_temp))[0]\n",
    "        if len(imatch) == 1:\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'LAT'] = FRS_facility_locs.loc[imatch[0],'LATITUDE83']\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'LON'] = FRS_facility_locs.loc[imatch[0],'LONGITUDE83']\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'found'] = 1\n",
    "        elif len(imatch)>1:\n",
    "            #no good matches in this case (do nothing)\n",
    "            continue\n",
    "    if EPA_nr_msw_noloc.loc[ifacility,'found'] ==0:\n",
    "        #check based on state and any matches between names\n",
    "        string_temp = [x for x in EPA_nr_msw_noloc.loc[ifacility,'Name'].upper().split() \\\n",
    "                       if x not in {'LANDFILL', 'SANITARY','COUNTY','CITY','TOWN','OF','LF','WASTE'}]\n",
    "        #string_temp = EPA_nr_msw_noloc.loc[ifacility,'Name'].upper().split()[0:2]\n",
    "        string_temp = '|'.join(string_temp)\n",
    "        #print(string_temp)\n",
    "        string_temp = string_temp.replace(\"(\",\"\")\n",
    "        string_temp = string_temp.replace(\")\",\"\")\n",
    "        string_temp = string_temp.replace(\"&\",\"\")\n",
    "        string_temp = string_temp.replace(\"/\",\"\")\n",
    "        #print(string_temp, state_temp)\n",
    "        city_temp = EPA_nr_msw_noloc.loc[ifacility,'WBJ City'].upper()\n",
    "        imatch = np.where((FRS_facility_locs['PRIMARY_NAME'].str.contains(string_temp))\\\n",
    "                      & (FRS_facility_locs['STATE_CODE']==state_temp))[0]\n",
    "        #print(imatch)\n",
    "        if len(imatch) == 1:\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'LAT'] = FRS_facility_locs.loc[imatch[0],'LATITUDE83']\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'LON'] = FRS_facility_locs.loc[imatch[0],'LONGITUDE83']\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'found'] = 1\n",
    "        elif len(imatch) >1:\n",
    "            #no good matches\n",
    "            continue\n",
    "    if EPA_nr_msw_noloc.loc[ifacility,'found'] ==0:\n",
    "        continue\n",
    "print('Count',len(EPA_nr_msw_noloc[EPA_nr_msw_noloc['found']==0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2.5 Find Locations by geocoding remaining EPA Non-Reporting Landfills (without locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRY GEOCODING\n",
    "# Try Geocoding to convert facility addresses into lat/lon values. \n",
    "# This uses the free openstreetmaps api (not as good as google maps, but free)\n",
    "# only need to get locations for facilities where found = 0\n",
    "#if this doesn't work with run using 'run all', try running individually \n",
    "\n",
    "#locator = Nominatim(user_agent=\"myGeocoder\")\n",
    "#print(locator)\n",
    "geolocator = Nominatim(user_agent=\"myGeocode\")\n",
    "geopy.geocoders.options.default_timeout = None\n",
    "print(geolocator.timeout)\n",
    "\n",
    "#Examples: #print((location.latitude, location.longitude))\n",
    "#location = locator.geocode(\"Champ de Mars, Paris, France\")\n",
    "#location = geolocator.geocode(\"1726 N Cochran Ave Charlotte MI 48813\")\n",
    "\n",
    "#food_beverage_facilities_locs['geo_match'] = 0\n",
    "for ifacility in np.arange(0,len(EPA_nr_msw_noloc)):\n",
    "    if EPA_nr_msw_noloc.loc[ifacility,'found'] ==0:\n",
    "        location = geolocator.geocode(EPA_nr_msw_noloc['Full_Address'][ifacility])\n",
    "        if location is None:\n",
    "            continue\n",
    "        else:\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'LAT'] = location.latitude\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'LON'] = location.longitude\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'found']=1\n",
    "            \n",
    "print('First Try - Percentage found:',(np.sum(EPA_nr_msw_noloc['found']))/len(EPA_nr_msw_noloc))\n",
    "print('Missing Count',len(EPA_nr_msw_noloc[EPA_nr_msw_noloc['found']==0]))\n",
    "\n",
    "for ifacility in np.arange(0,len(EPA_nr_msw_noloc)):\n",
    "    if EPA_nr_msw_noloc.loc[ifacility,'found'] ==0:\n",
    "        #if still no match, remove the address portion and just allocate based on city, state\n",
    "        location = geolocator.geocode(EPA_nr_msw_noloc['Partial_Address'][ifacility])\n",
    "        if location is None:\n",
    "            continue\n",
    "        else:\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'LAT'] = location.latitude\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'LON'] = location.longitude\n",
    "            EPA_nr_msw_noloc.loc[ifacility,'found']=1\n",
    "            \n",
    "print('Second Try - Percentage found:',(np.sum(EPA_nr_msw_noloc['found']))/len(EPA_nr_msw_noloc))\n",
    "print('Missing Count',len(EPA_nr_msw_noloc[EPA_nr_msw_noloc['found']==0]))\n",
    "\n",
    "#recombine into a single dataframe\n",
    "EPA_nr_msw_final = EPA_nr_msw_loc.append(EPA_nr_msw_noloc)\n",
    "EPA_nr_msw_final.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2.6 Calculate Emissions from Non-Reporting Landfills (those w/locations only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In this step, emissions for all non-reporting landfills are recalculated, removing\n",
    "# the landfills with missing locations from the analysis. \n",
    "\n",
    "print('QA/QC: report final landfill values to be placed on CONUS grid')\n",
    "for iyear in np.arange(0, num_years):\n",
    "    ghgrp_temp = ghgrp_msw[ghgrp_msw['Year'] == year_range[iyear]]\n",
    "    ghgrp_temp.reset_index(inplace=True, drop=True)\n",
    "    ghgrp_count = len(ghgrp_temp)\n",
    "\n",
    "    EPA_nr_msw_final['emis_'+year_range_str[iyear]] = 0\n",
    "    imatch =np.where(((EPA_nr_msw_final['Last GHGRP Year'] == 0) | \\\n",
    "                                   (EPA_nr_msw_final['Last GHGRP Year'] > year_range[iyear])) & \\\n",
    "                                   (EPA_nr_msw_final['LAT'] != 0))[0]\n",
    "\n",
    "    WIP_sum = np.sum(EPA_nr_msw_final.loc[imatch,'WIP_MT'])\n",
    "    EPA_nr_msw_final.loc[imatch,'emis_'+year_range_str[iyear]] = nonrepoting_total_emis[iyear]*\\\n",
    "                          EPA_nr_msw_final.loc[imatch,'WIP_MT']/WIP_sum   \n",
    "\n",
    "    print('Year:', year_range[iyear])\n",
    "    print('Total Landfills (counts):                          ',ghgrp_count+len(EPA_nr_msw_final.iloc[imatch,0]))\n",
    "    print('Total Landfill Emissions (kt):                     ',ghgrp_total_emis[iyear]+nonrepoting_total_emis[iyear])\n",
    "    print('Total GHGRP Landfills (counts):                    ',ghgrp_count)\n",
    "    print('Total GHGRP Landfill Emissions (kt):               ',ghgrp_total_emis[iyear] )\n",
    "    print('Total Non-Reporting Landfills w/location (counts): ',len(EPA_nr_msw_final.iloc[imatch,0]))\n",
    "    print('Total Non-Reporting Landfills w/location Emis (kt):',np.sum(EPA_nr_msw_final.loc[imatch,'emis_'+year_range_str[iyear]]))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2.7 Add Non-Reporting MSW Landfill Emissions to CONUS grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Place data on CONUS grid\n",
    "\n",
    "for iyear in np.arange(0, num_years):\n",
    "    for ifacility in np.arange(0, len(EPA_nr_msw_final)):\n",
    "        if EPA_nr_msw_final['LON'][ifacility] > Lon_left and \\\n",
    "            EPA_nr_msw_final['LON'][ifacility] < Lon_right and \\\n",
    "            EPA_nr_msw_final['LAT'][ifacility] > Lat_low and \\\n",
    "            EPA_nr_msw_final['LAT'][ifacility] < Lat_up:\n",
    "\n",
    "            ilat = int((EPA_nr_msw_final['LAT'][ifacility] - Lat_low)/Res01)\n",
    "            ilon = int((EPA_nr_msw_final['LON'][ifacility] - Lon_left)/Res01)\n",
    "            map_msw_emis[ilat,ilon,iyear] += EPA_nr_msw_final['emis_'+year_range_str[iyear]][ifacility]\n",
    "        else:\n",
    "            map_msw_emis_nongrid[iyear] += EPA_nr_msw_final['emis_'+year_range_str[iyear]][ifacility]\n",
    "    print(year_range[iyear],'Emissions (kt):',np.sum(map_msw_emis[:,:,iyear])+map_msw_emis_nongrid[iyear])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3 Industrial Landfill Proxy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.1 Read In State-Level Industrial Landfill Proxy Emissions Data - for both subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 1 - Read in the state-level emissions for industrial landfills from pulp & paper and food & beverage\n",
    "# Place emissions onto state array, with an extra dimension to account for these two ind. subgroups categories\n",
    "\n",
    "names = pd.read_excel(EPA_IndState_inputfile, sheet_name = \"P&P State Emissions\", usecols = \"B:AF\", skiprows = 5, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "EPA_state_ind_pp = pd.read_excel(EPA_IndState_inputfile, sheet_name = \"P&P State Emissions\", usecols = \"B:AF\", skiprows = 5, names = colnames)\n",
    "EPA_state_ind_pp = EPA_state_ind_pp.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_state_ind_pp.reset_index(inplace=True, drop=True)\n",
    "\n",
    "names = pd.read_excel(EPA_IndState_inputfile, sheet_name = \"F&B State Emissions\", usecols = \"B:AF\", skiprows = 5, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "EPA_state_ind_fb = pd.read_excel(EPA_IndState_inputfile, sheet_name = \"F&B State Emissions\", usecols = \"B:AF\", skiprows = 5, names = colnames)\n",
    "EPA_state_ind_fb = EPA_state_ind_fb.drop(columns = [*range(1990, start_year,1)])\n",
    "EPA_state_ind_fb.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#place in state x year array\n",
    "state_ind_pp = np.zeros([len(State_ANSI),num_years])\n",
    "state_ind_fb = np.zeros([len(State_ANSI),num_years])\n",
    "\n",
    "for istate in np.arange(0,len(State_ANSI)):\n",
    "    imatch = np.where(State_ANSI['abbr'][istate] == EPA_state_ind_pp['State'])[0]\n",
    "    imatch2 = np.where(State_ANSI['abbr'][istate] == EPA_state_ind_fb['State'])[0]\n",
    "    if len(imatch) > 0 and len(imatch2) > 0: \n",
    "        for iyear in np.arange(0, num_years):\n",
    "            state_ind_pp[istate,iyear] = EPA_state_ind_pp.loc[imatch,year_range[iyear]]*1e3 #mmt to kt\n",
    "            state_ind_fb[istate,iyear] = EPA_state_ind_fb.loc[imatch2,year_range[iyear]]*1e3 #mmt to kt\n",
    "\n",
    "state_ind_emis = np.zeros([2,len(State_ANSI),num_years])\n",
    "state_ind_emis[0,:,:] = state_ind_pp\n",
    "state_ind_emis[1,:,:] = state_ind_fb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.2. Create Pulp & Paper Manufacturing - grid proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.2.1 Read in pulp and paper industrial landfill GHGRP emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in GHGRP Subpart TT Emissions, extract Pulp & Paper NAICS codes only and place onto CONUS grid\n",
    "\n",
    "#a) Read in the GHGRP facility data\n",
    "facility_info = pd.read_csv(ghgrp_facility_tt_inputfile)\n",
    "facility_emis = pd.read_csv(ghgrp_emi_tt_inputfile)\n",
    "\n",
    "#filter emissions data for methane only (in metric tonnes CH4) and for years of interest\n",
    "facility_emis = facility_emis[facility_emis['TT_SUBPART_GHG_INFO.GHG_NAME'] == 'METHANE']\n",
    "facility_emis = facility_emis[facility_emis['TT_SUBPART_GHG_INFO.REPORTING_YEAR'].isin(year_range)]\n",
    "facility_info = facility_info[facility_info['V_GHG_EMITTER_FACILITIES.YEAR'].isin(year_range)]\n",
    "facility_info.reset_index(inplace=True, drop=True)\n",
    "facility_emis.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#rename common columns and merge into one dataframe\n",
    "facility_info.rename(columns={'V_GHG_EMITTER_FACILITIES.YEAR':'Year', \\\n",
    "                             'V_GHG_EMITTER_FACILITIES.FACILITY_ID':'Facility_ID', \\\n",
    "                             'V_GHG_EMITTER_FACILITIES.LONGITUDE':'LONGITUDE',\n",
    "                             'V_GHG_EMITTER_FACILITIES.LATITUDE':'LATITUDE',\n",
    "                             'V_GHG_EMITTER_FACILITIES.PRIMARY_NAICS_CODE':'NAICS_CODE',\n",
    "                             'V_GHG_EMITTER_FACILITIES.COUNTY':'COUNTY',\n",
    "                             'V_GHG_EMITTER_FACILITIES.CITY':'CITY',\n",
    "                             'V_GHG_EMITTER_FACILITIES.STATE':'STATE'},inplace=True)\n",
    "facility_emis.rename(columns={'TT_SUBPART_GHG_INFO.REPORTING_YEAR':'Year', \\\n",
    "                              'TT_SUBPART_GHG_INFO.FACILITY_ID':'Facility_ID'},inplace=True)\n",
    "ghgrp_ind = pd.merge(facility_info, facility_emis)\n",
    "ghgrp_ind['emis_kt_tot'] = ghgrp_ind['TT_SUBPART_GHG_INFO.GHG_QUANTITY']/1e3 #convert metric tonnes to kt\n",
    "ghgrp_ind['COUNTY'] = ghgrp_ind['COUNTY'].str.upper()\n",
    "ghgrp_ind['CITY'] = ghgrp_ind['CITY'].str.upper()\n",
    "ghgrp_ind['COUNTY'] = ghgrp_ind['COUNTY'].str.replace(\"COUNTY\",\"\")\n",
    "\n",
    "ghgrp_ind['NAICS_CODE'] = ghgrp_ind['NAICS_CODE'].astype(str)\n",
    "ghgrp_ind = ghgrp_ind[(ghgrp_ind['NAICS_CODE'].str.startswith('321')) | (ghgrp_ind['NAICS_CODE'].str.startswith('322'))]\n",
    "ghgrp_ind.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.2.2 Read in pulp and paper mill counts and find emissions/locations by matching to GHGRP facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Mills Online Database\n",
    "# This is the activity database used to apportion national emissions in the GHGI down to the state level\n",
    "# Emissions are assumed to be proportional to the number of mills in each state\n",
    "# Mills are matched to the GHGRP dataset based on the county and state of each mill\n",
    "# If a GHGRP facility is not in the Mills dataset, the GHGRP facility is added to the full list of facilities\n",
    "\n",
    "names = pd.read_excel(Mills_OnLine_inputdata, skiprows = 3, header = 0, nrows = 1)\n",
    "colnames = names.columns.values\n",
    "mills_locs = pd.read_excel(Mills_OnLine_inputdata, skiprows = 3, names = colnames)\n",
    "mills_locs = mills_locs[mills_locs['Pulp and Paper Mill'] == 'Yes']\n",
    "mills_locs.loc[:,'State_abbr'] = ''\n",
    "mills_locs['County'] = mills_locs['County'].astype(str)\n",
    "mills_locs.reset_index(inplace=True, drop=True)\n",
    "\n",
    "mills_locs.loc[:,'ghgrp_match'] = 0\n",
    "mills_locs.loc[:,'Lat'] = 0\n",
    "mills_locs.loc[:,'Lon'] = 0\n",
    "\n",
    "for imill in np.arange(0, len(mills_locs)):\n",
    "    mills_locs.loc[imill,'State_abbr'] = State_ANSI['abbr'][np.where(State_ANSI['name'] == mills_locs.loc[imill,'State'])[0][0]]\n",
    "num_mills = len(mills_locs)\n",
    "for iyear in np.arange(0, num_years):\n",
    "    mills_locs.loc[:,'emi_kt_'+year_range_str[iyear]] = 0\n",
    "\n",
    "ghgrp_ind['found']=0\n",
    "\n",
    "for iyear in np.arange(0, num_years):\n",
    "    for imill in np.arange(0,num_mills):\n",
    "        imatch = np.where((ghgrp_ind['Year'] == year_range[iyear]) & \\\n",
    "                          (ghgrp_ind['STATE'] == mills_locs.loc[imill,'State_abbr']) & \\\n",
    "                          (ghgrp_ind['COUNTY'].str.contains(mills_locs.loc[imill,'County'].upper())))[0]\n",
    "        if len(imatch)==1:\n",
    "            mills_locs.loc[imill,'ghgrp_match'] = 1\n",
    "            mills_locs.loc[imill,'Lat'] = ghgrp_ind.loc[imatch[0],'LATITUDE']\n",
    "            mills_locs.loc[imill,'Lon'] = ghgrp_ind.loc[imatch[0],'LONGITUDE']\n",
    "            mills_locs.loc[imill,'emi_kt_'+year_range_str[iyear]] = ghgrp_ind.loc[imatch[0],'emis_kt_tot']\n",
    "            ghgrp_ind.loc[imatch[0],'found'] = 1\n",
    "        if len(imatch) > 1:\n",
    "            new_match = np.where((ghgrp_ind['Year'] == year_range[iyear]) & \\\n",
    "                                 (ghgrp_ind['STATE'] == mills_locs.loc[imill,'State_abbr']) &\\\n",
    "                                 (ghgrp_ind['COUNTY'].str.contains(mills_locs.loc[imill,'County'].upper())) &\\\n",
    "                             (ghgrp_ind['CITY'].str.contains(mills_locs.loc[imill,'City'].upper())))[0]\n",
    "            #print(imill, new_match)\n",
    "            if len(new_match)>0:\n",
    "                mills_locs.loc[imill,'ghgrp_match'] = 1\n",
    "                mills_locs.loc[imill,'Lat'] = ghgrp_ind.loc[new_match[0],'LATITUDE']\n",
    "                mills_locs.loc[imill,'Lon'] = ghgrp_ind.loc[new_match[0],'LONGITUDE']\n",
    "                mills_locs.loc[imill,'emi_kt_'+year_range_str[iyear]] = ghgrp_ind.loc[new_match[0],'emis_kt_tot']\n",
    "                ghgrp_ind.loc[new_match[0],'found'] = 1\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "        #need to find location and allocate emissions\n",
    "    print('Found (%) Year',year_range[iyear],':',100*np.sum(mills_locs['ghgrp_match']/len(mills_locs)))\n",
    "    \n",
    "    \n",
    "#add additional GHGRP facilities (to capture all GHGRP emissions - deviates from GHGI methods)\n",
    "# for each extra facility in the ghgrp dataset, append an extra row, then fill in the emissions values for each year\n",
    "for ifacility in np.arange(0,len(ghgrp_ind)):\n",
    "    if ghgrp_ind.loc[ifacility,'found'] ==0:\n",
    "        #print(ifacility)\n",
    "        facility_id = ghgrp_ind.loc[ifacility,'Facility_ID']\n",
    "        df2 = {'State_abbr':ghgrp_ind.loc[ifacility,'STATE'], 'City':ghgrp_ind.loc[ifacility,'CITY'],\\\n",
    "               'County':ghgrp_ind.loc[ifacility,'COUNTY'],'ghgrp_match': 1, 'Lat': ghgrp_ind.loc[ifacility,'LATITUDE'], \\\n",
    "               'Lon':ghgrp_ind.loc[ifacility,'LONGITUDE']}\n",
    "        mills_locs = mills_locs.append(df2, ignore_index = True)\n",
    "        for iyear in np.arange(0, num_years):\n",
    "            ighgrp = np.where((ghgrp_ind['Year'] == year_range[iyear]) & (ghgrp_ind['Facility_ID'] == facility_id))[0]\n",
    "            #print(ighgrp)\n",
    "            if len(ighgrp)==1:\n",
    "                ghgrp_ind.loc[ighgrp[0],'found']=1\n",
    "                mills_locs.loc[len(mills_locs)-1,'emi_kt_'+year_range_str[iyear]] = ghgrp_ind.loc[ighgrp[0],'emis_kt_tot']\n",
    "            else:\n",
    "                mills_locs.loc[len(mills_locs)-1,'emi_kt_'+year_range_str[iyear]] = 0\n",
    "        #display(mills_locs)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.2.3 Try finding missing locations by matching to the FRS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) find locations of remaining mills - by comparing to FRS. NAICS Codes starting with 321 and 322\n",
    "\n",
    "#Read in FRS data to get location information for landfills with missing location information \n",
    " \n",
    "FRS_facility_locs = pd.read_csv(FRS_inputfile, usecols = [2,3,5,7,8,10,17,20,21,26,27,28,31,32,34,35,36],low_memory=False)\n",
    "FRS_facility_locs.fillna(0, inplace = True)\n",
    "FRS_facility_locs = FRS_facility_locs[FRS_facility_locs['LATITUDE83'] > 0]\n",
    "FRS_facility_locs = FRS_facility_locs[FRS_facility_locs['NAICS_CODES'] != 0]\n",
    "FRS_facility_locs.reset_index(inplace=True, drop=True)\n",
    "\n",
    "FRS_facility_locs = FRS_facility_locs[(FRS_facility_locs['NAICS_CODES'].str.startswith('321')) | (FRS_facility_locs['NAICS_CODES'].str.startswith('322'))]\n",
    "\n",
    "print('Total FRS Pulp & Paper Facilities: ',len(FRS_facility_locs))\n",
    "FRS_facility_locs.reset_index(inplace=True, drop=True)\n",
    "\n",
    "FRS_facility_locs['CITY_NAME'] = FRS_facility_locs['CITY_NAME'].replace(0,'NaN')\n",
    "FRS_facility_locs.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "#try to match mills database with FRS database, based on state and city - \n",
    "# only need to find locations for where 'ghgrp_match' = 0\n",
    "mills_locs['FRS_match'] = 0\n",
    "for ifacility in np.arange(0, len(mills_locs)):\n",
    "    if mills_locs.loc[ifacility,'ghgrp_match']==0:\n",
    "        imatch = np.where((mills_locs.loc[ifacility,'State_abbr'] == FRS_facility_locs['STATE_CODE']) &\\\n",
    "                         ((mills_locs.loc[ifacility,'City'].upper() == FRS_facility_locs['CITY_NAME'])))[0]\n",
    "        if len(imatch)==1:\n",
    "            mills_locs.loc[ifacility,'Lat'] = FRS_facility_locs.loc[imatch[0],'LATITUDE83']\n",
    "            mills_locs.loc[ifacility,'Lon'] = FRS_facility_locs.loc[imatch[0],'LONGITUDE83']\n",
    "            mills_locs.loc[ifacility,'FRS_match'] = 1\n",
    "        elif len(imatch)>1:\n",
    "            FRS_temp = FRS_facility_locs.loc[imatch,:]\n",
    "            new_match = np.where(np.max(FRS_temp['ACCURACY_VALUE']))[0]\n",
    "            if len(new_match) >0:\n",
    "                mills_locs.loc[ifacility,'Lat'] = FRS_facility_locs.loc[imatch[new_match[0]],'LATITUDE83']\n",
    "                mills_locs.loc[ifacility,'Lon'] = FRS_facility_locs.loc[imatch[new_match[0]],'LONGITUDE83']\n",
    "                mills_locs.loc[ifacility,'FRS_match'] = 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "print('Not Found:',len(mills_locs)-(np.sum(mills_locs.loc[:,'FRS_match'])+np.sum(mills_locs.loc[:,'ghgrp_match'])), 'of',len(mills_locs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.2.4 Assign/Calculate emissions for non-reporting facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the difference in emissions between the GHGI and avaialble GHGRP facilities (for each year)\n",
    "# assign this emissions difference uniformly to all remaining non-ghgrp mills \n",
    "\n",
    "print('Check Sum Against National Emissions')\n",
    "for iyear in np.arange(0, num_years):\n",
    "    sum_emis = np.sum(mills_locs.loc[:,'emi_kt_'+year_range_str[iyear]])\n",
    "    epa_emis = np.sum(state_ind_emis[0,:,iyear]) #sum epa pulp and paper data across all states\n",
    "    num_facility_missing = np.sum(mills_locs.loc[:,'FRS_match'])#len(mills_locs)-np.sum(mills_locs.loc[:,'ghgrp_match'])+\n",
    "    emis_diff = epa_emis - sum_emis\n",
    "    missing_fac_emis = emis_diff/num_facility_missing\n",
    "    #print(missing_fac_emis)\n",
    "    for ifacility in np.arange(0, len(mills_locs)):\n",
    "        if mills_locs.loc[ifacility,'FRS_match']==1 and mills_locs.loc[ifacility,'ghgrp_match']==0:\n",
    "            mills_locs.loc[ifacility,'emi_kt_'+year_range_str[iyear]] = missing_fac_emis\n",
    "    diff_emis = (np.sum(mills_locs['emi_kt_'+year_range_str[iyear]])- epa_emis)/((np.sum(mills_locs['emi_kt_'+year_range_str[iyear]])+epa_emis)/2)\n",
    "    if diff_emis <= 0.0001:\n",
    "        print('Year',year_range[iyear],': PASS')\n",
    "    else:\n",
    "        print('Year',year_range[iyear],': CHECK')                                                                         \n",
    "    #display(mills_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.2.5 Place calculated pulp and paper emissions on the CONUS grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place pulp and Paper manufacturing emissions onto map\n",
    "\n",
    "#Since Pulp & Paper is a sub-group of the IND landfills emission group, this proxy has dimensions of\n",
    "# (subgroup x lat x lon x years), where subgroup = 2. This emissions group is first allocated to the state-\n",
    "# level, so the gridded proxy is at 0.01x0.01 degree resolution \n",
    "\n",
    "#initialize arrays and reset to zero\n",
    "map_ind_emis = np.zeros([2,len(lat001),len(lon001),num_years])\n",
    "map_ind_emis_nongrid = np.zeros([2,num_years])\n",
    "\n",
    "map_ind_emis[0,:,:,:] = 0\n",
    "map_ind_emis_nongrid[0,:]=0\n",
    "\n",
    "for ifacility in np.arange(0, len(mills_locs)):\n",
    "    if mills_locs['Lon'][ifacility] > Lon_left and \\\n",
    "        mills_locs['Lon'][ifacility] < Lon_right and \\\n",
    "        mills_locs['Lat'][ifacility] > Lat_low and \\\n",
    "        mills_locs['Lat'][ifacility] < Lat_up:\n",
    "\n",
    "        ilat = int((mills_locs['Lat'][ifacility] - Lat_low)/Res_01)\n",
    "        ilon = int((mills_locs['Lon'][ifacility] - Lon_left)/Res_01)\n",
    "        for iyear in np.arange(0, num_years):\n",
    "            map_ind_emis[0,ilat,ilon,iyear] += mills_locs['emi_kt_'+year_range_str[iyear]][ifacility]\n",
    "    elif mills_locs.loc[ifacility,'State_abbr'] in (['AK','HI']):\n",
    "        #this pulls out AK/HI contributions without also pulling values from facilities where we couldn't find the location\n",
    "        for iyear in np.arange(0, num_years):\n",
    "            map_ind_emis_nongrid[0,iyear] += mills_locs['emi_kt_'+year_range_str[iyear]][ifacility]\n",
    "\n",
    "print('Annual Pulp & Paper Manufacturing Emissions')\n",
    "for iyear in np.arange(0, num_years):\n",
    "    print('Year:',year_range[iyear])\n",
    "    print('P&P Emissions (kt) ongrid:', np.sum(map_ind_emis[0,:,:,iyear]))\n",
    "    print('P&P Emissions (kt) offgrid:', np.sum(map_ind_emis_nongrid[0,iyear]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "##### Step 2.3.3 Industrial Landfills -  Food and Beverage Maufacturing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.3.1 Read in GHGRP Subpart TT emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in GHGRP Subpart TT Emissions, extract Food & Beverage NAICS codes only #and place onto CONUS grid\n",
    "\n",
    "#a) Read in the GHGRP facility data\n",
    "facility_info = pd.read_csv(ghgrp_facility_tt_inputfile)\n",
    "facility_emis = pd.read_csv(ghgrp_emi_tt_inputfile)\n",
    "\n",
    "#filter emissions data for methane only (in metric tonnes CH4) and for years of interest\n",
    "facility_emis = facility_emis[facility_emis['TT_SUBPART_GHG_INFO.GHG_NAME'] == 'METHANE']\n",
    "facility_emis = facility_emis[facility_emis['TT_SUBPART_GHG_INFO.REPORTING_YEAR'].isin(year_range)]\n",
    "facility_info = facility_info[facility_info['V_GHG_EMITTER_FACILITIES.YEAR'].isin(year_range)]\n",
    "facility_info.reset_index(inplace=True, drop=True)\n",
    "facility_emis.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#rename common columns and merge into one dataframe\n",
    "facility_info.rename(columns={'V_GHG_EMITTER_FACILITIES.YEAR':'Year', \\\n",
    "                             'V_GHG_EMITTER_FACILITIES.FACILITY_ID':'Facility_ID', \\\n",
    "                             'V_GHG_EMITTER_FACILITIES.LONGITUDE':'LONGITUDE',\n",
    "                             'V_GHG_EMITTER_FACILITIES.LATITUDE':'LATITUDE',\n",
    "                             'V_GHG_EMITTER_FACILITIES.PRIMARY_NAICS_CODE':'NAICS_CODE',\n",
    "                             'V_GHG_EMITTER_FACILITIES.COUNTY':'COUNTY',\n",
    "                             'V_GHG_EMITTER_FACILITIES.CITY':'CITY',\n",
    "                             'V_GHG_EMITTER_FACILITIES.STATE':'STATE'},inplace=True)\n",
    "facility_emis.rename(columns={'TT_SUBPART_GHG_INFO.REPORTING_YEAR':'Year', \\\n",
    "                              'TT_SUBPART_GHG_INFO.FACILITY_ID':'Facility_ID'},inplace=True)\n",
    "ghgrp_ind = pd.merge(facility_info, facility_emis)\n",
    "ghgrp_ind['emis_kt_tot'] = ghgrp_ind['TT_SUBPART_GHG_INFO.GHG_QUANTITY']/1e3 #convert metric tonnes to kt\n",
    "ghgrp_ind['COUNTY'] = ghgrp_ind['COUNTY'].str.upper()\n",
    "ghgrp_ind['CITY'] = ghgrp_ind['CITY'].str.upper()\n",
    "ghgrp_ind['COUNTY'] = ghgrp_ind['COUNTY'].str.replace(\"COUNTY\",\"\")\n",
    "\n",
    "ghgrp_ind['NAICS_CODE'] = ghgrp_ind['NAICS_CODE'].astype(str)\n",
    "fb_naics = ['311612','311421','311513','312140','311611','311615','311225','311613','311710','311221','311224','311314','311313']\n",
    "ghgrp_ind = ghgrp_ind[(ghgrp_ind['NAICS_CODE'].str.contains('|'.join(fb_naics)))]\n",
    "ghgrp_ind.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.3.2 Read in List of Food & Beverage Maufacturing Facilities & Match to the GHGRP Facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in and Estimate Emissions for Food & Beverage Manufacturing Facilities\n",
    "# This is the list of facilities that are assumed to contribute to the industrial food & beverage waste\n",
    "# in the national GHGI. The average excess food waste from these facilities is used to allocate national\n",
    "# GHGI emissions down to the state-level in the official state GHGI\n",
    "# By allocating gridded emissions to these same facilities (weighted by relative excess food waste), we are \n",
    "# assuming that these facilities landfill their waste at sites within 10km of the manufacturing facility. \n",
    "# There is no national database of industrial landfills, so this is the best assumption for now, but could be \n",
    "# improved upon as more information becomes avaialble on where industrial landfills are located. \n",
    "\n",
    "# Step 1 - Read in Data\n",
    "# Read in EPA Food Opportunities Map F&B Maufacturing Data\n",
    "# Data are filtered based on NAICS codes that are references in Appendix F of the GHGI State Methodology Report\n",
    "# These are the food & beverage manufacturing soureces contributing to industrial waste emissions\n",
    "\n",
    "names = pd.read_excel(FoodBeverage_inputdata, sheet_name = \"Data\", usecols = \"B:J\",skiprows = 0, header = 0)\n",
    "colnames = names.columns.values\n",
    "food_beverage_facilities = pd.read_excel(FoodBeverage_inputdata, sheet_name = \"Data\", usecols = \"B:J\", skiprows = 0, names = colnames)\n",
    "\n",
    "#filter for relevant NIACS codes (based on GHGI)\n",
    "food_beverage_facilities = (food_beverage_facilities[food_beverage_facilities['NAICS_CODE'].isin([311612,311421,311513,312140,311611,311615,\\\n",
    "                                                                             311225,311613,311710,311221,311224,311314,311313])])\n",
    "#remove the facilities that don't report excess food waste\n",
    "food_beverage_facilities = food_beverage_facilities[~np.isnan(food_beverage_facilities['EXCESSFOOD_TONYEAR_LOWEST'])]\n",
    "food_beverage_facilities.reset_index(inplace=True, drop=True)\n",
    "food_beverage_facilities['ADDRESS'].fillna('', inplace=True)\n",
    "food_beverage_facilities['CITY'].fillna('', inplace=True)\n",
    "food_beverage_facilities['COUNTY'].fillna('', inplace=True)\n",
    "food_beverage_facilities['Full_Address'] = ''\n",
    "food_beverage_facilities[\"Full_Address\"] = food_beverage_facilities[\"ADDRESS\"].astype(str) + ' '+\\\n",
    "                                         food_beverage_facilities[\"CITY\"].astype(str) +' '+\\\n",
    "                                        food_beverage_facilities[\"COUNTY\"].astype(str)+' '+\\\n",
    "                                        food_beverage_facilities[\"STATE\"].astype(str)+' '+\\\n",
    "                                        food_beverage_facilities[\"ZIP_CODE\"].astype(str)\n",
    "food_beverage_facilities['Partial_Address'] = ''\n",
    "food_beverage_facilities['Partial_Address'] = food_beverage_facilities[\"CITY\"].astype(str) +' '+\\\n",
    "                                        food_beverage_facilities[\"COUNTY\"].astype(str)+' '+\\\n",
    "                                        food_beverage_facilities[\"STATE\"].astype(str)+' '+\\\n",
    "                                        food_beverage_facilities[\"ZIP_CODE\"].astype(str)\n",
    "#create copy to store location information\n",
    "food_beverage_facilities_locs = food_beverage_facilities.copy()\n",
    "food_beverage_facilities_locs['Lat'] = 0\n",
    "food_beverage_facilities_locs['Lon'] = 0\n",
    "#Calculate the average food waste (tons) at each facility\n",
    "food_beverage_facilities_locs['Avg_Waste_Tons'] = 0\n",
    "food_beverage_facilities_locs['Avg_Waste_Frac'] = 0\n",
    "for ifacility in np.arange(0, len(food_beverage_facilities_locs)):\n",
    "    food_beverage_facilities_locs.loc[ifacility,'Avg_Waste_Tons'] = \\\n",
    "        np.mean([food_beverage_facilities_locs.loc[ifacility,'EXCESSFOOD_TONYEAR_LOWEST'],\\\n",
    "                 food_beverage_facilities_locs.loc[ifacility,'EXCESSFOOD_TONYEAR_HIGHEST']])\n",
    "\n",
    "    \n",
    "#) Step 2 -  try to match facilities to ghgrp based on county and city. \n",
    "# If there is no match with a GHGRP facility, add it to the full list of facilities\n",
    "for iyear in np.arange(0, num_years):\n",
    "    food_beverage_facilities_locs.loc[:,'emi_kt_'+year_range_str[iyear]] = 0\n",
    "\n",
    "food_beverage_facilities_locs['ghgrp_match'] = 0\n",
    "ghgrp_ind['found']=0\n",
    "\n",
    "for iyear in np.arange(0, num_years):\n",
    "    for ifacility in np.arange(0,len(food_beverage_facilities_locs)):\n",
    "        imatch = np.where((ghgrp_ind['Year'] == year_range[iyear]) & \\\n",
    "                          (ghgrp_ind['STATE'] == food_beverage_facilities_locs.loc[ifacility,'STATE']) & \\\n",
    "                          (ghgrp_ind['COUNTY'].str.contains(food_beverage_facilities_locs.loc[ifacility,'COUNTY'].upper())))[0]\n",
    "        if len(imatch)==1:\n",
    "            food_beverage_facilities_locs.loc[ifacility,'ghgrp_match'] = 1\n",
    "            food_beverage_facilities_locs.loc[ifacility,'Lat'] = ghgrp_ind.loc[imatch[0],'LATITUDE']\n",
    "            food_beverage_facilities_locs.loc[ifacility,'Lon'] = ghgrp_ind.loc[imatch[0],'LONGITUDE']\n",
    "            food_beverage_facilities_locs.loc[ifacility,'emi_kt_'+year_range_str[iyear]] = ghgrp_ind.loc[imatch[0],'emis_kt_tot']\n",
    "            ghgrp_ind.loc[imatch[0],'found'] = 1\n",
    "        if len(imatch) > 1:\n",
    "            new_match = np.where((ghgrp_ind['Year'] == year_range[iyear]) & \\\n",
    "                                 (ghgrp_ind['STATE'] == food_beverage_facilities_locs.loc[ifacility,'STATE']) &\\\n",
    "                                 (ghgrp_ind['COUNTY'].str.contains(food_beverage_facilities_locs.loc[ifacility,'COUNTY'].upper())) &\\\n",
    "                             (ghgrp_ind['CITY'].str.contains(food_beverage_facilities_locs.loc[ifacility,'CITY'].upper())))[0]\n",
    "            if len(new_match)>0:\n",
    "                food_beverage_facilities_locs.loc[ifacility,'ghgrp_match'] = 1\n",
    "                food_beverage_facilities_locs.loc[ifacility,'Lat'] = ghgrp_ind.loc[new_match[0],'LATITUDE']\n",
    "                food_beverage_facilities_locs.loc[ifacility,'Lon'] = ghgrp_ind.loc[new_match[0],'LONGITUDE']\n",
    "                food_beverage_facilities_locs.loc[ifacility,'emi_kt_'+year_range_str[iyear]] = ghgrp_ind.loc[new_match[0],'emis_kt_tot']\n",
    "                ghgrp_ind.loc[new_match[0],'found'] = 1\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "    print('Found (%) Year',year_range[iyear],':',100*np.sum(food_beverage_facilities_locs['ghgrp_match']/len(food_beverage_facilities_locs)))\n",
    "    \n",
    "# Step 3 - add additional GHGRP facilities (to capture all GHGRP emissions - deviates from GHGI methods)\n",
    "# for each extra facility in the ghgrp dataset, append an extra row, then fill in the emissions values for each year\n",
    "for ifacility in np.arange(0,len(ghgrp_ind)):\n",
    "    if ghgrp_ind.loc[ifacility,'found'] ==0:\n",
    "        facility_id = ghgrp_ind.loc[ifacility,'Facility_ID']\n",
    "        df2 = {'NAICS_CODE':ghgrp_ind.loc[ifacility,'NAICS_CODE'],'STATE':ghgrp_ind.loc[ifacility,'STATE'], 'CITY':ghgrp_ind.loc[ifacility,'CITY'],\\\n",
    "               'COUNTY':ghgrp_ind.loc[ifacility,'COUNTY'],'ghgrp_match': 1, 'Lat': ghgrp_ind.loc[ifacility,'LATITUDE'], \\\n",
    "               'Lon':ghgrp_ind.loc[ifacility,'LONGITUDE']}\n",
    "        food_beverage_facilities_locs = food_beverage_facilities_locs.append(df2, ignore_index = True)\n",
    "        for iyear in np.arange(0, num_years):\n",
    "            ighgrp = np.where((ghgrp_ind['Year'] == year_range[iyear]) & (ghgrp_ind['Facility_ID'] == facility_id))[0]\n",
    "            #print(ighgrp)\n",
    "            if len(ighgrp)==1:\n",
    "                ghgrp_ind.loc[ighgrp[0],'found']=1\n",
    "                food_beverage_facilities_locs.loc[len(food_beverage_facilities_locs)-1,'emi_kt_'+year_range_str[iyear]] = ghgrp_ind.loc[ighgrp[0],'emis_kt_tot']\n",
    "            else:\n",
    "                food_beverage_facilities_locs.loc[len(food_beverage_facilities_locs)-1,'emi_kt_'+year_range_str[iyear]] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.3.3 Find Missing Locations by Matching to the FRS database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4 - Try to match remaining facilities by comparing to FRS. F&B NAICS codes\n",
    "\n",
    "# 4a) Read in FRS data to get location information for landfills with missing location information \n",
    "FRS_facility_locs = pd.read_csv(FRS_inputfile, usecols = [2,3,5,7,8,10,17,20,21,26,27,28,31,32,34,35,36],low_memory=False)\n",
    "FRS_facility_locs.fillna(0, inplace = True)\n",
    "FRS_facility_locs = FRS_facility_locs[FRS_facility_locs['LATITUDE83'] > 0]\n",
    "FRS_facility_locs = FRS_facility_locs[FRS_facility_locs['NAICS_CODES'] != 0]\n",
    "FRS_facility_locs.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#fb_naics = ['311612','311421','311513','312140','311611','311615','311225','311613','311710','311221','311224','311314','311313']\n",
    "ghgrp_ind = ghgrp_ind[(ghgrp_ind['NAICS_CODE'].str.contains('|'.join(fb_naics)))]\n",
    "\n",
    "FRS_facility_locs = FRS_facility_locs[(FRS_facility_locs['NAICS_CODES'].str.contains('|'.join(fb_naics)))]\n",
    "print('Total FRS Food & Beverage Facilities: ',len(FRS_facility_locs))\n",
    "FRS_facility_locs.reset_index(inplace=True, drop=True)\n",
    "\n",
    "FRS_facility_locs['CITY_NAME'] = FRS_facility_locs['CITY_NAME'].replace(0,'NaN')\n",
    "\n",
    "\n",
    "# 4b - try to match mills database with FRS database, based on state and city - \n",
    "# only need to find locations for where 'ghgrp_match' = 0\n",
    "food_beverage_facilities_locs['FRS_match'] = 0\n",
    "for ifacility in np.arange(0, len(food_beverage_facilities_locs)):\n",
    "    if food_beverage_facilities_locs.loc[ifacility,'ghgrp_match']==0:\n",
    "        imatch = np.where((food_beverage_facilities_locs.loc[ifacility,'STATE'] == FRS_facility_locs['STATE_CODE']) &\\\n",
    "                         ((food_beverage_facilities_locs.loc[ifacility,'CITY'].upper() == FRS_facility_locs['CITY_NAME'])))[0]\n",
    "        if len(imatch)==1:\n",
    "            food_beverage_facilities_locs.loc[ifacility,'Lat'] = FRS_facility_locs.loc[imatch[0],'LATITUDE83']\n",
    "            food_beverage_facilities_locs.loc[ifacility,'Lon'] = FRS_facility_locs.loc[imatch[0],'LONGITUDE83']\n",
    "            food_beverage_facilities_locs.loc[ifacility,'FRS_match'] = 1\n",
    "        elif len(imatch)>1:\n",
    "            FRS_temp = FRS_facility_locs.loc[imatch,:]\n",
    "            new_match = np.where(np.max(FRS_temp['ACCURACY_VALUE']))[0]\n",
    "            if len(new_match) >0:\n",
    "                food_beverage_facilities_locs.loc[ifacility,'Lat'] = FRS_facility_locs.loc[imatch[new_match[0]],'LATITUDE83']\n",
    "                food_beverage_facilities_locs.loc[ifacility,'Lon'] = FRS_facility_locs.loc[imatch[new_match[0]],'LONGITUDE83']\n",
    "                food_beverage_facilities_locs.loc[ifacility,'FRS_match'] = 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "print('Not Found:',len(food_beverage_facilities_locs)-(np.sum(food_beverage_facilities_locs.loc[:,'FRS_match'])+np.sum(food_beverage_facilities_locs.loc[:,'ghgrp_match'])), 'of',len(food_beverage_facilities_locs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.3.4 Find Missing Locations by Geocoding Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Try Geocoding to convert facility addresses into lat/lon values. \n",
    "# This uses the free openstreetmaps api (not as good as google maps, but free)\n",
    "# only need to get locations for facilities where ghgrp_match and frs_match = 0\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"myGeocode\")\n",
    "geopy.geocoders.options.default_timeout = None\n",
    "print(geolocator.timeout)\n",
    "\n",
    "#Examples: #print((location.latitude, location.longitude))\n",
    "#location = locator.geocode(\"Champ de Mars, Paris, France\")\n",
    "#location = geolocator.geocode(\"1726 N Cochran Ave Charlotte MI 48813\")\n",
    "\n",
    "food_beverage_facilities_locs['geo_match'] = 0\n",
    "for ifacility in np.arange(0,len(food_beverage_facilities_locs)):\n",
    "    if food_beverage_facilities_locs.loc[ifacility,'FRS_match'] ==0 and food_beverage_facilities_locs.loc[ifacility,'ghgrp_match'] == 0:\n",
    "        location = geolocator.geocode(food_beverage_facilities_locs['Full_Address'][ifacility])\n",
    "        if location is None:\n",
    "            continue\n",
    "        else:\n",
    "            food_beverage_facilities_locs.loc[ifacility,'Lat'] = location.latitude\n",
    "            food_beverage_facilities_locs.loc[ifacility,'Lon'] = location.longitude\n",
    "            food_beverage_facilities_locs.loc[ifacility,'geo_match']=1\n",
    "            \n",
    "print('First Try - Percentage found:',(np.sum(food_beverage_facilities_locs['ghgrp_match'])+\\\n",
    "                                        np.sum(food_beverage_facilities_locs['FRS_match'])+\\\n",
    "                                        np.sum(food_beverage_facilities_locs['geo_match']))/len(food_beverage_facilities_locs))\n",
    "\n",
    "for ifacility in np.arange(0,len(food_beverage_facilities_locs)):\n",
    "    if food_beverage_facilities_locs.loc[ifacility,'FRS_match'] ==0 and food_beverage_facilities_locs.loc[ifacility,'ghgrp_match'] == 0 and \\\n",
    "        food_beverage_facilities_locs.loc[ifacility,'geo_match']==0:\n",
    "        #remove suite information from the address and try again \n",
    "        address_temp = food_beverage_facilities_locs['Full_Address'][ifacility]\n",
    "        address_temp = address_temp.replace('Ste','')\n",
    "        address_temp = address_temp.replace('Apt','')\n",
    "        address_temp = address_temp.replace('Unit','')\n",
    "        address_temp = address_temp.replace('Bldg','')\n",
    "        location = geolocator.geocode(address_temp)\n",
    "        if location is None:\n",
    "            #if still no match, remove the address portion and just allocate based on city, state, county, zip\n",
    "            #address_temp = food_beverage_facilities_locs.loc[ifacility,\"Partial_Address\"]\n",
    "            address_temp = food_beverage_facilities_locs.loc[ifacility,\"CITY\"] +' '+\\\n",
    "                                        food_beverage_facilities_locs.loc[ifacility,\"COUNTY\"]+' '+\\\n",
    "                                        food_beverage_facilities_locs.loc[ifacility,\"STATE\"]+' '+\\\n",
    "                                        food_beverage_facilities_locs.loc[ifacility,\"ZIP_CODE\"].astype(str)\n",
    "            location2 = geolocator.geocode(address_temp)\n",
    "            if location2 is None:\n",
    "                #print(ifacility,address_temp)\n",
    "                continue\n",
    "            else:\n",
    "                #count -= 1\n",
    "                food_beverage_facilities_locs.loc[ifacility,'Lat'] = location2.latitude\n",
    "                food_beverage_facilities_locs.loc[ifacility,'Lon'] = location2.longitude\n",
    "                food_beverage_facilities_locs.loc[ifacility,'geo_match']=1\n",
    "        else:\n",
    "            #count -= 1\n",
    "            food_beverage_facilities_locs.loc[ifacility,'Lat'] = location.latitude\n",
    "            food_beverage_facilities_locs.loc[ifacility,'Lon'] = location.longitude\n",
    "            food_beverage_facilities_locs.loc[ifacility,'geo_match']=1\n",
    "            \n",
    "print('Second Try - Percentage found:',(np.sum(food_beverage_facilities_locs['ghgrp_match'])+\\\n",
    "                                        np.sum(food_beverage_facilities_locs['FRS_match'])+\\\n",
    "                                        np.sum(food_beverage_facilities_locs['geo_match']))/len(food_beverage_facilities_locs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.3.5 Calculate the Average Waste produced by each facility and the fraction of the total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6. Calculate the fractional waste contributions for all non-matching facilities\n",
    "# need to calculate emissions for those where emissions are not already calculated (ghgrp_match = 0) \n",
    "# and where locations have been found (frs_match and geo_match =1)\n",
    "\n",
    "# a) first calculate the national total average waste (for all non-matching facilities)\n",
    "total_avg_waste = 0\n",
    "for ifacility in np.arange(0, len(food_beverage_facilities_locs)):\n",
    "    if (food_beverage_facilities_locs.loc[ifacility,'FRS_match'] ==1 or \\\n",
    "        food_beverage_facilities_locs.loc[ifacility,'geo_match'] == 1) and \\\n",
    "        food_beverage_facilities_locs.loc[ifacility,'ghgrp_match']==0:\n",
    "        #if food_beverage_facilities_locs.loc[ifacility,'ghgrp_match']==0:\n",
    "        total_avg_waste += food_beverage_facilities_locs.loc[ifacility,'Avg_Waste_Tons']\n",
    "\n",
    "# b) then calcualte the fractional contribution\n",
    "for ifacility in np.arange(0, len(food_beverage_facilities_locs)):\n",
    "    if (food_beverage_facilities_locs.loc[ifacility,'FRS_match'] ==1 or \\\n",
    "        food_beverage_facilities_locs.loc[ifacility,'geo_match'] == 1) and \\\n",
    "        food_beverage_facilities_locs.loc[ifacility,'ghgrp_match']==0:\n",
    "        food_beverage_facilities_locs.loc[ifacility,'Avg_Waste_Frac'] = \\\n",
    "                                    food_beverage_facilities_locs.loc[ifacility,'Avg_Waste_Tons']/total_avg_waste\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.3.6 Calculate the emissions at all non-GHGRP facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 7). Find the difference in emissions between the GHGI and avaialble GHGRP facilities (for each year)\n",
    "# assign this emissions difference to all remaining non-ghgrp facilities based on the relative average excess food waste \n",
    "print('Check Sum Against National Emissions')\n",
    "for iyear in np.arange(0,num_years):\n",
    "    sum_emis = np.sum(food_beverage_facilities_locs.loc[:,'emi_kt_'+year_range_str[iyear]])\n",
    "    epa_emis = np.sum(state_ind_emis[1,:,iyear]) #sum epa food & beverage data across all states\n",
    "    num_facility_missing = len(food_beverage_facilities_locs) - \\\n",
    "                            (np.sum(food_beverage_facilities_locs.loc[:,'geo_match'])+\\\n",
    "                            np.sum(food_beverage_facilities_locs.loc[:,'FRS_match'])+ \\\n",
    "                            np.sum(food_beverage_facilities_locs.loc[:,'ghgrp_match']))#len(mills_locs)-np.sum(mills_locs.loc[:,'ghgrp_match'])+\n",
    "    #print(num_facility_missing)\n",
    "    emis_diff = epa_emis - sum_emis\n",
    "    #missing_fac_emis = emis_diff/num_facility_missing\n",
    "    #print(epa_emis, sum_emis, emis_diff)\n",
    "    for ifacility in np.arange(0, len(food_beverage_facilities_locs)):\n",
    "        if (food_beverage_facilities_locs.loc[ifacility,'FRS_match'] ==1 or \\\n",
    "        food_beverage_facilities_locs.loc[ifacility,'geo_match'] == 1) and \\\n",
    "        food_beverage_facilities_locs.loc[ifacility,'ghgrp_match']==0:\n",
    "            food_beverage_facilities_locs.loc[ifacility,'emi_kt_'+year_range_str[iyear]] = \\\n",
    "                                emis_diff*food_beverage_facilities_locs.loc[ifacility,'Avg_Waste_Frac']\n",
    "    \n",
    "\n",
    "    diff_emis = (np.sum(food_beverage_facilities_locs['emi_kt_'+year_range_str[iyear]])- epa_emis)/((np.sum(food_beverage_facilities_locs['emi_kt_'+year_range_str[iyear]])+epa_emis)/2)\n",
    "    #print(diff_emis)\n",
    "    if abs(diff_emis) <= 0.0001:\n",
    "        print('Year',year_range[iyear],': PASS')\n",
    "        print(np.sum(food_beverage_facilities_locs['emi_kt_'+year_range_str[iyear]]))\n",
    "        print(epa_emis)\n",
    "    else:\n",
    "        print('Year',year_range[iyear],': CHECK')                                                                         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3.3.7 Place Food & Beverage Facility Emissions onto CONUS Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 8 - Place calculated F&B emissions onto map\n",
    "\n",
    "#ensure values are reset to zero\n",
    "map_ind_emis[1,:,:,:] = 0\n",
    "map_ind_emis_nongrid[1,:]=0  \n",
    "\n",
    "for ifacility in np.arange(0, len(food_beverage_facilities_locs)):\n",
    "    if food_beverage_facilities_locs['Lon'][ifacility] > Lon_left and \\\n",
    "        food_beverage_facilities_locs['Lon'][ifacility] < Lon_right and \\\n",
    "        food_beverage_facilities_locs['Lat'][ifacility] > Lat_low and \\\n",
    "        food_beverage_facilities_locs['Lat'][ifacility] < Lat_up:\n",
    "\n",
    "        ilat = int((food_beverage_facilities_locs['Lat'][ifacility] - Lat_low)/Res_01)\n",
    "        ilon = int((food_beverage_facilities_locs['Lon'][ifacility] - Lon_left)/Res_01)\n",
    "        for iyear in np.arange(0, num_years):\n",
    "            map_ind_emis[1,ilat,ilon,iyear] += food_beverage_facilities_locs['emi_kt_'+year_range_str[iyear]][ifacility]\n",
    "    elif food_beverage_facilities_locs.loc[ifacility,'STATE'] in (['AK','HI']):\n",
    "        #this pulls out AK/HI contributions without also pulling values from facilities where we couldn't find the location\n",
    "        for iyear in np.arange(0, num_years):\n",
    "            map_ind_emis_nongrid[1,iyear] += food_beverage_facilities_locs['emi_kt_'+year_range_str[iyear]][ifacility]\n",
    "\n",
    "print('Annual Food & Beverage Manufacturing Emissions')\n",
    "for iyear in np.arange(0, num_years):\n",
    "    print('Year:',year_range[iyear])\n",
    "    print('F&B Emissions (kt) ongrid:', np.sum(map_ind_emis[1,:,:,iyear]))\n",
    "    print('F&B Emissions (kt) offgrid:', np.sum(map_ind_emis_nongrid[1,iyear]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "## Step 3. Read in and Format US EPA GHGI Emissions\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read in the emissions data from the GHGI main inventory report (in kt)\n",
    "\n",
    "EPA_emi_landfill_CH4 = pd.read_csv(EPA_landfill_inputfile,skiprows=2,encoding= 'unicode_escape', header=0,nrows=8)\n",
    "EPA_emi_landfill_CH4 = EPA_emi_landfill_CH4.fillna('')\n",
    "EPA_emi_landfill_CH4 = EPA_emi_landfill_CH4.drop(columns = [str(n) for n in range(1990, start_year,1)])\n",
    "EPA_emi_landfill_CH4 = EPA_emi_landfill_CH4.drop(['Unnamed: 0'], axis=1)\n",
    "EPA_emi_landfill_CH4.rename(columns={EPA_emi_landfill_CH4.columns[0]:'Source'}, inplace=True)\n",
    "EPA_emi_landfill_CH4 = EPA_emi_landfill_CH4.apply(lambda x: x.str.replace(',',''))\n",
    "EPA_emi_landfill_CH4 = EPA_emi_landfill_CH4.apply(lambda x: x.str.replace(r\"\\)\",\"\"))\n",
    "EPA_emi_landfill_CH4 = EPA_emi_landfill_CH4.apply(lambda x: x.str.replace(r\"\\(\",\"\"))\n",
    "EPA_emi_landfill_CH4.iloc[:,1:] = EPA_emi_landfill_CH4.iloc[:,1:].apply(pd.to_numeric,errors='coerce')\n",
    "EPA_emi_landfill_CH4.reset_index(inplace=True, drop=True)\n",
    "\n",
    "temp = EPA_emi_landfill_CH4.iloc[:,1:].sum(axis=1)\n",
    "\n",
    "EPA_emi_landfill_total = EPA_emi_landfill_CH4[EPA_emi_landfill_CH4['Source'] == 'Total']\n",
    "EPA_emi_landfill_total.reset_index(inplace=True, drop=True)\n",
    "print(type(EPA_emi_landfill_total['2012']))\n",
    "print('EPA GHGI National CH4 Emissions (kt):')\n",
    "display(EPA_emi_landfill_total)\n",
    "\n",
    "display(EPA_emi_landfill_CH4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Split Emissions into Gridding Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split GHG emissions into gridding groups, based on Coal Proxy Mapping file\n",
    "\n",
    "DEBUG =1\n",
    "start_year_idx = EPA_emi_landfill_CH4.columns.get_loc(str(start_year))\n",
    "end_year_idx = EPA_emi_landfill_CH4.columns.get_loc(str(end_year))+1\n",
    "ghgi_landfill_groups = ghgi_landfill_map['GHGI_Emi_Group'].unique()\n",
    "sum_emi = np.zeros([num_years])\n",
    "\n",
    "for igroup in np.arange(0,len(ghgi_landfill_groups)): #loop through all groups, finding the GHGI sources in that group and summing emissions for that region, year        vars()[ghgi_prod_groups[igroup]] = np.zeros([num_regions-1,num_years])\n",
    "    ##DEBUG## print(ghgi_stat_groups[igroup])\n",
    "    vars()[ghgi_landfill_groups[igroup]] = np.zeros([num_years])\n",
    "    source_temp = ghgi_landfill_map.loc[ghgi_landfill_map['GHGI_Emi_Group'] == ghgi_landfill_groups[igroup], 'GHGI_Source']\n",
    "    pattern_temp  = '|'.join(source_temp) \n",
    "    #print(pattern_temp) \n",
    "    emi_temp =EPA_emi_landfill_CH4[EPA_emi_landfill_CH4['Source'].str.contains(pattern_temp)]\n",
    "    vars()[ghgi_landfill_groups[igroup]][:] = emi_temp.iloc[:,start_year_idx:].sum()\n",
    "        \n",
    "        \n",
    "#Check against total summary emissions \n",
    "print('QA/QC #1: Check Processing Emission Sum against GHGI Summary Emissions')\n",
    "for iyear in np.arange(0,num_years): \n",
    "    for igroup in np.arange(0,len(ghgi_landfill_groups)):\n",
    "        if iyear ==0:\n",
    "            vars()[ghgi_landfill_groups[igroup]][iyear] -= 0.5  ##NOTE: correct rounding error so sum of emissions = reported total emissions\n",
    "        sum_emi[iyear] += vars()[ghgi_landfill_groups[igroup]][iyear]\n",
    "        \n",
    "    summary_emi = EPA_emi_landfill_total.iloc[0,iyear+1]  \n",
    "    #Check 1 - make sure that the sums from all the regions equal the totals reported\n",
    "    diff1 = abs(sum_emi[iyear] - summary_emi)/((sum_emi[iyear] + summary_emi)/2)\n",
    "    if DEBUG==1:\n",
    "        print(summary_emi)\n",
    "        print(sum_emi[iyear])\n",
    "    if diff1 < 0.0001:\n",
    "        print('Year ', year_range[iyear],': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear],': FAIL (check Production & summary tabs): ', diff1,'%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Step 4. Grid Data\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1. Allocate emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.1.1 Assign the Appropriate Proxy Variable Names (state & grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names on the *left* need to match the 'ProxyMapping' 'State_Proxy_Group' names \n",
    "# (these are initialized in Step 2). \n",
    "# The names on the *right* are the variable names used to caluclate the proxies in this code.\n",
    "# Names on the right need to match those from the code in Step 2\n",
    "\n",
    "#state proxies are in dimensions (subgroup x state x year)\n",
    "# subgroup 0 = pulp and paper, subgroup 1 = food and beverage\n",
    "State_Ind_Landfills = state_ind_emis\n",
    "\n",
    "#state --> grid (0.01) proxies (subgroup x lat x lon x year OR lat x lon x year)\n",
    "Map_Emi_Ind_Landfills = map_ind_emis\n",
    "Map_Emi_MSW_Landfills = map_msw_emis\n",
    "Map_Emi_Ind_Landfills_nongrid = map_ind_emis_nongrid\n",
    "Map_Emi_MSW_Landfills_nongrid = map_msw_emis_nongrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.1.2. Allocate to the State level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate state-level emissions\n",
    "# Emissions in kt\n",
    "# State data = national GHGI emissions * state proxy/national total\n",
    "\n",
    "# Note that national emissions are retained for groups that do not have state proxies (identified in the mapping file)\n",
    "# and are gridded in the next step\n",
    "DEBUG = 1\n",
    "\n",
    "# Make placeholder emission arrays for each group\n",
    "# State SubGroup flag == 1 indicates that the proxy data contains information for two sub groups (e.g., P&P and F&B)\n",
    "for igroup in np.arange(0,len(proxy_landfill_map)):\n",
    "    if proxy_landfill_map.loc[igroup,'State_SubGroup_Flag'] ==1:\n",
    "        vars()['State_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([2,len(State_ANSI),num_years])\n",
    "        vars()['NonState_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([2,num_years])\n",
    "    else:\n",
    "        vars()['State_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(State_ANSI),num_years])\n",
    "        vars()['NonState_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([num_years])\n",
    "        \n",
    "#Loop over years\n",
    "for iyear in np.arange(num_years):\n",
    "    #Loop over states\n",
    "    for istate in np.arange(len(State_ANSI)):\n",
    "        for igroup in np.arange(0,len(proxy_landfill_map)):    \n",
    "            if proxy_landfill_map.loc[igroup,'State_Proxy_Group'] != '-' and proxy_landfill_map.loc[igroup,'GHGI_Emi_Group'] != 'Emi_not_mapped':\n",
    "                if proxy_landfill_map.loc[igroup,'State_SubGroup_Flag'] ==1:\n",
    "                    for isubgroup in np.arange(0,2):\n",
    "                        vars()['State_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][isubgroup,istate,iyear] = \\\n",
    "                                vars()[proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                        data_fn.safe_div(vars()[proxy_landfill_map.loc[igroup,'State_Proxy_Group']][isubgroup,istate,iyear], \\\n",
    "                                         np.sum(vars()[proxy_landfill_map.loc[igroup,'State_Proxy_Group']][:,:,iyear]))\n",
    "                    \n",
    "            else:\n",
    "                vars()['NonState_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][iyear] = vars()[proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "                \n",
    "# Check sum of all gridded emissions + emissions not included in state allocation\n",
    "print('QA/QC #1: Check weighted emissions against GHGI')   \n",
    "for iyear in np.arange(0,num_years):\n",
    "    summary_emi = EPA_emi_landfill_total.iloc[0,iyear+1] \n",
    "    calc_emi = 0\n",
    "    for igroup in np.arange(0,len(proxy_landfill_map)):\n",
    "        if proxy_landfill_map.loc[igroup,'State_SubGroup_Flag'] ==1:\n",
    "            calc_emi +=  np.sum(vars()['State_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])+\\\n",
    "                        np.sum(vars()['NonState_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][:,iyear])\n",
    "        else:\n",
    "            calc_emi +=  np.sum(vars()['State_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][:,iyear])\n",
    "            calc_emi += vars()['NonState_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][iyear] #np.sum(Emissions[:,iyear]) + Emissions_nongrid[iyear] + Emissions_nonstate[iyear]\n",
    "    if DEBUG ==1:\n",
    "        print(summary_emi)\n",
    "        print(calc_emi)\n",
    "    diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if diff < 0.0001:\n",
    "        print('Year ', year_range[iyear], ': PASS, difference < 0.01%')\n",
    "    else:\n",
    "        print('Year ', year_range[iyear], ': FAIL -- Difference = ', diff*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.3 Allocate emissions to the CONUS region (0.1x0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Allocate State-Level emissions (kt) onto a 0.1x0.1 grid using gridcell level 'Proxy_Groups'\n",
    "\n",
    "DEBUG =1\n",
    "#Define emission arrays\n",
    "Emissions_array_01 = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Emissions_array_01_temp = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Emissions_array_001 = np.zeros([len(lat001),len(lon001),num_years])\n",
    "\n",
    "Emissions_array_01_msw = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Emissions_array_01_temp_msw = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Emissions_array_001_msw = np.zeros([len(lat001),len(lon001),num_years])\n",
    "\n",
    "Emissions_array_01_ind = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Emissions_array_01_temp_ind = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Emissions_array_001_ind = np.zeros([len(lat001),len(lon001),num_years])\n",
    "\n",
    "Emissions_nongrid = np.zeros([num_years])\n",
    "\n",
    "# For each year, (2a) distribute state-level emissions onto a grid using proxies defined above ....\n",
    "# To speed up the code, masks are used rather than looping individually through each lat/lon. \n",
    "# In this case, a mask of 1's is made for the grid cells that match the ANSI values for a given state\n",
    "# The masked values are set to zero, remaining values = 1. \n",
    "# AK and HI and territories are removed from the analysis at this stage. \n",
    "# The emissions allocated to each state are at 0.01x0.01 degree resolution, as required to calculate accurate 'mask'\n",
    "# arrays for each state. \n",
    "# (2b) For emission groups that were not first allocated to states, national emissions for those groups are gridded\n",
    "# based on the relevant gridded proxy arrays (0.1x0.1 resolution). These emissions are at 0.1x0.1 degrees resolution. \n",
    "# (2c) - record 'not mapped' emission groups in the 'non-grid' array (not relevant here)\n",
    "\n",
    "print('**QA/QC Check: Sum of national gridded emissions vs. GHGI national emissions')\n",
    "running_sum = np.zeros([len(proxy_landfill_map),num_years])\n",
    "\n",
    "for igroup in np.arange(0,len(proxy_landfill_map)):\n",
    "    print(igroup, 'of', len(proxy_landfill_map))\n",
    "    proxy_temp = vars()[proxy_landfill_map.loc[igroup,'Proxy_Group']]\n",
    "    proxy_temp_nongrid = vars()[proxy_landfill_map.loc[igroup,'Proxy_Group']+'_nongrid']\n",
    "    vars()['Ext_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']+'_01'] = np.zeros([len(lat001),len(lon001),num_years])\n",
    "    vars()['Ext_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']+'_temp'] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "\n",
    "    #2a. Step through each state (if group was previously allocated to state level)\n",
    "    if proxy_landfill_map.loc[igroup,'State_Proxy_Group'] != '-' and \\\n",
    "        proxy_landfill_map.loc[igroup,'State_Proxy_Group'] != 'state_not_mapped':\n",
    "        for istate in np.arange(0,len(State_ANSI)):\n",
    "            if State_ANSI['abbr'][istate] not in {'AK','HI'} and istate < 51:\n",
    "                mask_state = np.ma.ones(np.shape(state_ANSI_map))\n",
    "                mask_state = np.ma.masked_where(state_ANSI_map != State_ANSI['ansi'][istate], mask_state)\n",
    "                mask_state = np.ma.filled(mask_state,0) \n",
    "                if proxy_landfill_map.loc[igroup, 'Grid_SubGroup_Flag'] ==1:\n",
    "                    if proxy_landfill_map.loc[igroup, 'State_SubGroup_Flag'] ==1:\n",
    "                        for iyear in np.arange(0, num_years):\n",
    "                            for isubgroup in np.arange(0,2):\n",
    "                                emi_temp = vars()['State_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][isubgroup,istate,iyear]\n",
    "                                if np.sum(mask_state*np.sum(proxy_temp[isubgroup,:,:,iyear],axis=0)) > 0 and emi_temp > 0: \n",
    "                                    # if state is on grid and proxy for that state is non-zero\n",
    "                                    weighted_array = data_fn.safe_div(mask_state*proxy_temp[isubgroup,:,:,iyear], \\\n",
    "                                                                  np.sum(mask_state*proxy_temp[isubgroup,:,:,iyear]))\n",
    "                                    if 'MSW' in proxy_landfill_map.loc[igroup, 'GHGI_Emi_Group']:\n",
    "                                        Emissions_array_001_msw[:,:,iyear] += emi_temp*weighted_array\n",
    "                                    elif 'Ind' in proxy_landfill_map.loc[igroup, 'GHGI_Emi_Group']:\n",
    "                                        Emissions_array_001_ind[:,:,iyear] += emi_temp*weighted_array\n",
    "                                    Emissions_array_001[:,:,iyear] += emi_temp*weighted_array#_01\n",
    "                                    vars()['Ext_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']+'_01'][:,:,iyear]+=emi_temp*weighted_array\n",
    "                                    running_sum[igroup,iyear] += np.sum(emi_temp*weighted_array)\n",
    "                                else:\n",
    "                                    Emissions_nongrid[iyear] += emi_temp\n",
    "                                    running_sum[igroup,iyear] += np.sum(emi_temp)\n",
    "            \n",
    "            else:\n",
    "                if proxy_landfill_map.loc[igroup, 'State_SubGroup_Flag'] ==1:\n",
    "                    for iyear in np.arange(0, num_years):\n",
    "                        Emissions_nongrid[iyear] += np.sum(vars()['State_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][:,istate,iyear])\n",
    "                        running_sum[igroup,iyear] += np.sum(vars()['State_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][:,istate,iyear])    \n",
    "         \n",
    "    #2b. if emissions were not allocated to state, allocate national total to grid here (these are in 0.1x0.1 resolution)\n",
    "    elif proxy_landfill_map.loc[igroup,'State_Proxy_Group'] == '-':\n",
    "        for iyear in np.arange(0,num_years):\n",
    "            temp_sum = np.sum(vars()[proxy_landfill_map.loc[igroup,'Proxy_Group']][:,:,iyear])+np.sum(vars()[proxy_landfill_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear])\n",
    "            emi_temp = vars()[proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                       data_fn.safe_div(vars()[proxy_landfill_map.loc[igroup,'Proxy_Group']][:,:,iyear], temp_sum)\n",
    "            Emissions_array_01_temp[:,:,iyear] += emi_temp\n",
    "            vars()['Ext_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']+'_temp'][:,:,iyear] += emi_temp\n",
    "            if 'MSW' in proxy_landfill_map.loc[igroup, 'GHGI_Emi_Group']:\n",
    "                emi_temp = vars()[proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                       data_fn.safe_div(vars()[proxy_landfill_map.loc[igroup,'Proxy_Group']][:,:,iyear], temp_sum)\n",
    "                Emissions_array_01_temp_msw[:,:,iyear] += emi_temp\n",
    "            elif 'Ind' in proxy_landfill_map.loc[igroup, 'GHGI_Emi_Group']:\n",
    "                emi_temp = vars()[proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                       data_fn.safe_div(vars()[proxy_landfill_map.loc[igroup,'Proxy_Group']][:,:,iyear], temp_sum)\n",
    "                Emissions_array_01_temp_ind[:,:,iyear]+=emi_temp\n",
    "            \n",
    "            Emissions_nongrid[iyear] += vars()[proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][iyear] *\\\n",
    "                        data_fn.safe_div(vars()[proxy_landfill_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear], temp_sum)\n",
    "            ##DEBUG## running_count += vars()[proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "            running_sum[igroup,iyear] += np.sum(vars()[proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][iyear] * \\\n",
    "                       data_fn.safe_div(vars()[proxy_landfill_map.loc[igroup,'Proxy_Group']][:,:,iyear], temp_sum)) + \\\n",
    "                        (vars()[proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][iyear] *\\\n",
    "                        data_fn.safe_div(vars()[proxy_landfill_map.loc[igroup,'Proxy_Group']+'_nongrid'][iyear], temp_sum))    \n",
    "\n",
    "    #2c. this is the case that GHGI emissions are not mapped (e.g., specified outside of CONUS in the GHGI)\n",
    "    elif proxy_landfill_map.loc[igroup,'Proxy_Group'] == 'Map_not_mapped':  \n",
    "        for iyear in np.arange(0, num_years):\n",
    "            Emissions_nongrid[iyear] += vars()[proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][iyear]\n",
    "            running_sum[igroup,iyear] += vars()[proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][iyear] \n",
    "\n",
    "    print()\n",
    "for igroup in np.arange(0, len(proxy_landfill_map)):\n",
    "    vars()['Ext_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']] = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "\n",
    "for iyear in np.arange(0, num_years):    \n",
    "    Emissions_array_01[:,:,iyear] = data_fn.regrid001_to_01(Emissions_array_001[:,:,iyear], Lat_01, Lon_01)\n",
    "    Emissions_array_01[:,:,iyear] += Emissions_array_01_temp[:,:,iyear]\n",
    "    Emissions_array_01_msw[:,:,iyear] = data_fn.regrid001_to_01(Emissions_array_001_msw[:,:,iyear], Lat_01, Lon_01)\n",
    "    Emissions_array_01_msw[:,:,iyear] += Emissions_array_01_temp_msw[:,:,iyear]\n",
    "    Emissions_array_01_ind[:,:,iyear] = data_fn.regrid001_to_01(Emissions_array_001_ind[:,:,iyear], Lat_01, Lon_01)\n",
    "    Emissions_array_01_ind[:,:,iyear] += Emissions_array_01_temp_ind[:,:,iyear]\n",
    "    calc_emi = np.sum(Emissions_array_01[:,:,iyear]) + np.sum(Emissions_nongrid[iyear]) \n",
    "    calc_emi2 = np.sum(Emissions_array_01_msw[:,:,iyear]) +np.sum(Emissions_array_01_ind[:,:,iyear])+ np.sum(Emissions_nongrid[iyear]) \n",
    "    calc_emi3 = 0\n",
    "    for igroup in np.arange(0, len(proxy_landfill_map)):\n",
    "        vars()['Ext_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] = data_fn.regrid001_to_01(vars()['Ext_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']+'_01'][:,:,iyear], Lat_01, Lon_01)\n",
    "        vars()['Ext_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear] += vars()['Ext_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']+'_temp'][:,:,iyear]\n",
    "        calc_emi3 += np.sum(vars()['Ext_'+proxy_landfill_map.loc[igroup,'GHGI_Emi_Group']][:,:,iyear])\n",
    "    calc_emi3 += np.sum(Emissions_nongrid[iyear])\n",
    "    summary_emi = EPA_emi_landfill_total.iloc[0,iyear+1] \n",
    "    emi_diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if DEBUG==1:\n",
    "        print(calc_emi)\n",
    "        print(calc_emi2)\n",
    "        print(calc_emi3)\n",
    "        print(summary_emi)\n",
    "    if abs(emi_diff) < 0.0001:\n",
    "        print('Year '+ year_range_str[iyear]+': Difference < 0.01%: PASS')\n",
    "    else: \n",
    "        print('Year '+ year_range_str[iyear]+': Difference > 0.01%: FAIL, diff: '+str(emi_diff))\n",
    "        \n",
    "ct = datetime.now() \n",
    "print(\"current time:\", ct)\n",
    "\n",
    "#del Emissions_array_001, Emissions_array_001_msw, Emissions_array_001_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1.4 Save gridded emissions (kt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save gridded emissions for each gridding group - for extension\n",
    "\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(grid_emi_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "\n",
    "unique_groups = np.unique(proxy_landfill_map['GHGI_Emi_Group'])\n",
    "unique_groups = unique_groups[unique_groups != 'Emi_not_mapped']\n",
    "\n",
    "nc_out = Dataset(grid_emi_outputfile, 'r+', format='NETCDF4')\n",
    "\n",
    "for igroup in np.arange(0,len(unique_groups)):\n",
    "    print('Ext_'+unique_groups[igroup])\n",
    "    if len(np.shape(vars()['Ext_'+unique_groups[igroup]])) ==4:\n",
    "        ghgi_temp = np.sum(vars()[unique_groups[igroup]],axis=3) #sum month data if data is monthly\n",
    "    else:\n",
    "        ghgi_temp = vars()['Ext_'+unique_groups[igroup]]\n",
    "\n",
    "    # Write data to netCDF\n",
    "    data_out = nc_out.createVariable('Ext_'+unique_groups[igroup], 'f8', ('lat', 'lon','year'), zlib=True)\n",
    "    data_out[:,:,:] = ghgi_temp[:,:,:]\n",
    "\n",
    "#save nongrid data to calculate non-grid fraction extension\n",
    "data_out = nc_out.createVariable('Emissions_nongrid', 'f8', ('year'), zlib=True)  \n",
    "data_out[:] = Emissions_nongrid[:]\n",
    "nc_out.close()\n",
    "\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded emissions (kt) written to file: {}\" .format(os.getcwd())+grid_emi_outputfile)\n",
    "print(' ')\n",
    "\n",
    "del data_out, ghgi_temp, nc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Calculate Gridded Emission Fluxes (molec./cm2/s) (0.1x0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert emissions to emission flux\n",
    "# convert kt to molec/cm2/s\n",
    "\n",
    "Flux_array_01_annual = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Flux_array_01_annual_msw = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "Flux_array_01_annual_ind = np.zeros([len(Lat_01),len(Lon_01),num_years])\n",
    "print('**QA/QC Check: Sum of national gridded emissions vs. GHGI national emissions')\n",
    "  \n",
    "for iyear in np.arange(0,num_years):\n",
    "    calc_emi = 0\n",
    "    if year_range[iyear]==2012 or year_range[iyear]==2016:\n",
    "        year_days = np.sum(month_day_leap)\n",
    "    else:\n",
    "        year_days = np.sum(month_day_nonleap)\n",
    "        \n",
    "    conversion_factor_01 = 10**9 * Avogadro / float(Molarch4 *year_days * 24 * 60 *60) / area_matrix_01\n",
    "    Flux_array_01_annual[:,:,iyear] = Emissions_array_01[:,:,iyear]*conversion_factor_01\n",
    "    Flux_array_01_annual_msw[:,:,iyear] = Emissions_array_01_msw[:,:,iyear]*conversion_factor_01\n",
    "    Flux_array_01_annual_ind[:,:,iyear] = Emissions_array_01_ind[:,:,iyear]*conversion_factor_01\n",
    "    #convert back to mass to check\n",
    "    conversion_factor_annual = 10**9 * Avogadro / float(Molarch4 *year_days * 24 * 60 *60) / area_matrix_01\n",
    "    calc_emi = np.sum(Flux_array_01_annual[:,:,iyear]/conversion_factor_annual)+np.sum(Emissions_nongrid[iyear])\n",
    "    calc_emi2 = np.sum(Flux_array_01_annual_msw[:,:,iyear]/conversion_factor_annual)+\\\n",
    "                np.sum(Flux_array_01_annual_ind[:,:,iyear]/conversion_factor_annual)+np.sum(Emissions_nongrid[iyear])\n",
    "    summary_emi = EPA_emi_landfill_total.iloc[0,iyear+1] \n",
    "    emi_diff = abs(summary_emi-calc_emi)/((summary_emi+calc_emi)/2)\n",
    "    if DEBUG==1:\n",
    "        print(calc_emi)\n",
    "        print(calc_emi2)\n",
    "        print(summary_emi)\n",
    "    if abs(emi_diff) < 0.0001:\n",
    "        print('Year '+ year_range_str[iyear]+': Difference < 0.01%: PASS')\n",
    "    else: \n",
    "        print('Year '+ year_range_str[iyear]+': Difference > 0.01%: FAIL, diff: '+str(emi_diff))\n",
    "        \n",
    "Flux_Emissions_Total_annual = Flux_array_01_annual\n",
    "Flux_Emissions_Total_annual_msw = Flux_array_01_annual_msw\n",
    "Flux_Emissions_Total_annual_ind = Flux_array_01_annual_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Step 5. Write netCDF\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearly data\n",
    "\n",
    "#Total\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(gridded_outputfile, netCDF_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "# Write data to netCDF\n",
    "nc_out = Dataset(gridded_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Total_annual\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded stationary combustion fluxes written to file: {}\" .format(os.getcwd())+gridded_outputfile)\n",
    "\n",
    "#MSW Landfills\n",
    "# yearly data\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(gridded_msw_outputfile, netCDF_msw_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "# Write data to netCDF\n",
    "nc_out = Dataset(gridded_msw_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Total_annual_msw\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded stationary combustion fluxes written to file: {}\" .format(os.getcwd())+gridded_msw_outputfile)\n",
    "\n",
    "#Industrial Landfills\n",
    "# yearly data\n",
    "#Initialize file\n",
    "data_IO_fn.initialize_netCDF(gridded_ind_outputfile, netCDF_ind_description, 0, year_range, loc_dimensions, Lat_01, Lon_01)\n",
    "# Write data to netCDF\n",
    "nc_out = Dataset(gridded_ind_outputfile, 'r+', format='NETCDF4')\n",
    "nc_out.variables['emi_ch4'][:,:,:] = Flux_Emissions_Total_annual_ind\n",
    "nc_out.close()\n",
    "#Confirm file location\n",
    "print('** SUCCESS **')\n",
    "print(\"Gridded stationary combustion fluxes written to file: {}\" .format(os.getcwd())+gridded_ind_outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Step 6. Plot Gridded Data\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.1. Plot Annual Emission Fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot Annual Data\n",
    "#Total\n",
    "scale_max = 10\n",
    "save_flag = 0\n",
    "save_fig = ''\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_str,scale_max,save_flag,save_fig)\n",
    "\n",
    "# MSW\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Total_annual_msw, Lat_01, Lon_01, year_range, title_str_msw,scale_max,save_flag,save_fig)\n",
    "\n",
    "#IND\n",
    "data_plot_fn.plot_annual_emission_flux_map(Flux_Emissions_Total_annual_ind, Lat_01, Lon_01, year_range, title_str_ind,scale_max,save_flag,save_fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.2 Plot Difference between first and last inventory year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot difference between last and first year\n",
    "\n",
    "#Total\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Total_annual, Lat_01, Lon_01, year_range, title_diff_str,save_flag,save_fig)\n",
    "\n",
    "#MSW\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Total_annual_msw, Lat_01, Lon_01, year_range, title_diff_str_msw,save_flag,save_fig)\n",
    "\n",
    "#IND\n",
    "data_plot_fn.plot_diff_emission_flux_map(Flux_Emissions_Total_annual_ind, Lat_01, Lon_01, year_range, title_diff_str_ind,save_flag,save_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = datetime.now() \n",
    "ft = ct.timestamp() \n",
    "time_elapsed = (ft-it)/(60*60)\n",
    "print('Time to run: '+str(time_elapsed)+' hours')\n",
    "print('** GEPA_5A1_Landfills: COMPLETE **')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
